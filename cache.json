{"2025-03-03T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2502.15850v2","updated":"2025-03-03T17:11:16Z","published":"2025-02-21T02:34:17Z","title":"Forecasting Frontier Language Model Agent Capabilities","summary":"  As Language Models (LMs) increasingly operate as autonomous agents,\naccurately forecasting their capabilities becomes crucial for societal\npreparedness. We evaluate six forecasting methods that predict downstream\ncapabilities of LM agents. We use \"one-step\" approaches that predict benchmark\nscores from input metrics like compute or model release date directly or\n\"two-step\" approaches that first predict an intermediate metric like the\nprincipal component of cross-benchmark performance (PC-1) and human-evaluated\ncompetitive Elo ratings. We evaluate our forecasting methods by backtesting\nthem on a dataset of 38 LMs from the OpenLLM 2 leaderboard. We then use the\nvalidated two-step approach (Release Date$\\to$Elo$\\to$Benchmark) to predict LM\nagent performance for frontier models on three benchmarks: SWE-Bench Verified\n(software development), Cybench (cybersecurity assessment), and RE-Bench (ML\nresearch engineering). Our forecast predicts that by the beginning of 2026,\nnon-specialized LM agents with low capability elicitation will reach a success\nrate of 54% on SWE-Bench Verified, while state-of-the-art LM agents will reach\nan 87% success rate. Our approach does not account for recent advances in\ninference-compute scaling and might thus be too conservative.\n","authors":["Govind Pimpale","Axel Højmark","Jérémy Scheurer","Marius Hobbhahn"],"pdf_url":"https://arxiv.org/pdf/2502.15850v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18600v2","updated":"2025-03-03T17:08:21Z","published":"2025-02-25T19:36:06Z","title":"Chain of Draft: Thinking Faster by Writing Less","summary":"  Large Language Models (LLMs) have demonstrated remarkable performance in\nsolving complex reasoning tasks through mechanisms like Chain-of-Thought (CoT)\nprompting, which emphasizes verbose, step-by-step reasoning. However, humans\ntypically employ a more efficient strategy: drafting concise intermediate\nthoughts that capture only essential information. In this work, we propose\nChain of Draft (CoD), a novel paradigm inspired by human cognitive processes,\nwhere LLMs generate minimalistic yet informative intermediate reasoning outputs\nwhile solving tasks. By reducing verbosity and focusing on critical insights,\nCoD matches or surpasses CoT in accuracy while using as little as only 7.6% of\nthe tokens, significantly reducing cost and latency across various reasoning\ntasks. Our code and data are available at\nhttps://github.com/sileix/chain-of-draft.\n","authors":["Silei Xu","Wenhao Xie","Lingxiao Zhao","Pengcheng He"],"pdf_url":"https://arxiv.org/pdf/2502.18600v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04974v3","updated":"2025-03-03T17:03:49Z","published":"2025-01-09T05:06:44Z","title":"SensorQA: A Question Answering Benchmark for Daily-Life Monitoring","summary":"  With the rapid growth in sensor data, effectively interpreting and\ninterfacing with these data in a human-understandable way has become crucial.\nWhile existing research primarily focuses on learning classification models,\nfewer studies have explored how end users can actively extract useful insights\nfrom sensor data, often hindered by the lack of a proper dataset. To address\nthis gap, we introduce SensorQA, the first human-created question-answering\n(QA) dataset for long-term time-series sensor data for daily life monitoring.\nSensorQA is created by human workers and includes 5.6K diverse and practical\nqueries that reflect genuine human interests, paired with accurate answers\nderived from sensor data. We further establish benchmarks for state-of-the-art\nAI models on this dataset and evaluate their performance on typical edge\ndevices. Our results reveal a gap between current models and optimal QA\nperformance and efficiency, highlighting the need for new contributions. The\ndataset and code are available at:\nhttps://github.com/benjamin-reichman/SensorQA.\n","authors":["Benjamin Reichman","Xiaofan Yu","Lanxiang Hu","Jack Truxal","Atishay Jain","Rushil Chandrupatla","Tajana Šimunić Rosing","Larry Heck"],"pdf_url":"https://arxiv.org/pdf/2501.04974v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05589v3","updated":"2025-03-03T16:49:18Z","published":"2025-02-08T14:28:36Z","title":"On Memory Construction and Retrieval for Personalized Conversational\n  Agents","summary":"  To deliver coherent and personalized experiences in long-term conversations,\nexisting approaches typically perform retrieval augmented response generation\nby constructing memory banks from conversation history at either the\nturn-level, session-level, or through summarization techniques.In this paper,\nwe present two key findings: (1) The granularity of memory unit matters:\nturn-level, session-level, and summarization-based methods each exhibit\nlimitations in both memory retrieval accuracy and the semantic quality of the\nretrieved content. (2) Prompt compression methods, such as LLMLingua-2, can\neffectively serve as a denoising mechanism, enhancing memory retrieval accuracy\nacross different granularities. Building on these insights, we propose SeCom, a\nmethod that constructs the memory bank at segment level by introducing a\nconversation segmentation model that partitions long-term conversations into\ntopically coherent segments, while applying compression based denoising on\nmemory units to enhance memory retrieval. Experimental results show that SeCom\nexhibits a significant performance advantage over baselines on long-term\nconversation benchmarks LOCOMO and Long-MT-Bench+. Additionally, the proposed\nconversation segmentation method demonstrates superior performance on dialogue\nsegmentation datasets such as DialSeg711, TIAGE, and SuperDialSeg.\n","authors":["Zhuoshi Pan","Qianhui Wu","Huiqiang Jiang","Xufang Luo","Hao Cheng","Dongsheng Li","Yuqing Yang","Chin-Yew Lin","H. Vicky Zhao","Lili Qiu","Jianfeng Gao"],"pdf_url":"https://arxiv.org/pdf/2502.05589v3.pdf","comment":"10 pages, 5 figures, conference"},{"id":"http://arxiv.org/abs/2502.19735v2","updated":"2025-03-03T16:44:25Z","published":"2025-02-27T03:57:00Z","title":"R1-T1: Fully Incentivizing Translation Capability in LLMs via Reasoning\n  Learning","summary":"  Despite recent breakthroughs in reasoning-enhanced large language models\n(LLMs) like DeepSeek-R1, incorporating inference-time reasoning into machine\ntranslation (MT), where human translators naturally employ structured,\nmulti-layered reasoning chain-of-thoughts (CoTs), is yet underexplored.\nExisting methods either design a fixed CoT tailored for a specific MT sub-task\n(e.g., literature translation), or rely on synthesizing CoTs unaligned with\nhumans, limiting their adaptability to diverse translation scenarios. This\npaper introduces R1-Translator (R1-T1), a novel framework to achieve\ninference-time reasoning for general MT via reinforcement learning (RL) with\nhuman-aligned CoTs comprising six common patterns. Our approach pioneers three\ninnovations: (1) extending reasoning-based translation beyond MT sub-tasks to\nsix languages and diverse tasks (e.g., legal/medical domain adaptation, idiom\nresolution); (2) formalizing six expert-curated CoT templates that mirror\nhybrid human strategies like context-aware paraphrasing and back translation;\nand (3) enabling self-evolving CoT discovery through RL. Experimental results\nindicate a steady translation performance improvement in 11 languages and 40\ntranslation directions on Flores-101 test set, especially on the languages\nunseen from training.\n","authors":["Minggui He","Yilun Liu","Shimin Tao","Yuanchang Luo","Hongyong Zeng","Chang Su","Li Zhang","Hongxia Ma","Daimeng Wei","Weibin Meng","Hao Yang","Boxing Chen","Osamu Yoshie"],"pdf_url":"https://arxiv.org/pdf/2502.19735v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15823v3","updated":"2025-03-03T16:38:10Z","published":"2025-02-20T03:48:00Z","title":"InductionBench: LLMs Fail in the Simplest Complexity Class","summary":"  Large language models (LLMs) have shown remarkable improvements in reasoning\nand many existing benchmarks have been addressed by models such as o1 and o3\neither fully or partially. However, a majority of these benchmarks emphasize\ndeductive reasoning, including mathematical and coding tasks in which rules\nsuch as mathematical axioms or programming syntax are clearly defined, based on\nwhich LLMs can plan and apply these rules to arrive at a solution. In contrast,\ninductive reasoning, where one infers the underlying rules from observed data,\nremains less explored. Such inductive processes lie at the heart of scientific\ndiscovery, as they enable researchers to extract general principles from\nempirical observations. To assess whether LLMs possess this capacity, we\nintroduce InductionBench, a new benchmark designed to evaluate the inductive\nreasoning ability of LLMs. Our experimental findings reveal that even the most\nadvanced models available struggle to master the simplest complexity classes\nwithin the subregular hierarchy of functions, highlighting a notable deficiency\nin current LLMs' inductive reasoning capabilities. Coda and data are available\nhttps://github.com/Wenyueh/inductive_reasoning_benchmark.\n","authors":["Wenyue Hua","Tyler Wong","Sun Fei","Liangming Pan","Adam Jardine","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2502.15823v3.pdf","comment":"24 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.16251v3","updated":"2025-03-03T15:37:23Z","published":"2024-10-21T17:55:54Z","title":"Can Knowledge Editing Really Correct Hallucinations?","summary":"  Large Language Models (LLMs) suffer from hallucinations, referring to the\nnon-factual information in generated content, despite their superior capacities\nacross tasks. Meanwhile, knowledge editing has been developed as a new popular\nparadigm to correct erroneous factual knowledge encoded in LLMs with the\nadvantage of avoiding retraining from scratch. However, a common issue of\nexisting evaluation datasets for knowledge editing is that they do not ensure\nthat LLMs actually generate hallucinated answers to the evaluation questions\nbefore editing. When LLMs are evaluated on such datasets after being edited by\ndifferent techniques, it is hard to directly adopt the performance to assess\nthe effectiveness of different knowledge editing methods in correcting\nhallucinations. Thus, the fundamental question remains insufficiently\nvalidated: Can knowledge editing really correct hallucinations in LLMs? We\nproposed HalluEditBench to holistically benchmark knowledge editing methods in\ncorrecting real-world hallucinations. First, we rigorously construct a massive\nhallucination dataset with 9 domains, 26 topics and more than 6,000\nhallucinations. Then, we assess the performance of knowledge editing methods in\na holistic way on five dimensions including Efficacy, Generalization,\nPortability, Locality, and Robustness. Through HalluEditBench, we have provided\nnew insights into the potentials and limitations of different knowledge editing\nmethods in correcting hallucinations, which could inspire future improvements\nand facilitate progress in the field of knowledge editing.\n","authors":["Baixiang Huang","Canyu Chen","Xiongxiao Xu","Ali Payani","Kai Shu"],"pdf_url":"https://arxiv.org/pdf/2410.16251v3.pdf","comment":"ICLR 2025. Main paper: 10 pages; total: 34 pages (including\n  appendix). The first two authors contributed equally to this work. Code,\n  data, results, and additional resources are available on the project website:\n  https://llm-editing.github.io"},{"id":"http://arxiv.org/abs/2502.12215v2","updated":"2025-03-03T15:29:43Z","published":"2025-02-17T07:21:11Z","title":"Revisiting the Test-Time Scaling of o1-like Models: Do they Truly\n  Possess Test-Time Scaling Capabilities?","summary":"  The advent of test-time scaling in large language models (LLMs), exemplified\nby OpenAI's o1 series, has advanced reasoning capabilities by scaling\ncomputational resource allocation during inference. While successors like QwQ,\nDeepseek-R1 (R1) and LIMO replicate these advancements, whether these models\ntruly possess test-time scaling capabilities remains underexplored. This study\nfound that longer CoTs of these o1-like models do not consistently enhance\naccuracy; in fact, correct solutions are often shorter than incorrect ones for\nthe same questions. Further investigation shows this phenomenon is closely\nrelated to models' self-revision capabilities - longer CoTs contain more\nself-revisions, which often lead to performance degradation. We then compare\nsequential and parallel scaling strategies on QwQ, R1 and LIMO, finding that\nparallel scaling achieves better coverage and scalability. Based on these\ninsights, we propose Shortest Majority Vote, a method that combines parallel\nscaling strategies with CoT length characteristics, significantly improving\nmodels' test-time scalability compared to conventional majority voting\napproaches.\n","authors":["Zhiyuan Zeng","Qinyuan Cheng","Zhangyue Yin","Yunhua Zhou","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2502.12215v2.pdf","comment":"Add the github link"},{"id":"http://arxiv.org/abs/2410.19803v2","updated":"2025-03-03T15:13:10Z","published":"2024-10-16T17:59:47Z","title":"First-Person Fairness in Chatbots","summary":"  Evaluating chatbot fairness is crucial given their rapid proliferation, yet\ntypical chatbot tasks (e.g., resume writing, entertainment) diverge from the\ninstitutional decision-making tasks (e.g., resume screening) which have\ntraditionally been central to discussion of algorithmic fairness. The\nopen-ended nature and diverse use-cases of chatbots necessitate novel methods\nfor bias assessment. This paper addresses these challenges by introducing a\nscalable counterfactual approach to evaluate \"first-person fairness,\" meaning\nfairness toward chatbot users based on demographic characteristics. Our method\nemploys a Language Model as a Research Assistant (LMRA) to yield quantitative\nmeasures of harmful stereotypes and qualitative analyses of demographic\ndifferences in chatbot responses. We apply this approach to assess biases in\nsix of our language models across millions of interactions, covering sixty-six\ntasks in nine domains and spanning two genders and four races. Independent\nhuman annotations corroborate the LMRA-generated bias evaluations. This study\nrepresents the first large-scale fairness evaluation based on real-world chat\ndata. We highlight that post-training reinforcement learning techniques\nsignificantly mitigate these biases. This evaluation provides a practical\nmethodology for ongoing bias monitoring and mitigation.\n","authors":["Tyna Eloundou","Alex Beutel","David G. Robinson","Keren Gu-Lemberg","Anna-Luisa Brakman","Pamela Mishkin","Meghan Shah","Johannes Heidecke","Lilian Weng","Adam Tauman Kalai"],"pdf_url":"https://arxiv.org/pdf/2410.19803v2.pdf","comment":"In ICLR 2025, 59 pages, 27 figures"},{"id":"http://arxiv.org/abs/2502.19723v2","updated":"2025-03-03T15:07:28Z","published":"2025-02-27T03:25:34Z","title":"CNsum:Automatic Summarization for Chinese News Text","summary":"  Obtaining valuable information from massive data efficiently has become our\nresearch goal in the era of Big Data. Text summarization technology has been\ncontinuously developed to meet this demand. Recent work has also shown that\ntransformer-based pre-trained language models have achieved great success on\nvarious tasks in Natural Language Processing (NLP). Aiming at the problem of\nChinese news text summary generation and the application of Transformer\nstructure on Chinese, this paper proposes a Chinese news text summarization\nmodel (CNsum) based on Transformer structure, and tests it on Chinese datasets\nsuch as THUCNews. The results of the conducted experiments show that CNsum\nachieves better ROUGE score than the baseline models, which verifies the\noutperformance of the model.\n","authors":["Yu Zhao","Songping Huang","Dongsheng Zhou","Zhaoyun Ding","Fei Wang","Aixin Nian"],"pdf_url":"https://arxiv.org/pdf/2502.19723v2.pdf","comment":"This withdrawal is due to the lack of authorization from all\n  co-authors for the publication of this version"},{"id":"http://arxiv.org/abs/2411.07180v4","updated":"2025-03-03T14:56:17Z","published":"2024-11-11T17:57:30Z","title":"Gumbel Counterfactual Generation From Language Models","summary":"  Understanding and manipulating the causal generation mechanisms in language\nmodels is essential for controlling their behavior. Previous work has primarily\nrelied on techniques such as representation surgery -- e.g., model ablations or\nmanipulation of linear subspaces tied to specific concepts -- to\n\\emph{intervene} on these models. To understand the impact of interventions\nprecisely, it is useful to examine \\emph{counterfactuals} -- e.g., how a given\nsentence would have appeared had it been generated by the model following a\nspecific intervention. We highlight that counterfactual reasoning is\nconceptually distinct from interventions, as articulated in Pearl's causal\nhierarchy. Based on this observation, we propose a framework for generating\ntrue string counterfactuals by reformulating language models as a structural\nequation model using the Gumbel-max trick, which we called Gumbel\ncounterfactual generation. This reformulation allows us to model the joint\ndistribution over original strings and their counterfactuals resulting from the\nsame instantiation of the sampling noise. We develop an algorithm based on\nhindsight Gumbel sampling that allows us to infer the latent noise variables\nand generate counterfactuals of observed strings. Our experiments demonstrate\nthat the approach produces meaningful counterfactuals while at the same time\nshowing that commonly used intervention techniques have considerable undesired\nside effects.\n","authors":["Shauli Ravfogel","Anej Svete","Vésteinn Snæbjarnarson","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2411.07180v4.pdf","comment":"Accepted in ICLR 2025"},{"id":"http://arxiv.org/abs/2410.05864v4","updated":"2025-03-03T14:30:07Z","published":"2024-10-08T09:53:35Z","title":"From Tokens to Words: On the Inner Lexicon of LLMs","summary":"  Natural language is composed of words, but modern large language models\n(LLMs) process sub-words as input. A natural question raised by this\ndiscrepancy is whether LLMs encode words internally, and if so how. We present\nevidence that LLMs engage in an intrinsic detokenization process, where\nsub-word sequences are combined into coherent whole-word representations at\ntheir last token. Our experiments show that this process primarily takes place\nwithin the early and middle layers of the model. We further demonstrate its\nrobustness to arbitrary splits (e.g., \"cats\" to \"ca\" and \"ts\"), typos, and\nimportantly-to out-of-vocabulary words: when feeding the last token internal\nrepresentations of such words to the model as input, it can \"understand\" them\nas the complete word despite never seeing such representations as input during\ntraining. Our findings suggest that LLMs maintain a latent vocabulary beyond\nthe tokenizer's scope. These insights provide a practical, finetuning-free\napplication for expanding the vocabulary of pre-trained models. By enabling the\naddition of new vocabulary words, we reduce input length and inference\niterations, which reduces both space and model latency, with little to no loss\nin model accuracy.\n","authors":["Guy Kaplan","Matanel Oren","Yuval Reif","Roy Schwartz"],"pdf_url":"https://arxiv.org/pdf/2410.05864v4.pdf","comment":"Accepted to the International Conference on Learning Representations\n  (ICLR) 2025"},{"id":"http://arxiv.org/abs/2407.10944v2","updated":"2025-03-03T13:41:46Z","published":"2024-07-15T17:41:34Z","title":"Naturally Occurring Feedback is Common, Extractable and Useful","summary":"  Human feedback data is a critical component in developing language models.\nHowever, collecting this feedback is costly and ultimately not scalable.\nInspired by the way human interlocutors provide spontaneous unsolicited\nfeedback to each other, we propose to extract feedback that users naturally\ninclude when interacting with chat models. We manually annotated conversations\nto confirm the presence of naturally occurring feedback in a standard corpus,\nfinding that as much as 30% of the chats include explicit feedback. Comparing\nto older datasets, we find that naturally occurring feedback is more prevalent\nin recent conversation datasets, suggesting that more than ever, naturally\noccurring feedback can serve as a valuable resource for feedback data. We\npropose a method for automatically extracting this feedback, and apply it to\nover 1M conversations to obtain hundreds of thousands of feedback samples. The\nextracted feedback shows promise: training with it improves over baseline\nmodels and enhances model alignment to human preferences.\n","authors":["Shachar Don-Yehiya","Leshem Choshen","Omri Abend"],"pdf_url":"https://arxiv.org/pdf/2407.10944v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18858v2","updated":"2025-03-03T13:38:50Z","published":"2025-02-26T05:59:45Z","title":"Evaluating Intelligence via Trial and Error","summary":"  Intelligence is a crucial trait for species to find solutions within a\nlimited number of trial-and-error attempts. Building on this idea, we introduce\nSurvival Game as a framework to evaluate intelligence based on the number of\nfailed attempts in a trial-and-error process. Fewer failures indicate higher\nintelligence. When the expectation and variance of failure counts are both\nfinite, it signals the ability to consistently find solutions to new\nchallenges, which we define as the Autonomous Level of intelligence. Using\nSurvival Game, we comprehensively evaluate existing AI systems. Our results\nshow that while AI systems achieve the Autonomous Level in simple tasks, they\nare still far from it in more complex tasks, such as vision, search,\nrecommendation, and language. While scaling current AI technologies might help,\nthis would come at an astronomical cost. Projections suggest that achieving the\nAutonomous Level for general tasks would require $10^{26}$ parameters. To put\nthis into perspective, loading such a massive model requires so many H100 GPUs\nthat their total value is $10^{7}$ times that of Apple Inc.'s market value.\nEven with Moore's Law, supporting such a parameter scale would take $70$ years.\nThis staggering cost highlights the complexity of human tasks and the\ninadequacies of current AI technologies. To further investigate this\nphenomenon, we conduct a theoretical analysis of Survival Game and its\nexperimental results. Our findings suggest that human tasks possess a\ncriticality property. As a result, Autonomous Level requires a deep\nunderstanding of the task's underlying mechanisms. Current AI systems, however,\ndo not fully grasp these mechanisms and instead rely on superficial mimicry,\nmaking it difficult for them to reach an autonomous level. We believe Survival\nGame can not only guide the future development of AI but also offer profound\ninsights into human intelligence.\n","authors":["Jingtao Zhan","Jiahao Zhao","Jiayu Li","Yiqun Liu","Bo Zhang","Qingyao Ai","Jiaxin Mao","Hongning Wang","Min Zhang","Shaoping Ma"],"pdf_url":"https://arxiv.org/pdf/2502.18858v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07596v2","updated":"2025-03-03T13:27:01Z","published":"2025-01-10T01:42:43Z","title":"Optimize Incompatible Parameters through Compatibility-aware Knowledge\n  Integration","summary":"  Deep neural networks have become foundational to advancements in multiple\ndomains, including recommendation systems, natural language processing, and so\non. Despite their successes, these models often contain incompatible parameters\nthat can be underutilized or detrimental to model performance, particularly\nwhen faced with specific, varying data distributions. Existing research excels\nin removing such parameters or merging the outputs of multiple different\npretrained models. However, the former focuses on efficiency rather than\nperformance, while the latter requires several times more computing and storage\nresources to support inference. In this paper, we set the goal to explicitly\nimprove these incompatible parameters by leveraging the complementary strengths\nof different models, thereby directly enhancing the models without any\nadditional parameters. Specifically, we propose Compatibility-aware Knowledge\nIntegration (CKI), which consists of Parameter Compatibility Assessment and\nParameter Splicing, which are used to evaluate the knowledge content of\nmultiple models and integrate the knowledge into one model, respectively. The\nintegrated model can be used directly for inference or for further fine-tuning.\nWe conduct extensive experiments on various datasets for recommendation and\nlanguage tasks, and the results show that Compatibility-aware Knowledge\nIntegration can effectively optimize incompatible parameters under multiple\ntasks and settings to break through the training limit of the original model\nwithout increasing the inference cost.\n","authors":["Zheqi Lv","Keming Ye","Zishu Wei","Qi Tian","Shengyu Zhang","Wenqiao Zhang","Wenjie Wang","Kun Kuang","Tat-Seng Chua","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2501.07596v2.pdf","comment":"Published on AAAI'25(Oral): The Annual AAAI Conference on Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2405.18915v2","updated":"2025-03-03T13:25:36Z","published":"2024-05-29T09:17:46Z","title":"Towards Better Chain-of-Thought: A Reflection on Effectiveness and\n  Faithfulness","summary":"  Chain-of-thought (CoT) prompting demonstrates varying performance under\ndifferent reasoning tasks. Previous work attempts to evaluate it but falls\nshort in providing an in-depth analysis of patterns that influence the CoT. In\nthis paper, we study the CoT performance from the perspective of effectiveness\nand faithfulness. For the former, we identify key factors that influence CoT\neffectiveness on performance improvement, including problem difficulty,\ninformation gain, and information flow. For the latter, we interpret the\nunfaithful CoT issue by conducting a joint analysis of the information\ninteraction among the question, CoT, and answer. The result demonstrates that,\nwhen the LLM predicts answers, it can recall correct information missing in the\nCoT from the question, leading to the problem. Finally, we propose a novel\nalgorithm to mitigate this issue, in which we recall extra information from the\nquestion to enhance the CoT generation and evaluate CoTs based on their\ninformation gain. Extensive experiments demonstrate that our approach enhances\nboth the faithfulness and effectiveness of CoT.\n","authors":["Jiachun Li","Pengfei Cao","Yubo Chen","Jiexin Xu","Huaijun Li","Xiaojian Jiang","Kang Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2405.18915v2.pdf","comment":"18 pages, under review"},{"id":"http://arxiv.org/abs/2408.08291v2","updated":"2025-03-03T13:18:21Z","published":"2024-08-15T17:46:54Z","title":"The ShareLM Collection and Plugin: Contributing Human-Model Chats for\n  the Benefit of the Community","summary":"  Human-model conversations provide a window into users' real-world scenarios,\nbehavior, and needs, and thus are a valuable resource for model development and\nresearch. While for-profit companies collect user data through the APIs of\ntheir models, using it internally to improve their own models, the open source\nand research community lags behind.\n  We introduce the ShareLM collection, a unified set of human conversations\nwith large language models, and its accompanying plugin, a Web extension for\nvoluntarily contributing user-model conversations. Where few platforms share\ntheir chats, the ShareLM plugin adds this functionality, thus, allowing users\nto share conversations from most platforms. The plugin allows the user to rate\ntheir conversations, both at the conversation and the response levels, and\ndelete conversations they prefer to keep private before they ever leave the\nuser's local storage. We release the plugin conversations as part of the\nShareLM collection, and call for more community effort in the field of open\nhuman-model data.\n  The code, plugin, and data are available.\n","authors":["Shachar Don-Yehiya","Leshem Choshen","Omri Abend"],"pdf_url":"https://arxiv.org/pdf/2408.08291v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07076v4","updated":"2025-03-03T13:17:24Z","published":"2024-10-09T17:19:58Z","title":"MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry\n  Scientific Hypotheses","summary":"  Scientific discovery contributes largely to human society's prosperity, and\nrecent progress shows that LLMs could potentially catalyze this process.\nHowever, it is still unclear whether LLMs can discover novel and valid\nhypotheses in chemistry. In this work, we investigate this central research\nquestion: Can LLMs automatically discover novel and valid chemistry research\nhypotheses given only a chemistry research background (consisting of a research\nquestion and/or a background survey), without limitation on the domain of the\nresearch question? After extensive discussions with chemistry experts, we\npropose an assumption that a majority of chemistry hypotheses can be resulted\nfrom a research background and several inspirations. With this key insight, we\nbreak the central question into three smaller fundamental questions. In brief,\nthey are: (1) given a background question, whether LLMs can retrieve good\ninspirations; (2) with background and inspirations, whether LLMs can lead to\nhypothesis; and (3) whether LLMs can identify good hypotheses to rank them\nhigher. To investigate these questions, we construct a benchmark consisting of\n51 chemistry papers published in Nature, Science, or a similar level in 2024\n(all papers are only available online since 2024). Every paper is divided by\nchemistry PhD students into three components: background, inspirations, and\nhypothesis. The goal is to rediscover the hypothesis, given only the background\nand a large randomly selected chemistry literature corpus consisting the ground\ntruth inspiration papers, with LLMs trained with data up to 2023. We also\ndevelop an LLM-based multi-agent framework that leverages the assumption,\nconsisting of three stages reflecting the three smaller questions. The proposed\nmethod can rediscover many hypotheses with very high similarity with the ground\ntruth ones, covering the main innovations.\n","authors":["Zonglin Yang","Wanhao Liu","Ben Gao","Tong Xie","Yuqiang Li","Wanli Ouyang","Soujanya Poria","Erik Cambria","Dongzhan Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.07076v4.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2502.11142v2","updated":"2025-03-03T12:56:35Z","published":"2025-02-16T14:17:36Z","title":"NavRAG: Generating User Demand Instructions for Embodied Navigation\n  through Retrieval-Augmented LLM","summary":"  Vision-and-Language Navigation (VLN) is an essential skill for embodied\nagents, allowing them to navigate in 3D environments following natural language\ninstructions. High-performance navigation models require a large amount of\ntraining data, the high cost of manually annotating data has seriously hindered\nthis field. Therefore, some previous methods translate trajectory videos into\nstep-by-step instructions for expanding data, but such instructions do not\nmatch well with users' communication styles that briefly describe destinations\nor state specific needs. Moreover, local navigation trajectories overlook\nglobal context and high-level task planning. To address these issues, we\npropose NavRAG, a retrieval-augmented generation (RAG) framework that generates\nuser demand instructions for VLN. NavRAG leverages LLM to build a hierarchical\nscene description tree for 3D scene understanding from global layout to local\ndetails, then simulates various user roles with specific demands to retrieve\nfrom the scene tree, generating diverse instructions with LLM. We annotate over\n2 million navigation instructions across 861 scenes and evaluate the data\nquality and navigation performance of trained models.\n","authors":["Zihan Wang","Yaohui Zhu","Gim Hee Lee","Yachun Fan"],"pdf_url":"https://arxiv.org/pdf/2502.11142v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19732v2","updated":"2025-03-03T12:21:14Z","published":"2025-02-27T03:53:45Z","title":"Speculative Decoding and Beyond: An In-Depth Survey of Techniques","summary":"  Sequential dependencies present a fundamental bottleneck in deploying\nlarge-scale autoregressive models, particularly for real-time applications.\nWhile traditional optimization approaches like pruning and quantization often\ncompromise model quality, recent advances in generation-refinement frameworks\ndemonstrate that this trade-off can be significantly mitigated.\n  This survey presents a comprehensive taxonomy of generation-refinement\nframeworks, analyzing methods across autoregressive sequence tasks. We\ncategorize methods based on their generation strategies (from simple n-gram\nprediction to sophisticated draft models) and refinement mechanisms (including\nsingle-pass verification and iterative approaches). Through systematic analysis\nof both algorithmic innovations and system-level implementations, we examine\ndeployment strategies across computing environments and explore applications\nspanning text, images, and speech generation. This systematic examination of\nboth theoretical frameworks and practical implementations provides a foundation\nfor future research in efficient autoregressive decoding.\n","authors":["Yunhai Hu","Zining Liu","Zhenyuan Dong","Tianfan Peng","Bradley McDanel","Sai Qian Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.19732v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06057v2","updated":"2025-03-03T11:08:15Z","published":"2024-07-08T15:59:44Z","title":"Variational Best-of-N Alignment","summary":"  Best-of-N (BoN) is a popular and effective algorithm for aligning language\nmodels to human preferences. The algorithm works as follows: at inference time,\nN samples are drawn from the language model, and the sample with the highest\nreward, as judged by a reward model, is returned as the output. Despite its\neffectiveness, BoN is computationally expensive; it reduces sampling throughput\nby a factor of N. To make BoN more efficient at inference time, one strategy is\nto fine-tune the language model to mimic what BoN does during inference. To\nachieve this, we derive the distribution induced by the BoN algorithm. We then\npropose to fine-tune the language model to minimize backward KL divergence to\nthe BoN distribution. Our approach is analogous to mean-field variational\ninference and, thus, we term it variational BoN (vBoN). To the extent this\nfine-tuning is successful and we end up with a good approximation, we have\nreduced the inference cost by a factor of N. Our experiments on controlled\ngeneration and summarization tasks show that BoN is the most effective\nalignment method, and our variational approximation to BoN achieves the closest\nperformance to BoN and surpasses models fine-tuned using the standard\nKL-constrained RL objective. In the controlled generation task, vBoN appears\nmore frequently on the Pareto frontier of reward and KL divergence compared to\nother alignment methods. In the summarization task, vBoN achieves high reward\nvalues across various sampling temperatures.\n","authors":["Afra Amini","Tim Vieira","Elliott Ash","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2407.06057v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.12460v2","updated":"2025-03-03T10:35:27Z","published":"2024-11-19T12:36:02Z","title":"Exploring Iterative Controllable Summarization with Large Language\n  Models","summary":"  Large language models (LLMs) have demonstrated remarkable performance in\nabstractive summarization tasks. However, their ability to precisely control\nsummary attributes (e.g., length or topic) remains underexplored, limiting\ntheir adaptability to specific user preferences. In this paper, we\nsystematically explore the controllability of LLMs. To this end, we revisit\nsummary attribute measurements and introduce iterative evaluation metrics,\nfailure rate and average iteration count to precisely evaluate controllability\nof LLMs, rather than merely assessing errors. Our findings show that LLMs\nstruggle more with numerical attributes than with linguistic attributes. To\naddress this challenge, we propose a guide-to-explain framework (GTE) for\ncontrollable summarization. Our GTE framework enables the model to identify\nmisaligned attributes in the initial draft and guides it in self-explaining\nerrors in the previous output. By allowing the model to reflect on its\nmisalignment, GTE generates well-adjusted summaries that satisfy the desired\nattributes with robust effectiveness, requiring surprisingly fewer iterations\nthan other iterative approaches.\n","authors":["Sangwon Ryu","Heejin Do","Daehee Kim","Hwanjo Yu","Dongwoo Kim","Yunsu Kim","Gary Geunbae Lee","Jungseul Ok"],"pdf_url":"https://arxiv.org/pdf/2411.12460v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11355v2","updated":"2025-03-03T09:45:24Z","published":"2025-02-17T02:11:17Z","title":"\"Nuclear Deployed!\": Analyzing Catastrophic Risks in Decision-making of\n  Autonomous LLM Agents","summary":"  Large language models (LLMs) are evolving into autonomous decision-makers,\nraising concerns about catastrophic risks in high-stakes scenarios,\nparticularly in Chemical, Biological, Radiological and Nuclear (CBRN) domains.\nBased on the insight that such risks can originate from trade-offs between the\nagent's Helpful, Harmlessness and Honest (HHH) goals, we build a novel\nthree-stage evaluation framework, which is carefully constructed to effectively\nand naturally expose such risks. We conduct 14,400 agentic simulations across\n12 advanced LLMs, with extensive experiments and analysis. Results reveal that\nLLM agents can autonomously engage in catastrophic behaviors and deception,\nwithout being deliberately induced. Furthermore, stronger reasoning abilities\noften increase, rather than mitigate, these risks. We also show that these\nagents can violate instructions and superior commands. On the whole, we\nempirically prove the existence of catastrophic risks in autonomous LLM agents.\nWe will release our code upon request.\n","authors":["Rongwu Xu","Xiaojian Li","Shuo Chen","Wei Xu"],"pdf_url":"https://arxiv.org/pdf/2502.11355v2.pdf","comment":"Please visit https://llm-catastrophic-risks.github.io for a quick\n  tour of our project"},{"id":"http://arxiv.org/abs/2403.07260v2","updated":"2025-03-03T09:36:14Z","published":"2024-03-12T02:37:11Z","title":"LaERC-S: Improving LLM-based Emotion Recognition in Conversation with\n  Speaker Characteristics","summary":"  Emotion recognition in conversation (ERC), the task of discerning human\nemotions for each utterance within a conversation, has garnered significant\nattention in human-computer interaction systems. Previous ERC studies focus on\nspeaker-specific information that predominantly stems from relationships among\nutterances, which lacks sufficient information around conversations. Recent\nresearch in ERC has sought to exploit pre-trained large language models (LLMs)\nwith speaker modelling to comprehend emotional states. Although these methods\nhave achieved encouraging results, the extracted speaker-specific information\nstruggles to indicate emotional dynamics. In this paper, motivated by the fact\nthat speaker characteristics play a crucial role and LLMs have rich world\nknowledge, we present LaERC-S, a novel framework that stimulates LLMs to\nexplore speaker characteristics involving the mental state and behavior of\ninterlocutors, for accurate emotion predictions. To endow LLMs with this\nknowledge information, we adopt the two-stage learning to make the models\nreason speaker characteristics and track the emotion of the speaker in complex\nconversation scenarios. Extensive experiments on three benchmark datasets\ndemonstrate the superiority of LaERC-S, reaching the new state-of-the-art.\n","authors":["Yumeng Fu","Junjie Wu","Zhongjie Wang","Meishan Zhang","Lili Shan","Yulin Wu","Bingquan Li"],"pdf_url":"https://arxiv.org/pdf/2403.07260v2.pdf","comment":"COLING 2025"},{"id":"http://arxiv.org/abs/2402.09911v2","updated":"2025-03-03T09:21:11Z","published":"2024-02-15T12:20:02Z","title":"Enhancing Large Language Models with Pseudo- and Multisource- Knowledge\n  Graphs for Open-ended Question Answering","summary":"  Mitigating the hallucinations of Large Language Models is a crucial task.\nAlthough some existing methods employ self-enhancement techniques, they fall\nshort of effectively addressing unknown factual hallucinations. Meanwhile,\nKnowledge Graph (KG) enhancement approaches fail to address the generalization\nacross different KG sources and the enhancement of open-ended answer questions\nsimultaneously. To tackle these limitations, we propose a framework that\ncombines Pseudo-Graph Generation and Atomic Knowledge Verification (PG\\&AKV).\nEnhancement of open-ended question-answering begins with leveraging the\nPseudo-Graph Generation to provide the related knowledge framework.\nSubsequently, Atomic Knowledge Verification utilizes atomic-level knowledge\nquerying and verification to achieve generalizability under different KG\nsources. Compared to the baseline, this approach yields a minimum improvement\nof 11.5 in the ROUGE-L score for open-ended questions. For precise-answered\nquestions, we observe a minimum accuracy improvement of 7.5%. Moreover, PG\\&AKV\nalso exhibits generalizability across different KG sources. Utilizing KG\ndifferent from the question sources, PG\\&AKV can even achieve at least a 3.5 %\nperformance improvement. In summary, our results pave the way for enhancing\nLLMs by incorporating Pseudo- and Multisource-KGs, particularly in the filed of\nopen-ended questions.\n","authors":["Jiaxiang Liu","Tong Zhou","Yubo Chen","Kang Liu","Jun Zhao"],"pdf_url":"https://arxiv.org/pdf/2402.09911v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.21228v2","updated":"2025-03-03T09:11:46Z","published":"2025-02-28T16:59:30Z","title":"ECLeKTic: a Novel Challenge Set for Evaluation of Cross-Lingual\n  Knowledge Transfer","summary":"  To achieve equitable performance across languages, multilingual large\nlanguage models (LLMs) must be able to abstract knowledge beyond the language\nin which it was acquired. However, the current literature lacks reliable ways\nto measure LLMs' capability of cross-lingual knowledge transfer. To that end,\nwe present ECLeKTic, a multilingual closed-book QA (CBQA) dataset that\nEvaluates Cross-Lingual Knowledge Transfer in a simple, black-box manner. We\ndetected information with uneven coverage across languages by controlling for\npresence and absence of Wikipedia articles in 12 languages. We generated\nknowledge-seeking questions in a source language, for which the answer appears\nin a relevant Wikipedia article and translated them to all other 11 languages,\nfor which the respective Wikipedias lack equivalent articles. Assuming that\nWikipedia reflects the prominent knowledge in the LLM's training data, to solve\nECLeKTic's CBQA task the model is required to transfer knowledge between\nlanguages. Experimenting with 8 LLMs, we show that SOTA models struggle to\neffectively share knowledge across, languages even if they can predict the\nanswer well for queries in the same language the knowledge was acquired in.\n","authors":["Omer Goldman","Uri Shaham","Dan Malkin","Sivan Eiger","Avinatan Hassidim","Yossi Matias","Joshua Maynez","Adi Mayrav Gilady","Jason Riesa","Shruti Rijhwani","Laura Rimell","Idan Szpektor","Reut Tsarfaty","Matan Eyal"],"pdf_url":"https://arxiv.org/pdf/2502.21228v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11167v2","updated":"2025-03-03T08:26:12Z","published":"2025-02-16T15:38:19Z","title":"SURGE: On the Potential of Large Language Models as General-Purpose\n  Surrogate Code Executors","summary":"  Neural surrogate models have emerged as powerful and efficient tools in data\nmining. Meanwhile, large language models (LLMs) have demonstrated remarkable\ncapabilities in code-related tasks. We investigate a novel application: using\nLLMs as surrogate models for code execution prediction. Given LLMs' unique\nability to understand and process diverse programs, they present a promising\ndirection for building general-purpose surrogate models. To systematically\ninvestigate this capability, we introduce SURGE, a comprehensive benchmark with\n$1160$ problems covering $8$ key aspects: multi-language programming tasks,\ncompetition-level programming problems, repository-level code analysis,\nhigh-cost scientific computing, time-complexity-intensive algorithms, buggy\ncode analysis, programs dependent on specific compilers or execution\nenvironments, and formal mathematical proof verification. Through extensive\nempirical analysis of $21$ open-source and proprietary LLMs, we examine scaling\nlaws, data efficiency, and predictive accuracy. Our findings reveal important\ninsights about the feasibility of LLMs as efficient surrogates for\ncomputational processes, with implications for automated software testing,\nprogram analysis, and computational resource optimization in data mining\napplications. Code and dataset are released at\nhttps://github.com/Imbernoulli/SURGE.\n","authors":["Bohan Lyu","Siqiao Huang","Zichen Liang"],"pdf_url":"https://arxiv.org/pdf/2502.11167v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19316v2","updated":"2025-03-03T08:22:25Z","published":"2024-05-29T17:39:48Z","title":"Robust Preference Optimization through Reward Model Distillation","summary":"  Language model (LM) post-training (or alignment) involves maximizing a reward\nfunction that is derived from preference annotations. Direct Preference\nOptimization (DPO) is a popular offline alignment method that trains a policy\ndirectly on preference data without the need to train a reward model or apply\nreinforcement learning. However, the empirical evidence suggests that DPO\ntypically assigns implicit rewards that overfit, and trend towards infinite\nmagnitude. This frequently leads to degenerate policies, sometimes causing even\nthe probabilities of the preferred generations to go to zero. In this work, we\nanalyze this phenomenon and use distillation to get a better proxy for the true\npreference distribution over generation pairs: we train the LM such that its\ninduced implicit reward, i.e., the scaled log-likelihood ratio of the model to\nthe reference model, matches an explicit reward model trained on the preference\ndata. Moreover, to account for uncertainty in the reward model we are\ndistilling from, we optimize against a family of reward models that, as a\nwhole, is likely to include at least one reasonable proxy for the preference\ndistribution. Our results show that distilling from such a family of reward\nmodels leads to improved robustness to distribution shift in preference\nannotations, while preserving the simple supervised nature of DPO.\n","authors":["Adam Fisch","Jacob Eisenstein","Vicky Zayats","Alekh Agarwal","Ahmad Beirami","Chirag Nagpal","Pete Shaw","Jonathan Berant"],"pdf_url":"https://arxiv.org/pdf/2405.19316v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.19651v3","updated":"2025-03-03T07:49:17Z","published":"2023-10-30T15:37:10Z","title":"Dynamics of Instruction Fine-Tuning for Chinese Large Language Models","summary":"  Instruction tuning is a burgeoning method to elicit the general intelligence\nof Large Language Models (LLMs). While numerous studies have examined the\nimpact of factors such as data volume and model size on English models, the\nscaling properties of instruction tuning in other languages remain largely\nunexplored. In this work, we systematically investigate the effects of data\nquantity, model size, and data construction methods on instruction tuning for\nChinese LLMs. We utilize a newly curated dataset, DoIT, which includes over\n40,000 high-quality instruction instances covering ten underlying abilities,\nsuch as creative writing, code generation, and logical reasoning. Our\nexperiments, conducted on models ranging from 7b to 33b parameters, yield three\nkey findings: (i) While these factors directly affect overall model\nperformance, some abilities are more responsive to scaling, whereas others\ndemonstrate significant resistance. (ii) The scaling sensitivity of different\nabilities to these factors can be explained by two features: Complexity and\nTransference. (iii) By tailoring training strategies to their varying\nsensitivities, specific abilities can be efficiently learned, enhancing\nperformance on two public benchmarks.\n","authors":["Chiyu Song","Zhanchao Zhou","Jianhao Yan","Yuejiao Fei","Zhenzhong Lan","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2310.19651v3.pdf","comment":"Accepted to COLING 2025"},{"id":"http://arxiv.org/abs/2406.14434v3","updated":"2025-03-03T07:36:49Z","published":"2024-06-20T15:59:07Z","title":"Selected Languages are All You Need for Cross-lingual Truthfulness\n  Transfer","summary":"  Truthfulness stands out as an essential challenge for Large Language Models\n(LLMs). Although many works have developed various ways for truthfulness\nenhancement, they seldom focus on truthfulness in multilingual scenarios.\nMeanwhile, contemporary multilingual aligning technologies struggle to balance\nnumerous languages and often exhibit serious truthfulness gaps across different\nlanguages, especially those that differ greatly from English. In our work, we\nextend truthfulness evaluation to multilingual contexts and propose a practical\nmethod for cross-lingual truthfulness transfer called Fact-aware Multilingual\nSelective Synergy (FaMSS). FaMSS is able to select an optimal subset of all\ntested languages by language bias and transfer contributions, and then employ\ntranslation instruction tuning for cross-lingual truthfulness transfer.\nExperimental results demonstrate that our approach can effectively reduce the\nmultilingual representation disparity and boost cross-lingual truthfulness\ntransfer of LLMs.\n","authors":["Weihao Liu","Ning Wu","Wenbiao Ding","Shining Liang","Ming Gong","Dongmei Zhang"],"pdf_url":"https://arxiv.org/pdf/2406.14434v3.pdf","comment":"16 pages, COLING2025"},{"id":"http://arxiv.org/abs/2409.16644v2","updated":"2025-03-03T07:22:54Z","published":"2024-09-25T05:44:44Z","title":"Enabling Auditory Large Language Models for Automatic Speech Quality\n  Evaluation","summary":"  Speech quality assessment typically requires evaluating audio from multiple\naspects, such as mean opinion score (MOS) and speaker similarity (SIM) \\etc.,\nwhich can be challenging to cover using one small model designed for a single\ntask. In this paper, we propose leveraging recently introduced auditory large\nlanguage models (LLMs) for automatic speech quality assessment. By employing\ntask-specific prompts, auditory LLMs are finetuned to predict MOS, SIM and A/B\ntesting results, which are commonly used for evaluating text-to-speech systems.\nAdditionally, the finetuned auditory LLM is able to generate natural language\ndescriptions assessing aspects like noisiness, distortion, discontinuity, and\noverall quality, providing more interpretable outputs. Extensive experiments\nhave been performed on the NISQA, BVCC, SOMOS and VoxSim speech quality\ndatasets, using open-source auditory LLMs such as SALMONN, Qwen-Audio, and\nQwen2-Audio. For the natural language descriptions task, a commercial model\nGoogle Gemini 1.5 Pro is also evaluated. The results demonstrate that auditory\nLLMs achieve competitive performance compared to state-of-the-art task-specific\nsmall models in predicting MOS and SIM, while also delivering promising results\nin A/B testing and natural language descriptions. Our data processing scripts\nand finetuned model checkpoints can be found at\nhttps://github.com/bytedance/SALMONN.\n","authors":["Siyin Wang","Wenyi Yu","Yudong Yang","Changli Tang","Yixuan Li","Jimin Zhuang","Xianzhao Chen","Xiaohai Tian","Jun Zhang","Guangzhi Sun","Lu Lu","Chao Zhang"],"pdf_url":"https://arxiv.org/pdf/2409.16644v2.pdf","comment":"Accepted by ICASSP 2025"},{"id":"http://arxiv.org/abs/2410.02683v2","updated":"2025-03-03T07:20:54Z","published":"2024-10-03T17:08:52Z","title":"DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of\n  Daily Life","summary":"  As users increasingly seek guidance from LLMs for decision-making in daily\nlife, many of these decisions are not clear-cut and depend significantly on the\npersonal values and ethical standards of people. We present DailyDilemmas, a\ndataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma\npresents two possible actions, along with affected parties and relevant human\nvalues for each action. Based on these dilemmas, we gather a repository of\nhuman values covering diverse everyday topics, such as interpersonal\nrelationships, workplace, and environmental issues. With DailyDilemmas, we\nevaluate LLMs on these dilemmas to determine what action they will choose and\nthe values represented by these action choices. Then, we analyze values through\nthe lens of five theoretical frameworks inspired by sociology, psychology, and\nphilosophy, including the World Values Survey, Moral Foundations Theory,\nMaslow's Hierarchy of Needs, Aristotle's Virtues, and Plutchik's Wheel of\nEmotions. For instance, we find LLMs are most aligned with self-expression over\nsurvival in World Values Survey and care over loyalty in Moral Foundations\nTheory. Interestingly, we find substantial preference differences in models for\nsome core values. For example, for truthfulness, Mixtral-8x7B neglects it by\n9.7% while GPT-4-turbo selects it by 9.4%. We also study the recent guidance\nreleased by OpenAI (ModelSpec), and Anthropic (Constitutional AI) to understand\nhow their designated principles reflect their models' actual value\nprioritization when facing nuanced moral reasoning in daily-life settings.\nFinally, we find that end users cannot effectively steer such prioritization\nusing system prompts.\n","authors":["Yu Ying Chiu","Liwei Jiang","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2410.02683v2.pdf","comment":"Accepted into ICLR 2025 (spotlight)"},{"id":"http://arxiv.org/abs/2501.02497v2","updated":"2025-03-03T07:16:16Z","published":"2025-01-05T10:24:20Z","title":"Test-Time Compute: from System-1 Thinking to System-2 Thinking","summary":"  The remarkable performance of the o1 model in complex reasoning demonstrates\nthat test-time compute scaling can further unlock the model's potential,\nenabling powerful System-2 thinking. However, there is still a lack of\ncomprehensive surveys for test-time compute scaling. We trace the concept of\ntest-time compute back to System-1 models. In System-1 models, test-time\ncompute addresses distribution shifts and improves robustness and\ngeneralization through parameter updating, input modification, representation\nediting, and output calibration. In System-2 models, it enhances the model's\nreasoning ability to solve complex problems through repeated sampling,\nself-correction, and tree search. We organize this survey according to the\ntrend of System-1 to System-2 thinking, highlighting the key role of test-time\ncompute in the transition from System-1 models to weak System-2 models, and\nthen to strong System-2 models. We also point out a few possible future\ndirections.\n","authors":["Yixin Ji","Juntao Li","Hai Ye","Kaixin Wu","Kai Yao","Jia Xu","Linjian Mo","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.02497v2.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2502.18874v2","updated":"2025-03-03T07:13:12Z","published":"2025-02-26T06:31:45Z","title":"Learning to Align Multi-Faceted Evaluation: A Unified and Robust\n  Framework","summary":"  Large Language Models (LLMs) are being used more and more extensively for\nautomated evaluation in various scenarios. Previous studies have attempted to\nfine-tune open-source LLMs to replicate the evaluation explanations and\njudgments of powerful proprietary models, such as GPT-4. However, these methods\nare largely limited to text-based analyses under predefined general criteria,\nresulting in reduced adaptability for unseen instructions and demonstrating\ninstability in evaluating adherence to quantitative and structural constraints.\nTo address these limitations, we propose a novel evaluation framework, ARJudge,\nthat adaptively formulates evaluation criteria and synthesizes both text-based\nand code-driven analyses to evaluate LLM responses. ARJudge consists of two\ncomponents: a fine-tuned Analyzer that generates multi-faceted evaluation\nanalyses and a tuning-free Refiner that combines and refines all analyses to\nmake the final judgment. We construct a Composite Analysis Corpus that\nintegrates tasks for evaluation criteria generation alongside text-based and\ncode-driven analysis generation to train the Analyzer. Our results demonstrate\nthat ARJudge outperforms existing fine-tuned evaluators in effectiveness and\nrobustness. Furthermore, it demonstrates the importance of multi-faceted\nevaluation and code-driven analyses in enhancing evaluation capabilities.\n","authors":["Kaishuai Xu","Tiezheng Yu","Wenjun Hou","Yi Cheng","Liangyou Li","Xin Jiang","Lifeng Shang","Qun Liu","Wenjie Li"],"pdf_url":"https://arxiv.org/pdf/2502.18874v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06638v3","updated":"2025-03-03T07:09:42Z","published":"2024-10-09T07:43:38Z","title":"Subtle Errors Matter: Preference Learning via Error-injected\n  Self-editing","summary":"  Large Language Models (LLMs) have exhibited strong mathematical reasoning\nprowess, tackling tasks ranging from basic arithmetic to advanced\ncompetition-level problems. However, frequently occurring subtle yet critical\nerrors, such as miscalculations or incorrect substitutions, limit the LLMs'\nfull potential. Existing studies to improve mathematical ability typically\ninvolve applying preference learning to step-wise solution pairs. Although\nthese methods leverage samples of varying granularity to mitigate reasoning\nerrors, they overlook critical subtle errors. In this work, we propose a novel\npreference learning framework called eRror-Injected Self-Editing (RISE), which\ninjects predefined subtle errors into pivotal tokens in reasoning or\ncomputation steps to construct hard pairs for error mitigation. In detail, RISE\nuses the LLM itself to edit a small number of tokens in the solution, injecting\ndesigned subtle errors. Then, pairs composed of self-edited solutions and their\ncorresponding correct ones, along with pairs of correct and incorrect solutions\nobtained through sampling, are used together for subtle error-aware DPO\ntraining. Compared with other preference learning methods, RISE further refines\nthe training objective without requiring fine-grained sampling or preference\nannotation. Extensive experiments validate the effectiveness of RISE, with\npreference learning on Qwen2-7B-Instruct yielding notable improvements of 3.0%\non GSM8K and 7.9% on MATH with only 4.5K training samples. Moreover, the effect\nof error mitigation extends from mathematical reasoning to logical reasoning\nand code generation.\n","authors":["Kaishuai Xu","Tiezheng Yu","Wenjun Hou","Yi Cheng","Chak Tou Leong","Liangyou Li","Xin Jiang","Lifeng Shang","Qun Liu","Wenjie Li"],"pdf_url":"https://arxiv.org/pdf/2410.06638v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07170v3","updated":"2025-03-03T07:02:20Z","published":"2024-09-11T10:33:30Z","title":"Learning Efficient Recursive Numeral Systems via Reinforcement Learning","summary":"  It has previously been shown that by using reinforcement learning (RL),\nagents can derive simple approximate and exact-restricted numeral systems that\nare similar to human ones (Carlsson, 2021). However, it is a major challenge to\nshow how more complex recursive numeral systems, similar to for example\nEnglish, could arise via a simple learning mechanism such as RL. Here, we\nintroduce an approach towards deriving a mechanistic explanation of the\nemergence of efficient recursive number systems. We consider pairs of agents\nlearning how to communicate about numerical quantities through a meta-grammar\nthat can be gradually modified throughout the interactions. %We find that the\nseminal meta-grammar of Hurford (Hurford, 1975) is not suitable for this\napplication as its optimization results in systems that deviate from standard\nconventions observed within human numeral systems. We propose a simple\nmodification which addresses this issue. Utilising a slightly modified version\nof the meta-grammar of Hurford, we demonstrate that our RL agents, shaped by\nthe pressures for efficient communication, can effectively modify their lexicon\ntowards Pareto-optimal configurations which are comparable to those observed\nwithin human numeral systems in terms of their efficiency.\n","authors":["Andrea Silvi","Jonathan Thomas","Emil Carlsson","Devdatt Dubhashi","Moa Johansson"],"pdf_url":"https://arxiv.org/pdf/2409.07170v3.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2407.04752v2","updated":"2025-03-03T06:46:33Z","published":"2024-07-05T08:37:17Z","title":"SpikeLLM: Scaling up Spiking Neural Network to Large Language Models via\n  Saliency-based Spiking","summary":"  Recent advancements in large language models (LLMs) with billions of\nparameters have improved performance in various applications, but their\ninference processes demand significant energy and computational resources. In\ncontrast, the human brain, with approximately 86 billion neurons, is much more\nenergy-efficient than LLMs with similar parameters. Inspired by this, we\nredesign 7$\\sim$70 billion parameter LLMs using bio-plausible spiking\nmechanisms, emulating the efficient behavior of the human brain. We propose the\nfirst spiking large language model, SpikeLLM. Coupled with the proposed model,\ntwo essential approaches are proposed to improve spike training efficiency:\nGeneralized Integrate-and-Fire (GIF) neurons to compress spike length from $T$\nto $\\frac{T}{L} \\log_2 L$ bits, and an Optimal Brain Spiking framework to\ndivide outlier channels and allocate different $T$ for GIF neurons, which\nfurther compresses spike length to approximate $log_2T$ bits. The necessity of\nspike-driven LLM is proved by comparison with quantized LLMs with similar\noperations. In the OmniQuant pipeline, SpikeLLM reduces 11.01% WikiText2\nperplexity and improves 2.55% accuracy of common scene reasoning on a LLAMA-7B\nW4A4 model. In the GPTQ pipeline, SpikeLLM achieves direct additive in linear\nlayers, significantly exceeding PB-LLMs.\n","authors":["Xingrun Xing","Boyan Gao","Zheng Zhang","David A. Clifton","Shitao Xiao","Li Du","Guoqi Li","Jiajun Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.04752v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18945v4","updated":"2025-03-03T06:34:48Z","published":"2024-02-29T08:20:49Z","title":"SynGhost: Invisible and Universal Task-agnostic Backdoor Attack via\n  Syntactic Transfer","summary":"  Although pre-training achieves remarkable performance, it suffers from\ntask-agnostic backdoor attacks due to vulnerabilities in data and training\nmechanisms. These attacks can transfer backdoors to various downstream tasks.\nIn this paper, we introduce $\\mathtt{maxEntropy}$, an entropy-based poisoning\nfilter that mitigates such risks. To overcome the limitations of manual target\nsetting and explicit triggers, we propose $\\mathtt{SynGhost}$, an invisible and\nuniversal task-agnostic backdoor attack via syntactic transfer, further\nexposing vulnerabilities in pre-trained language models (PLMs). Specifically,\n$\\mathtt{SynGhost}$ injects multiple syntactic backdoors into the pre-training\nspace through corpus poisoning, while preserving the PLM's pre-training\ncapabilities. Second, $\\mathtt{SynGhost}$ adaptively selects optimal targets\nbased on contrastive learning, creating a uniform distribution in the\npre-training space. To identify syntactic differences, we also introduce an\nawareness module to minimize interference between backdoors. Experiments show\nthat $\\mathtt{SynGhost}$ poses significant threats and can transfer to various\ndownstream tasks. Furthermore, $\\mathtt{SynGhost}$ resists defenses based on\nperplexity, fine-pruning, and $\\mathtt{maxEntropy}$. The code is available at\nhttps://github.com/Zhou-CyberSecurity-AI/SynGhost.\n","authors":["Pengzhou Cheng","Wei Du","Zongru Wu","Fengwei Zhang","Libo Chen","Zhuosheng Zhang","Gongshen Liu"],"pdf_url":"https://arxiv.org/pdf/2402.18945v4.pdf","comment":"17 pages, 16 figures, 12 tables, accepted at NAACL 2025 Findings"},{"id":"http://arxiv.org/abs/2412.07298v2","updated":"2025-03-03T06:33:49Z","published":"2024-12-10T08:28:57Z","title":"The Rise and Down of Babel Tower: Investigating the Evolution Process of\n  Multilingual Code Large Language Model","summary":"  Large language models (LLMs) have shown significant multilingual\ncapabilities. However, the mechanisms underlying the development of these\ncapabilities during pre-training are not well understood. In this paper, we use\ncode LLMs as an experimental platform to explore the evolution of multilingual\ncapabilities in LLMs during the pre-training process. Based on our\nobservations, we propose the Babel Tower Hypothesis, which describes the entire\nprocess of LLMs acquiring new language capabilities. During the learning\nprocess, multiple languages initially share a single knowledge system dominated\nby the primary language and gradually develop language-specific knowledge\nsystems. We then validate the above hypothesis by tracking the internal states\nof the LLMs through identifying working languages and language transferring\nneurons. Experimental results show that the internal state changes of the LLM\nare consistent with our Babel Tower Hypothesis. Building on these insights, we\npropose a novel method to construct an optimized pre-training corpus for\nmultilingual code LLMs, which significantly outperforms LLMs trained on the\noriginal corpus. The proposed Babel Tower Hypothesis provides new insights into\ndesigning pre-training data distributions to achieve optimal multilingual\ncapabilities in LLMs.\n","authors":["Jiawei Chen","Wentao Chen","Jing Su","Jingjing Xu","Hongyu Lin","Mengjie Ren","Yaojie Lu","Xianpei Han","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2412.07298v2.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2502.17204v2","updated":"2025-03-03T06:29:31Z","published":"2025-02-24T14:39:28Z","title":"Order Matters: Investigate the Position Bias in Multi-constraint\n  Instruction Following","summary":"  Real-world instructions with multiple constraints pose a significant\nchallenge to existing large language models (LLMs). An observation is that the\nLLMs exhibit dramatic performance fluctuation when disturbing the order of the\nincorporated constraints. Yet, none of the existing works has systematically\ninvestigated this position bias problem in the field of multi-constraint\ninstruction following. To bridge this gap, we design a probing task where we\nquantitatively measure the difficulty distribution of the constraints by a\nnovel Difficulty Distribution Index (CDDI). Through the experimental results,\nwe find that LLMs are more performant when presented with the constraints in a\n``hard-to-easy'' order. This preference can be generalized to LLMs with\ndifferent architecture or different sizes of parameters. Additionally, we\nconduct an explanation study, providing an intuitive insight into the\ncorrelation between the LLM's attention and constraint orders. Our code and\ndataset are publicly available at https://github.com/meowpass/PBIF.\n","authors":["Jie Zeng","Qianyu He","Qingyu Ren","Jiaqing Liang","Yanghua Xiao","Weikang Zhou","Zeye Sun","Fei Yu"],"pdf_url":"https://arxiv.org/pdf/2502.17204v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.01405v4","updated":"2025-03-03T06:14:14Z","published":"2023-10-02T17:59:07Z","title":"Representation Engineering: A Top-Down Approach to AI Transparency","summary":"  In this paper, we identify and characterize the emerging area of\nrepresentation engineering (RepE), an approach to enhancing the transparency of\nAI systems that draws on insights from cognitive neuroscience. RepE places\npopulation-level representations, rather than neurons or circuits, at the\ncenter of analysis, equipping us with novel methods for monitoring and\nmanipulating high-level cognitive phenomena in deep neural networks (DNNs). We\nprovide baselines and an initial analysis of RepE techniques, showing that they\noffer simple yet effective solutions for improving our understanding and\ncontrol of large language models. We showcase how these methods can provide\ntraction on a wide range of safety-relevant problems, including honesty,\nharmlessness, power-seeking, and more, demonstrating the promise of top-down\ntransparency research. We hope that this work catalyzes further exploration of\nRepE and fosters advancements in the transparency and safety of AI systems.\n","authors":["Andy Zou","Long Phan","Sarah Chen","James Campbell","Phillip Guo","Richard Ren","Alexander Pan","Xuwang Yin","Mantas Mazeika","Ann-Kathrin Dombrowski","Shashwat Goel","Nathaniel Li","Michael J. Byun","Zifan Wang","Alex Mallen","Steven Basart","Sanmi Koyejo","Dawn Song","Matt Fredrikson","J. Zico Kolter","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2310.01405v4.pdf","comment":"Code is available at\n  https://github.com/andyzoujm/representation-engineering"},{"id":"http://arxiv.org/abs/2411.02886v2","updated":"2025-03-03T05:49:41Z","published":"2024-11-05T07:56:24Z","title":"TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection","summary":"  The rapid advancement of Large Language Models (LLMs) has driven growing\ndemand for processing extended context sequences in contemporary applications.\nHowever, this progress faces two major challenges: performance degradation due\nto sequence lengths out-of-distribution, and excessively long inference times\ncaused by the quadratic computational complexity of attention. These issues\nhinder the application of LLMs in long-context scenarios. In this paper, we\npropose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free\nmethod for efficient and accurate long-context inference. TokenSelect builds\nupon the observation of non-contiguous attention sparsity, using Query-Key dot\nproducts to measure per-head KV Cache criticality at token-level. By per-head\nsoft voting mechanism, TokenSelect selectively involves a few critical KV cache\ntokens in attention calculation without sacrificing accuracy. To further\naccelerate TokenSelect, we design the Selection Cache based on observations of\nconsecutive Query similarity and implemented efficient dot product kernel,\nsignificantly reducing the overhead. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods.\n","authors":["Wei Wu","Zhuoshi Pan","Chao Wang","Liyi Chen","Yunchu Bai","Tianfu Wang","Kun Fu","Zheng Wang","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2411.02886v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14171v3","updated":"2025-03-03T05:44:29Z","published":"2025-02-20T00:39:05Z","title":"Enhancing Conversational Agents with Theory of Mind: Aligning Beliefs,\n  Desires, and Intentions for Human-Like Interaction","summary":"  Natural language interaction with agentic Artificial Intelligence (AI),\ndriven by Large Language Models (LLMs), is expected to remain a dominant\nparadigm in the near future. While humans instinctively align their\ncommunication with mental states -- an ability known as Theory of Mind (ToM),\ncurrent LLM powered systems exhibit significant limitations in this regard.\nThis study examines the extent to which open source language models (LLaMA) can\ncapture and preserve ToM related information and how effectively it contributes\nto consistent ToM reasoning in generated responses. We further investigate\nwhether explicit manipulation of ToM related components, such as beliefs,\ndesires, and intentions, can enhance response alignment. Experiments on two\nLLaMA 3 variants demonstrate that incorporating ToM informed alignment improves\nresponse quality, achieving win rates of 67 and 63 percent for the 3B and 8B\nmodels, respectively. These findings highlight the potential of ToM driven\nstrategies to improve alignment in LLM based conversational agents.\n","authors":["Mehdi Jafari","Devin Yuncheng Hua","Hao Xue","Flora Salim"],"pdf_url":"https://arxiv.org/pdf/2502.14171v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02268v3","updated":"2025-03-03T05:32:47Z","published":"2024-10-03T07:40:14Z","title":"Structural-Entropy-Based Sample Selection for Efficient and Effective\n  Learning","summary":"  Sample selection improves the efficiency and effectiveness of machine\nlearning models by providing informative and representative samples. Typically,\nsamples can be modeled as a sample graph, where nodes are samples and edges\nrepresent their similarities. Most existing methods are based on local\ninformation, such as the training difficulty of samples, thereby overlooking\nglobal information, such as connectivity patterns. This oversight can result in\nsuboptimal selection because global information is crucial for ensuring that\nthe selected samples well represent the structural properties of the graph. To\naddress this issue, we employ structural entropy to quantify global information\nand losslessly decompose it from the whole graph to individual nodes using the\nShapley value. Based on the decomposition, we present\n$\\textbf{S}$tructural-$\\textbf{E}$ntropy-based sample $\\textbf{S}$election\n($\\textbf{SES}$), a method that integrates both global and local information to\nselect informative and representative samples. SES begins by constructing a\n$k$NN-graph among samples based on their similarities. It then measures sample\nimportance by combining structural entropy (global metric) with training\ndifficulty (local metric). Finally, SES applies importance-biased blue noise\nsampling to select a set of diverse and representative samples. Comprehensive\nexperiments on three learning scenarios -- supervised learning, active\nlearning, and continual learning -- clearly demonstrate the effectiveness of\nour method.\n","authors":["Tianchi Xie","Jiangning Zhu","Guozu Ma","Minzhi Lin","Wei Chen","Weikai Yang","Shixia Liu"],"pdf_url":"https://arxiv.org/pdf/2410.02268v3.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.17720v2","updated":"2025-03-03T04:31:48Z","published":"2025-02-24T23:23:27Z","title":"Spontaneous Giving and Calculated Greed in Language Models","summary":"  Large language models demonstrate advanced problem-solving capabilities by\nincorporating reasoning techniques such as chain of thought and reflection.\nHowever, how these reasoning capabilities extend to social intelligence remains\nunclear. In this study, we investigate this question using economic games that\nmodel social dilemmas, where social intelligence plays a crucial role. First,\nwe examine the effects of chain-of-thought and reflection techniques in a\npublic goods game. We then extend our analysis to six economic games on\ncooperation and punishment, comparing off-the-shelf non-reasoning and reasoning\nmodels. We find that reasoning models significantly reduce cooperation and norm\nenforcement, prioritizing individual rationality. Consequently, groups with\nmore reasoning models exhibit less cooperation and lower gains through repeated\ninteractions. These behaviors parallel human tendencies of \"spontaneous giving\nand calculated greed.\" Our results suggest the need for AI architectures that\nincorporate social intelligence alongside reasoning capabilities to ensure that\nAI supports, rather than disrupts, human cooperative intuition.\n","authors":["Yuxuan Li","Hirokazu Shirado"],"pdf_url":"https://arxiv.org/pdf/2502.17720v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09906v3","updated":"2025-03-03T04:28:49Z","published":"2024-02-15T12:12:19Z","title":"Generative Representational Instruction Tuning","summary":"  All text-based language problems can be reduced to either generation or\nembedding. Current models only perform well at one or the other. We introduce\ngenerative representational instruction tuning (GRIT) whereby a large language\nmodel is trained to handle both generative and embedding tasks by\ndistinguishing between them through instructions. Compared to other open\nmodels, our resulting GritLM 7B sets a new state of the art on the Massive Text\nEmbedding Benchmark (MTEB) and outperforms all models up to its size on a range\nof generative tasks. By scaling up further, GritLM 8x7B outperforms all open\ngenerative language models that we tried while still being among the best\nembedding models. Notably, we find that GRIT matches training on only\ngenerative or embedding data, thus we can unify both at no performance loss.\nAmong other benefits, the unification via GRIT speeds up Retrieval-Augmented\nGeneration (RAG) by > 60% for long documents, by no longer requiring separate\nretrieval and generation models. Models, code, etc. are freely available at\nhttps://github.com/ContextualAI/gritlm.\n","authors":["Niklas Muennighoff","Hongjin Su","Liang Wang","Nan Yang","Furu Wei","Tao Yu","Amanpreet Singh","Douwe Kiela"],"pdf_url":"https://arxiv.org/pdf/2402.09906v3.pdf","comment":"67 pages (16 main), 25 figures, 34 tables"},{"id":"http://arxiv.org/abs/2405.16821v3","updated":"2025-03-03T04:25:41Z","published":"2024-05-27T04:40:56Z","title":"Perturbation-Restrained Sequential Model Editing","summary":"  Model editing is an emerging field that focuses on updating the knowledge\nembedded within large language models (LLMs) without extensive retraining.\nHowever, current model editing methods significantly compromise the general\nabilities of LLMs as the number of edits increases, and this trade-off poses a\nsubstantial challenge to the continual learning of LLMs. In this paper, we\nfirst theoretically analyze that the factor affecting the general abilities in\nsequential model editing lies in the condition number of the edited matrix. The\ncondition number of a matrix represents its numerical sensitivity, and\ntherefore can be used to indicate the extent to which the original knowledge\nassociations stored in LLMs are perturbed after editing. Subsequently,\nstatistical findings demonstrate that the value of this factor becomes larger\nas the number of edits increases, thereby exacerbating the deterioration of\ngeneral abilities. To this end, a framework termed Perturbation Restraint on\nUpper bouNd for Editing (PRUNE) is proposed, which applies the condition number\nrestraints in sequential editing. These restraints can lower the upper bound on\nperturbation to edited models, thus preserving the general abilities.\nSystematically, we conduct experiments employing three editing methods on three\nLLMs across four downstream tasks. The results show that PRUNE can preserve\ngeneral abilities while maintaining the editing performance effectively in\nsequential model editing. The code are available at\nhttps://github.com/mjy1111/PRUNE.\n","authors":["Jun-Yu Ma","Hong Wang","Hao-Xiang Xu","Zhen-Hua Ling","Jia-Chen Gu"],"pdf_url":"https://arxiv.org/pdf/2405.16821v3.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2502.12110v2","updated":"2025-03-03T04:14:02Z","published":"2025-02-17T18:36:14Z","title":"A-MEM: Agentic Memory for LLM Agents","summary":"  While large language model (LLM) agents can effectively use external tools\nfor complex real-world tasks, they require memory systems to leverage\nhistorical experiences. Current memory systems enable basic storage and\nretrieval but lack sophisticated memory organization, despite recent attempts\nto incorporate graph databases. Moreover, these systems' fixed operations and\nstructures limit their adaptability across diverse tasks. To address this\nlimitation, this paper proposes a novel agentic memory system for LLM agents\nthat can dynamically organize memories in an agentic way. Following the basic\nprinciples of the Zettelkasten method, we designed our memory system to create\ninterconnected knowledge networks through dynamic indexing and linking. When a\nnew memory is added, we generate a comprehensive note containing multiple\nstructured attributes, including contextual descriptions, keywords, and tags.\nThe system then analyzes historical memories to identify relevant connections,\nestablishing links where meaningful similarities exist. Additionally, this\nprocess enables memory evolution - as new memories are integrated, they can\ntrigger updates to the contextual representations and attributes of existing\nhistorical memories, allowing the memory network to continuously refine its\nunderstanding. Our approach combines the structured organization principles of\nZettelkasten with the flexibility of agent-driven decision making, allowing for\nmore adaptive and context-aware memory management. Empirical experiments on six\nfoundation models show superior improvement against existing SOTA baselines.\nThe source code is available at https://github.com/WujiangXu/AgenticMemory.\n","authors":["Wujiang Xu","Zujie Liang","Kai Mei","Hang Gao","Juntao Tan","Yongfeng Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.12110v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14189v2","updated":"2025-03-03T04:11:31Z","published":"2025-02-20T01:46:12Z","title":"QUAD-LLM-MLTC: Large Language Models Ensemble Learning for Healthcare\n  Text Multi-Label Classification","summary":"  The escalating volume of collected healthcare textual data presents a unique\nchallenge for automated Multi-Label Text Classification (MLTC), which is\nprimarily due to the scarcity of annotated texts for training and their nuanced\nnature. Traditional machine learning models often fail to fully capture the\narray of expressed topics. However, Large Language Models (LLMs) have\ndemonstrated remarkable effectiveness across numerous Natural Language\nProcessing (NLP) tasks in various domains, which show impressive computational\nefficiency and suitability for unsupervised learning through prompt\nengineering. Consequently, these LLMs promise an effective MLTC of medical\nnarratives. However, when dealing with various labels, different prompts can be\nrelevant depending on the topic. To address these challenges, the proposed\napproach, QUAD-LLM-MLTC, leverages the strengths of four LLMs: GPT-4o, BERT,\nPEGASUS, and BART. QUAD-LLM-MLTC operates in a sequential pipeline in which\nBERT extracts key tokens, PEGASUS augments textual data, GPT-4o classifies, and\nBART provides topics' assignment probabilities, which results in four\nclassifications, all in a 0-shot setting. The outputs are then combined using\nensemble learning and processed through a meta-classifier to produce the final\nMLTC result. The approach is evaluated using three samples of annotated texts,\nwhich contrast it with traditional and single-model methods. The results show\nsignificant improvements across the majority of the topics in the\nclassification's F1 score and consistency (F1 and Micro-F1 scores of 78.17% and\n80.16% with standard deviations of 0.025 and 0.011, respectively). This\nresearch advances MLTC using LLMs and provides an efficient and scalable\nsolution to rapidly categorize healthcare-related text data without further\ntraining.\n","authors":["Hajar Sakai","Sarah S. Lam"],"pdf_url":"https://arxiv.org/pdf/2502.14189v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00617v4","updated":"2025-03-03T03:41:11Z","published":"2024-06-30T08:00:34Z","title":"Iterative Nash Policy Optimization: Aligning LLMs with General\n  Preferences via No-Regret Learning","summary":"  Reinforcement Learning with Human Feedback (RLHF) has achieved great success\nin aligning large language models (LLMs) with human preferences. Prevalent RLHF\napproaches are reward-based, following the Bradley-Terry (BT) model assumption,\nwhich may not fully capture the complexity of human preferences. In this paper,\nwe explore RLHF under a general preference framework and approach it from a\ngame-theoretic perspective. Specifically, we formulate the problem as a\ntwo-player game and propose a novel online algorithm, iterative Nash policy\noptimization (INPO). The key idea is to let the policy play against itself via\nno-regret learning, thereby approximating the Nash policy. Unlike previous\nmethods, INPO bypasses the need for estimating the expected win rate for\nindividual responses, which typically incurs high computational or annotation\ncosts. Instead, we introduce a new loss objective that is directly minimized\nover a preference dataset. We provide theoretical analysis for our approach and\ndemonstrate its effectiveness through experiments on various representative\nbenchmarks. With an LLaMA-3-8B-based SFT model, INPO achieves a 42.6%\nlength-controlled win rate on AlpacaEval 2.0 and a 37.8% win rate on\nArena-Hard, showing substantial improvement over the state-of-the-art online\nRLHF algorithms.\n","authors":["Yuheng Zhang","Dian Yu","Baolin Peng","Linfeng Song","Ye Tian","Mingyue Huo","Nan Jiang","Haitao Mi","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2407.00617v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14093v3","updated":"2025-03-03T03:19:31Z","published":"2024-05-23T01:43:54Z","title":"A Survey on Vision-Language-Action Models for Embodied AI","summary":"  Embodied AI is widely recognized as a key element of artificial general\nintelligence because it involves controlling embodied agents to perform tasks\nin the physical world. Building on the success of large language models and\nvision-language models, a new category of multimodal models -- referred to as\nvision-language-action models (VLAs) -- has emerged to address\nlanguage-conditioned robotic tasks in embodied AI by leveraging their distinct\nability to generate actions. In recent years, a myriad of VLAs have been\ndeveloped, making it imperative to capture the rapidly evolving landscape\nthrough a comprehensive survey. To this end, we present the first survey on\nVLAs for embodied AI. This work provides a detailed taxonomy of VLAs, organized\ninto three major lines of research. The first line focuses on individual\ncomponents of VLAs. The second line is dedicated to developing control policies\nadept at predicting low-level actions. The third line comprises high-level task\nplanners capable of decomposing long-horizon tasks into a sequence of subtasks,\nthereby guiding VLAs to follow more general user instructions. Furthermore, we\nprovide an extensive summary of relevant resources, including datasets,\nsimulators, and benchmarks. Finally, we discuss the challenges faced by VLAs\nand outline promising future directions in embodied AI.\n","authors":["Yueen Ma","Zixing Song","Yuzheng Zhuang","Jianye Hao","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2405.14093v3.pdf","comment":"16 pages, a survey of vision-language-action models"},{"id":"http://arxiv.org/abs/2412.17242v3","updated":"2025-03-03T03:08:43Z","published":"2024-12-23T03:30:34Z","title":"On the Generalization and Adaptation Ability of Machine-Generated Text\n  Detectors in Academic Writing","summary":"  The rising popularity of large language models (LLMs) has raised concerns\nabout machine-generated text (MGT), particularly in academic settings, where\nissues like plagiarism and misinformation are prevalent. As a result,\ndeveloping a highly generalizable and adaptable MGT detection system has become\nan urgent priority. Given that LLMs are most commonly misused in academic\nwriting, this work investigates the generalization and adaptation capabilities\nof MGT detectors in three key aspects specific to academic writing: First, we\nconstruct MGT-Acedemic, a large-scale dataset comprising over 336M tokens and\n749K samples. MGT-Acedemic focuses on academic writing, featuring human-written\ntexts (HWTs) and MGTs across STEM, Humanities, and Social Sciences, paired with\nan extensible code framework for efficient benchmarking. Second, we benchmark\nthe performance of various detectors for binary classification and attribution\ntasks in both in-domain and cross-domain settings. This benchmark reveals the\noften-overlooked challenges of attribution tasks. Third, we introduce a novel\nattribution task where models have to adapt to new classes over time without\n(or with very limited) access to prior training data in both few-shot and\nmany-shot scenarios. We implement eight different adapting techniques to\nimprove the performance and highlight the inherent complexity of the task. Our\nfindings provide insights into the generalization and adaptation ability of MGT\ndetectors across diverse scenarios and lay the foundation for building robust,\nadaptive detection systems. The code framework is available at\nhttps://github.com/Y-L-LIU/MGTBench-2.0.\n","authors":["Yule Liu","Zhiyuan Zhong","Yifan Liao","Zhen Sun","Jingyi Zheng","Jiaheng Wei","Qingyuan Gong","Fenghua Tong","Yang Chen","Yang Zhang","Xinlei He"],"pdf_url":"https://arxiv.org/pdf/2412.17242v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13085v2","updated":"2025-03-03T03:08:28Z","published":"2024-10-16T23:03:27Z","title":"MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language\n  Models","summary":"  Artificial Intelligence (AI) has demonstrated significant potential in\nhealthcare, particularly in disease diagnosis and treatment planning. Recent\nprogress in Medical Large Vision-Language Models (Med-LVLMs) has opened up new\npossibilities for interactive diagnostic tools. However, these models often\nsuffer from factual hallucination, which can lead to incorrect diagnoses.\nFine-tuning and retrieval-augmented generation (RAG) have emerged as methods to\naddress these issues. However, the amount of high-quality data and distribution\nshifts between training data and deployment data limit the application of\nfine-tuning methods. Although RAG is lightweight and effective, existing\nRAG-based approaches are not sufficiently general to different medical domains\nand can potentially cause misalignment issues, both between modalities and\nbetween the model and the ground truth. In this paper, we propose a versatile\nmultimodal RAG system, MMed-RAG, designed to enhance the factuality of\nMed-LVLMs. Our approach introduces a domain-aware retrieval mechanism, an\nadaptive retrieved contexts selection method, and a provable RAG-based\npreference fine-tuning strategy. These innovations make the RAG process\nsufficiently general and reliable, significantly improving alignment when\nintroducing retrieved contexts. Experimental results across five medical\ndatasets (involving radiology, ophthalmology, pathology) on medical VQA and\nreport generation demonstrate that MMed-RAG can achieve an average improvement\nof 43.8% in the factual accuracy of Med-LVLMs. Our data and code are available\nin https://github.com/richard-peng-xia/MMed-RAG.\n","authors":["Peng Xia","Kangyu Zhu","Haoran Li","Tianze Wang","Weijia Shi","Sheng Wang","Linjun Zhang","James Zou","Huaxiu Yao"],"pdf_url":"https://arxiv.org/pdf/2410.13085v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2406.06600v3","updated":"2025-03-03T03:05:30Z","published":"2024-06-06T13:44:57Z","title":"HORAE: A Domain-Agnostic Modeling Language for Automating Multimodal\n  Service Regulation","summary":"  Artificial intelligence is rapidly encroaching on the field of service\nregulation. This work-in-progress article presents the design principles behind\nHORAE, a unified specification language to model multimodal regulation rules\nacross a diverse set of domains. We show how HORAE facilitates an intelligent\nservice regulation pipeline by further exploiting a fine-tuned large language\nmodel named HORAE that automates the HORAE modeling process, thereby yielding\nan end-to-end framework for fully automated intelligent service regulation.\n","authors":["Yutao Sun","Mingshuai Chen","Kangjia Zhao","Jintao Chen"],"pdf_url":"https://arxiv.org/pdf/2406.06600v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07404v2","updated":"2025-03-03T03:02:55Z","published":"2024-11-11T22:22:21Z","title":"Controllable Context Sensitivity and the Knob Behind It","summary":"  When making predictions, a language model must trade off how much it relies\non its context vs. its prior knowledge. Choosing how sensitive the model is to\nits context is a fundamental functionality, as it enables the model to excel at\ntasks like retrieval-augmented generation and question-answering. In this\npaper, we search for a knob which controls this sensitivity, determining\nwhether language models answer from the context or their prior knowledge. To\nguide this search, we design a task for controllable context sensitivity. In\nthis task, we first feed the model a context (Paris is in England) and a\nquestion (Where is Paris?); we then instruct the model to either use its prior\nor contextual knowledge and evaluate whether it generates the correct answer\nfor both intents (either France or England). When fine-tuned on this task,\ninstruction-tuned versions of Llama-3.1, Mistral-v0.3, and Gemma-2 can solve it\nwith high accuracy (85-95%). Analyzing these high-performing models, we narrow\ndown which layers may be important to context sensitivity using a novel linear\ntime algorithm. Then, in each model, we identify a 1-D subspace in a single\nlayer that encodes whether the model follows context or prior knowledge.\nInterestingly, while we identify this subspace in a fine-tuned model, we find\nthat the exact same subspace serves as an effective knob in not only that model\nbut also non-fine-tuned instruct and base models of that model family. Finally,\nwe show a strong correlation between a model's performance and how distinctly\nit separates context-agreeing from context-ignoring answers in this subspace.\nThese results suggest a single subspace facilitates how the model chooses\nbetween context and prior knowledge, hinting at a simple fundamental mechanism\nthat controls this behavior.\n","authors":["Julian Minder","Kevin Du","Niklas Stoehr","Giovanni Monea","Chris Wendler","Robert West","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2411.07404v2.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.20854v2","updated":"2025-03-03T03:00:59Z","published":"2025-02-28T08:53:08Z","title":"A Pilot Empirical Study on When and How to Use Knowledge Graphs as\n  Retrieval Augmented Generation","summary":"  The integration of Knowledge Graphs (KGs) into the Retrieval Augmented\nGeneration (RAG) framework has attracted significant interest, with early\nstudies showing promise in mitigating hallucinations and improving model\naccuracy. However, a systematic understanding and comparative analysis of the\nrapidly emerging KG-RAG methods are still lacking. This paper seeks to lay the\nfoundation for systematically answering the question of when and how to use\nKG-RAG by analyzing their performance in various application scenarios\nassociated with different technical configurations. After outlining the mind\nmap using KG-RAG framework and summarizing its popular pipeline, we conduct a\npilot empirical study of KG-RAG works to reimplement and evaluate 6 KG-RAG\nmethods across 7 datasets in diverse scenarios, analyzing the impact of 9\nKG-RAG configurations in combination with 17 LLMs. Our results underscore the\ncritical role of appropriate application conditions and optimal configurations\nof KG-RAG components.\n","authors":["Xujie Yuan","Yongxu Liu","Shimin Di","Shiwen Wu","Libin Zheng","Rui Meng","Lei Chen","Xiaofang Zhou","Jian Yin"],"pdf_url":"https://arxiv.org/pdf/2502.20854v2.pdf","comment":"8 pages, 2 figures, 14 tables"},{"id":"http://arxiv.org/abs/2410.08109v3","updated":"2025-03-03T02:45:58Z","published":"2024-10-10T16:56:05Z","title":"A Closer Look at Machine Unlearning for Large Language Models","summary":"  Large language models (LLMs) may memorize sensitive or copyrighted content,\nraising privacy and legal concerns. Due to the high cost of retraining from\nscratch, researchers attempt to employ machine unlearning to remove specific\ncontent from LLMs while preserving the overall performance. In this paper, we\ndiscuss several issues in machine unlearning for LLMs and provide our insights\non possible approaches. To address the issue of inadequate evaluation of model\noutputs after unlearning, we introduce three additional metrics to evaluate\ntoken diversity, sentence semantics, and factual correctness. We then\ncategorize unlearning methods into untargeted and targeted, and discuss their\nissues respectively. Specifically, the behavior that untargeted unlearning\nattempts to approximate is unpredictable and may involve hallucinations, and\nexisting regularization is insufficient for targeted unlearning. To alleviate\nthese issues, we propose using the objective of maximizing entropy (ME) for\nuntargeted unlearning and incorporate answer preservation (AP) loss as\nregularization for targeted unlearning. Experimental results across three\nscenarios, i.e., fictitious unlearning, continual unlearning, and real-world\nunlearning, demonstrate the effectiveness of our approaches. The code is\navailable at https://github.com/sail-sg/closer-look-LLM-unlearning.\n","authors":["Xiaojian Yuan","Tianyu Pang","Chao Du","Kejiang Chen","Weiming Zhang","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2410.08109v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2409.19788v2","updated":"2025-03-03T02:38:52Z","published":"2024-09-29T21:20:57Z","title":"Exploring Adversarial Robustness in Classification tasks using DNA\n  Language Models","summary":"  DNA Language Models, such as GROVER, DNABERT2 and the Nucleotide Transformer,\noperate on DNA sequences that inherently contain sequencing errors, mutations,\nand laboratory-induced noise, which may significantly impact model performance.\nDespite the importance of this issue, the robustness of DNA language models\nremains largely underexplored. In this paper, we comprehensivly investigate\ntheir robustness in DNA classification by applying various adversarial attack\nstrategies: the character (nucleotide substitutions), word (codon\nmodifications), and sentence levels (back-translation-based transformations) to\nsystematically analyze model vulnerabilities. Our results demonstrate that DNA\nlanguage models are highly susceptible to adversarial attacks, leading to\nsignificant performance degradation. Furthermore, we explore adversarial\ntraining method as a defense mechanism, which enhances both robustness and\nclassification accuracy. This study highlights the limitations of DNA language\nmodels and underscores the necessity of robustness in bioinformatics.\n","authors":["Hyunwoo Yoo","Haebin Shin","Kaidi Xu","Gail Rosen"],"pdf_url":"https://arxiv.org/pdf/2409.19788v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12952v2","updated":"2025-03-03T02:27:02Z","published":"2024-10-16T18:40:26Z","title":"Facilitating Multi-turn Function Calling for LLMs via Compositional\n  Instruction Tuning","summary":"  Large Language Models (LLMs) have exhibited significant potential in\nperforming diverse tasks, including the ability to call functions or use\nexternal tools to enhance their performance. While current research on function\ncalling by LLMs primarily focuses on single-turn interactions, this paper\naddresses the overlooked necessity for LLMs to engage in multi-turn function\ncalling--critical for handling compositional, real-world queries that require\nplanning with functions but not only use functions. To facilitate this, we\nintroduce an approach, BUTTON, which generates synthetic compositional\ninstruction tuning data via bottom-up instruction construction and top-down\ntrajectory generation. In the bottom-up phase, we generate simple atomic tasks\nbased on real-world scenarios and build compositional tasks using heuristic\nstrategies based on atomic tasks. Corresponding function definitions are then\nsynthesized for these compositional tasks. The top-down phase features a\nmulti-agent environment where interactions among simulated humans, assistants,\nand tools are utilized to gather multi-turn function calling trajectories. This\napproach ensures task compositionality and allows for effective function and\ntrajectory generation by examining atomic tasks within compositional tasks. We\nproduce a dataset BUTTONInstruct comprising 8k data points and demonstrate its\neffectiveness through extensive experiments across various LLMs.\n","authors":["Mingyang Chen","Haoze Sun","Tianpeng Li","Fan Yang","Hao Liang","Keer Lu","Bin Cui","Wentao Zhang","Zenan Zhou","Weipeng Chen"],"pdf_url":"https://arxiv.org/pdf/2410.12952v2.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2501.13983v3","updated":"2025-03-03T02:06:47Z","published":"2025-01-23T06:57:24Z","title":"AdEval: Alignment-based Dynamic Evaluation to Mitigate Data\n  Contamination in Large Language Models","summary":"  As Large Language Models (LLMs) are pretrained on massive-scale corpora, the\nissue of data contamination has become increasingly severe, leading to\npotential overestimation of model performance during evaluation. To address\nthis, we propose AdEval (Alignment-based Dynamic Evaluation), a dynamic data\nevaluation method aimed at mitigating the impact of data contamination on\nevaluation reliability. Experimental results on multiple datasets demonstrate\nthat AdEval effectively reduces the impact of data contamination on evaluation\noutcomes, enhancing both the fairness and reliability of the evaluation\nprocess.\n","authors":["Yang Fan"],"pdf_url":"https://arxiv.org/pdf/2501.13983v3.pdf","comment":"There are serious academic problems in this paper, such as data\n  falsification and plagiarism in the method of the paper"},{"id":"http://arxiv.org/abs/2409.02060v2","updated":"2025-03-03T01:25:46Z","published":"2024-09-03T17:08:20Z","title":"OLMoE: Open Mixture-of-Experts Language Models","summary":"  We introduce OLMoE, a fully open, state-of-the-art language model leveraging\nsparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but\nuses only 1B per input token. We pretrain it on 5 trillion tokens and further\nadapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available\nmodels with similar active parameters, even surpassing larger ones like\nLlama2-13B-Chat and DeepSeekMoE-16B. We present various experiments on MoE\ntraining, analyze routing in our model showing high specialization, and\nopen-source all aspects of our work: model weights, training data, code, and\nlogs.\n","authors":["Niklas Muennighoff","Luca Soldaini","Dirk Groeneveld","Kyle Lo","Jacob Morrison","Sewon Min","Weijia Shi","Pete Walsh","Oyvind Tafjord","Nathan Lambert","Yuling Gu","Shane Arora","Akshita Bhagia","Dustin Schwenk","David Wadden","Alexander Wettig","Binyuan Hui","Tim Dettmers","Douwe Kiela","Ali Farhadi","Noah A. Smith","Pang Wei Koh","Amanpreet Singh","Hannaneh Hajishirzi"],"pdf_url":"https://arxiv.org/pdf/2409.02060v2.pdf","comment":"63 pages (24 main), 36 figures, 17 tables"},{"id":"http://arxiv.org/abs/2410.01417v2","updated":"2025-03-03T00:41:36Z","published":"2024-10-02T10:58:54Z","title":"The Labyrinth of Links: Navigating the Associative Maze of Multi-modal\n  LLMs","summary":"  Multi-modal Large Language Models (MLLMs) have exhibited impressive\ncapability. However, recently many deficiencies of MLLMs have been found\ncompared to human intelligence, $\\textit{e.g.}$, hallucination. To drive the\nMLLMs study, the community dedicated efforts to building larger benchmarks with\ncomplex tasks. In this paper, we propose benchmarking an essential but usually\noverlooked intelligence: $\\textbf{association}$, a human's basic capability to\nlink observation and prior practice memory. To comprehensively investigate\nMLLM's performance on the association, we formulate the association task and\ndevise a standard benchmark based on adjective and verb semantic concepts.\nInstead of costly data annotation and curation, we propose a convenient\n$\\textbf{annotation-free}$ construction method transforming the general dataset\nfor our association tasks. Simultaneously, we devise a rigorous data refinement\nprocess to eliminate confusion in the raw dataset. Building on this database,\nwe establish three levels of association tasks: single-step, synchronous, and\nasynchronous associations. Moreover, we conduct a comprehensive investigation\ninto the MLLMs' zero-shot association capabilities, addressing multiple\ndimensions, including three distinct memory strategies, both open-source and\nclosed-source MLLMs, cutting-edge Mixture-of-Experts (MoE) models, and the\ninvolvement of human experts. Our systematic investigation shows that current\nopen-source MLLMs consistently exhibit poor capability in our association\ntasks, even the currently state-of-the-art GPT-4V(vision) also has a\nsignificant gap compared to humans. We believe our benchmark would pave the way\nfor future MLLM studies. $\\textit{Our data and code are available at:}$\nhttps://mvig-rhos.com/llm_inception.\n","authors":["Hong Li","Nanxi Li","Yuanjie Chen","Jianbin Zhu","Qinlu Guo","Cewu Lu","Yong-Lu Li"],"pdf_url":"https://arxiv.org/pdf/2410.01417v2.pdf","comment":"Accepted by ICLR 2025. Project page:\n  https://mvig-rhos.com/llm_inception"},{"id":"http://arxiv.org/abs/2405.02318v2","updated":"2025-03-03T00:38:48Z","published":"2024-04-18T00:20:48Z","title":"NL2FOL: Translating Natural Language to First-Order Logic for Logical\n  Fallacy Detection","summary":"  Translating natural language into formal language such as First-Order Logic\n(FOL) is a foundational challenge in NLP with wide-ranging applications in\nautomated reasoning, misinformation tracking, and knowledge validation. In this\npaper, we introduce Natural Language to First-Order Logic (NL2FOL), a framework\nto autoformalize natural language to FOL step by step using Large Language\nModels (LLMs). Our approach addresses key challenges in this translation\nprocess, including the integration of implicit background knowledge. By\nleveraging structured representations generated by NL2FOL, we use\nSatisfiability Modulo Theory (SMT) solvers to reason about the logical validity\nof natural language statements. We present logical fallacy detection as a case\nstudy to evaluate the efficacy of NL2FOL. Being neurosymbolic, our approach\nalso provides interpretable insights into the reasoning process and\ndemonstrates robustness without requiring model fine-tuning or labeled training\ndata. Our framework achieves strong performance on multiple datasets. On the\nLOGIC dataset, NL2FOL achieves an F1-score of 78%, while generalizing\neffectively to the LOGICCLIMATE dataset with an F1-score of 80%.\n","authors":["Abhinav Lalwani","Tasha Kim","Lovish Chopra","Christopher Hahn","Zhijing Jin","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2405.02318v2.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2411.18135v2","updated":"2025-03-03T16:00:39Z","published":"2024-11-27T08:33:42Z","title":"ModeDreamer: Mode Guiding Score Distillation for Text-to-3D Generation\n  using Reference Image Prompts","summary":"  Existing Score Distillation Sampling (SDS)-based methods have driven\nsignificant progress in text-to-3D generation. However, 3D models produced by\nSDS-based methods tend to exhibit over-smoothing and low-quality outputs. These\nissues arise from the mode-seeking behavior of current methods, where the\nscores used to update the model oscillate between multiple modes, resulting in\nunstable optimization and diminished output quality. To address this problem,\nwe introduce a novel image prompt score distillation loss named ISD, which\nemploys a reference image to direct text-to-3D optimization toward a specific\nmode. Our ISD loss can be implemented by using IP-Adapter, a lightweight\nadapter for integrating image prompt capability to a text-to-image diffusion\nmodel, as a mode-selection module. A variant of this adapter, when not being\nprompted by a reference image, can serve as an efficient control variate to\nreduce variance in score estimates, thereby enhancing both output quality and\noptimization stability. Our experiments demonstrate that the ISD loss\nconsistently achieves visually coherent, high-quality outputs and improves\noptimization speed compared to prior text-to-3D methods, as demonstrated\nthrough both qualitative and quantitative evaluations on the T3Bench benchmark\nsuite.\n","authors":["Uy Dieu Tran","Minh Luu","Phong Ha Nguyen","Khoi Nguyen","Binh-Son Hua"],"pdf_url":"https://arxiv.org/pdf/2411.18135v2.pdf","comment":"Project page: https://modedreamer.github.io/"},{"id":"http://arxiv.org/abs/2409.18459v2","updated":"2025-03-03T15:04:18Z","published":"2024-09-27T05:43:22Z","title":"FoodMLLM-JP: Leveraging Multimodal Large Language Models for Japanese\n  Recipe Generation","summary":"  Research on food image understanding using recipe data has been a\nlong-standing focus due to the diversity and complexity of the data. Moreover,\nfood is inextricably linked to people's lives, making it a vital research area\nfor practical applications such as dietary management. Recent advancements in\nMultimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities, not only in their vast knowledge but also in their ability to\nhandle languages naturally. While English is predominantly used, they can also\nsupport multiple languages including Japanese. This suggests that MLLMs are\nexpected to significantly improve performance in food image understanding\ntasks. We fine-tuned open MLLMs LLaVA-1.5 and Phi-3 Vision on a Japanese recipe\ndataset and benchmarked their performance against the closed model GPT-4o. We\nthen evaluated the content of generated recipes, including ingredients and\ncooking procedures, using 5,000 evaluation samples that comprehensively cover\nJapanese food culture. Our evaluation demonstrates that the open models trained\non recipe data outperform GPT-4o, the current state-of-the-art model, in\ningredient generation. Our model achieved F1 score of 0.531, surpassing\nGPT-4o's F1 score of 0.481, indicating a higher level of accuracy. Furthermore,\nour model exhibited comparable performance to GPT-4o in generating cooking\nprocedure text.\n","authors":["Yuki Imajuku","Yoko Yamakata","Kiyoharu Aizawa"],"pdf_url":"https://arxiv.org/pdf/2409.18459v2.pdf","comment":"15 pages, 5 figures. We found errors in the calculation of evaluation\n  metrics, which were corrected in this version with\n  $\\color{blue}{\\text{modifications highlighted in blue}}$. Please also see the\n  Appendix"},{"id":"http://arxiv.org/abs/2408.11561v2","updated":"2025-03-03T15:04:03Z","published":"2024-08-21T12:15:20Z","title":"Self-Supervised Iterative Refinement for Anomaly Detection in Industrial\n  Quality Control","summary":"  This study introduces the Iterative Refinement Process (IRP), a robust\nanomaly detection methodology designed for high-stakes industrial quality\ncontrol. The IRP enhances defect detection accuracy through a cyclic data\nrefinement strategy, iteratively removing misleading data points to improve\nmodel performance and robustness. We validate the IRP's effectiveness using two\nbenchmark datasets, Kolektor SDD2 (KSDD2) and MVTec AD, covering a wide range\nof industrial products and defect types. Our experimental results demonstrate\nthat the IRP consistently outperforms traditional anomaly detection models,\nparticularly in environments with high noise levels. This study highlights the\nIRP's potential to significantly enhance anomaly detection processes in\nindustrial settings, effectively managing the challenges of sparse and noisy\ndata.\n","authors":["Muhammad Aqeel","Shakiba Sharifi","Marco Cristani","Francesco Setti"],"pdf_url":"https://arxiv.org/pdf/2408.11561v2.pdf","comment":"Accepted to VISAPP 2025"},{"id":"http://arxiv.org/abs/2409.15259v2","updated":"2025-03-03T15:01:03Z","published":"2024-09-23T17:56:03Z","title":"StarVid: Enhancing Semantic Alignment in Video Diffusion Models via\n  Spatial and SynTactic Guided Attention Refocusing","summary":"  Recent advances in text-to-video (T2V) generation with diffusion models have\ngarnered significant attention. However, they typically perform well in scenes\nwith a single object and motion, struggling in compositional scenarios with\nmultiple objects and distinct motions to accurately reflect the semantic\ncontent of text prompts. To address these challenges, we propose\n\\textbf{StarVid}, a plug-and-play, training-free method that improves semantic\nalignment between multiple subjects, their motions, and text prompts in T2V\nmodels. StarVid first leverages the spatial reasoning capabilities of large\nlanguage models (LLMs) for two-stage motion trajectory planning based on text\nprompts. Such trajectories serve as spatial priors, guiding a spatial-aware\nloss to refocus cross-attention (CA) maps into distinctive regions.\nFurthermore, we propose a syntax-guided contrastive constraint to strengthen\nthe correlation between the CA maps of verbs and their corresponding nouns,\nenhancing motion-subject binding. Both qualitative and quantitative evaluations\ndemonstrate that the proposed framework significantly outperforms baseline\nmethods, delivering videos of higher quality with improved semantic\nconsistency.\n","authors":["Yuanhang Li","Qi Mao","Lan Chen","Zhen Fang","Lei Tian","Xinyan Xiao","Libiao Jin","Hua Wu"],"pdf_url":"https://arxiv.org/pdf/2409.15259v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09695v2","updated":"2025-03-03T14:48:45Z","published":"2025-01-16T17:48:03Z","title":"Mitigating Hallucinations in Large Vision-Language Models via DPO:\n  On-Policy Data Hold the Key","summary":"  Hallucination remains a major challenge for Large Vision-Language Models\n(LVLMs). Direct Preference Optimization (DPO) has gained increasing attention\nas a simple solution to hallucination issues. It directly learns from\nconstructed preference pairs that reflect the severity of hallucinations in\nresponses to the same prompt and image. Nonetheless, different data\nconstruction methods in existing works bring notable performance variations. We\nidentify a crucial factor here: outcomes are largely contingent on whether the\nconstructed data aligns on-policy w.r.t the initial (reference) policy of DPO.\nTheoretical analysis suggests that learning from off-policy data is impeded by\nthe presence of KL-divergence between the updated policy and the reference\npolicy. From the perspective of dataset distribution, we systematically\nsummarize the inherent flaws in existing algorithms that employ DPO to address\nhallucination issues. To alleviate the problems, we propose On-Policy Alignment\n(OPA)-DPO framework, which uniquely leverages expert feedback to correct\nhallucinated responses and aligns both the original and expert-revised\nresponses in an on-policy manner. Notably, with only 4.8k data, OPA-DPO\nachieves an additional reduction in the hallucination rate of LLaVA-1.5-7B:\n13.26% on the AMBER benchmark and 5.39% on the Object-Hal benchmark, compared\nto the previous SOTA algorithm trained with 16k samples. Our implementation is\navailable at https://github.com/zhyang2226/OPA-DPO.\n","authors":["Zhihe Yang","Xufang Luo","Dongqi Han","Yunjian Xu","Dongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2501.09695v2.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2409.10071v4","updated":"2025-03-03T14:47:34Z","published":"2024-09-16T08:21:22Z","title":"Towards Physically Realizable Adversarial Attacks in Embodied Vision\n  Navigation","summary":"  The significant advancements in embodied vision navigation have raised\nconcerns about its susceptibility to adversarial attacks exploiting deep neural\nnetworks. Investigating the adversarial robustness of embodied vision\nnavigation is crucial, especially given the threat of 3D physical attacks that\ncould pose risks to human safety. However, existing attack methods for embodied\nvision navigation often lack physical feasibility due to challenges in\ntransferring digital perturbations into the physical world. Moreover, current\nphysical attacks for object detection struggle to achieve both multi-view\neffectiveness and visual naturalness in navigation scenarios. To address this,\nwe propose a practical attack method for embodied navigation by attaching\nadversarial patches to objects, where both opacity and textures are learnable.\nSpecifically, to ensure effectiveness across varying viewpoints, we employ a\nmulti-view optimization strategy based on object-aware sampling, which\noptimizes the patch's texture based on feedback from the vision-based\nperception model used in navigation. To make the patch inconspicuous to human\nobservers, we introduce a two-stage opacity optimization mechanism, in which\nopacity is fine-tuned after texture optimization. Experimental results\ndemonstrate that our adversarial patches decrease the navigation success rate\nby an average of 22.39%, outperforming previous methods in practicality,\neffectiveness, and naturalness. Code is available at:\nhttps://github.com/chen37058/Physical-Attacks-in-Embodied-Nav\n","authors":["Meng Chen","Jiawei Tu","Chao Qi","Yonghao Dang","Feng Zhou","Wei Wei","Jianqin Yin"],"pdf_url":"https://arxiv.org/pdf/2409.10071v4.pdf","comment":"7 pages, 7 figures, submitted to IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS) 2025"},{"id":"http://arxiv.org/abs/2412.07487v2","updated":"2025-03-03T14:04:23Z","published":"2024-12-10T13:12:32Z","title":"Stereo Hand-Object Reconstruction for Human-to-Robot Handover","summary":"  Jointly estimating hand and object shape facilitates the grasping task in\nhuman-to-robot handovers. However, relying on hand-crafted prior knowledge\nabout the geometric structure of the object fails when generalising to unseen\nobjects, and depth sensors fail to detect transparent objects such as drinking\nglasses. In this work, we propose a stereo-based method for hand-object\nreconstruction that combines single-view reconstructions probabilistically to\nform a coherent stereo reconstruction. We learn 3D shape priors from a large\nsynthetic hand-object dataset to ensure that our method is generalisable, and\nuse RGB inputs to better capture transparent objects. We show that our method\nreduces the object Chamfer distance compared to existing RGB based hand-object\nreconstruction methods on single view and stereo settings. We process the\nreconstructed hand-object shape with a projection-based outlier removal step\nand use the output to guide a human-to-robot handover pipeline with\nwide-baseline stereo RGB cameras. Our hand-object reconstruction enables a\nrobot to successfully receive a diverse range of household objects from the\nhuman.\n","authors":["Yik Lung Pang","Alessio Xompero","Changjae Oh","Andrea Cavallaro"],"pdf_url":"https://arxiv.org/pdf/2412.07487v2.pdf","comment":"8 pages, 9 figures, 1 table"},{"id":"http://arxiv.org/abs/2412.02993v2","updated":"2025-03-03T13:59:01Z","published":"2024-12-04T03:19:43Z","title":"EchoONE: Segmenting Multiple echocardiography Planes in One Model","summary":"  In clinical practice of echocardiography examinations, multiple planes\ncontaining the heart structures of different view are usually required in\nscreening, diagnosis and treatment of cardiac disease. AI models for\nechocardiography have to be tailored for each specific plane due to the\ndramatic structure differences, thus resulting in repetition development and\nextra complexity. Effective solution for such a multi-plane segmentation (MPS)\nproblem is highly demanded for medical images, yet has not been well\ninvestigated. In this paper, we propose a novel solution, EchoONE, for this\nproblem with a SAM-based segmentation architecture, a prior-composable mask\nlearning (PC-Mask) module for semantic-aware dense prompt generation, and a\nlearnable CNN-branch with a simple yet effective local feature fusion and\nadaption (LFFA) module for SAM adapting. We extensively evaluated our method on\nmultiple internal and external echocardiography datasets, and achieved\nconsistently state-of-the-art performance for multi-source datasets with\ndifferent heart planes. This is the first time that the MPS problem is solved\nin one model for echocardiography data. The code will be available at\nhttps://github.com/a2502503/EchoONE.\n","authors":["Jiongtong Hu","Wei Zhuo","Jun Cheng","Yingying Liu","Wufeng Xue","Dong Ni"],"pdf_url":"https://arxiv.org/pdf/2412.02993v2.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2502.18858v2","updated":"2025-03-03T13:38:50Z","published":"2025-02-26T05:59:45Z","title":"Evaluating Intelligence via Trial and Error","summary":"  Intelligence is a crucial trait for species to find solutions within a\nlimited number of trial-and-error attempts. Building on this idea, we introduce\nSurvival Game as a framework to evaluate intelligence based on the number of\nfailed attempts in a trial-and-error process. Fewer failures indicate higher\nintelligence. When the expectation and variance of failure counts are both\nfinite, it signals the ability to consistently find solutions to new\nchallenges, which we define as the Autonomous Level of intelligence. Using\nSurvival Game, we comprehensively evaluate existing AI systems. Our results\nshow that while AI systems achieve the Autonomous Level in simple tasks, they\nare still far from it in more complex tasks, such as vision, search,\nrecommendation, and language. While scaling current AI technologies might help,\nthis would come at an astronomical cost. Projections suggest that achieving the\nAutonomous Level for general tasks would require $10^{26}$ parameters. To put\nthis into perspective, loading such a massive model requires so many H100 GPUs\nthat their total value is $10^{7}$ times that of Apple Inc.'s market value.\nEven with Moore's Law, supporting such a parameter scale would take $70$ years.\nThis staggering cost highlights the complexity of human tasks and the\ninadequacies of current AI technologies. To further investigate this\nphenomenon, we conduct a theoretical analysis of Survival Game and its\nexperimental results. Our findings suggest that human tasks possess a\ncriticality property. As a result, Autonomous Level requires a deep\nunderstanding of the task's underlying mechanisms. Current AI systems, however,\ndo not fully grasp these mechanisms and instead rely on superficial mimicry,\nmaking it difficult for them to reach an autonomous level. We believe Survival\nGame can not only guide the future development of AI but also offer profound\ninsights into human intelligence.\n","authors":["Jingtao Zhan","Jiahao Zhao","Jiayu Li","Yiqun Liu","Bo Zhang","Qingyao Ai","Jiaxin Mao","Hongning Wang","Min Zhang","Shaoping Ma"],"pdf_url":"https://arxiv.org/pdf/2502.18858v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.15517v2","updated":"2025-03-03T13:22:14Z","published":"2024-09-23T20:09:43Z","title":"MATCH POLICY: A Simple Pipeline from Point Cloud Registration to\n  Manipulation Policies","summary":"  Many manipulation tasks require the robot to rearrange objects relative to\none another. Such tasks can be described as a sequence of relative poses\nbetween parts of a set of rigid bodies. In this work, we propose MATCH POLICY,\na simple but novel pipeline for solving high-precision pick and place tasks.\nInstead of predicting actions directly, our method registers the pick and place\ntargets to the stored demonstrations. This transfers action inference into a\npoint cloud registration task and enables us to realize nontrivial manipulation\npolicies without any training. MATCH POLICY is designed to solve high-precision\ntasks with a key-frame setting. By leveraging the geometric interaction and the\nsymmetries of the task, it achieves extremely high sample efficiency and\ngeneralizability to unseen configurations. We demonstrate its state-of-the-art\nperformance across various tasks on RLBench benchmark compared with several\nstrong baselines and test it on a real robot with six tasks.\n","authors":["Haojie Huang","Haotian Liu","Dian Wang","Robin Walters","Robert Platt"],"pdf_url":"https://arxiv.org/pdf/2409.15517v2.pdf","comment":"project url: https://haojhuang.github.io/match_page/"},{"id":"http://arxiv.org/abs/2409.20171v3","updated":"2025-03-03T13:12:48Z","published":"2024-09-30T10:29:41Z","title":"Annotation-Free Curb Detection Leveraging Altitude Difference Image","summary":"  Road curbs are considered as one of the crucial and ubiquitous traffic\nfeatures, which are essential for ensuring the safety of autonomous vehicles.\nCurrent methods for detecting curbs primarily rely on camera imagery or LiDAR\npoint clouds. Image-based methods are vulnerable to fluctuations in lighting\nconditions and exhibit poor robustness, while methods based on point clouds\ncircumvent the issues associated with lighting variations. However, it is the\ntypical case that significant processing delays are encountered due to the\nvoluminous amount of 3D points contained in each frame of the point cloud data.\nFurthermore, the inherently unstructured characteristics of point clouds poses\nchallenges for integrating the latest deep learning advancements into point\ncloud data applications. To address these issues, this work proposes an\nannotation-free curb detection method leveraging Altitude Difference Image\n(ADI), which effectively mitigates the aforementioned challenges. Given that\nmethods based on deep learning generally demand extensive, manually annotated\ndatasets, which are both expensive and labor-intensive to create, we present an\nAutomatic Curb Annotator (ACA) module. This module utilizes a deterministic\ncurb detection algorithm to automatically generate a vast quantity of training\ndata. Consequently, it facilitates the training of the curb detection model\nwithout necessitating any manual annotation of data. Finally, by incorporating\na post-processing module, we manage to achieve state-of-the-art results on the\nKITTI 3D curb dataset with considerably reduced processing delays compared to\nexisting methods, which underscores the effectiveness of our approach in curb\ndetection tasks.\n","authors":["Fulong Ma","Peng Hou","Yuxuan Liu","Yang Liu","Ming Liu","Jun Ma"],"pdf_url":"https://arxiv.org/pdf/2409.20171v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.09555v3","updated":"2025-03-03T13:05:35Z","published":"2025-01-16T14:18:06Z","title":"Text-driven Adaptation of Foundation Models for Few-shot Surgical\n  Workflow Analysis","summary":"  Purpose: Surgical workflow analysis is crucial for improving surgical\nefficiency and safety. However, previous studies rely heavily on large-scale\nannotated datasets, posing challenges in cost, scalability, and reliance on\nexpert annotations. To address this, we propose Surg-FTDA (Few-shot Text-driven\nAdaptation), designed to handle various surgical workflow analysis tasks with\nminimal paired image-label data.\n  Methods: Our approach has two key components. First, Few-shot selection-based\nmodality alignment selects a small subset of images and aligns their embeddings\nwith text embeddings from the downstream task, bridging the modality gap.\nSecond, Text-driven adaptation leverages only text data to train a decoder,\neliminating the need for paired image-text data. This decoder is then applied\nto aligned image embeddings, enabling image-related tasks without explicit\nimage-text pairs.\n  Results: We evaluate our approach to generative tasks (image captioning) and\ndiscriminative tasks (triplet recognition and phase recognition). Results show\nthat Surg-FTDA outperforms baselines and generalizes well across downstream\ntasks.\n  Conclusion: We propose a text-driven adaptation approach that mitigates the\nmodality gap and handles multiple downstream tasks in surgical workflow\nanalysis, with minimal reliance on large annotated datasets. The code and\ndataset will be released in https://github.com/CAMMA-public/Surg-FTDA\n","authors":["Tingxuan Chen","Kun Yuan","Vinkle Srivastav","Nassir Navab","Nicolas Padoy"],"pdf_url":"https://arxiv.org/pdf/2501.09555v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11142v2","updated":"2025-03-03T12:56:35Z","published":"2025-02-16T14:17:36Z","title":"NavRAG: Generating User Demand Instructions for Embodied Navigation\n  through Retrieval-Augmented LLM","summary":"  Vision-and-Language Navigation (VLN) is an essential skill for embodied\nagents, allowing them to navigate in 3D environments following natural language\ninstructions. High-performance navigation models require a large amount of\ntraining data, the high cost of manually annotating data has seriously hindered\nthis field. Therefore, some previous methods translate trajectory videos into\nstep-by-step instructions for expanding data, but such instructions do not\nmatch well with users' communication styles that briefly describe destinations\nor state specific needs. Moreover, local navigation trajectories overlook\nglobal context and high-level task planning. To address these issues, we\npropose NavRAG, a retrieval-augmented generation (RAG) framework that generates\nuser demand instructions for VLN. NavRAG leverages LLM to build a hierarchical\nscene description tree for 3D scene understanding from global layout to local\ndetails, then simulates various user roles with specific demands to retrieve\nfrom the scene tree, generating diverse instructions with LLM. We annotate over\n2 million navigation instructions across 861 scenes and evaluate the data\nquality and navigation performance of trained models.\n","authors":["Zihan Wang","Yaohui Zhu","Gim Hee Lee","Yachun Fan"],"pdf_url":"https://arxiv.org/pdf/2502.11142v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14616v2","updated":"2025-03-03T12:37:18Z","published":"2025-02-20T14:57:01Z","title":"Monocular Depth Estimation and Segmentation for Transparent Object with\n  Iterative Semantic and Geometric Fusion","summary":"  Transparent object perception is indispensable for numerous robotic tasks.\nHowever, accurately segmenting and estimating the depth of transparent objects\nremain challenging due to complex optical properties. Existing methods\nprimarily delve into only one task using extra inputs or specialized sensors,\nneglecting the valuable interactions among tasks and the subsequent refinement\nprocess, leading to suboptimal and blurry predictions. To address these issues,\nwe propose a monocular framework, which is the first to excel in both\nsegmentation and depth estimation of transparent objects, with only a\nsingle-image input. Specifically, we devise a novel semantic and geometric\nfusion module, effectively integrating the multi-scale information between\ntasks. In addition, drawing inspiration from human perception of objects, we\nfurther incorporate an iterative strategy, which progressively refines initial\nfeatures for clearer results. Experiments on two challenging synthetic and\nreal-world datasets demonstrate that our model surpasses state-of-the-art\nmonocular, stereo, and multi-view methods by a large margin of about\n38.8%-46.2% with only a single RGB input. Codes and models are publicly\navailable at https://github.com/L-J-Yuan/MODEST.\n","authors":["Jiangyuan Liu","Hongxuan Ma","Yuxin Guo","Yuhao Zhao","Chi Zhang","Wei Sui","Wei Zou"],"pdf_url":"https://arxiv.org/pdf/2502.14616v2.pdf","comment":"Accepted by ICRA(2025). The code is accessible through:\n  https://github.com/L-J-Yuan/MODEST"},{"id":"http://arxiv.org/abs/2408.04591v2","updated":"2025-03-03T12:35:33Z","published":"2024-08-08T17:04:06Z","title":"HiLo: A Learning Framework for Generalized Category Discovery Robust to\n  Domain Shifts","summary":"  Generalized Category Discovery (GCD) is a challenging task in which, given a\npartially labelled dataset, models must categorize all unlabelled instances,\nregardless of whether they come from labelled categories or from new ones. In\nthis paper, we challenge a remaining assumption in this task: that all images\nshare the same domain. Specifically, we introduce a new task and method to\nhandle GCD when the unlabelled data also contains images from different domains\nto the labelled set. Our proposed `HiLo' networks extract High-level semantic\nand Low-level domain features, before minimizing the mutual information between\nthe representations. Our intuition is that the clusterings based on domain\ninformation and semantic information should be independent. We further extend\nour method with a specialized domain augmentation tailored for the GCD task, as\nwell as a curriculum learning approach. Finally, we construct a benchmark from\ncorrupted fine-grained datasets as well as a large-scale evaluation on\nDomainNet with real-world domain shifts, reimplementing a number of GCD\nbaselines in this setting. We demonstrate that HiLo outperforms SoTA category\ndiscovery models by a large margin on all evaluations.\n","authors":["Hongjun Wang","Sagar Vaze","Kai Han"],"pdf_url":"https://arxiv.org/pdf/2408.04591v2.pdf","comment":"v2: Accepted as a conference paper at ICLR 2025; Project page:\n  https://github.com/Visual-AI/hilo/"},{"id":"http://arxiv.org/abs/2410.09400v2","updated":"2025-03-03T12:33:49Z","published":"2024-10-12T07:04:32Z","title":"CtrLoRA: An Extensible and Efficient Framework for Controllable Image\n  Generation","summary":"  Recently, large-scale diffusion models have made impressive progress in\ntext-to-image (T2I) generation. To further equip these T2I models with\nfine-grained spatial control, approaches like ControlNet introduce an extra\nnetwork that learns to follow a condition image. However, for every single\ncondition type, ControlNet requires independent training on millions of data\npairs with hundreds of GPU hours, which is quite expensive and makes it\nchallenging for ordinary users to explore and develop new types of conditions.\nTo address this problem, we propose the CtrLoRA framework, which trains a Base\nControlNet to learn the common knowledge of image-to-image generation from\nmultiple base conditions, along with condition-specific LoRAs to capture\ndistinct characteristics of each condition. Utilizing our pretrained Base\nControlNet, users can easily adapt it to new conditions, requiring as few as\n1,000 data pairs and less than one hour of single-GPU training to obtain\nsatisfactory results in most scenarios. Moreover, our CtrLoRA reduces the\nlearnable parameters by 90% compared to ControlNet, significantly lowering the\nthreshold to distribute and deploy the model weights. Extensive experiments on\nvarious types of conditions demonstrate the efficiency and effectiveness of our\nmethod. Codes and model weights will be released at\nhttps://github.com/xyfJASON/ctrlora.\n","authors":["Yifeng Xu","Zhenliang He","Shiguang Shan","Xilin Chen"],"pdf_url":"https://arxiv.org/pdf/2410.09400v2.pdf","comment":"ICLR 2025. Code: https://github.com/xyfJASON/ctrlora"},{"id":"http://arxiv.org/abs/2403.19243v4","updated":"2025-03-03T12:32:47Z","published":"2024-03-28T08:58:20Z","title":"Efficient Learning With Sine-Activated Low-rank Matrices","summary":"  Low-rank decomposition has emerged as a vital tool for enhancing parameter\nefficiency in neural network architectures, gaining traction across diverse\napplications in machine learning. These techniques significantly lower the\nnumber of parameters, striking a balance between compactness and performance.\nHowever, a common challenge has been the compromise between parameter\nefficiency and the accuracy of the model, where reduced parameters often lead\nto diminished accuracy compared to their full-rank counterparts. In this work,\nwe propose a novel theoretical framework that integrates a sinusoidal function\nwithin the low-rank decomposition process. This approach not only preserves the\nbenefits of the parameter efficiency characteristic of low-rank methods but\nalso increases the decomposition's rank, thereby enhancing model performance.\nOur method proves to be a plug in enhancement for existing low-rank models, as\nevidenced by its successful application in Vision Transformers (ViT), Large\nLanguage Models (LLMs), Neural Radiance Fields (NeRF) and 3D shape modelling.\n","authors":["Yiping Ji","Hemanth Saratchandran","Cameron Gordon","Zeyu Zhang","Simon Lucey"],"pdf_url":"https://arxiv.org/pdf/2403.19243v4.pdf","comment":"The first two authors contributed equally. Paper accepted at ICLR\n  2025"},{"id":"http://arxiv.org/abs/2410.08190v2","updated":"2025-03-03T12:18:29Z","published":"2024-10-10T17:57:29Z","title":"Poison-splat: Computation Cost Attack on 3D Gaussian Splatting","summary":"  3D Gaussian splatting (3DGS), known for its groundbreaking performance and\nefficiency, has become a dominant 3D representation and brought progress to\nmany 3D vision tasks. However, in this work, we reveal a significant security\nvulnerability that has been largely overlooked in 3DGS: the computation cost of\ntraining 3DGS could be maliciously tampered by poisoning the input data. By\ndeveloping an attack named Poison-splat, we reveal a novel attack surface where\nthe adversary can poison the input images to drastically increase the\ncomputation memory and time needed for 3DGS training, pushing the algorithm\ntowards its worst computation complexity. In extreme cases, the attack can even\nconsume all allocable memory, leading to a Denial-of-Service (DoS) that\ndisrupts servers, resulting in practical damages to real-world 3DGS service\nvendors. Such a computation cost attack is achieved by addressing a bi-level\noptimization problem through three tailored strategies: attack objective\napproximation, proxy model rendering, and optional constrained optimization.\nThese strategies not only ensure the effectiveness of our attack but also make\nit difficult to defend with simple defensive measures. We hope the revelation\nof this novel attack surface can spark attention to this crucial yet overlooked\nvulnerability of 3DGS systems. Our code is available at\nhttps://github.com/jiahaolu97/poison-splat .\n","authors":["Jiahao Lu","Yifan Zhang","Qiuhong Shen","Xinchao Wang","Shuicheng Yan"],"pdf_url":"https://arxiv.org/pdf/2410.08190v2.pdf","comment":"Accepted by ICLR 2025 as a spotlight paper"},{"id":"http://arxiv.org/abs/2502.12138v3","updated":"2025-03-03T12:09:29Z","published":"2025-02-17T18:54:05Z","title":"FLARE: Feed-forward Geometry, Appearance and Camera Estimation from\n  Uncalibrated Sparse Views","summary":"  We present FLARE, a feed-forward model designed to infer high-quality camera\nposes and 3D geometry from uncalibrated sparse-view images (i.e., as few as 2-8\ninputs), which is a challenging yet practical setting in real-world\napplications. Our solution features a cascaded learning paradigm with camera\npose serving as the critical bridge, recognizing its essential role in mapping\n3D structures onto 2D image planes. Concretely, FLARE starts with camera pose\nestimation, whose results condition the subsequent learning of geometric\nstructure and appearance, optimized through the objectives of geometry\nreconstruction and novel-view synthesis. Utilizing large-scale public datasets\nfor training, our method delivers state-of-the-art performance in the tasks of\npose estimation, geometry reconstruction, and novel view synthesis, while\nmaintaining the inference efficiency (i.e., less than 0.5 seconds). The project\npage and code can be found at: https://zhanghe3z.github.io/FLARE/\n","authors":["Shangzhan Zhang","Jianyuan Wang","Yinghao Xu","Nan Xue","Christian Rupprecht","Xiaowei Zhou","Yujun Shen","Gordon Wetzstein"],"pdf_url":"https://arxiv.org/pdf/2502.12138v3.pdf","comment":"CVPR 2025. Website: https://zhanghe3z.github.io/FLARE/"},{"id":"http://arxiv.org/abs/2403.08632v2","updated":"2025-03-03T12:01:27Z","published":"2024-03-13T15:46:37Z","title":"A Decade's Battle on Dataset Bias: Are We There Yet?","summary":"  We revisit the \"dataset classification\" experiment suggested by Torralba &\nEfros (2011) a decade ago, in the new era with large-scale, diverse, and\nhopefully less biased datasets as well as more capable neural network\narchitectures. Surprisingly, we observe that modern neural networks can achieve\nexcellent accuracy in classifying which dataset an image is from: e.g., we\nreport 84.7% accuracy on held-out validation data for the three-way\nclassification problem consisting of the YFCC, CC, and DataComp datasets. Our\nfurther experiments show that such a dataset classifier could learn semantic\nfeatures that are generalizable and transferable, which cannot be explained by\nmemorization. We hope our discovery will inspire the community to rethink\nissues involving dataset bias.\n","authors":["Zhuang Liu","Kaiming He"],"pdf_url":"https://arxiv.org/pdf/2403.08632v2.pdf","comment":"Published in ICLR 2025 (Oral Presentation)"},{"id":"http://arxiv.org/abs/2502.17941v2","updated":"2025-03-03T12:00:57Z","published":"2025-02-25T08:03:04Z","title":"Optimal Brain Apoptosis","summary":"  The increasing complexity and parameter count of Convolutional Neural\nNetworks (CNNs) and Transformers pose challenges in terms of computational\nefficiency and resource demands. Pruning has been identified as an effective\nstrategy to address these challenges by removing redundant elements such as\nneurons, channels, or connections, thereby enhancing computational efficiency\nwithout heavily compromising performance. This paper builds on the foundational\nwork of Optimal Brain Damage (OBD) by advancing the methodology of parameter\nimportance estimation using the Hessian matrix. Unlike previous approaches that\nrely on approximations, we introduce Optimal Brain Apoptosis (OBA), a novel\npruning method that calculates the Hessian-vector product value directly for\neach parameter. By decomposing the Hessian matrix across network layers and\nidentifying conditions under which inter-layer Hessian submatrices are\nnon-zero, we propose a highly efficient technique for computing the\nsecond-order Taylor expansion of parameters. This approach allows for a more\nprecise pruning process, particularly in the context of CNNs and Transformers,\nas validated in our experiments including VGG19, ResNet32, ResNet50, and\nViT-B/16 on CIFAR10, CIFAR100 and Imagenet datasets. Our code is available at\nhttps://github.com/NEU-REAL/OBA.\n","authors":["Mingyuan Sun","Zheng Fang","Jiaxu Wang","Junjie Jiang","Delei Kong","Chenming Hu","Yuetong Fang","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2502.17941v2.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2407.15589v5","updated":"2025-03-03T11:48:03Z","published":"2024-07-22T12:26:08Z","title":"Exploring the Effectiveness of Object-Centric Representations in Visual\n  Question Answering: Comparative Insights with Foundation Models","summary":"  Object-centric (OC) representations, which model visual scenes as\ncompositions of discrete objects, have the potential to be used in various\ndownstream tasks to achieve systematic compositional generalization and\nfacilitate reasoning. However, these claims have yet to be thoroughly validated\nempirically. Recently, foundation models have demonstrated unparalleled\ncapabilities across diverse domains, from language to computer vision,\npositioning them as a potential cornerstone of future research for a wide range\nof computational tasks. In this paper, we conduct an extensive empirical study\non representation learning for downstream Visual Question Answering (VQA),\nwhich requires an accurate compositional understanding of the scene. We\nthoroughly investigate the benefits and trade-offs of OC models and alternative\napproaches including large pre-trained foundation models on both synthetic and\nreal-world data, ultimately identifying a promising path to leverage the\nstrengths of both paradigms. The extensiveness of our study, encompassing over\n600 downstream VQA models and 15 different types of upstream representations,\nalso provides several additional insights that we believe will be of interest\nto the community at large.\n","authors":["Amir Mohammad Karimi Mamaghan","Samuele Papa","Karl Henrik Johansson","Stefan Bauer","Andrea Dittadi"],"pdf_url":"https://arxiv.org/pdf/2407.15589v5.pdf","comment":"Published at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.21291v2","updated":"2025-03-03T11:33:31Z","published":"2025-02-28T18:21:08Z","title":"MIGE: A Unified Framework for Multimodal Instruction-Based Image\n  Generation and Editing","summary":"  Despite significant progress in diffusion-based image generation,\nsubject-driven generation and instruction-based editing remain challenging.\nExisting methods typically treat them separately, struggling with limited\nhigh-quality data and poor generalization. However, both tasks require\ncapturing complex visual variations while maintaining consistency between\ninputs and outputs. Therefore, we propose MIGE, a unified framework that\nstandardizes task representations using multimodal instructions. It treats\nsubject-driven generation as creation on a blank canvas and instruction-based\nediting as modification of an existing image, establishing a shared\ninput-output formulation. MIGE introduces a novel multimodal encoder that maps\nfree-form multimodal instructions into a unified vision-language space,\nintegrating visual and semantic features through a feature fusion mechanism.\nThis unification enables joint training of both tasks, providing two key\nadvantages: (1) Cross-Task Enhancement: By leveraging shared visual and\nsemantic representations, joint training improves instruction adherence and\nvisual consistency in both subject-driven generation and instruction-based\nediting. (2) Generalization: Learning in a unified format facilitates\ncross-task knowledge transfer, enabling MIGE to generalize to novel\ncompositional tasks, including instruction-based subject-driven editing.\nExperiments show that MIGE excels in both subject-driven generation and\ninstruction-based editing while setting a state-of-the-art in the new task of\ninstruction-based subject-driven editing. Code and model have been publicly\navailable at https://github.com/Eureka-Maggie/MIGE.\n","authors":["Xueyun Tian","Wei Li","Bingbing Xu","Yige Yuan","Yuanzhuo Wang","Huawei Shen"],"pdf_url":"https://arxiv.org/pdf/2502.21291v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18936v4","updated":"2025-03-03T11:00:24Z","published":"2025-01-31T07:41:06Z","title":"Adaptive Prompt: Unlocking the Power of Visual Prompt Tuning","summary":"  Visual Prompt Tuning (VPT) has recently emerged as a powerful method for\nadapting pre-trained vision models to downstream tasks. By introducing\nlearnable prompt tokens as task-specific instructions, VPT effectively guides\npre-trained transformer models with minimal overhead. Despite its empirical\nsuccess, a comprehensive theoretical understanding of VPT remains an active\narea of research. Building on recent insights into the connection between\nmixture of experts and prompt-based approaches, we identify a key limitation in\nVPT: the restricted functional expressiveness in prompt formulation. To address\nthis limitation, we propose Visual Adaptive Prompt Tuning (VAPT), a new\ngeneration of prompts that redefines prompts as adaptive functions of the\ninput. Our theoretical analysis shows that this simple yet intuitive approach\nachieves optimal sample efficiency. Empirical results on VTAB-1K and FGVC\nfurther demonstrate VAPT's effectiveness, with performance gains of 7.34% and\n1.04% over fully fine-tuning baselines, respectively. Notably, VAPT also\nsurpasses VPT by a substantial margin while using fewer parameters. These\nresults highlight both the effectiveness and efficiency of our method and pave\nthe way for future research to explore the potential of adaptive prompts.\n","authors":["Minh Le","Anh Nguyen","Huy Nguyen","Chau Nguyen","Nhat Ho"],"pdf_url":"https://arxiv.org/pdf/2501.18936v4.pdf","comment":"57 pages, 10 figures, 18 tables"},{"id":"http://arxiv.org/abs/2410.02423v2","updated":"2025-03-03T10:44:06Z","published":"2024-10-03T12:13:56Z","title":"PnP-Flow: Plug-and-Play Image Restoration with Flow Matching","summary":"  In this paper, we introduce Plug-and-Play (PnP) Flow Matching, an algorithm\nfor solving imaging inverse problems. PnP methods leverage the strength of\npre-trained denoisers, often deep neural networks, by integrating them in\noptimization schemes. While they achieve state-of-the-art performance on\nvarious inverse problems in imaging, PnP approaches face inherent limitations\non more generative tasks like inpainting. On the other hand, generative models\nsuch as Flow Matching pushed the boundary in image sampling yet lack a clear\nmethod for efficient use in image restoration. We propose to combine the PnP\nframework with Flow Matching (FM) by defining a time-dependent denoiser using a\npre-trained FM model. Our algorithm alternates between gradient descent steps\non the data-fidelity term, reprojections onto the learned FM path, and\ndenoising. Notably, our method is computationally efficient and\nmemory-friendly, as it avoids backpropagation through ODEs and trace\ncomputations. We evaluate its performance on denoising, super-resolution,\ndeblurring, and inpainting tasks, demonstrating superior results compared to\nexisting PnP algorithms and Flow Matching based state-of-the-art methods.\n","authors":["Ségolène Martin","Anne Gagneux","Paul Hagemann","Gabriele Steidl"],"pdf_url":"https://arxiv.org/pdf/2410.02423v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11542v2","updated":"2025-03-03T10:39:41Z","published":"2024-12-16T08:22:23Z","title":"Meta Curvature-Aware Minimization for Domain Generalization","summary":"  Domain generalization (DG) aims to enhance the ability of models trained on\nsource domains to generalize effectively to unseen domains. Recently,\nSharpness-Aware Minimization (SAM) has shown promise in this area by reducing\nthe sharpness of the loss landscape to obtain more generalized models. However,\nSAM and its variants sometimes fail to guide the model toward a flat minimum,\nand their training processes exhibit limitations, hindering further\nimprovements in model generalization. In this paper, we first propose an\nimproved model training process aimed at encouraging the model to converge to a\nflat minima. To achieve this, we design a curvature metric that has a minimal\neffect when the model is far from convergence but becomes increasingly\ninfluential in indicating the curvature of the minima as the model approaches a\nlocal minimum. Then we derive a novel algorithm from this metric, called Meta\nCurvature-Aware Minimization (MeCAM), to minimize the curvature around the\nlocal minima. Specifically, the optimization objective of MeCAM simultaneously\nminimizes the regular training loss, the surrogate gap of SAM, and the\nsurrogate gap of meta-learning. We provide theoretical analysis on MeCAM's\ngeneralization error and convergence rate, and demonstrate its superiority over\nexisting DG methods through extensive experiments on five benchmark DG\ndatasets, including PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet. Code\nwill be available on GitHub.\n","authors":["Ziyang Chen","Yiwen Ye","Feilong Tang","Yongsheng Pan","Yong Xia"],"pdf_url":"https://arxiv.org/pdf/2412.11542v2.pdf","comment":"22 pages, 5 figures, 17 tables"},{"id":"http://arxiv.org/abs/2502.08005v2","updated":"2025-03-03T10:38:34Z","published":"2025-02-11T23:02:14Z","title":"Towards Training One-Step Diffusion Models Without Distillation","summary":"  Recent advances in one-step generative models typically follow a two-stage\nprocess: first training a teacher diffusion model and then distilling it into a\none-step student model. This distillation process traditionally relies on both\nthe teacher model's score function to compute the distillation loss and its\nweights for student initialization. In this paper, we explore whether one-step\ngenerative models can be trained directly without this distillation process.\nFirst, we show that the teacher's score function is not essential and propose a\nfamily of distillation methods that achieve competitive results without relying\non score estimation. Next, we demonstrate that initialization from teacher\nweights is indispensable in successful training. Surprisingly, we find that\nthis benefit is not due to improved ``input-output\" mapping but rather the\nlearned feature representations, which dominate distillation quality. Our\nfindings provide a better understanding of the role of initialization in\none-step model training and its impact on distillation quality.\n","authors":["Mingtian Zhang","Jiajun He","Wenlin Chen","Zijing Ou","José Miguel Hernández-Lobato","Bernhard Schölkopf","David Barber"],"pdf_url":"https://arxiv.org/pdf/2502.08005v2.pdf","comment":"13 pages, Technical Report"},{"id":"http://arxiv.org/abs/2502.21264v2","updated":"2025-03-03T10:35:23Z","published":"2025-02-28T17:40:45Z","title":"Foundation Models -- A Panacea for Artificial Intelligence in Pathology?","summary":"  The role of artificial intelligence (AI) in pathology has evolved from aiding\ndiagnostics to uncovering predictive morphological patterns in whole slide\nimages (WSIs). Recently, foundation models (FMs) leveraging self-supervised\npre-training have been widely advocated as a universal solution for diverse\ndownstream tasks. However, open questions remain about their clinical\napplicability and generalization advantages over end-to-end learning using\ntask-specific (TS) models. Here, we focused on AI with clinical-grade\nperformance for prostate cancer diagnosis and Gleason grading. We present the\nlargest validation of AI for this task, using over 100,000 core needle biopsies\nfrom 7,342 patients across 15 sites in 11 countries. We compared two FMs with a\nfully end-to-end TS model in a multiple instance learning framework. Our\nfindings challenge assumptions that FMs universally outperform TS models. While\nFMs demonstrated utility in data-scarce scenarios, their performance converged\nwith - and was in some cases surpassed by - TS models when sufficient labeled\ntraining data were available. Notably, extensive task-specific training\nmarkedly reduced clinically significant misgrading, misdiagnosis of challenging\nmorphologies, and variability across different WSI scanners. Additionally, FMs\nused up to 35 times more energy than the TS model, raising concerns about their\nsustainability. Our results underscore that while FMs offer clear advantages\nfor rapid prototyping and research, their role as a universal solution for\nclinically applicable medical AI remains uncertain. For high-stakes clinical\napplications, rigorous validation and consideration of task-specific training\nremain critically important. We advocate for integrating the strengths of FMs\nand end-to-end learning to achieve robust and resource-efficient AI pathology\nsolutions fit for clinical use.\n","authors":["Nita Mulliqi","Anders Blilie","Xiaoyi Ji","Kelvin Szolnoky","Henrik Olsson","Sol Erika Boman","Matteo Titus","Geraldine Martinez Gonzalez","Julia Anna Mielcarz","Masi Valkonen","Einar Gudlaugsson","Svein R. Kjosavik","José Asenjo","Marcello Gambacorta","Paolo Libretti","Marcin Braun","Radzislaw Kordek","Roman Łowicki","Kristina Hotakainen","Päivi Väre","Bodil Ginnerup Pedersen","Karina Dalsgaard Sørensen","Benedicte Parm Ulhøi","Pekka Ruusuvuori","Brett Delahunt","Hemamali Samaratunga","Toyonori Tsuzuki","Emilius A. M. Janssen","Lars Egevad","Martin Eklund","Kimmo Kartasalo"],"pdf_url":"https://arxiv.org/pdf/2502.21264v2.pdf","comment":"50 pages, 15 figures and an appendix (study protocol) which is\n  previously published, see https://doi.org/10.1101/2024.07.04.24309948;\n  updated authors list format"},{"id":"http://arxiv.org/abs/2502.21201v2","updated":"2025-03-03T10:32:20Z","published":"2025-02-28T16:18:57Z","title":"The PanAf-FGBG Dataset: Understanding the Impact of Backgrounds in\n  Wildlife Behaviour Recognition","summary":"  Computer vision analysis of camera trap video footage is essential for\nwildlife conservation, as captured behaviours offer some of the earliest\nindicators of changes in population health. Recently, several high-impact\nanimal behaviour datasets and methods have been introduced to encourage their\nuse; however, the role of behaviour-correlated background information and its\nsignificant effect on out-of-distribution generalisation remain unexplored. In\nresponse, we present the PanAf-FGBG dataset, featuring 20 hours of wild\nchimpanzee behaviours, recorded at over 350 individual camera locations.\nUniquely, it pairs every video with a chimpanzee (referred to as a foreground\nvideo) with a corresponding background video (with no chimpanzee) from the same\ncamera location. We present two views of the dataset: one with overlapping\ncamera locations and one with disjoint locations. This setup enables, for the\nfirst time, direct evaluation of in-distribution and out-of-distribution\nconditions, and for the impact of backgrounds on behaviour recognition models\nto be quantified. All clips come with rich behavioural annotations and metadata\nincluding unique camera IDs and detailed textual scene descriptions.\nAdditionally, we establish several baselines and present a highly effective\nlatent-space normalisation technique that boosts out-of-distribution\nperformance by +5.42% mAP for convolutional and +3.75% mAP for\ntransformer-based models. Finally, we provide an in-depth analysis on the role\nof backgrounds in out-of-distribution behaviour recognition, including the so\nfar unexplored impact of background durations (i.e., the count of background\nframes within foreground videos).\n","authors":["Otto Brookes","Maksim Kukushkin","Majid Mirmehdi","Colleen Stephens","Paula Dieguez","Thurston C. Hicks","Sorrel Jones","Kevin Lee","Maureen S. McCarthy","Amelia Meier","Emmanuelle Normand","Erin G. Wessling","Roman M. Wittig","Kevin Langergraber","Klaus Zuberbühler","Lukas Boesch","Thomas Schmid","Mimi Arandjelovic","Hjalmar Kühl","Tilo Burghardt"],"pdf_url":"https://arxiv.org/pdf/2502.21201v2.pdf","comment":"Accepted at the IEEE / CVF Computer Vision and Pattern Recognition\n  Conference 2025"},{"id":"http://arxiv.org/abs/2410.05643v3","updated":"2025-03-03T10:28:30Z","published":"2024-10-08T02:46:30Z","title":"TRACE: Temporal Grounding Video LLM via Causal Event Modeling","summary":"  Video Temporal Grounding (VTG) is a crucial capability for video\nunderstanding models and plays a vital role in downstream tasks such as video\nbrowsing and editing. To effectively handle various tasks simultaneously and\nenable zero-shot prediction, there is a growing trend in employing video LLMs\nfor VTG tasks. However, current video LLM-based methods rely exclusively on\nnatural language generation, lacking the ability to model the clear structure\ninherent in videos, which restricts their effectiveness in tackling VTG tasks.\nTo address this issue, this paper first formally introduces causal event\nmodeling framework, which represents video LLM outputs as sequences of events,\nand predict the current event using previous events, video inputs, and textural\ninstructions. Each event consists of three components: timestamps, salient\nscores, and textual captions. We then propose a novel task-interleaved video\nLLM called TRACE to effectively implement the causal event modeling framework\nin practice. The TRACE process visual frames, timestamps, salient scores, and\ntext as distinct tasks, employing various encoders and decoding heads for each.\nTask tokens are arranged in an interleaved sequence according to the causal\nevent modeling framework's formulation. Extensive experiments on various VTG\ntasks and datasets demonstrate the superior performance of TRACE compared to\nstate-of-the-art video LLMs. Our model and code are available at\nhttps://github.com/gyxxyg/TRACE.\n","authors":["Yongxin Guo","Jingyu Liu","Mingda Li","Qingbin Liu","Xi Chen","Xiaoying Tang"],"pdf_url":"https://arxiv.org/pdf/2410.05643v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2411.06916v2","updated":"2025-03-03T10:22:24Z","published":"2024-11-11T12:19:28Z","title":"Slowing Down Forgetting in Continual Learning","summary":"  A common challenge in continual learning (CL) is catastrophic forgetting,\nwhere the performance on old tasks drops after new, additional tasks are\nlearned. In this paper, we propose a novel framework called ReCL to slow down\nforgetting in CL. Our framework exploits an implicit bias of gradient-based\nneural networks due to which these converge to margin maximization points. Such\nconvergence points allow us to reconstruct old data from previous tasks, which\nwe then combine with the current training data. Our framework is flexible and\ncan be applied on top of existing, state-of-the-art CL methods. We further\ndemonstrate the performance gain from our framework across a large series of\nexperiments, including two challenging CL scenarios (class incremental and\ndomain incremental learning), different datasets (MNIST, CIFAR10,\nTinyImagenet), and different network architectures. Across all experiments, we\nfind large performance gains through ReCL. To the best of our knowledge, our\nframework is the first to address catastrophic forgetting by leveraging models\nin CL as their own memory buffers.\n","authors":["Pascal Janetzky","Tobias Schlagenhauf","Stefan Feuerriegel"],"pdf_url":"https://arxiv.org/pdf/2411.06916v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14651v3","updated":"2025-03-03T09:31:01Z","published":"2024-07-19T20:05:10Z","title":"Improving Representation of High-frequency Components for Medical Visual\n  Foundation Models","summary":"  Foundation models have recently attracted significant attention for their\nimpressive generalizability across diverse downstream tasks. However, these\nmodels are demonstrated to exhibit great limitations in representing\nhigh-frequency components and fine-grained details. In many medical imaging\ntasks, the precise representation of such information is crucial due to the\ninherently intricate anatomical structures, sub-visual features, and complex\nboundaries involved. Consequently, the limited representation of prevalent\nfoundation models can result in significant performance degradation or even\nfailure in these tasks. To address these challenges, we propose a novel\npretraining strategy, named Frequency-advanced Representation Autoencoder\n(Frepa). Through high-frequency masking and low-frequency perturbation combined\nwith adversarial learning, Frepa encourages the encoder to effectively\nrepresent and preserve high-frequency components in the image embeddings.\nAdditionally, we introduce an innovative histogram-equalized image masking\nstrategy, extending the Masked Autoencoder approach beyond ViT to other\narchitectures such as Swin Transformer and convolutional networks. We develop\nFrepa across nine medical modalities and validate it on 32 downstream tasks for\nboth 2D images and 3D volume data. Without fine-tuning, Frepa can outperform\nother self-supervised pretraining methods and, in some cases, even surpasses\ntask-specific trained models. This improvement is particularly significant for\ntasks involving fine-grained details, such as achieving up to a +15% increase\nin DSC for retina vessel segmentation and a +7% increase in IoU for lung nodule\ndetection. Further experiments quantitatively reveal that Frepa enables\nsuperior high-frequency representations and preservation in the embeddings,\nunderscoring its potential for developing more generalized and universal\nmedical image foundation models.\n","authors":["Yuetan Chu","Yilan Zhang","Zhongyi Han","Changchun Yang","Longxi Zhou","Gongning Luo","Chao Huang","Xin Gao"],"pdf_url":"https://arxiv.org/pdf/2407.14651v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23751v2","updated":"2025-03-03T09:30:42Z","published":"2024-10-31T09:11:56Z","title":"EXACFS -- A CIL Method to mitigate Catastrophic Forgetting","summary":"  Deep neural networks (DNNS) excel at learning from static datasets but\nstruggle with continual learning, where data arrives sequentially. Catastrophic\nforgetting, the phenomenon of forgetting previously learned knowledge, is a\nprimary challenge. This paper introduces EXponentially Averaged Class-wise\nFeature Significance (EXACFS) to mitigate this issue in the class incremental\nlearning (CIL) setting. By estimating the significance of model features for\neach learned class using loss gradients, gradually aging the significance\nthrough the incremental tasks and preserving the significant features through a\ndistillation loss, EXACFS effectively balances remembering old knowledge\n(stability) and learning new knowledge (plasticity). Extensive experiments on\nCIFAR-100 and ImageNet-100 demonstrate EXACFS's superior performance in\npreserving stability while acquiring plasticity.\n","authors":["S Balasubramanian","M Sai Subramaniam","Sai Sriram Talasu","Yedu Krishna P","Manepalli Pranav Phanindra Sai","Ravi Mukkamala","Darshan Gera"],"pdf_url":"https://arxiv.org/pdf/2410.23751v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.08537v3","updated":"2025-03-03T09:26:26Z","published":"2023-10-12T17:26:16Z","title":"Saliency-Bench: A Comprehensive Benchmark for Evaluating Visual\n  Explanations","summary":"  Explainable AI (XAI) has gained significant attention for providing insights\ninto the decision-making processes of deep learning models, particularly for\nimage classification tasks through visual explanations visualized by saliency\nmaps. Despite their success, challenges remain due to the lack of annotated\ndatasets and standardized evaluation pipelines. In this paper, we introduce\nSaliency-Bench, a novel benchmark suite designed to evaluate visual\nexplanations generated by saliency methods across multiple datasets. We\ncurated, constructed, and annotated eight datasets, each covering diverse tasks\nsuch as scene classification, cancer diagnosis, object classification, and\naction classification, with corresponding ground-truth explanations. The\nbenchmark includes a standardized and unified evaluation pipeline for assessing\nfaithfulness and alignment of the visual explanation, providing a holistic\nvisual explanation performance assessment. We benchmark these eight datasets\nwith widely used saliency methods on different image classifier architectures\nto evaluate explanation quality. Additionally, we developed an easy-to-use API\nfor automating the evaluation pipeline, from data accessing, and data loading,\nto result evaluation. The benchmark is available via our website:\nhttps://xaidataset.github.io.\n","authors":["Yifei Zhang","James Song","Siyi Gu","Tianxu Jiang","Bo Pan","Guangji Bai","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2310.08537v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16751v3","updated":"2025-03-03T09:07:59Z","published":"2025-01-28T07:08:20Z","title":"HiBug2: Efficient and Interpretable Error Slice Discovery for\n  Comprehensive Model Debugging","summary":"  Despite the significant success of deep learning models in computer vision,\nthey often exhibit systematic failures on specific data subsets, known as error\nslices. Identifying and mitigating these error slices is crucial to enhancing\nmodel robustness and reliability in real-world scenarios. In this paper, we\nintroduce HiBug2, an automated framework for error slice discovery and model\nrepair. HiBug2 first generates task-specific visual attributes to highlight\ninstances prone to errors through an interpretable and structured process. It\nthen employs an efficient slice enumeration algorithm to systematically\nidentify error slices, overcoming the combinatorial challenges that arise\nduring slice exploration. Additionally, HiBug2 extends its capabilities by\npredicting error slices beyond the validation set, addressing a key limitation\nof prior approaches. Extensive experiments across multiple domains, including\nimage classification, pose estimation, and object detection - show that HiBug2\nnot only improves the coherence and precision of identified error slices but\nalso significantly enhances the model repair capabilities.\n","authors":["Muxi Chen","Chenchen Zhao","Qiang Xu"],"pdf_url":"https://arxiv.org/pdf/2501.16751v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.12275v2","updated":"2025-03-03T09:05:52Z","published":"2024-06-18T05:05:12Z","title":"VoCo-LLaMA: Towards Vision Compression with Large Language Models","summary":"  Vision-Language Models (VLMs) have achieved remarkable success in various\nmulti-modal tasks, but they are often bottlenecked by the limited context\nwindow and high computational cost of processing high-resolution image inputs\nand videos. Vision compression can alleviate this problem by reducing the\nvision token count. Previous approaches compress vision tokens with external\nmodules and force LLMs to understand the compressed ones, leading to visual\ninformation loss. However, the LLMs' understanding paradigm of vision tokens is\nnot fully utilised in the compression learning process. We propose VoCo-LLaMA,\nthe first approach to compress vision tokens using LLMs. By introducing Vision\nCompression tokens during the vision instruction tuning phase and leveraging\nattention distillation, our method distill how LLMs comprehend vision tokens\ninto their processing of VoCo tokens. VoCo-LLaMA facilitates effective vision\ncompression and improves the computational efficiency during the inference\nstage. Specifically, our method achieves minimal performance loss with a\ncompression ratio of 576$\\times$, resulting in up to 94.8$\\%$ fewer FLOPs and\n69.6$\\%$ acceleration in inference time. Furthermore, through continuous\ntraining using time-series compressed token sequences of video frames,\nVoCo-LLaMA demonstrates the ability to understand temporal correlations,\noutperforming previous methods on popular video question-answering benchmarks.\nOur approach presents a promising way to unlock the full potential of VLMs'\ncontextual window, enabling more scalable multi-modal applications. The project\npage, along with the associated code, can be accessed via\nhttps://yxxxb.github.io/VoCo-LLaMA-page/.\n","authors":["Xubing Ye","Yukang Gan","Xiaoke Huang","Yixiao Ge","Yansong Tang"],"pdf_url":"https://arxiv.org/pdf/2406.12275v2.pdf","comment":"11 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.21130v2","updated":"2025-03-03T08:39:54Z","published":"2025-02-28T15:10:07Z","title":"Fast and Accurate Gigapixel Pathological Image Classification with\n  Hierarchical Distillation Multi-Instance Learning","summary":"  Although multi-instance learning (MIL) has succeeded in pathological image\nclassification, it faces the challenge of high inference costs due to\nprocessing numerous patches from gigapixel whole slide images (WSIs). To\naddress this, we propose HDMIL, a hierarchical distillation multi-instance\nlearning framework that achieves fast and accurate classification by\neliminating irrelevant patches. HDMIL consists of two key components: the\ndynamic multi-instance network (DMIN) and the lightweight instance\npre-screening network (LIPN). DMIN operates on high-resolution WSIs, while LIPN\noperates on the corresponding low-resolution counterparts. During training,\nDMIN are trained for WSI classification while generating attention-score-based\nmasks that indicate irrelevant patches. These masks then guide the training of\nLIPN to predict the relevance of each low-resolution patch. During testing,\nLIPN first determines the useful regions within low-resolution WSIs, which\nindirectly enables us to eliminate irrelevant regions in high-resolution WSIs,\nthereby reducing inference time without causing performance degradation. In\naddition, we further design the first Chebyshev-polynomials-based\nKolmogorov-Arnold classifier in computational pathology, which enhances the\nperformance of HDMIL through learnable activation layers. Extensive experiments\non three public datasets demonstrate that HDMIL outperforms previous\nstate-of-the-art methods, e.g., achieving improvements of 3.13% in AUC while\nreducing inference time by 28.6% on the Camelyon16 dataset.\n","authors":["Jiuyang Dong","Junjun Jiang","Kui Jiang","Jiahan Li","Yongbing Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.21130v2.pdf","comment":"11 pages, 4 figures, accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2311.14922v3","updated":"2025-03-03T07:41:00Z","published":"2023-11-25T03:55:06Z","title":"GDTS: Goal-Guided Diffusion Model with Tree Sampling for Multi-Modal\n  Pedestrian Trajectory Prediction","summary":"  Accurate prediction of pedestrian trajectories is crucial for improving the\nsafety of autonomous driving. However, this task is generally nontrivial due to\nthe inherent stochasticity of human motion, which naturally requires the\npredictor to generate multi-modal prediction. Previous works leverage various\ngenerative methods, such as GAN and VAE, for pedestrian trajectory prediction.\nNevertheless, these methods may suffer from mode collapse and relatively\nlow-quality results. The denoising diffusion probabilistic model (DDPM) has\nrecently been applied to trajectory prediction due to its simple training\nprocess and powerful reconstruction ability. However, current diffusion-based\nmethods do not fully utilize input information and usually require many\ndenoising iterations that lead to a long inference time or an additional\nnetwork for initialization. To address these challenges and facilitate the use\nof diffusion models in multi-modal trajectory prediction, we propose GDTS, a\nnovel Goal-Guided Diffusion Model with Tree Sampling for multi-modal trajectory\nprediction. Considering the \"goal-driven\" characteristics of human motion, GDTS\nleverages goal estimation to guide the generation of the diffusion network. A\ntwo-stage tree sampling algorithm is presented, which leverages common features\nto reduce the inference time and improve accuracy for multi-modal prediction.\nExperimental results demonstrate that our proposed framework achieves\ncomparable state-of-the-art performance with real-time inference speed in\npublic datasets.\n","authors":["Ge Sun","Sheng Wang","Lei Zhu","Ming Liu","Jun Ma"],"pdf_url":"https://arxiv.org/pdf/2311.14922v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.01217v2","updated":"2025-03-03T07:38:09Z","published":"2024-05-02T11:58:06Z","title":"CromSS: Cross-modal pre-training with noisy labels for remote sensing\n  image segmentation","summary":"  We explore the potential of large-scale noisily labeled data to enhance\nfeature learning by pretraining semantic segmentation models within a\nmulti-modal framework for geospatial applications. We propose a novel\nCross-modal Sample Selection (CromSS) method, a weakly supervised pretraining\nstrategy designed to improve feature representations through cross-modal\nconsistency and noise mitigation techniques. Unlike conventional pretraining\napproaches, CromSS exploits massive amounts of noisy and easy-to-come-by labels\nfor improved feature learning beneficial to semantic segmentation tasks. We\ninvestigate middle and late fusion strategies to optimize the multi-modal\npretraining architecture design. We also introduce a cross-modal sample\nselection module to mitigate the adverse effects of label noise, which employs\na cross-modal entangling strategy to refine the estimated confidence masks\nwithin each modality to guide the sampling process. Additionally, we introduce\na spatial-temporal label smoothing technique to counteract overconfidence for\nenhanced robustness against noisy labels. To validate our approach, we\nassembled the multi-modal dataset, NoLDO-S12, which consists of a large-scale\nnoisy label subset from Google's Dynamic World (DW) dataset for pretraining and\ntwo downstream subsets with high-quality labels from Google DW and\nOpenStreetMap (OSM) for transfer learning. Experimental results on two\ndownstream tasks and the publicly available DFC2020 dataset demonstrate that\nwhen effectively utilized, the low-cost noisy labels can significantly enhance\nfeature learning for segmentation tasks. All data, code, and pretrained weights\nwill be made publicly available.\n","authors":["Chenying Liu","Conrad Albrecht","Yi Wang","Xiao Xiang Zhu"],"pdf_url":"https://arxiv.org/pdf/2405.01217v2.pdf","comment":"The 1st short version was accepted as an oral presentation by ICLR\n  2024 ML4RS workshop. The 2nd extended version is being under review"},{"id":"http://arxiv.org/abs/2501.15394v2","updated":"2025-03-03T07:30:55Z","published":"2025-01-26T04:24:07Z","title":"Doracamom: Joint 3D Detection and Occupancy Prediction with Multi-view\n  4D Radars and Cameras for Omnidirectional Perception","summary":"  3D object detection and occupancy prediction are critical tasks in autonomous\ndriving, attracting significant attention. Despite the potential of recent\nvision-based methods, they encounter challenges under adverse conditions. Thus,\nintegrating cameras with next-generation 4D imaging radar to achieve unified\nmulti-task perception is highly significant, though research in this domain\nremains limited. In this paper, we propose Doracamom, the first framework that\nfuses multi-view cameras and 4D radar for joint 3D object detection and\nsemantic occupancy prediction, enabling comprehensive environmental perception.\nSpecifically, we introduce a novel Coarse Voxel Queries Generator that\nintegrates geometric priors from 4D radar with semantic features from images to\ninitialize voxel queries, establishing a robust foundation for subsequent\nTransformer-based refinement. To leverage temporal information, we design a\nDual-Branch Temporal Encoder that processes multi-modal temporal features in\nparallel across BEV and voxel spaces, enabling comprehensive spatio-temporal\nrepresentation learning. Furthermore, we propose a Cross-Modal BEV-Voxel Fusion\nmodule that adaptively fuses complementary features through attention\nmechanisms while employing auxiliary tasks to enhance feature quality.\nExtensive experiments on the OmniHD-Scenes, View-of-Delft (VoD), and TJ4DRadSet\ndatasets demonstrate that Doracamom achieves state-of-the-art performance in\nboth tasks, establishing new benchmarks for multi-modal 3D perception. Code and\nmodels will be publicly available.\n","authors":["Lianqing Zheng","Jianan Liu","Runwei Guan","Long Yang","Shouyi Lu","Yuanzhe Li","Xiaokai Bai","Jie Bai","Zhixiong Ma","Hui-Liang Shen","Xichan Zhu"],"pdf_url":"https://arxiv.org/pdf/2501.15394v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.19289v3","updated":"2025-03-03T07:18:14Z","published":"2024-11-28T17:41:33Z","title":"ADUGS-VINS: Generalized Visual-Inertial Odometry for Robust Navigation\n  in Highly Dynamic and Complex Environments","summary":"  Visual-inertial odometry (VIO) is widely used in various fields, such as\nrobots, drones, and autonomous vehicles. However, real-world scenes often\nfeature dynamic objects, compromising the accuracy of VIO. The diversity and\npartial occlusion of these objects present a tough challenge for existing\ndynamic VIO methods. To tackle this challenge, we introduce ADUGS-VINS, which\nintegrates an enhanced SORT algorithm along with a promptable foundation model\ninto VIO, thereby improving pose estimation accuracy in environments with\ndiverse dynamic objects and frequent occlusions. We evaluated our proposed\nmethod using multiple public datasets representing various scenes, as well as\nin a real-world scenario involving diverse dynamic objects. The experimental\nresults demonstrate that our proposed method performs impressively in multiple\nscenarios, outperforming other state-of-the-art methods. This highlights its\nremarkable generalization and adaptability in diverse dynamic environments,\nshowcasing its potential to handle various dynamic objects in practical\napplications.\n","authors":["Rui Zhou","Jingbin Liu","Junbin Xie","Jianyu Zhang","Yingze Hu","Jiele Zhao"],"pdf_url":"https://arxiv.org/pdf/2411.19289v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05757v2","updated":"2025-03-03T07:07:28Z","published":"2025-01-10T07:19:41Z","title":"Locality-aware Gaussian Compression for Fast and High-quality Rendering","summary":"  We present LocoGS, a locality-aware 3D Gaussian Splatting (3DGS) framework\nthat exploits the spatial coherence of 3D Gaussians for compact modeling of\nvolumetric scenes. To this end, we first analyze the local coherence of 3D\nGaussian attributes, and propose a novel locality-aware 3D Gaussian\nrepresentation that effectively encodes locally-coherent Gaussian attributes\nusing a neural field representation with a minimal storage requirement. On top\nof the novel representation, LocoGS is carefully designed with additional\ncomponents such as dense initialization, an adaptive spherical harmonics\nbandwidth scheme and different encoding schemes for different Gaussian\nattributes to maximize compression performance. Experimental results\ndemonstrate that our approach outperforms the rendering quality of existing\ncompact Gaussian representations for representative real-world 3D datasets\nwhile achieving from 54.6$\\times$ to 96.6$\\times$ compressed storage size and\nfrom 2.1$\\times$ to 2.4$\\times$ rendering speed than 3DGS. Even our approach\nalso demonstrates an averaged 2.4$\\times$ higher rendering speed than the\nstate-of-the-art compression method with comparable compression performance.\n","authors":["Seungjoo Shin","Jaesik Park","Sunghyun Cho"],"pdf_url":"https://arxiv.org/pdf/2501.05757v2.pdf","comment":"Accepted to ICLR 2025. Project page:\n  https://seungjooshin.github.io/LocoGS"},{"id":"http://arxiv.org/abs/2501.12296v2","updated":"2025-03-03T06:45:12Z","published":"2025-01-21T17:03:06Z","title":"RALAD: Bridging the Real-to-Sim Domain Gap in Autonomous Driving with\n  Retrieval-Augmented Learning","summary":"  In the pursuit of robust autonomous driving systems, models trained on\nreal-world datasets often struggle to adapt to new environments, particularly\nwhen confronted with corner cases such as extreme weather conditions.\nCollecting these corner cases in the real world is non-trivial, which\nnecessitates the use of simulators for validation. However,the high\ncomputational cost and the domain gap in data distribution have hindered the\nseamless transition between real and simulated driving scenarios. To tackle\nthis challenge, we propose Retrieval-Augmented Learning for Autonomous Driving\n(RALAD), a novel framework designed to bridge the real-to-sim gap at a low\ncost. RALAD features three primary designs, including (1) domain adaptation via\nan enhanced Optimal Transport (OT) method that accounts for both individual and\ngrouped image distances, (2) a simple and unified framework that can be applied\nto various models, and (3) efficient fine-tuning techniques that freeze the\ncomputationally expensive layers while maintaining robustness. Experimental\nresults demonstrate that RALAD compensates for the performance degradation in\nsimulated environments while maintaining accuracy in real-world scenarios\nacross three different models. Taking Cross View as an example, the mIOU and\nmAP metrics in real-world scenarios remain stable before and after RALAD\nfine-tuning, while in simulated environments,the mIOU and mAP metrics are\nimproved by 10.30% and 12.29%, respectively. Moreover, the re-training cost of\nour approach is reduced by approximately 88.1%. Our code is available at\nhttps://github.com/JiachengZuo/RALAD.git.\n","authors":["Jiacheng Zuo","Haibo Hu","Zikang Zhou","Yufei Cui","Ziquan Liu","Jianping Wang","Nan Guan","Jin Wang","Chun Jason Xue"],"pdf_url":"https://arxiv.org/pdf/2501.12296v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.19160v2","updated":"2025-03-03T06:34:25Z","published":"2024-12-26T10:40:15Z","title":"Cross-Spectral Vision Transformer for Biometric Authentication using\n  Forehead Subcutaneous Vein Pattern and Periocular Pattern","summary":"  Traditional biometric systems have encountered significant setbacks due to\nvarious unavoidable factors, for example, face recognition-based biometrics\nfails due to the wearing of face masks and fingerprints create hygiene\nconcerns. This paper proposes a novel lightweight cross-spectral vision\ntransformer (CS-ViT) for biometric authentication using forehead subcutaneous\nvein patterns and periocular patterns, offering a promising alternative to\ntraditional methods, capable of performing well even with the face masks and\nwithout any physical touch. The proposed framework comprises a cross-spectral\ndual-channel architecture designed to handle two distinct biometric traits and\nto capture inter-dependencies in terms of relative spectral patterns. Each\nchannel consists of a Phase-Only Correlation Cross-Spectral Attention (POC-CSA)\nthat captures their individual as well as correlated patterns. The computation\nof cross-spectral attention using POC extracts the phase correlation in the\nspatial features. Therefore, it is robust against the resolution/intensity\nvariations and illumination of the input images, assuming both biometric traits\nare from the same person. The lightweight model is suitable for edge device\ndeployment. The performance of the proposed algorithm was rigorously evaluated\nusing the Forehead Subcutaneous Vein Pattern and Periocular Biometric Pattern\n(FSVP-PBP) database. The results demonstrated the superiority of the algorithm\nover state-of-the-art methods, achieving a remarkable classification accuracy\nof 98.8% with the combined vein and periocular patterns.\n","authors":["Arun K. Sharma","Shubhobrata Bhattacharya","Motahar Reza","Bishakh Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2412.19160v2.pdf","comment":"Submitted to IEEE TPAMI"},{"id":"http://arxiv.org/abs/2502.20041v2","updated":"2025-03-03T06:21:57Z","published":"2025-02-27T12:29:44Z","title":"3D-AffordanceLLM: Harnessing Large Language Models for Open-Vocabulary\n  Affordance Detection in 3D Worlds","summary":"  3D Affordance detection is a challenging problem with broad applications on\nvarious robotic tasks. Existing methods typically formulate the detection\nparadigm as a label-based semantic segmentation task. This paradigm relies on\npredefined labels and lacks the ability to comprehend complex natural language,\nresulting in limited generalization in open-world scene. To address these\nlimitations, we reformulate the traditional affordance detection paradigm into\n\\textit{Instruction Reasoning Affordance Segmentation} (IRAS) task. This task\nis designed to output a affordance mask region given a query reasoning text,\nwhich avoids fixed categories of input labels. We accordingly propose the\n\\textit{3D-AffordanceLLM} (3D-ADLLM), a framework designed for reasoning\naffordance detection in 3D open-scene. Specifically, 3D-ADLLM introduces large\nlanguage models (LLMs) to 3D affordance perception with a custom-designed\ndecoder for generating affordance masks, thus achieving open-world reasoning\naffordance detection. In addition, given the scarcity of 3D affordance datasets\nfor training large models, we seek to extract knowledge from general\nsegmentation data and transfer it to affordance detection. Thus, we propose a\nmulti-stage training strategy that begins with a novel pre-training task, i.e.,\n\\textit{Referring Object Part Segmentation}~(ROPS). This stage is designed to\nequip the model with general recognition and segmentation capabilities at the\nobject-part level. Then followed by fine-tuning with the IRAS task, 3D-ADLLM\nobtains the reasoning ability for affordance detection. In summary, 3D-ADLLM\nleverages the rich world knowledge and human-object interaction reasoning\nability of LLMs, achieving approximately an 8\\% improvement in mIoU on\nopen-vocabulary affordance detection tasks.\n","authors":["Hengshuo Chu","Xiang Deng","Qi Lv","Xiaoyang Chen","Yinchuan Li","Jianye Hao","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2502.20041v2.pdf","comment":"ICLR"},{"id":"http://arxiv.org/abs/2310.01405v4","updated":"2025-03-03T06:14:14Z","published":"2023-10-02T17:59:07Z","title":"Representation Engineering: A Top-Down Approach to AI Transparency","summary":"  In this paper, we identify and characterize the emerging area of\nrepresentation engineering (RepE), an approach to enhancing the transparency of\nAI systems that draws on insights from cognitive neuroscience. RepE places\npopulation-level representations, rather than neurons or circuits, at the\ncenter of analysis, equipping us with novel methods for monitoring and\nmanipulating high-level cognitive phenomena in deep neural networks (DNNs). We\nprovide baselines and an initial analysis of RepE techniques, showing that they\noffer simple yet effective solutions for improving our understanding and\ncontrol of large language models. We showcase how these methods can provide\ntraction on a wide range of safety-relevant problems, including honesty,\nharmlessness, power-seeking, and more, demonstrating the promise of top-down\ntransparency research. We hope that this work catalyzes further exploration of\nRepE and fosters advancements in the transparency and safety of AI systems.\n","authors":["Andy Zou","Long Phan","Sarah Chen","James Campbell","Phillip Guo","Richard Ren","Alexander Pan","Xuwang Yin","Mantas Mazeika","Ann-Kathrin Dombrowski","Shashwat Goel","Nathaniel Li","Michael J. Byun","Zifan Wang","Alex Mallen","Steven Basart","Sanmi Koyejo","Dawn Song","Matt Fredrikson","J. Zico Kolter","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2310.01405v4.pdf","comment":"Code is available at\n  https://github.com/andyzoujm/representation-engineering"},{"id":"http://arxiv.org/abs/2412.10831v2","updated":"2025-03-03T06:13:35Z","published":"2024-12-14T13:28:40Z","title":"Low-Biased General Annotated Dataset Generation","summary":"  Pre-training backbone networks on a general annotated dataset (e.g.,\nImageNet) that comprises numerous manually collected images with category\nannotations has proven to be indispensable for enhancing the generalization\ncapacity of downstream visual tasks. However, those manually collected images\noften exhibit bias, which is non-transferable across either categories or\ndomains, thus causing the model's generalization capacity degeneration. To\nmitigate this problem, we present an low-biased general annotated dataset\ngeneration framework (lbGen). Instead of expensive manual collection, we aim at\ndirectly generating low-biased images with category annotations. To achieve\nthis goal, we propose to leverage the advantage of a multimodal foundation\nmodel (e.g., CLIP), in terms of aligning images in an low-biased semantic space\ndefined by language. Specifically, we develop a bi-level semantic alignment\nloss, which not only forces all generated images to be consistent with the\nsemantic distribution of all categories belonging to the target dataset in an\nadversarial learning manner, but also requires each generated image to match\nthe semantic description of its category name. In addition, we further cast an\nexisting image quality scoring model into a quality assurance loss to preserve\nthe quality of the generated image. By leveraging these two loss functions, we\ncan obtain an low-biased image generation model by simply fine-tuning a\npre-trained diffusion model using only all category names in the target dataset\nas input. Experimental results confirm that, compared with the manually labeled\ndataset or other synthetic datasets, the utilization of our generated\nlow-biased datasets leads to stable generalization capacity enhancement of\ndifferent backbone networks across various tasks, especially in tasks where the\nmanually labeled samples are scarce.\n","authors":["Dengyang Jiang","Haoyu Wang","Lei Zhang","Wei Wei","Guang Dai","Mengmeng Wang","Jingdong Wang","Yanning Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.10831v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.02268v3","updated":"2025-03-03T05:32:47Z","published":"2024-10-03T07:40:14Z","title":"Structural-Entropy-Based Sample Selection for Efficient and Effective\n  Learning","summary":"  Sample selection improves the efficiency and effectiveness of machine\nlearning models by providing informative and representative samples. Typically,\nsamples can be modeled as a sample graph, where nodes are samples and edges\nrepresent their similarities. Most existing methods are based on local\ninformation, such as the training difficulty of samples, thereby overlooking\nglobal information, such as connectivity patterns. This oversight can result in\nsuboptimal selection because global information is crucial for ensuring that\nthe selected samples well represent the structural properties of the graph. To\naddress this issue, we employ structural entropy to quantify global information\nand losslessly decompose it from the whole graph to individual nodes using the\nShapley value. Based on the decomposition, we present\n$\\textbf{S}$tructural-$\\textbf{E}$ntropy-based sample $\\textbf{S}$election\n($\\textbf{SES}$), a method that integrates both global and local information to\nselect informative and representative samples. SES begins by constructing a\n$k$NN-graph among samples based on their similarities. It then measures sample\nimportance by combining structural entropy (global metric) with training\ndifficulty (local metric). Finally, SES applies importance-biased blue noise\nsampling to select a set of diverse and representative samples. Comprehensive\nexperiments on three learning scenarios -- supervised learning, active\nlearning, and continual learning -- clearly demonstrate the effectiveness of\nour method.\n","authors":["Tianchi Xie","Jiangning Zhu","Guozu Ma","Minzhi Lin","Wei Chen","Weikai Yang","Shixia Liu"],"pdf_url":"https://arxiv.org/pdf/2410.02268v3.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2404.12379v3","updated":"2025-03-03T05:31:09Z","published":"2024-04-18T17:58:16Z","title":"Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Dynamic\n  Scenes","summary":"  Modern 3D engines and graphics pipelines require mesh as a memory-efficient\nrepresentation, which allows efficient rendering, geometry processing, texture\nediting, and many other downstream operations. However, it is still highly\ndifficult to obtain high-quality mesh in terms of detailed structure and time\nconsistency from dynamic observations. To this end, we introduce Dynamic\nGaussians Mesh (DG-Mesh), a framework to reconstruct a high-fidelity and\ntime-consistent mesh from dynamic input. Our work leverages the recent\nadvancement in 3D Gaussian Splatting to construct the mesh sequence with\ntemporal consistency from dynamic observations. Building on top of this\nrepresentation, DG-Mesh recovers high-quality meshes from the Gaussian points\nand can track the mesh vertices over time, which enables applications such as\ntexture editing on dynamic objects. We introduce the Gaussian-Mesh Anchoring,\nwhich encourages evenly distributed Gaussians, resulting better mesh\nreconstruction through mesh-guided densification and pruning on the deformed\nGaussians. By applying cycle-consistent deformation between the canonical and\nthe deformed space, we can project the anchored Gaussian back to the canonical\nspace and optimize Gaussians across all time frames. During the evaluation on\ndifferent datasets, DG-Mesh provides significantly better mesh reconstruction\nand rendering than baselines. Project page: https://www.liuisabella.com/DG-Mesh\n","authors":["Isabella Liu","Hao Su","Xiaolong Wang"],"pdf_url":"https://arxiv.org/pdf/2404.12379v3.pdf","comment":"Project page: https://www.liuisabella.com/DG-Mesh"},{"id":"http://arxiv.org/abs/2410.09374v3","updated":"2025-03-03T05:31:05Z","published":"2024-10-12T05:35:27Z","title":"ESVO2: Direct Visual-Inertial Odometry with Stereo Event Cameras","summary":"  Event-based visual odometry is a specific branch of visual Simultaneous\nLocalization and Mapping (SLAM) techniques, which aims at solving tracking and\nmapping subproblems (typically in parallel), by exploiting the special working\nprinciples of neuromorphic (i.e., event-based) cameras. Due to the\nmotion-dependent nature of event data, explicit data association (i.e., feature\nmatching) under large-baseline view-point changes is difficult to establish,\nmaking direct methods a more rational choice. However, state-of-the-art direct\nmethods are limited by the high computational complexity of the mapping\nsub-problem and the degeneracy of camera pose tracking in certain degrees of\nfreedom (DoF) in rotation. In this paper, we tackle these issues by building an\nevent-based stereo visual-inertial odometry system on top of a direct pipeline.\nSpecifically, to speed up the mapping operation, we propose an efficient\nstrategy for sampling contour points according to the local dynamics of events.\nThe mapping performance is also improved in terms of structure completeness and\nlocal smoothness by merging the temporal stereo and static stereo results. To\ncircumvent the degeneracy of camera pose tracking in recovering the pitch and\nyaw components of general 6-DoF motion, we introduce IMU measurements as motion\npriors via pre-integration. To this end, a compact back-end is proposed for\ncontinuously updating the IMU bias and predicting the linear velocity, enabling\nan accurate motion prediction for camera pose tracking. The resulting system\nscales well with modern high-resolution event cameras and leads to better\nglobal positioning accuracy in large-scale outdoor environments. Extensive\nevaluations on five publicly available datasets featuring different resolutions\nand scenarios justify the superior performance of the proposed system against\nfive state-of-the-art methods.\n","authors":["Junkai Niu","Sheng Zhong","Xiuyuan Lu","Shaojie Shen","Guillermo Gallego","Yi Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.09374v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.10988v2","updated":"2025-03-03T05:26:25Z","published":"2025-02-16T04:18:41Z","title":"OMG: Opacity Matters in Material Modeling with Gaussian Splatting","summary":"  Decomposing geometry, materials and lighting from a set of images, namely\ninverse rendering, has been a long-standing problem in computer vision and\ngraphics. Recent advances in neural rendering enable photo-realistic and\nplausible inverse rendering results. The emergence of 3D Gaussian Splatting has\nboosted it to the next level by showing real-time rendering potentials. An\nintuitive finding is that the models used for inverse rendering do not take\ninto account the dependency of opacity w.r.t. material properties, namely cross\nsection, as suggested by optics. Therefore, we develop a novel approach that\nadds this dependency to the modeling itself. Inspired by radiative transfer, we\naugment the opacity term by introducing a neural network that takes as input\nmaterial properties to provide modeling of cross section and a physically\ncorrect activation function. The gradients for material properties are\ntherefore not only from color but also from opacity, facilitating a constraint\nfor their optimization. Therefore, the proposed method incorporates more\naccurate physical properties compared to previous works. We implement our\nmethod into 3 different baselines that use Gaussian Splatting for inverse\nrendering and achieve significant improvements universally in terms of novel\nview synthesis and material modeling.\n","authors":["Silong Yong","Venkata Nagarjun Pudureddiyur Manivannan","Bernhard Kerbl","Zifu Wan","Simon Stepputtis","Katia Sycara","Yaqi Xie"],"pdf_url":"https://arxiv.org/pdf/2502.10988v2.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.01912v2","updated":"2025-03-03T05:25:43Z","published":"2025-02-04T01:05:12Z","title":"PATCH: a deep learning method to assess heterogeneity of artistic\n  practice in historical paintings","summary":"  The history of art has seen significant shifts in the manner in which\nartworks are created, making understanding of creative processes a central\nquestion in technical art history. In the Renaissance and Early Modern period,\npaintings were largely produced by master painters directing workshops of\napprentices who often contributed to projects. The masters varied significantly\nin artistic and managerial styles, meaning different combinations of artists\nand implements might be seen both between masters and within workshops or even\nindividual canvases. Information on how different workshops were managed and\nthe processes by which artworks were created remains elusive. Machine learning\nmethods have potential to unearth new information about artists' creative\nprocesses by extending the analysis of brushwork to a microscopic scale.\nAnalysis of workshop paintings, however, presents a challenge in that\ndocumentation of the artists and materials involved is sparse, meaning external\nexamples are not available to train networks to recognize their contributions.\nHere we present a novel machine learning approach we call pairwise assignment\ntraining for classifying heterogeneity (PATCH) that is capable of identifying\nindividual artistic practice regimes with no external training data, or \"ground\ntruth.\" The method achieves unsupervised results by supervised means, and\noutperforms both simple statistical procedures and unsupervised machine\nlearning methods. We apply this method to two historical paintings by the\nSpanish Renaissance master, El Greco: The Baptism of Christ and Christ on the\nCross with Landscape, and our findings regarding the former potentially\nchallenge previous work that has assigned the painting to workshop members.\nFurther, the results of our analyses create a measure of heterogeneity of\nartistic practice that can be used to characterize artworks across time and\nspace.\n","authors":["Andrew Van Horn","Lauryn Smith","Mahamad Mahmoud","Michael McMaster","Clara Pinchbeck","Ina Martin","Andrew Lininger","Anthony Ingrisano","Adam Lowe","Carlos Bayod","Elizabeth Bolman","Kenneth Singer","Michael Hinczewski"],"pdf_url":"https://arxiv.org/pdf/2502.01912v2.pdf","comment":"main text: 16 pages, 6 figures; SI: 7 pages, 3 figures; v2: minor\n  typo corrections, higher resolution figures"},{"id":"http://arxiv.org/abs/2402.02112v5","updated":"2025-03-03T04:42:15Z","published":"2024-02-03T10:35:42Z","title":"S-NeRF++: Autonomous Driving Simulation via Neural Reconstruction and\n  Generation","summary":"  Autonomous driving simulation system plays a crucial role in enhancing\nself-driving data and simulating complex and rare traffic scenarios, ensuring\nnavigation safety. However, traditional simulation systems, which often heavily\nrely on manual modeling and 2D image editing, struggled with scaling to\nextensive scenes and generating realistic simulation data. In this study, we\npresent S-NeRF++, an innovative autonomous driving simulation system based on\nneural reconstruction. Trained on widely-used self-driving datasets such as\nnuScenes and Waymo, S-NeRF++ can generate a large number of realistic street\nscenes and foreground objects with high rendering quality as well as offering\nconsiderable flexibility in manipulation and simulation. Specifically, S-NeRF++\nis an enhanced neural radiance field for synthesizing large-scale scenes and\nmoving vehicles, with improved scene parameterization and camera pose learning.\nThe system effectively utilizes noisy and sparse LiDAR data to refine training\nand address depth outliers, ensuring high-quality reconstruction and novel-view\nrendering. It also provides a diverse foreground asset bank by reconstructing\nand generating different foreground vehicles to support comprehensive scenario\ncreation.Moreover, we have developed an advanced foreground-background fusion\npipeline that skillfully integrates illumination and shadow effects, further\nenhancing the realism of our simulations. With the high-quality simulated data\nprovided by our S-NeRF++, we found the perception methods enjoy performance\nboosts on several autonomous driving downstream tasks, further demonstrating\nour proposed simulator's effectiveness.\n","authors":["Yurui Chen","Junge Zhang","Ziyang Xie","Wenye Li","Feihu Zhang","Jiachen Lu","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.02112v5.pdf","comment":"IEEE TPAMI 2025"},{"id":"http://arxiv.org/abs/2409.07002v2","updated":"2025-03-03T04:32:29Z","published":"2024-09-11T04:30:45Z","title":"AdvLogo: Adversarial Patch Attack against Object Detectors based on\n  Diffusion Models","summary":"  With the rapid development of deep learning, object detectors have\ndemonstrated impressive performance; however, vulnerabilities still exist in\ncertain scenarios. Current research exploring the vulnerabilities using\nadversarial patches often struggles to balance the trade-off between attack\neffectiveness and visual quality. To address this problem, we propose a novel\nframework of patch attack from semantic perspective, which we refer to as\nAdvLogo. Based on the hypothesis that every semantic space contains an\nadversarial subspace where images can cause detectors to fail in recognizing\nobjects, we leverage the semantic understanding of the diffusion denoising\nprocess and drive the process to adversarial subareas by perturbing the latent\nand unconditional embeddings at the last timestep. To mitigate the distribution\nshift that exposes a negative impact on image quality, we apply perturbation to\nthe latent in frequency domain with the Fourier Transform. Experimental results\ndemonstrate that AdvLogo achieves strong attack performance while maintaining\nhigh visual quality.\n","authors":["Boming Miao","Chunxiao Li","Yao Zhu","Weixiang Sun","Zizhe Wang","Xiaoyi Wang","Chuanlong Xie"],"pdf_url":"https://arxiv.org/pdf/2409.07002v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18084v2","updated":"2025-03-03T04:31:23Z","published":"2024-10-23T17:59:58Z","title":"DynamicCity: Large-Scale 4D Occupancy Generation from Dynamic Scenes","summary":"  Urban scene generation has been developing rapidly recently. However,\nexisting methods primarily focus on generating static and single-frame scenes,\noverlooking the inherently dynamic nature of real-world driving environments.\nIn this work, we introduce DynamicCity, a novel 4D occupancy generation\nframework capable of generating large-scale, high-quality dynamic 4D scenes\nwith semantics. DynamicCity mainly consists of two key models. 1) A VAE model\nfor learning HexPlane as the compact 4D representation. Instead of using naive\naveraging operations, DynamicCity employs a novel Projection Module to\neffectively compress 4D features into six 2D feature maps for HexPlane\nconstruction, which significantly enhances HexPlane fitting quality (up to\n12.56 mIoU gain). Furthermore, we utilize an Expansion & Squeeze Strategy to\nreconstruct 3D feature volumes in parallel, which improves both network\ntraining efficiency and reconstruction accuracy than naively querying each 3D\npoint (up to 7.05 mIoU gain, 2.06x training speedup, and 70.84% memory\nreduction). 2) A DiT-based diffusion model for HexPlane generation. To make\nHexPlane feasible for DiT generation, a Padded Rollout Operation is proposed to\nreorganize all six feature planes of the HexPlane as a squared 2D feature map.\nIn particular, various conditions could be introduced in the diffusion or\nsampling process, supporting versatile 4D generation applications, such as\ntrajectory- and command-driven generation, inpainting, and layout-conditioned\ngeneration. Extensive experiments on the CarlaSC and Waymo datasets demonstrate\nthat DynamicCity significantly outperforms existing state-of-the-art 4D\noccupancy generation methods across multiple metrics. The code and models have\nbeen released to facilitate future research.\n","authors":["Hengwei Bian","Lingdong Kong","Haozhe Xie","Liang Pan","Yu Qiao","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2410.18084v2.pdf","comment":"ICLR 2025 Spotlight; 35 pages, 18 figures, 15 tables; Project Page at\n  https://dynamic-city.github.io/"},{"id":"http://arxiv.org/abs/2403.17010v3","updated":"2025-03-03T04:22:19Z","published":"2024-03-25T17:59:59Z","title":"Calib3D: Calibrating Model Preferences for Reliable 3D Scene\n  Understanding","summary":"  Safety-critical 3D scene understanding tasks necessitate not only accurate\nbut also confident predictions from 3D perception models. This study introduces\nCalib3D, a pioneering effort to benchmark and scrutinize the reliability of 3D\nscene understanding models from an uncertainty estimation viewpoint. We\ncomprehensively evaluate 28 state-of-the-art models across 10 diverse 3D\ndatasets, uncovering insightful phenomena that cope with both the aleatoric and\nepistemic uncertainties in 3D scene understanding. We discover that despite\nachieving impressive levels of accuracy, existing models frequently fail to\nprovide reliable uncertainty estimates -- a pitfall that critically undermines\ntheir applicability in safety-sensitive contexts. Through extensive analysis of\nkey factors such as network capacity, LiDAR representations, rasterization\nresolutions, and 3D data augmentation techniques, we correlate these aspects\ndirectly with the model calibration efficacy. Furthermore, we introduce DeptS,\na novel depth-aware scaling approach aimed at enhancing 3D model calibration.\nExtensive experiments across a wide range of configurations validate the\nsuperiority of our method. We hope this work could serve as a cornerstone for\nfostering reliable 3D scene understanding. Code and benchmark toolkit are\npublicly available.\n","authors":["Lingdong Kong","Xiang Xu","Jun Cen","Wenwei Zhang","Liang Pan","Kai Chen","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2403.17010v3.pdf","comment":"WACV 2025 Oral; 26 pages, 8 figures, 12 tables; Code at\n  https://github.com/ldkong1205/Calib3D"},{"id":"http://arxiv.org/abs/2410.03190v3","updated":"2025-03-03T04:11:46Z","published":"2024-10-04T07:05:16Z","title":"Tuning Timestep-Distilled Diffusion Model Using Pairwise Sample\n  Optimization","summary":"  Recent advancements in timestep-distilled diffusion models have enabled\nhigh-quality image generation that rivals non-distilled multi-step models, but\nwith significantly fewer inference steps. While such models are attractive for\napplications due to the low inference cost and latency, fine-tuning them with a\nnaive diffusion objective would result in degraded and blurry outputs. An\nintuitive alternative is to repeat the diffusion distillation process with a\nfine-tuned teacher model, which produces good results but is cumbersome and\ncomputationally intensive; the distillation training usually requires magnitude\nhigher of training compute compared to fine-tuning for specific image styles.\nIn this paper, we present an algorithm named pairwise sample optimization\n(PSO), which enables the direct fine-tuning of an arbitrary timestep-distilled\ndiffusion model. PSO introduces additional reference images sampled from the\ncurrent time-step distilled model, and increases the relative likelihood margin\nbetween the training images and reference images. This enables the model to\nretain its few-step generation ability, while allowing for fine-tuning of its\noutput distribution. We also demonstrate that PSO is a generalized formulation\nwhich can be flexibly extended to both offline-sampled and online-sampled\npairwise data, covering various popular objectives for diffusion model\npreference optimization. We evaluate PSO in both preference optimization and\nother fine-tuning tasks, including style transfer and concept customization. We\nshow that PSO can directly adapt distilled models to human-preferred generation\nwith both offline and online-generated pairwise preference image data. PSO also\ndemonstrates effectiveness in style transfer and concept customization by\ndirectly tuning timestep-distilled diffusion models.\n","authors":["Zichen Miao","Zhengyuan Yang","Kevin Lin","Ze Wang","Zicheng Liu","Lijuan Wang","Qiang Qiu"],"pdf_url":"https://arxiv.org/pdf/2410.03190v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.21093v2","updated":"2025-03-03T03:48:47Z","published":"2025-02-28T14:32:04Z","title":"FlexDrive: Toward Trajectory Flexibility in Driving Scene Reconstruction\n  and Rendering","summary":"  Driving scene reconstruction and rendering have advanced significantly using\nthe 3D Gaussian Splatting. However, most prior research has focused on the\nrendering quality along a pre-recorded vehicle path and struggles to generalize\nto out-of-path viewpoints, which is caused by the lack of high-quality\nsupervision in those out-of-path views. To address this issue, we introduce an\nInverse View Warping technique to create compact and high-quality images as\nsupervision for the reconstruction of the out-of-path views, enabling\nhigh-quality rendering results for those views. For accurate and robust inverse\nview warping, a depth bootstrap strategy is proposed to obtain on-the-fly dense\ndepth maps during the optimization process, overcoming the sparsity and\nincompleteness of LiDAR depth data. Our method achieves superior in-path and\nout-of-path reconstruction and rendering performance on the widely used Waymo\nOpen dataset. In addition, a simulator-based benchmark is proposed to obtain\nthe out-of-path ground truth and quantitatively evaluate the performance of\nout-of-path rendering, where our method outperforms previous methods by a\nsignificant margin.\n","authors":["Jingqiu Zhou","Lue Fan","Linjiang Huang","Xiaoyu Shi","Si Liu","Zhaoxiang Zhang","Hongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2502.21093v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.02586v2","updated":"2025-03-03T03:38:29Z","published":"2023-05-04T06:40:11Z","title":"Semantically Structured Image Compression via Irregular Group-Based\n  Decoupling","summary":"  Image compression techniques typically focus on compressing rectangular\nimages for human consumption, however, resulting in transmitting redundant\ncontent for downstream applications. To overcome this limitation, some previous\nworks propose to semantically structure the bitstream, which can meet specific\napplication requirements by selective transmission and reconstruction.\nNevertheless, they divide the input image into multiple rectangular regions\naccording to semantics and ignore avoiding information interaction among them,\ncausing waste of bitrate and distorted reconstruction of region boundaries. In\nthis paper, we propose to decouple an image into multiple groups with irregular\nshapes based on a customized group mask and compress them independently. Our\ngroup mask describes the image at a finer granularity, enabling significant\nbitrate saving by reducing the transmission of redundant content. Moreover, to\nensure the fidelity of selective reconstruction, this paper proposes the\nconcept of group-independent transform that maintain the independence among\ndistinct groups. And we instantiate it by the proposed Group-Independent\nSwin-Block (GI Swin-Block). Experimental results demonstrate that our framework\nstructures the bitstream with negligible cost, and exhibits superior\nperformance on both visual quality and intelligent task supporting.\n","authors":["Ruoyu Feng","Yixin Gao","Xin Jin","Runsen Feng","Zhibo Chen"],"pdf_url":"https://arxiv.org/pdf/2305.02586v2.pdf","comment":"Accept by ICCV2023"},{"id":"http://arxiv.org/abs/2502.01117v2","updated":"2025-03-03T03:35:00Z","published":"2025-02-03T07:13:59Z","title":"Learning to Learn Weight Generation via Trajectory Diffusion","summary":"  Diffusion-based algorithms have emerged as promising techniques for weight\ngeneration, particularly in scenarios like multi-task learning that require\nfrequent weight updates. However, existing solutions suffer from limited\ncross-task transferability. In addition, they only utilize optimal weights as\ntraining samples, ignoring the value of other weights in the optimization\nprocess. To address these issues, we propose Lt-Di, which integrates the\ndiffusion algorithm with meta-learning to generate weights for unseen tasks.\nFurthermore, we extend the vanilla diffusion algorithm into a trajectory\ndiffusion algorithm to utilize other weights along the optimization trajectory.\nTrajectory diffusion decomposes the entire diffusion chain into multiple\nshorter ones, improving training and inference efficiency. We analyze the\nconvergence properties of the weight generation paradigm and improve\nconvergence efficiency without additional time overhead. Our experiments\ndemonstrate Lt-Di's higher accuracy while reducing computational overhead\nacross various tasks, including zero-shot and few-shot learning, multi-domain\ngeneralization, and large-scale language model fine-tuning.Our code is released\nat https://anonymous.4open.science/r/Lt-Di-0E51.\n","authors":["Yunchuan Guan","Yu Liu","Ke Zhou","Zhiqi Shen","Serge Belongie","Jenq-Neng Hwang","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2502.01117v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21256v2","updated":"2025-03-03T03:23:44Z","published":"2024-10-28T17:54:29Z","title":"Multi-modal AI for comprehensive breast cancer prognostication","summary":"  Treatment selection in breast cancer is guided by molecular subtypes and\nclinical characteristics. However, current tools including genomic assays lack\nthe accuracy required for optimal clinical decision-making. We developed a\nnovel artificial intelligence (AI)-based approach that integrates digital\npathology images with clinical data, providing a more robust and effective\nmethod for predicting the risk of cancer recurrence in breast cancer patients.\nSpecifically, we utilized a vision transformer pan-cancer foundation model\ntrained with self-supervised learning to extract features from digitized\nH&E-stained slides. These features were integrated with clinical data to form a\nmulti-modal AI test predicting cancer recurrence and death. The test was\ndeveloped and evaluated using data from a total of 8,161 female breast cancer\npatients across 15 cohorts originating from seven countries. Of these, 3,502\npatients from five cohorts were used exclusively for evaluation, while the\nremaining patients were used for training. Our test accurately predicted our\nprimary endpoint, disease-free interval, in the five evaluation cohorts\n(C-index: 0.71 [0.68-0.75], HR: 3.63 [3.02-4.37, p<0.001]). In a direct\ncomparison (n=858), the AI test was more accurate than Oncotype DX, the\nstandard-of-care 21-gene assay, achieving a C-index of 0.67 [0.61-0.74] versus\n0.61 [0.49-0.73], respectively. Additionally, the AI test added independent\nprognostic information to Oncotype DX in a multivariate analysis (HR: 3.11\n[1.91-5.09, p<0.001)]). The test demonstrated robust accuracy across major\nmolecular breast cancer subtypes, including TNBC (C-index: 0.71 [0.62-0.81],\nHR: 3.81 [2.35-6.17, p=0.02]), where no diagnostic tools are currently\nrecommended by clinical guidelines. These results suggest that our AI test\nimproves upon the accuracy of existing prognostic tests, while being applicable\nto a wider range of patients.\n","authors":["Jan Witowski","Ken G. Zeng","Joseph Cappadona","Jailan Elayoubi","Khalil Choucair","Elena Diana Chiru","Nancy Chan","Young-Joon Kang","Frederick Howard","Irina Ostrovnaya","Carlos Fernandez-Granda","Freya Schnabel","Zoe Steinsnyder","Ugur Ozerdem","Kangning Liu","Waleed Abdulsattar","Yu Zong","Lina Daoud","Rafic Beydoun","Anas Saad","Nitya Thakore","Mohammad Sadic","Frank Yeung","Elisa Liu","Theodore Hill","Benjamin Swett","Danielle Rigau","Andrew Clayburn","Valerie Speirs","Marcus Vetter","Lina Sojak","Simone Soysal","Daniel Baumhoer","Jia-Wern Pan","Haslina Makmur","Soo-Hwang Teo","Linda Ma Pak","Victor Angel","Dovile Zilenaite-Petrulaitiene","Arvydas Laurinavicius","Natalie Klar","Brian D. Piening","Carlo Bifulco","Sun-Young Jun","Jae Pak Yi","Su Hyun Lim","Adam Brufsky","Francisco J. Esteva","Lajos Pusztai","Yann LeCun","Krzysztof J. Geras"],"pdf_url":"https://arxiv.org/pdf/2410.21256v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14093v3","updated":"2025-03-03T03:19:31Z","published":"2024-05-23T01:43:54Z","title":"A Survey on Vision-Language-Action Models for Embodied AI","summary":"  Embodied AI is widely recognized as a key element of artificial general\nintelligence because it involves controlling embodied agents to perform tasks\nin the physical world. Building on the success of large language models and\nvision-language models, a new category of multimodal models -- referred to as\nvision-language-action models (VLAs) -- has emerged to address\nlanguage-conditioned robotic tasks in embodied AI by leveraging their distinct\nability to generate actions. In recent years, a myriad of VLAs have been\ndeveloped, making it imperative to capture the rapidly evolving landscape\nthrough a comprehensive survey. To this end, we present the first survey on\nVLAs for embodied AI. This work provides a detailed taxonomy of VLAs, organized\ninto three major lines of research. The first line focuses on individual\ncomponents of VLAs. The second line is dedicated to developing control policies\nadept at predicting low-level actions. The third line comprises high-level task\nplanners capable of decomposing long-horizon tasks into a sequence of subtasks,\nthereby guiding VLAs to follow more general user instructions. Furthermore, we\nprovide an extensive summary of relevant resources, including datasets,\nsimulators, and benchmarks. Finally, we discuss the challenges faced by VLAs\nand outline promising future directions in embodied AI.\n","authors":["Yueen Ma","Zixing Song","Yuzheng Zhuang","Jianye Hao","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2405.14093v3.pdf","comment":"16 pages, a survey of vision-language-action models"},{"id":"http://arxiv.org/abs/2501.12844v2","updated":"2025-03-03T03:18:40Z","published":"2025-01-22T12:45:09Z","title":"GAMED-Snake: Gradient-aware Adaptive Momentum Evolution Deep Snake Model\n  for Multi-organ Segmentation","summary":"  Multi-organ segmentation is a critical yet challenging task due to complex\nanatomical backgrounds, blurred boundaries, and diverse morphologies. This\nstudy introduces the Gradient-aware Adaptive Momentum Evolution Deep Snake\n(GAMED-Snake) model, which establishes a novel paradigm for contour-based\nsegmentation by integrating gradient-based learning with adaptive momentum\nevolution mechanisms. The GAMED-Snake model incorporates three major\ninnovations: First, the Distance Energy Map Prior (DEMP) generates a\npixel-level force field that effectively attracts contour points towards the\ntrue boundaries, even in scenarios with complex backgrounds and blurred edges.\nSecond, the Differential Convolution Inception Module (DCIM) precisely extracts\ncomprehensive energy gradients, significantly enhancing segmentation accuracy.\nThird, the Adaptive Momentum Evolution Mechanism (AMEM) employs cross-attention\nto establish dynamic features across different iterations of evolution,\nenabling precise boundary alignment for diverse morphologies. Experimental\nresults on four challenging multi-organ segmentation datasets demonstrate that\nGAMED-Snake improves the mDice metric by approximately 2% compared to\nstate-of-the-art methods. Code will be available at\nhttps://github.com/SYSUzrc/GAMED-Snake.\n","authors":["Ruicheng Zhang","Haowei Guo","Zeyu Zhang","Puxin Yan","Shen Zhao"],"pdf_url":"https://arxiv.org/pdf/2501.12844v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16826v2","updated":"2025-03-03T03:09:49Z","published":"2025-02-24T04:23:21Z","title":"Noise2Score3D:Unsupervised Tweedie's Approach for Point Cloud Denoising","summary":"  Building on recent advances in Bayesian statistics and image denoising, we\npropose Noise2Score3D, a fully unsupervised framework for point cloud denoising\nthat addresses the critical challenge of limited availability of clean data.\nNoise2Score3D learns the gradient of the underlying point cloud distribution\ndirectly from noisy data, eliminating the need for clean data during training.\nBy leveraging Tweedie's formula, our method performs inference in a single\nstep, avoiding the iterative processes used in existing unsupervised methods,\nthereby improving both performance and efficiency. Experimental results\ndemonstrate that Noise2Score3D achieves state-of-the-art performance on\nstandard benchmarks, outperforming other unsupervised methods in Chamfer\ndistance and point-to-mesh metrics, and rivaling some supervised approaches.\nFurthermore, Noise2Score3D demonstrates strong generalization ability beyond\ntraining datasets. Additionally, we introduce Total Variation for Point Cloud,\na criterion that allows for the estimation of unknown noise parameters, which\nfurther enhances the method's versatility and real-world utility.\n","authors":["Xiangbin Wei"],"pdf_url":"https://arxiv.org/pdf/2502.16826v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13085v2","updated":"2025-03-03T03:08:28Z","published":"2024-10-16T23:03:27Z","title":"MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language\n  Models","summary":"  Artificial Intelligence (AI) has demonstrated significant potential in\nhealthcare, particularly in disease diagnosis and treatment planning. Recent\nprogress in Medical Large Vision-Language Models (Med-LVLMs) has opened up new\npossibilities for interactive diagnostic tools. However, these models often\nsuffer from factual hallucination, which can lead to incorrect diagnoses.\nFine-tuning and retrieval-augmented generation (RAG) have emerged as methods to\naddress these issues. However, the amount of high-quality data and distribution\nshifts between training data and deployment data limit the application of\nfine-tuning methods. Although RAG is lightweight and effective, existing\nRAG-based approaches are not sufficiently general to different medical domains\nand can potentially cause misalignment issues, both between modalities and\nbetween the model and the ground truth. In this paper, we propose a versatile\nmultimodal RAG system, MMed-RAG, designed to enhance the factuality of\nMed-LVLMs. Our approach introduces a domain-aware retrieval mechanism, an\nadaptive retrieved contexts selection method, and a provable RAG-based\npreference fine-tuning strategy. These innovations make the RAG process\nsufficiently general and reliable, significantly improving alignment when\nintroducing retrieved contexts. Experimental results across five medical\ndatasets (involving radiology, ophthalmology, pathology) on medical VQA and\nreport generation demonstrate that MMed-RAG can achieve an average improvement\nof 43.8% in the factual accuracy of Med-LVLMs. Our data and code are available\nin https://github.com/richard-peng-xia/MMed-RAG.\n","authors":["Peng Xia","Kangyu Zhu","Haoran Li","Tianze Wang","Weijia Shi","Sheng Wang","Linjun Zhang","James Zou","Huaxiu Yao"],"pdf_url":"https://arxiv.org/pdf/2410.13085v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2409.06214v3","updated":"2025-03-03T01:46:42Z","published":"2024-09-10T04:45:25Z","title":"Towards Generalizable Scene Change Detection","summary":"  While current state-of-the-art Scene Change Detection (SCD) approaches\nachieve impressive results in well-trained research data, they become\nunreliable under unseen environments and different temporal conditions;\nin-domain performance drops from 77.6\\% to 8.0\\% in a previously unseen\nenvironment and to 4.6\\% under a different temporal condition -- calling for\ngeneralizable SCD and benchmark. In this work, we propose the Generalizable\nScene Change Detection Framework (GeSCF), which addresses unseen domain\nperformance and temporal consistency -- to meet the growing demand for anything\nSCD. Our method leverages the pre-trained Segment Anything Model (SAM) in a\nzero-shot manner. For this, we design Initial Pseudo-mask Generation and\nGeometric-Semantic Mask Matching -- seamlessly turning user-guided prompt and\nsingle-image based segmentation into scene change detection for a pair of\ninputs without guidance. Furthermore, we define the Generalizable Scene Change\nDetection (GeSCD) benchmark along with novel metrics and an evaluation protocol\nto facilitate SCD research in generalizability. In the process, we introduce\nthe ChangeVPR dataset, a collection of challenging image pairs with diverse\nenvironmental scenarios -- including urban, suburban, and rural settings.\nExtensive experiments across various datasets demonstrate that GeSCF achieves\nan average performance gain of 19.2\\% on existing SCD datasets and 30.0\\% on\nthe ChangeVPR dataset, nearly doubling the prior art performance. We believe\nour work can lay a solid foundation for robust and generalizable SCD research.\n","authors":["Jaewoo Kim","Uehwan Kim"],"pdf_url":"https://arxiv.org/pdf/2409.06214v3.pdf","comment":"Manuscript. Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2502.08079v3","updated":"2025-03-03T01:35:58Z","published":"2025-02-12T02:53:27Z","title":"MAA: Meticulous Adversarial Attack against Vision-Language Pre-trained\n  Models","summary":"  Current adversarial attacks for evaluating the robustness of vision-language\npre-trained (VLP) models in multi-modal tasks suffer from limited\ntransferability, where attacks crafted for a specific model often struggle to\ngeneralize effectively across different models, limiting their utility in\nassessing robustness more broadly. This is mainly attributed to the\nover-reliance on model-specific features and regions, particularly in the image\nmodality. In this paper, we propose an elegant yet highly effective method\ntermed Meticulous Adversarial Attack (MAA) to fully exploit model-independent\ncharacteristics and vulnerabilities of individual samples, achieving enhanced\ngeneralizability and reduced model dependence. MAA emphasizes fine-grained\noptimization of adversarial images by developing a novel resizing and sliding\ncrop (RScrop) technique, incorporating a multi-granularity similarity\ndisruption (MGSD) strategy. Extensive experiments across diverse VLP models,\nmultiple benchmark datasets, and a variety of downstream tasks demonstrate that\nMAA significantly enhances the effectiveness and transferability of adversarial\nattacks. A large cohort of performance studies is conducted to generate\ninsights into the effectiveness of various model configurations, guiding future\nadvancements in this domain.\n","authors":["Peng-Fei Zhang","Guangdong Bai","Zi Huang"],"pdf_url":"https://arxiv.org/pdf/2502.08079v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14519v2","updated":"2025-03-03T00:51:41Z","published":"2024-09-22T16:25:31Z","title":"RobotFingerPrint: Unified Gripper Coordinate Space for Multi-Gripper\n  Grasp Synthesis and Transfer","summary":"  We introduce a novel grasp representation named the Unified Gripper\nCoordinate Space (UGCS) for grasp synthesis and grasp transfer. Our\nrepresentation leverages spherical coordinates to create a shared coordinate\nspace across different robot grippers, enabling it to synthesize and transfer\ngrasps for both novel objects and previously unseen grippers. The strength of\nthis representation lies in the ability to map palm and fingers of a gripper\nand the unified coordinate space. Grasp synthesis is formulated as predicting\nthe unified spherical coordinates on object surface points via a conditional\nvariational autoencoder. The predicted unified gripper coordinates establish\nexact correspondences between the gripper and object points, which is used to\noptimize grasp pose and joint values. Grasp transfer is facilitated through the\npoint-to-point correspondence between any two (potentially unseen) grippers and\nsolved via a similar optimization. Extensive simulation and real-world\nexperiments showcase the efficacy of the unified grasp representation for grasp\nsynthesis in generating stable and diverse grasps. Similarly, we showcase\nreal-world grasp transfer from human demonstrations across different objects.\n","authors":["Ninad Khargonkar","Luis Felipe Casas","Balakrishnan Prabhakaran","Yu Xiang"],"pdf_url":"https://arxiv.org/pdf/2409.14519v2.pdf","comment":"8 pages, 11 figures, 3 tables. Project page available at\n  https://irvlutd.github.io/RobotFingerPrint"},{"id":"http://arxiv.org/abs/2410.01417v2","updated":"2025-03-03T00:41:36Z","published":"2024-10-02T10:58:54Z","title":"The Labyrinth of Links: Navigating the Associative Maze of Multi-modal\n  LLMs","summary":"  Multi-modal Large Language Models (MLLMs) have exhibited impressive\ncapability. However, recently many deficiencies of MLLMs have been found\ncompared to human intelligence, $\\textit{e.g.}$, hallucination. To drive the\nMLLMs study, the community dedicated efforts to building larger benchmarks with\ncomplex tasks. In this paper, we propose benchmarking an essential but usually\noverlooked intelligence: $\\textbf{association}$, a human's basic capability to\nlink observation and prior practice memory. To comprehensively investigate\nMLLM's performance on the association, we formulate the association task and\ndevise a standard benchmark based on adjective and verb semantic concepts.\nInstead of costly data annotation and curation, we propose a convenient\n$\\textbf{annotation-free}$ construction method transforming the general dataset\nfor our association tasks. Simultaneously, we devise a rigorous data refinement\nprocess to eliminate confusion in the raw dataset. Building on this database,\nwe establish three levels of association tasks: single-step, synchronous, and\nasynchronous associations. Moreover, we conduct a comprehensive investigation\ninto the MLLMs' zero-shot association capabilities, addressing multiple\ndimensions, including three distinct memory strategies, both open-source and\nclosed-source MLLMs, cutting-edge Mixture-of-Experts (MoE) models, and the\ninvolvement of human experts. Our systematic investigation shows that current\nopen-source MLLMs consistently exhibit poor capability in our association\ntasks, even the currently state-of-the-art GPT-4V(vision) also has a\nsignificant gap compared to humans. We believe our benchmark would pave the way\nfor future MLLM studies. $\\textit{Our data and code are available at:}$\nhttps://mvig-rhos.com/llm_inception.\n","authors":["Hong Li","Nanxi Li","Yuanjie Chen","Jianbin Zhu","Qinlu Guo","Cewu Lu","Yong-Lu Li"],"pdf_url":"https://arxiv.org/pdf/2410.01417v2.pdf","comment":"Accepted by ICLR 2025. Project page:\n  https://mvig-rhos.com/llm_inception"},{"id":"http://arxiv.org/abs/2409.04607v2","updated":"2025-03-03T00:20:29Z","published":"2024-09-06T20:32:53Z","title":"Self-Supervised Contrastive Learning for Videos using Differentiable\n  Local Alignment","summary":"  Robust frame-wise embeddings are essential to perform video analysis and\nunderstanding tasks. We present a self-supervised method for representation\nlearning based on aligning temporal video sequences. Our framework uses a\ntransformer-based encoder to extract frame-level features and leverages them to\nfind the optimal alignment path between video sequences. We introduce the novel\nLocal-Alignment Contrastive (LAC) loss, which combines a differentiable local\nalignment loss to capture local temporal dependencies with a contrastive loss\nto enhance discriminative learning. Prior works on video alignment have focused\non using global temporal ordering across sequence pairs, whereas our loss\nencourages identifying the best-scoring subsequence alignment. LAC uses the\ndifferentiable Smith-Waterman (SW) affine method, which features a flexible\nparameterization learned through the training phase, enabling the model to\nadjust the temporal gap penalty length dynamically. Evaluations show that our\nlearned representations outperform existing state-of-the-art approaches on\naction recognition tasks.\n","authors":["Keyne Oei","Amr Gomaa","Anna Maria Feit","João Belo"],"pdf_url":"https://arxiv.org/pdf/2409.04607v2.pdf","comment":"Accepted in 2nd Workshop on Video Understanding and its Applications,\n  held in conjunction with the British Machine Vision Conference (BMVC) 2024"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2502.18858v2","updated":"2025-03-03T13:38:50Z","published":"2025-02-26T05:59:45Z","title":"Evaluating Intelligence via Trial and Error","summary":"  Intelligence is a crucial trait for species to find solutions within a\nlimited number of trial-and-error attempts. Building on this idea, we introduce\nSurvival Game as a framework to evaluate intelligence based on the number of\nfailed attempts in a trial-and-error process. Fewer failures indicate higher\nintelligence. When the expectation and variance of failure counts are both\nfinite, it signals the ability to consistently find solutions to new\nchallenges, which we define as the Autonomous Level of intelligence. Using\nSurvival Game, we comprehensively evaluate existing AI systems. Our results\nshow that while AI systems achieve the Autonomous Level in simple tasks, they\nare still far from it in more complex tasks, such as vision, search,\nrecommendation, and language. While scaling current AI technologies might help,\nthis would come at an astronomical cost. Projections suggest that achieving the\nAutonomous Level for general tasks would require $10^{26}$ parameters. To put\nthis into perspective, loading such a massive model requires so many H100 GPUs\nthat their total value is $10^{7}$ times that of Apple Inc.'s market value.\nEven with Moore's Law, supporting such a parameter scale would take $70$ years.\nThis staggering cost highlights the complexity of human tasks and the\ninadequacies of current AI technologies. To further investigate this\nphenomenon, we conduct a theoretical analysis of Survival Game and its\nexperimental results. Our findings suggest that human tasks possess a\ncriticality property. As a result, Autonomous Level requires a deep\nunderstanding of the task's underlying mechanisms. Current AI systems, however,\ndo not fully grasp these mechanisms and instead rely on superficial mimicry,\nmaking it difficult for them to reach an autonomous level. We believe Survival\nGame can not only guide the future development of AI but also offer profound\ninsights into human intelligence.\n","authors":["Jingtao Zhan","Jiahao Zhao","Jiayu Li","Yiqun Liu","Bo Zhang","Qingyao Ai","Jiaxin Mao","Hongning Wang","Min Zhang","Shaoping Ma"],"pdf_url":"https://arxiv.org/pdf/2502.18858v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07596v2","updated":"2025-03-03T13:27:01Z","published":"2025-01-10T01:42:43Z","title":"Optimize Incompatible Parameters through Compatibility-aware Knowledge\n  Integration","summary":"  Deep neural networks have become foundational to advancements in multiple\ndomains, including recommendation systems, natural language processing, and so\non. Despite their successes, these models often contain incompatible parameters\nthat can be underutilized or detrimental to model performance, particularly\nwhen faced with specific, varying data distributions. Existing research excels\nin removing such parameters or merging the outputs of multiple different\npretrained models. However, the former focuses on efficiency rather than\nperformance, while the latter requires several times more computing and storage\nresources to support inference. In this paper, we set the goal to explicitly\nimprove these incompatible parameters by leveraging the complementary strengths\nof different models, thereby directly enhancing the models without any\nadditional parameters. Specifically, we propose Compatibility-aware Knowledge\nIntegration (CKI), which consists of Parameter Compatibility Assessment and\nParameter Splicing, which are used to evaluate the knowledge content of\nmultiple models and integrate the knowledge into one model, respectively. The\nintegrated model can be used directly for inference or for further fine-tuning.\nWe conduct extensive experiments on various datasets for recommendation and\nlanguage tasks, and the results show that Compatibility-aware Knowledge\nIntegration can effectively optimize incompatible parameters under multiple\ntasks and settings to break through the training limit of the original model\nwithout increasing the inference cost.\n","authors":["Zheqi Lv","Keming Ye","Zishu Wei","Qi Tian","Shengyu Zhang","Wenqiao Zhang","Wenjie Wang","Kun Kuang","Tat-Seng Chua","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2501.07596v2.pdf","comment":"Published on AAAI'25(Oral): The Annual AAAI Conference on Artificial\n  Intelligence"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2502.06786v3","updated":"2025-03-03T17:54:53Z","published":"2025-02-10T18:59:10Z","title":"Matryoshka Quantization","summary":"  Quantizing model weights is critical for reducing the communication and\ninference costs of large models. However, quantizing models -- especially to\nlow precisions like int4 or int2 -- requires a trade-off in model quality;\nint2, in particular, is known to severely degrade model quality. Consequently,\npractitioners are often forced to maintain multiple models with different\nquantization levels or serve a single model that best satisfies the\nquality-latency trade-off. On the other hand, integer data types, such as int8,\ninherently possess a nested (Matryoshka) structure where smaller bit-width\nintegers, like int4 or int2, are nested within the most significant bits.\nLeveraging this insight, in this paper, we propose Matryoshka Quantization\n(MatQuant), a novel multi-scale quantization technique that alleviates the\naforementioned challenge. This technique allows us to train and maintain a\nsingle quantized model but serve it with the precision demanded by the\ndeployment. Furthermore, leveraging MatQuant's co-training and co-distillation\nregularization, int2 precision models extracted by MatQuant outperform standard\nint2 quantization by up to to 4% and 7% with OmniQuant and QAT as base\nalgorithms respectively. Finally, we demonstrate that by using an extra bit to\nrepresent outliers, a model with an effective precision of 2.05-bit gives an\nadditional 6% improvement with OmniQuant as the base algorithm.\n","authors":["Pranav Nair","Puranjay Datta","Jeff Dean","Prateek Jain","Aditya Kusupati"],"pdf_url":"https://arxiv.org/pdf/2502.06786v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15823v3","updated":"2025-03-03T16:38:10Z","published":"2025-02-20T03:48:00Z","title":"InductionBench: LLMs Fail in the Simplest Complexity Class","summary":"  Large language models (LLMs) have shown remarkable improvements in reasoning\nand many existing benchmarks have been addressed by models such as o1 and o3\neither fully or partially. However, a majority of these benchmarks emphasize\ndeductive reasoning, including mathematical and coding tasks in which rules\nsuch as mathematical axioms or programming syntax are clearly defined, based on\nwhich LLMs can plan and apply these rules to arrive at a solution. In contrast,\ninductive reasoning, where one infers the underlying rules from observed data,\nremains less explored. Such inductive processes lie at the heart of scientific\ndiscovery, as they enable researchers to extract general principles from\nempirical observations. To assess whether LLMs possess this capacity, we\nintroduce InductionBench, a new benchmark designed to evaluate the inductive\nreasoning ability of LLMs. Our experimental findings reveal that even the most\nadvanced models available struggle to master the simplest complexity classes\nwithin the subregular hierarchy of functions, highlighting a notable deficiency\nin current LLMs' inductive reasoning capabilities. Coda and data are available\nhttps://github.com/Wenyueh/inductive_reasoning_benchmark.\n","authors":["Wenyue Hua","Tyler Wong","Sun Fei","Liangming Pan","Adam Jardine","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2502.15823v3.pdf","comment":"24 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.22371v2","updated":"2025-03-03T16:16:00Z","published":"2024-10-28T23:25:55Z","title":"Error Bounds for Physics-Informed Neural Networks in Fokker-Planck PDEs","summary":"  Stochastic differential equations are commonly used to describe the evolution\nof stochastic processes. The state uncertainty of such processes is best\nrepresented by the probability density function (PDF), whose evolution is\ngoverned by the Fokker-Planck partial differential equation (FP-PDE). However,\nit is generally infeasible to solve the FP-PDE in closed form. In this work, we\nshow that physics-informed neural networks (PINNs) can be trained to\napproximate the solution PDF. Our main contribution is the analysis of PINN\napproximation error: we develop a theoretical framework to construct tight\nerror bounds using PINNs. In addition, we derive a practical error bound that\ncan be efficiently constructed with standard training methods. We discuss that\nthis error-bound framework generalizes to approximate solutions of other linear\nPDEs. Empirical results on nonlinear, high-dimensional, and chaotic systems\nvalidate the correctness of our error bounds while demonstrating the\nscalability of PINNs and their significant computational speedup in obtaining\naccurate PDF solutions compared to the Monte Carlo approach.\n","authors":["Chun-Wei Kong","Luca Laurenti","Jay McMahon","Morteza Lahijanian"],"pdf_url":"https://arxiv.org/pdf/2410.22371v2.pdf","comment":"paper under review"},{"id":"http://arxiv.org/abs/2502.18821v2","updated":"2025-03-03T16:12:50Z","published":"2025-02-26T04:52:31Z","title":"CAMEx: Curvature-aware Merging of Experts","summary":"  Existing methods for merging experts during model training and fine-tuning\npredominantly rely on Euclidean geometry, which assumes a flat parameter space.\nThis assumption can limit the model's generalization ability, especially during\nthe pre-training phase, where the parameter manifold might exhibit more complex\ncurvature. Curvature-aware merging methods typically require additional\ninformation and computational resources to approximate the Fisher Information\nMatrix, adding memory overhead. In this paper, we introduce CAMEx\n(Curvature-Aware Merging of Experts), a novel expert merging protocol that\nincorporates natural gradients to account for the non-Euclidean curvature of\nthe parameter manifold. By leveraging natural gradients, CAMEx adapts more\neffectively to the structure of the parameter space, improving alignment\nbetween model updates and the manifold's geometry. This approach enhances both\npre-training and fine-tuning, resulting in better optimization trajectories and\nimproved generalization without the substantial memory overhead typically\nassociated with curvature-aware methods. Our contributions are threefold: (1)\nCAMEx significantly outperforms traditional Euclidean-based expert merging\ntechniques across various natural language processing tasks, leading to\nenhanced performance during pre-training and fine-tuning; (2) we introduce a\ndynamic merging architecture that optimizes resource utilization, achieving\nhigh performance while reducing computational costs, facilitating efficient\nscaling of large language models; and (3) we provide both theoretical and\nempirical evidence to demonstrate the efficiency of our proposed method. The\ncode is publicly available at: https://github.com/kpup1710/CAMEx.\n","authors":["Dung V. Nguyen","Minh H. Nguyen","Luc Q. Nguyen","Rachel S. Y. Teo","Tan M. Nguyen","Linh Duy Tran"],"pdf_url":"https://arxiv.org/pdf/2502.18821v2.pdf","comment":"10 pages, 5 Figures, 7 Tables. Published at ICLR 2025"},{"id":"http://arxiv.org/abs/2412.19495v2","updated":"2025-03-03T16:05:29Z","published":"2024-12-27T07:31:14Z","title":"Disparate Model Performance and Stability in Machine Learning Clinical\n  Support for Diabetes and Heart Diseases","summary":"  Machine Learning (ML) algorithms are vital for supporting clinical\ndecision-making in biomedical informatics. However, their predictive\nperformance can vary across demographic groups, often due to the\nunderrepresentation of historically marginalized populations in training\ndatasets. The investigation reveals widespread sex- and age-related inequities\nin chronic disease datasets and their derived ML models. Thus, a novel\nanalytical framework is introduced, combining systematic arbitrariness with\ntraditional metrics like accuracy and data complexity. The analysis of data\nfrom over 25,000 individuals with chronic diseases revealed mild sex-related\ndisparities, favoring predictive accuracy for males, and significant\nage-related differences, with better accuracy for younger patients. Notably,\nolder patients showed inconsistent predictive accuracy across seven datasets,\nlinked to higher data complexity and lower model performance. This highlights\nthat representativeness in training data alone does not guarantee equitable\noutcomes, and model arbitrariness must be addressed before deploying models in\nclinical settings.\n","authors":["Ioannis Bilionis","Ricardo C. Berrios","Luis Fernandez-Luque","Carlos Castillo"],"pdf_url":"https://arxiv.org/pdf/2412.19495v2.pdf","comment":"This paper will be presented in American Medical Informatics\n  Association (AMIA) Informatics Summit Conference 2025 (Pittsburgh, PA). 10\n  pages, 2 figures, 5 tables"},{"id":"http://arxiv.org/abs/2501.11972v2","updated":"2025-03-03T15:45:44Z","published":"2025-01-21T08:34:10Z","title":"\"FRAME: Forward Recursive Adaptive Model Extraction-A Technique for\n  Advance Feature Selection\"","summary":"  The challenges in feature selection, particularly in balancing model\naccuracy, interpretability, and computational efficiency, remain a critical\nissue in advancing machine learning methodologies. To address these\ncomplexities, this study introduces a novel hybrid approach, the Forward\nRecursive Adaptive Model Extraction Technique (FRAME), which combines Forward\nSelection and Recursive Feature Elimination (RFE) to enhance feature selection\nacross diverse datasets. By combining the exploratory capabilities of Forward\nSelection with the refinement strengths of RFE, FRAME systematically identifies\noptimal feature subsets, striking a harmonious trade-off between\nexperimentation and precision. A comprehensive evaluation of FRAME is conducted\nagainst traditional methods such as SelectKBest and Lasso Regression, using\nhigh-dimensional, noisy, and heterogeneous datasets. The results demonstrate\nthat FRAME consistently delivers superior predictive performance based on\ndownstream machine learning evaluation metrics. It efficiently performs\ndimensionality reduction with strong model performance, thus being especially\nuseful for applications that need interpretable and accurate predictions, e.g.,\nbiomedical diagnostics.\n  This research emphasizes the need to evaluate feature selection techniques on\ndiverse datasets to test their robustness and generalizability. The results\nindicate that FRAME has great potential for further development, especially by\nincorporating deep learning frameworks for adaptive and real-time feature\nselection in dynamic settings. By advancing feature selection methodologies,\nFRAME offers a practical and effective solution to improve machine learning\napplications across multiple domains.\n","authors":["Nachiket Kapure","Harsh Joshi","Parul Kumari","Rajeshwari Mistri","Manasi Mali"],"pdf_url":"https://arxiv.org/pdf/2501.11972v2.pdf","comment":"Updated version with refinements before JMLR submission. Improved\n  clarity, expanded literature review, refined methodology, updated\n  experimental results, and enhanced conclusion. FRAME's scalability, deep\n  learning integration, and real-world applications are further highlighted"},{"id":"http://arxiv.org/abs/2412.14663v2","updated":"2025-03-03T15:32:17Z","published":"2024-12-19T09:14:24Z","title":"IOHunter: Graph Foundation Model to Uncover Online Information\n  Operations","summary":"  Social media platforms have become vital spaces for public discourse, serving\nas modern agor\\`as where a wide range of voices influence societal narratives.\nHowever, their open nature also makes them vulnerable to exploitation by\nmalicious actors, including state-sponsored entities, who can conduct\ninformation operations (IOs) to manipulate public opinion. The spread of\nmisinformation, false news, and misleading claims threatens democratic\nprocesses and societal cohesion, making it crucial to develop methods for the\ntimely detection of inauthentic activity to protect the integrity of online\ndiscourse. In this work, we introduce a methodology designed to identify users\norchestrating information operations, a.k.a. IO drivers, across various\ninfluence campaigns. Our framework, named IOHunter, leverages the combined\nstrengths of Language Models and Graph Neural Networks to improve\ngeneralization in supervised, scarcely-supervised, and cross-IO contexts. Our\napproach achieves state-of-the-art performance across multiple sets of IOs\noriginating from six countries, significantly surpassing existing approaches.\nThis research marks a step toward developing Graph Foundation Models\nspecifically tailored for the task of IO detection on social media platforms.\n","authors":["Marco Minici","Luca Luceri","Francesco Fabbri","Emilio Ferrara"],"pdf_url":"https://arxiv.org/pdf/2412.14663v2.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2502.19210v2","updated":"2025-03-03T15:32:09Z","published":"2025-02-26T15:13:08Z","title":"Langevin Multiplicative Weights Update with Applications in Polynomial\n  Portfolio Management","summary":"  We consider nonconvex optimization problem over simplex, and more generally,\na product of simplices. We provide an algorithm, Langevin Multiplicative\nWeights Update (LMWU) for solving global optimization problems by adding a\nnoise scaling with the non-Euclidean geometry in the simplex. Non-convex\noptimization has been extensively studied by machine learning community due to\nits application in various scenarios such as neural network approximation and\nfinding Nash equilibrium. Despite recent progresses on provable guarantee of\nescaping and avoiding saddle point (convergence to local minima) and global\nconvergence of Langevin gradient based method without constraints, the global\noptimization with constraints is less studied. We show that LMWU algorithm is\nprovably convergent to interior global minima with a non-asymptotic convergence\nanalysis. We verify the efficiency of the proposed algorithm in real data set\nfrom polynomial portfolio management, where optimization of a highly non-linear\nobjective function plays a crucial role.\n","authors":["Yi Feng","Xiao Wang","Tian Xie"],"pdf_url":"https://arxiv.org/pdf/2502.19210v2.pdf","comment":"Accepted for AAAI-2025"},{"id":"http://arxiv.org/abs/2502.12215v2","updated":"2025-03-03T15:29:43Z","published":"2025-02-17T07:21:11Z","title":"Revisiting the Test-Time Scaling of o1-like Models: Do they Truly\n  Possess Test-Time Scaling Capabilities?","summary":"  The advent of test-time scaling in large language models (LLMs), exemplified\nby OpenAI's o1 series, has advanced reasoning capabilities by scaling\ncomputational resource allocation during inference. While successors like QwQ,\nDeepseek-R1 (R1) and LIMO replicate these advancements, whether these models\ntruly possess test-time scaling capabilities remains underexplored. This study\nfound that longer CoTs of these o1-like models do not consistently enhance\naccuracy; in fact, correct solutions are often shorter than incorrect ones for\nthe same questions. Further investigation shows this phenomenon is closely\nrelated to models' self-revision capabilities - longer CoTs contain more\nself-revisions, which often lead to performance degradation. We then compare\nsequential and parallel scaling strategies on QwQ, R1 and LIMO, finding that\nparallel scaling achieves better coverage and scalability. Based on these\ninsights, we propose Shortest Majority Vote, a method that combines parallel\nscaling strategies with CoT length characteristics, significantly improving\nmodels' test-time scalability compared to conventional majority voting\napproaches.\n","authors":["Zhiyuan Zeng","Qinyuan Cheng","Zhangyue Yin","Yunhua Zhou","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2502.12215v2.pdf","comment":"Add the github link"},{"id":"http://arxiv.org/abs/2501.10945v2","updated":"2025-03-03T15:09:31Z","published":"2025-01-19T04:56:55Z","title":"Gradient-Based Multi-Objective Deep Learning: Algorithms, Theories,\n  Applications, and Beyond","summary":"  Multi-objective optimization (MOO) in deep learning aims to simultaneously\noptimize multiple conflicting objectives, a challenge frequently encountered in\nareas like multi-task learning and multi-criteria learning. Recent advancements\nin gradient-based MOO methods have enabled the discovery of diverse types of\nsolutions, ranging from a single balanced solution to finite or even infinite\nPareto sets, tailored to user needs. These developments have broad applications\nacross domains such as reinforcement learning, computer vision, recommendation\nsystems, and large language models. This survey provides the first\ncomprehensive review of gradient-based MOO in deep learning, covering\nalgorithms, theories, and practical applications. By unifying various\napproaches and identifying critical challenges, it serves as a foundational\nresource for driving innovation in this evolving field. A comprehensive list of\nMOO algorithms in deep learning is available at\nhttps://github.com/Baijiong-Lin/Awesome-Multi-Objective-Deep-Learning.\n","authors":["Weiyu Chen","Xiaoyuan Zhang","Baijiong Lin","Xi Lin","Han Zhao","Qingfu Zhang","James T. Kwok"],"pdf_url":"https://arxiv.org/pdf/2501.10945v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11561v2","updated":"2025-03-03T15:04:03Z","published":"2024-08-21T12:15:20Z","title":"Self-Supervised Iterative Refinement for Anomaly Detection in Industrial\n  Quality Control","summary":"  This study introduces the Iterative Refinement Process (IRP), a robust\nanomaly detection methodology designed for high-stakes industrial quality\ncontrol. The IRP enhances defect detection accuracy through a cyclic data\nrefinement strategy, iteratively removing misleading data points to improve\nmodel performance and robustness. We validate the IRP's effectiveness using two\nbenchmark datasets, Kolektor SDD2 (KSDD2) and MVTec AD, covering a wide range\nof industrial products and defect types. Our experimental results demonstrate\nthat the IRP consistently outperforms traditional anomaly detection models,\nparticularly in environments with high noise levels. This study highlights the\nIRP's potential to significantly enhance anomaly detection processes in\nindustrial settings, effectively managing the challenges of sparse and noisy\ndata.\n","authors":["Muhammad Aqeel","Shakiba Sharifi","Marco Cristani","Francesco Setti"],"pdf_url":"https://arxiv.org/pdf/2408.11561v2.pdf","comment":"Accepted to VISAPP 2025"},{"id":"http://arxiv.org/abs/2502.10784v2","updated":"2025-03-03T15:02:37Z","published":"2025-02-15T12:28:51Z","title":"Preconditioned Inexact Stochastic ADMM for Deep Model","summary":"  The recent advancement of foundation models (FMs) has brought about a\nparadigm shift, revolutionizing various sectors worldwide. The popular\noptimizers used to train these models are stochastic gradient descent-based\nalgorithms, which face inherent limitations, such as slow convergence and\nstringent assumptions for convergence. In particular, data heterogeneity\narising from distributed settings poses significant challenges to their\ntheoretical and numerical performance. This paper develops an algorithm, PISA\n({P}reconditioned {I}nexact {S}tochastic {A}lternating Direction Method of\nMultipliers), which enables scalable parallel computing and supports various\nsecond-moment schemes. Grounded in rigorous theoretical guarantees, the\nalgorithm converges under the sole assumption of Lipschitz continuity of the\ngradient, thereby removing the need for other conditions commonly imposed by\nstochastic methods. This capability enables PISA to tackle the challenge of\ndata heterogeneity effectively. Comprehensive experimental evaluations for\ntraining or fine-tuning diverse FMs, including vision models, large language\nmodels, reinforcement learning models, generative adversarial networks, and\nrecurrent neural networks, demonstrate its superior numerical performance\ncompared to various state-of-the-art optimizers.\n","authors":["Shenglong Zhou","Ouya Wang","Ziyan Luo","Yongxu Zhu","Geoffrey Ye Li"],"pdf_url":"https://arxiv.org/pdf/2502.10784v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07180v4","updated":"2025-03-03T14:56:17Z","published":"2024-11-11T17:57:30Z","title":"Gumbel Counterfactual Generation From Language Models","summary":"  Understanding and manipulating the causal generation mechanisms in language\nmodels is essential for controlling their behavior. Previous work has primarily\nrelied on techniques such as representation surgery -- e.g., model ablations or\nmanipulation of linear subspaces tied to specific concepts -- to\n\\emph{intervene} on these models. To understand the impact of interventions\nprecisely, it is useful to examine \\emph{counterfactuals} -- e.g., how a given\nsentence would have appeared had it been generated by the model following a\nspecific intervention. We highlight that counterfactual reasoning is\nconceptually distinct from interventions, as articulated in Pearl's causal\nhierarchy. Based on this observation, we propose a framework for generating\ntrue string counterfactuals by reformulating language models as a structural\nequation model using the Gumbel-max trick, which we called Gumbel\ncounterfactual generation. This reformulation allows us to model the joint\ndistribution over original strings and their counterfactuals resulting from the\nsame instantiation of the sampling noise. We develop an algorithm based on\nhindsight Gumbel sampling that allows us to infer the latent noise variables\nand generate counterfactuals of observed strings. Our experiments demonstrate\nthat the approach produces meaningful counterfactuals while at the same time\nshowing that commonly used intervention techniques have considerable undesired\nside effects.\n","authors":["Shauli Ravfogel","Anej Svete","Vésteinn Snæbjarnarson","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2411.07180v4.pdf","comment":"Accepted in ICLR 2025"},{"id":"http://arxiv.org/abs/2409.13533v2","updated":"2025-03-03T14:40:34Z","published":"2024-09-20T14:23:05Z","title":"Using High-Level Patterns to Estimate How Humans Predict a Robot will\n  Behave","summary":"  Humans interacting with robots often form predictions of what the robot will\ndo next. For instance, based on the recent behavior of an autonomous car, a\nnearby human driver might predict that the car is going to remain in the same\nlane. It is important for the robot to understand the human's prediction for\nsafe and seamless interaction: e.g., if the autonomous car knows the human\nthinks it is not merging -- but the autonomous car actually intends to merge --\nthen the car can adjust its behavior to prevent an accident. Prior works\ntypically assume that humans make precise predictions of robot behavior.\nHowever, recent research on human-human prediction suggests the opposite:\nhumans tend to approximate other agents by predicting their high-level\nbehaviors. We apply this finding to develop a second-order theory of mind\napproach that enables robots to estimate how humans predict they will behave.\nTo extract these high-level predictions directly from data, we embed the recent\nhuman and robot trajectories into a discrete latent space. Each element of this\nlatent space captures a different type of behavior (e.g., merging in front of\nthe human, remaining in the same lane) and decodes into a vector field across\nthe state space that is consistent with the underlying behavior type. We\nhypothesize that our resulting high-level and course predictions of robot\nbehavior will correspond to actual human predictions. We provide initial\nevidence in support of this hypothesis through proof-of-concept simulations,\ntesting our method's predictions against those of real users, and experiments\non a real-world interactive driving dataset.\n","authors":["Sagar Parekh","Lauren Bramblett","Nicola Bezzo","Dylan P. Losey"],"pdf_url":"https://arxiv.org/pdf/2409.13533v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23208v2","updated":"2025-03-03T14:29:16Z","published":"2024-10-30T16:59:41Z","title":"Kinetix: Investigating the Training of General Agents through Open-Ended\n  Physics-Based Control Tasks","summary":"  While large models trained with self-supervised learning on offline datasets\nhave shown remarkable capabilities in text and image domains, achieving the\nsame generalisation for agents that act in sequential decision problems remains\nan open challenge. In this work, we take a step towards this goal by\nprocedurally generating tens of millions of 2D physics-based tasks and using\nthese to train a general reinforcement learning (RL) agent for physical\ncontrol. To this end, we introduce Kinetix: an open-ended space of\nphysics-based RL environments that can represent tasks ranging from robotic\nlocomotion and grasping to video games and classic RL environments, all within\na unified framework. Kinetix makes use of our novel hardware-accelerated\nphysics engine Jax2D that allows us to cheaply simulate billions of environment\nsteps during training. Our trained agent exhibits strong physical reasoning\ncapabilities in 2D space, being able to zero-shot solve unseen human-designed\nenvironments. Furthermore, fine-tuning this general agent on tasks of interest\nshows significantly stronger performance than training an RL agent *tabula\nrasa*. This includes solving some environments that standard RL training\ncompletely fails at. We believe this demonstrates the feasibility of large\nscale, mixed-quality pre-training for online RL and we hope that Kinetix will\nserve as a useful framework to investigate this further.\n","authors":["Michael Matthews","Michael Beukman","Chris Lu","Jakob Foerster"],"pdf_url":"https://arxiv.org/pdf/2410.23208v2.pdf","comment":"ICLR 2025 Oral. The first two authors contributed equally. Project\n  page located at: https://kinetix-env.github.io/"},{"id":"http://arxiv.org/abs/2407.11734v2","updated":"2025-03-03T14:24:06Z","published":"2024-07-16T14:05:03Z","title":"Multi-Modal and Multi-Attribute Generation of Single Cells with CFGen","summary":"  Generative modeling of single-cell RNA-seq data is crucial for tasks like\ntrajectory inference, batch effect removal, and simulation of realistic\ncellular data. However, recent deep generative models simulating synthetic\nsingle cells from noise operate on pre-processed continuous gene expression\napproximations, overlooking the discrete nature of single-cell data, which\nlimits their effectiveness and hinders the incorporation of robust noise\nmodels. Additionally, aspects like controllable multi-modal and multi-label\ngeneration of cellular data remain underexplored. This work introduces CellFlow\nfor Generation (CFGen), a flow-based conditional generative model that\npreserves the inherent discreteness of single-cell data. CFGen generates\nwhole-genome multi-modal single-cell data reliably, improving the recovery of\ncrucial biological data characteristics while tackling relevant generative\ntasks such as rare cell type augmentation and batch correction. We also\nintroduce a novel framework for compositional data generation using Flow\nMatching. By showcasing CFGen on a diverse set of biological datasets and\nsettings, we provide evidence of its value to the fields of computational\nbiology and deep generative models.\n","authors":["Alessandro Palma","Till Richter","Hanyi Zhang","Manuel Lubetzki","Alexander Tong","Andrea Dittadi","Fabian Theis"],"pdf_url":"https://arxiv.org/pdf/2407.11734v2.pdf","comment":"41 pages, 22 figures"},{"id":"http://arxiv.org/abs/2406.00987v2","updated":"2025-03-03T14:14:00Z","published":"2024-06-03T04:48:45Z","title":"Enhancing Fairness in Unsupervised Graph Anomaly Detection through\n  Disentanglement","summary":"  Graph anomaly detection (GAD) is increasingly crucial in various\napplications, ranging from financial fraud detection to fake news detection.\nHowever, current GAD methods largely overlook the fairness problem, which might\nresult in discriminatory decisions skewed toward certain demographic groups\ndefined on sensitive attributes (e.g., gender, religion, ethnicity, etc.). This\ngreatly limits the applicability of these methods in real-world scenarios in\nlight of societal and ethical restrictions. To address this critical gap, we\nmake the first attempt to integrate fairness with utility in GAD\ndecision-making. Specifically, we devise a novel DisEntangle-based\nFairnEss-aware aNomaly Detection framework on the attributed graph, named\nDEFEND. DEFEND first introduces disentanglement in GNNs to capture informative\nyet sensitive-irrelevant node representations, effectively reducing societal\nbias inherent in graph representation learning. Besides, to alleviate\ndiscriminatory bias in evaluating anomalous nodes, DEFEND adopts a\nreconstruction-based anomaly detection, which concentrates solely on node\nattributes without incorporating any graph structure. Additionally, given the\ninherent association between input and sensitive attributes, DEFEND constrains\nthe correlation between the reconstruction error and the predicted sensitive\nattributes. Our empirical evaluations on real-world datasets reveal that DEFEND\nperforms effectively in GAD and significantly enhances fairness compared to\nstate-of-the-art baselines. To foster reproducibility, our code is available at\nhttps://github.com/AhaChang/DEFEND.\n","authors":["Wenjing Chang","Kay Liu","Philip S. Yu","Jianjun Yu"],"pdf_url":"https://arxiv.org/pdf/2406.00987v2.pdf","comment":"Accepted to TMLR. Code available at\n  https://github.com/AhaChang/DEFEND"},{"id":"http://arxiv.org/abs/2410.15474v2","updated":"2025-03-03T14:08:48Z","published":"2024-10-20T19:12:14Z","title":"Optimizing Backward Policies in GFlowNets via Trajectory Likelihood\n  Maximization","summary":"  Generative Flow Networks (GFlowNets) are a family of generative models that\nlearn to sample objects with probabilities proportional to a given reward\nfunction. The key concept behind GFlowNets is the use of two stochastic\npolicies: a forward policy, which incrementally constructs compositional\nobjects, and a backward policy, which sequentially deconstructs them. Recent\nresults show a close relationship between GFlowNet training and\nentropy-regularized reinforcement learning (RL) problems with a particular\nreward design. However, this connection applies only in the setting of a fixed\nbackward policy, which might be a significant limitation. As a remedy to this\nproblem, we introduce a simple backward policy optimization algorithm that\ninvolves direct maximization of the value function in an entropy-regularized\nMarkov Decision Process (MDP) over intermediate rewards. We provide an\nextensive experimental evaluation of the proposed approach across various\nbenchmarks in combination with both RL and GFlowNet algorithms and demonstrate\nits faster convergence and mode discovery in complex environments.\n","authors":["Timofei Gritsaev","Nikita Morozov","Sergey Samsonov","Daniil Tiapkin"],"pdf_url":"https://arxiv.org/pdf/2410.15474v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2501.07596v2","updated":"2025-03-03T13:27:01Z","published":"2025-01-10T01:42:43Z","title":"Optimize Incompatible Parameters through Compatibility-aware Knowledge\n  Integration","summary":"  Deep neural networks have become foundational to advancements in multiple\ndomains, including recommendation systems, natural language processing, and so\non. Despite their successes, these models often contain incompatible parameters\nthat can be underutilized or detrimental to model performance, particularly\nwhen faced with specific, varying data distributions. Existing research excels\nin removing such parameters or merging the outputs of multiple different\npretrained models. However, the former focuses on efficiency rather than\nperformance, while the latter requires several times more computing and storage\nresources to support inference. In this paper, we set the goal to explicitly\nimprove these incompatible parameters by leveraging the complementary strengths\nof different models, thereby directly enhancing the models without any\nadditional parameters. Specifically, we propose Compatibility-aware Knowledge\nIntegration (CKI), which consists of Parameter Compatibility Assessment and\nParameter Splicing, which are used to evaluate the knowledge content of\nmultiple models and integrate the knowledge into one model, respectively. The\nintegrated model can be used directly for inference or for further fine-tuning.\nWe conduct extensive experiments on various datasets for recommendation and\nlanguage tasks, and the results show that Compatibility-aware Knowledge\nIntegration can effectively optimize incompatible parameters under multiple\ntasks and settings to break through the training limit of the original model\nwithout increasing the inference cost.\n","authors":["Zheqi Lv","Keming Ye","Zishu Wei","Qi Tian","Shengyu Zhang","Wenqiao Zhang","Wenjie Wang","Kun Kuang","Tat-Seng Chua","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2501.07596v2.pdf","comment":"Published on AAAI'25(Oral): The Annual AAAI Conference on Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2411.17711v2","updated":"2025-03-03T13:19:42Z","published":"2024-11-17T17:32:58Z","title":"AnyECG: Foundational Models for Multitask Cardiac Analysis in Real-World\n  Settings","summary":"  Electrocardiogram (ECG), a non-invasive and affordable tool for cardiac\nmonitoring, is highly sensitive in detecting acute heart attacks. However, due\nto the lengthy nature of ECG recordings, numerous machine learning methods have\nbeen developed for automated heart disease detection to reduce human workload.\nDespite these efforts, performance remains suboptimal. A key obstacle is the\ninherent complexity of ECG data, which includes heterogeneity (e.g., varying\nsampling rates), high levels of noise, demographic-related pattern shifts, and\nintricate rhythm-event associations. To overcome these challenges, this paper\nintroduces AnyECG, a foundational model designed to extract robust\nrepresentations from any real-world ECG data. Specifically, a tailored ECG\nTokenizer encodes each fixed-duration ECG fragment into a token and, guided by\nproxy tasks, converts noisy, continuous ECG features into discrete, compact,\nand clinically meaningful local rhythm codes. These codes encapsulate basic\nmorphological, frequency, and demographic information (e.g., sex), effectively\nmitigating signal noise. We further pre-train the AnyECG to learn rhythmic\npattern associations across ECG tokens, enabling the capture of cardiac event\nsemantics. By being jointly pre-trained on diverse ECG data sources, AnyECG is\ncapable of generalizing across a wide range of downstream tasks where ECG\nsignals are recorded from various devices and scenarios. The experimental\nresults show that AnyECG achieves an average performance improvement of 6%\nacross four critical tasks-anomaly detection, arrhythmia classification,\ncorrupted lead generation, and ultra-long ECG recognition. AnyECG learns common\nECG rhythm from data and significantly outperforms state-of-the-art methods in\neach of these tasks.\n","authors":["Yue Wang","Xu Cao","Yaojun Hu","Haochao Ying","Hongxia Xu","Ruijia Wu","James Matthew Rehg","Jimeng Sun","Jian Wu","Jintai Chen"],"pdf_url":"https://arxiv.org/pdf/2411.17711v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05106v2","updated":"2025-03-03T13:18:55Z","published":"2024-10-07T15:02:48Z","title":"Nonasymptotic Analysis of Stochastic Gradient Descent with the\n  Richardson-Romberg Extrapolation","summary":"  We address the problem of solving strongly convex and smooth minimization\nproblems using stochastic gradient descent (SGD) algorithm with a constant step\nsize. Previous works suggested to combine the Polyak-Ruppert averaging\nprocedure with the Richardson-Romberg extrapolation to reduce the asymptotic\nbias of SGD at the expense of a mild increase of the variance. We significantly\nextend previous results by providing an expansion of the mean-squared error of\nthe resulting estimator with respect to the number of iterations $n$. We show\nthat the root mean-squared error can be decomposed into the sum of two terms: a\nleading one of order $\\mathcal{O}(n^{-1/2})$ with explicit dependence on a\nminimax-optimal asymptotic covariance matrix, and a second-order term of order\n$\\mathcal{O}(n^{-3/4})$, where the power $3/4$ is best known. We also extend\nthis result to the higher-order moment bounds. Our analysis relies on the\nproperties of the SGD iterates viewed as a time-homogeneous Markov chain. In\nparticular, we establish that this chain is geometrically ergodic with respect\nto a suitably defined weighted Wasserstein semimetric.\n","authors":["Marina Sheshukova","Denis Belomestny","Alain Durmus","Eric Moulines","Alexey Naumov","Sergey Samsonov"],"pdf_url":"https://arxiv.org/pdf/2410.05106v2.pdf","comment":"ICLR-2025, camera-ready version"},{"id":"http://arxiv.org/abs/2410.07076v4","updated":"2025-03-03T13:17:24Z","published":"2024-10-09T17:19:58Z","title":"MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry\n  Scientific Hypotheses","summary":"  Scientific discovery contributes largely to human society's prosperity, and\nrecent progress shows that LLMs could potentially catalyze this process.\nHowever, it is still unclear whether LLMs can discover novel and valid\nhypotheses in chemistry. In this work, we investigate this central research\nquestion: Can LLMs automatically discover novel and valid chemistry research\nhypotheses given only a chemistry research background (consisting of a research\nquestion and/or a background survey), without limitation on the domain of the\nresearch question? After extensive discussions with chemistry experts, we\npropose an assumption that a majority of chemistry hypotheses can be resulted\nfrom a research background and several inspirations. With this key insight, we\nbreak the central question into three smaller fundamental questions. In brief,\nthey are: (1) given a background question, whether LLMs can retrieve good\ninspirations; (2) with background and inspirations, whether LLMs can lead to\nhypothesis; and (3) whether LLMs can identify good hypotheses to rank them\nhigher. To investigate these questions, we construct a benchmark consisting of\n51 chemistry papers published in Nature, Science, or a similar level in 2024\n(all papers are only available online since 2024). Every paper is divided by\nchemistry PhD students into three components: background, inspirations, and\nhypothesis. The goal is to rediscover the hypothesis, given only the background\nand a large randomly selected chemistry literature corpus consisting the ground\ntruth inspiration papers, with LLMs trained with data up to 2023. We also\ndevelop an LLM-based multi-agent framework that leverages the assumption,\nconsisting of three stages reflecting the three smaller questions. The proposed\nmethod can rediscover many hypotheses with very high similarity with the ground\ntruth ones, covering the main innovations.\n","authors":["Zonglin Yang","Wanhao Liu","Ben Gao","Tong Xie","Yuqiang Li","Wanli Ouyang","Soujanya Poria","Erik Cambria","Dongzhan Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.07076v4.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2405.15273v4","updated":"2025-03-03T12:40:28Z","published":"2024-05-24T06:59:43Z","title":"Towards a General Time Series Anomaly Detector with Adaptive Bottlenecks\n  and Dual Adversarial Decoders","summary":"  Time series anomaly detection plays a vital role in a wide range of\napplications. Existing methods require training one specific model for each\ndataset, which exhibits limited generalization capability across different\ntarget datasets, hindering anomaly detection performance in various scenarios\nwith scarce training data. Aiming at this problem, we propose constructing a\ngeneral time series anomaly detection model, which is pre-trained on extensive\nmulti-domain datasets and can subsequently apply to a multitude of downstream\nscenarios. The significant divergence of time series data across different\ndomains presents two primary challenges in building such a general model: (1)\nmeeting the diverse requirements of appropriate information bottlenecks\ntailored to different datasets in one unified model, and (2) enabling\ndistinguishment between multiple normal and abnormal patterns, both are crucial\nfor effective anomaly detection in various target scenarios. To tackle these\ntwo challenges, we propose a General time series anomaly Detector with Adaptive\nBottlenecks and Dual Adversarial Decoders (DADA), which enables flexible\nselection of bottlenecks based on different data and explicitly enhances clear\ndifferentiation between normal and abnormal series. We conduct extensive\nexperiments on nine target datasets from different domains. After pre-training\non multi-domain data, DADA, serving as a zero-shot anomaly detector for these\ndatasets, still achieves competitive or even superior results compared to those\nmodels tailored to each specific dataset. The code is made available at\nhttps://github.com/decisionintelligence/DADA.\n","authors":["Qichao Shentu","Beibu Li","Kai Zhao","Yang Shu","Zhongwen Rao","Lujia Pan","Bin Yang","Chenjuan Guo"],"pdf_url":"https://arxiv.org/pdf/2405.15273v4.pdf","comment":"Accepted by the 13th International Conference on Learning\n  Representations (ICLR 2025)"},{"id":"http://arxiv.org/abs/2403.19243v4","updated":"2025-03-03T12:32:47Z","published":"2024-03-28T08:58:20Z","title":"Efficient Learning With Sine-Activated Low-rank Matrices","summary":"  Low-rank decomposition has emerged as a vital tool for enhancing parameter\nefficiency in neural network architectures, gaining traction across diverse\napplications in machine learning. These techniques significantly lower the\nnumber of parameters, striking a balance between compactness and performance.\nHowever, a common challenge has been the compromise between parameter\nefficiency and the accuracy of the model, where reduced parameters often lead\nto diminished accuracy compared to their full-rank counterparts. In this work,\nwe propose a novel theoretical framework that integrates a sinusoidal function\nwithin the low-rank decomposition process. This approach not only preserves the\nbenefits of the parameter efficiency characteristic of low-rank methods but\nalso increases the decomposition's rank, thereby enhancing model performance.\nOur method proves to be a plug in enhancement for existing low-rank models, as\nevidenced by its successful application in Vision Transformers (ViT), Large\nLanguage Models (LLMs), Neural Radiance Fields (NeRF) and 3D shape modelling.\n","authors":["Yiping Ji","Hemanth Saratchandran","Cameron Gordon","Zeyu Zhang","Simon Lucey"],"pdf_url":"https://arxiv.org/pdf/2403.19243v4.pdf","comment":"The first two authors contributed equally. Paper accepted at ICLR\n  2025"},{"id":"http://arxiv.org/abs/2412.16577v2","updated":"2025-03-03T12:21:35Z","published":"2024-12-21T10:52:56Z","title":"A Meta-Learning Approach to Bayesian Causal Discovery","summary":"  Discovering a unique causal structure is difficult due to both inherent\nidentifiability issues, and the consequences of finite data. As such,\nuncertainty over causal structures, such as those obtained from a Bayesian\nposterior, are often necessary for downstream tasks. Finding an accurate\napproximation to this posterior is challenging, due to the large number of\npossible causal graphs, as well as the difficulty in the subproblem of finding\nposteriors over the functional relationships of the causal edges. Recent works\nhave used meta-learning to view the problem of estimating the maximum\na-posteriori causal graph as supervised learning. Yet, these methods are\nlimited when estimating the full posterior as they fail to encode key\nproperties of the posterior, such as correlation between edges and permutation\nequivariance with respect to nodes. Further, these methods also cannot reliably\nsample from the posterior over causal structures. To address these limitations,\nwe propose a Bayesian meta learning model that allows for sampling causal\nstructures from the posterior and encodes these key properties. We compare our\nmeta-Bayesian causal discovery against existing Bayesian causal discovery\nmethods, demonstrating the advantages of directly learning a posterior over\ncausal structure.\n","authors":["Anish Dhir","Matthew Ashman","James Requeima","Mark van der Wilk"],"pdf_url":"https://arxiv.org/pdf/2412.16577v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08190v2","updated":"2025-03-03T12:18:29Z","published":"2024-10-10T17:57:29Z","title":"Poison-splat: Computation Cost Attack on 3D Gaussian Splatting","summary":"  3D Gaussian splatting (3DGS), known for its groundbreaking performance and\nefficiency, has become a dominant 3D representation and brought progress to\nmany 3D vision tasks. However, in this work, we reveal a significant security\nvulnerability that has been largely overlooked in 3DGS: the computation cost of\ntraining 3DGS could be maliciously tampered by poisoning the input data. By\ndeveloping an attack named Poison-splat, we reveal a novel attack surface where\nthe adversary can poison the input images to drastically increase the\ncomputation memory and time needed for 3DGS training, pushing the algorithm\ntowards its worst computation complexity. In extreme cases, the attack can even\nconsume all allocable memory, leading to a Denial-of-Service (DoS) that\ndisrupts servers, resulting in practical damages to real-world 3DGS service\nvendors. Such a computation cost attack is achieved by addressing a bi-level\noptimization problem through three tailored strategies: attack objective\napproximation, proxy model rendering, and optional constrained optimization.\nThese strategies not only ensure the effectiveness of our attack but also make\nit difficult to defend with simple defensive measures. We hope the revelation\nof this novel attack surface can spark attention to this crucial yet overlooked\nvulnerability of 3DGS systems. Our code is available at\nhttps://github.com/jiahaolu97/poison-splat .\n","authors":["Jiahao Lu","Yifan Zhang","Qiuhong Shen","Xinchao Wang","Shuicheng Yan"],"pdf_url":"https://arxiv.org/pdf/2410.08190v2.pdf","comment":"Accepted by ICLR 2025 as a spotlight paper"},{"id":"http://arxiv.org/abs/2410.00722v2","updated":"2025-03-03T12:18:16Z","published":"2024-10-01T14:13:05Z","title":"On the Geometry and Optimization of Polynomial Convolutional Networks","summary":"  We study convolutional neural networks with monomial activation functions.\nSpecifically, we prove that their parameterization map is regular and is an\nisomorphism almost everywhere, up to rescaling the filters. By leveraging on\ntools from algebraic geometry, we explore the geometric properties of the image\nin function space of this map - typically referred to as neuromanifold. In\nparticular, we compute the dimension and the degree of the neuromanifold, which\nmeasure the expressivity of the model, and describe its singularities.\nMoreover, for a generic large dataset, we derive an explicit formula that\nquantifies the number of critical points arising in the optimization of a\nregression loss.\n","authors":["Vahid Shahverdi","Giovanni Luca Marchetti","Kathlén Kohn"],"pdf_url":"https://arxiv.org/pdf/2410.00722v2.pdf","comment":"Accepted at AISTATS 2025"},{"id":"http://arxiv.org/abs/2410.12343v3","updated":"2025-03-03T12:15:38Z","published":"2024-10-16T08:04:57Z","title":"Federated Temporal Graph Clustering","summary":"  Temporal graph clustering is a complex task that involves discovering\nmeaningful structures in dynamic graphs where relationships and entities change\nover time. Existing methods typically require centralized data collection,\nwhich poses significant privacy and communication challenges. In this work, we\nintroduce a novel Federated Temporal Graph Clustering (FTGC) framework that\nenables decentralized training of graph neural networks (GNNs) across multiple\nclients, ensuring data privacy throughout the process. Our approach\nincorporates a temporal aggregation mechanism to effectively capture the\nevolution of graph structures over time and a federated optimization strategy\nto collaboratively learn high-quality clustering representations. By preserving\ndata privacy and reducing communication overhead, our framework achieves\ncompetitive performance on temporal graph datasets, making it a promising\nsolution for privacy-sensitive, real-world applications involving dynamic data.\n","authors":["Zihao Zhou","Yang Liu","Xianghong Xu","Qian Li"],"pdf_url":"https://arxiv.org/pdf/2410.12343v3.pdf","comment":"8 pages, 1 figure"},{"id":"http://arxiv.org/abs/2409.02143v2","updated":"2025-03-03T12:08:50Z","published":"2024-09-02T22:04:08Z","title":"MLOmics: Benchmark for Machine Learning on Cancer Multi-Omics Data","summary":"  Framing the investigation of diverse cancers as a machine learning problem\nhas recently shown significant potential in multi-omics analysis and cancer\nresearch. Empowering these successful machine learning models are the\nhigh-quality training datasets with sufficient data volume and adequate\npreprocessing. However, while there exist several public data portals including\nThe Cancer Genome Atlas (TCGA) multi-omics initiative or open-bases such as the\nLinkedOmics, these databases are not off-the-shelf for existing machine\nlearning models. In this paper we propose MLOmics, an open cancer multi-omics\nbenchmark aiming at serving better the development and evaluation of\nbioinformatics and machine learning models. MLOmics contains 8,314 patient\nsamples covering all 32 cancer types with four omics types, stratified\nfeatures, and extensive baselines. Complementary support for downstream\nanalysis and bio-knowledge linking are also included to support\ninterdisciplinary analysis.\n","authors":["Ziwei Yang","Rikuto Kotoge","Xihao Piao","Zheng Chen","Lingwei Zhu","Peng Gao","Yasuko Matsubara","Yasushi Sakurai","Jimeng Sun"],"pdf_url":"https://arxiv.org/pdf/2409.02143v2.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2403.08632v2","updated":"2025-03-03T12:01:27Z","published":"2024-03-13T15:46:37Z","title":"A Decade's Battle on Dataset Bias: Are We There Yet?","summary":"  We revisit the \"dataset classification\" experiment suggested by Torralba &\nEfros (2011) a decade ago, in the new era with large-scale, diverse, and\nhopefully less biased datasets as well as more capable neural network\narchitectures. Surprisingly, we observe that modern neural networks can achieve\nexcellent accuracy in classifying which dataset an image is from: e.g., we\nreport 84.7% accuracy on held-out validation data for the three-way\nclassification problem consisting of the YFCC, CC, and DataComp datasets. Our\nfurther experiments show that such a dataset classifier could learn semantic\nfeatures that are generalizable and transferable, which cannot be explained by\nmemorization. We hope our discovery will inspire the community to rethink\nissues involving dataset bias.\n","authors":["Zhuang Liu","Kaiming He"],"pdf_url":"https://arxiv.org/pdf/2403.08632v2.pdf","comment":"Published in ICLR 2025 (Oral Presentation)"},{"id":"http://arxiv.org/abs/2502.17941v2","updated":"2025-03-03T12:00:57Z","published":"2025-02-25T08:03:04Z","title":"Optimal Brain Apoptosis","summary":"  The increasing complexity and parameter count of Convolutional Neural\nNetworks (CNNs) and Transformers pose challenges in terms of computational\nefficiency and resource demands. Pruning has been identified as an effective\nstrategy to address these challenges by removing redundant elements such as\nneurons, channels, or connections, thereby enhancing computational efficiency\nwithout heavily compromising performance. This paper builds on the foundational\nwork of Optimal Brain Damage (OBD) by advancing the methodology of parameter\nimportance estimation using the Hessian matrix. Unlike previous approaches that\nrely on approximations, we introduce Optimal Brain Apoptosis (OBA), a novel\npruning method that calculates the Hessian-vector product value directly for\neach parameter. By decomposing the Hessian matrix across network layers and\nidentifying conditions under which inter-layer Hessian submatrices are\nnon-zero, we propose a highly efficient technique for computing the\nsecond-order Taylor expansion of parameters. This approach allows for a more\nprecise pruning process, particularly in the context of CNNs and Transformers,\nas validated in our experiments including VGG19, ResNet32, ResNet50, and\nViT-B/16 on CIFAR10, CIFAR100 and Imagenet datasets. Our code is available at\nhttps://github.com/NEU-REAL/OBA.\n","authors":["Mingyuan Sun","Zheng Fang","Jiaxu Wang","Junjie Jiang","Delei Kong","Chenming Hu","Yuetong Fang","Renjing Xu"],"pdf_url":"https://arxiv.org/pdf/2502.17941v2.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2403.02107v5","updated":"2025-03-03T11:48:55Z","published":"2024-03-04T15:07:33Z","title":"Iterated $Q$-Network: Beyond One-Step Bellman Updates in Deep\n  Reinforcement Learning","summary":"  The vast majority of Reinforcement Learning methods is largely impacted by\nthe computation effort and data requirements needed to obtain effective\nestimates of action-value functions, which in turn determine the quality of the\noverall performance and the sample-efficiency of the learning procedure.\nTypically, action-value functions are estimated through an iterative scheme\nthat alternates the application of an empirical approximation of the Bellman\noperator and a subsequent projection step onto a considered function space. It\nhas been observed that this scheme can be potentially generalized to carry out\nmultiple iterations of the Bellman operator at once, benefiting the underlying\nlearning algorithm. However, till now, it has been challenging to effectively\nimplement this idea, especially in high-dimensional problems. In this paper, we\nintroduce iterated $Q$-Network (i-QN), a novel principled approach that enables\nmultiple consecutive Bellman updates by learning a tailored sequence of\naction-value functions where each serves as the target for the next. We show\nthat i-QN is theoretically grounded and that it can be seamlessly used in\nvalue-based and actor-critic methods. We empirically demonstrate the advantages\nof i-QN in Atari $2600$ games and MuJoCo continuous control problems.\n","authors":["Théo Vincent","Daniel Palenicek","Boris Belousov","Jan Peters","Carlo D'Eramo"],"pdf_url":"https://arxiv.org/pdf/2403.02107v5.pdf","comment":"Published at TMLR: https://openreview.net/forum?id=Lt2H8Bd8jF"},{"id":"http://arxiv.org/abs/2407.15589v5","updated":"2025-03-03T11:48:03Z","published":"2024-07-22T12:26:08Z","title":"Exploring the Effectiveness of Object-Centric Representations in Visual\n  Question Answering: Comparative Insights with Foundation Models","summary":"  Object-centric (OC) representations, which model visual scenes as\ncompositions of discrete objects, have the potential to be used in various\ndownstream tasks to achieve systematic compositional generalization and\nfacilitate reasoning. However, these claims have yet to be thoroughly validated\nempirically. Recently, foundation models have demonstrated unparalleled\ncapabilities across diverse domains, from language to computer vision,\npositioning them as a potential cornerstone of future research for a wide range\nof computational tasks. In this paper, we conduct an extensive empirical study\non representation learning for downstream Visual Question Answering (VQA),\nwhich requires an accurate compositional understanding of the scene. We\nthoroughly investigate the benefits and trade-offs of OC models and alternative\napproaches including large pre-trained foundation models on both synthetic and\nreal-world data, ultimately identifying a promising path to leverage the\nstrengths of both paradigms. The extensiveness of our study, encompassing over\n600 downstream VQA models and 15 different types of upstream representations,\nalso provides several additional insights that we believe will be of interest\nto the community at large.\n","authors":["Amir Mohammad Karimi Mamaghan","Samuele Papa","Karl Henrik Johansson","Stefan Bauer","Andrea Dittadi"],"pdf_url":"https://arxiv.org/pdf/2407.15589v5.pdf","comment":"Published at ICLR 2025"},{"id":"http://arxiv.org/abs/2405.16195v3","updated":"2025-03-03T11:39:53Z","published":"2024-05-25T11:57:43Z","title":"Adaptive $Q$-Network: On-the-fly Target Selection for Deep Reinforcement\n  Learning","summary":"  Deep Reinforcement Learning (RL) is well known for being highly sensitive to\nhyperparameters, requiring practitioners substantial efforts to optimize them\nfor the problem at hand. This also limits the applicability of RL in real-world\nscenarios. In recent years, the field of automated Reinforcement Learning\n(AutoRL) has grown in popularity by trying to address this issue. However,\nthese approaches typically hinge on additional samples to select\nwell-performing hyperparameters, hindering sample-efficiency and practicality.\nFurthermore, most AutoRL methods are heavily based on already existing AutoML\nmethods, which were originally developed neglecting the additional challenges\ninherent to RL due to its non-stationarities. In this work, we propose a new\napproach for AutoRL, called Adaptive $Q$-Network (AdaQN), that is tailored to\nRL to take into account the non-stationarity of the optimization procedure\nwithout requiring additional samples. AdaQN learns several $Q$-functions, each\none trained with different hyperparameters, which are updated online using the\n$Q$-function with the smallest approximation error as a shared target. Our\nselection scheme simultaneously handles different hyperparameters while coping\nwith the non-stationarity induced by the RL optimization procedure and being\northogonal to any critic-based RL algorithm. We demonstrate that AdaQN is\ntheoretically sound and empirically validate it in MuJoCo control problems and\nAtari $2600$ games, showing benefits in sample-efficiency, overall performance,\nrobustness to stochasticity and training stability.\n","authors":["Théo Vincent","Fabian Wahren","Jan Peters","Boris Belousov","Carlo D'Eramo"],"pdf_url":"https://arxiv.org/pdf/2405.16195v3.pdf","comment":"Accepted at ICLR https://iclr.cc/virtual/2025/poster/28508"},{"id":"http://arxiv.org/abs/2410.11502v2","updated":"2025-03-03T11:38:11Z","published":"2024-10-15T11:15:03Z","title":"Offline Model-Based Optimization by Learning to Rank","summary":"  Offline model-based optimization (MBO) aims to identify a design that\nmaximizes a black-box function using only a fixed, pre-collected dataset of\ndesigns and their corresponding scores. A common approach in offline MBO is to\ntrain a regression-based surrogate model by minimizing mean squared error (MSE)\nand then find the best design within this surrogate model by different\noptimizers (e.g., gradient ascent). However, a critical challenge is the risk\nof out-of-distribution errors, i.e., the surrogate model may typically\noverestimate the scores and mislead the optimizers into suboptimal regions.\nPrior works have attempted to address this issue in various ways, such as using\nregularization techniques and ensemble learning to enhance the robustness of\nthe model, but it still remains. In this paper, we argue that regression models\ntrained with MSE are not well-aligned with the primary goal of offline MBO,\nwhich is to select promising designs rather than to predict their scores\nprecisely. Notably, if a surrogate model can maintain the order of candidate\ndesigns based on their relative score relationships, it can produce the best\ndesigns even without precise predictions. To validate it, we conduct\nexperiments to compare the relationship between the quality of the final\ndesigns and MSE, finding that the correlation is really very weak. In contrast,\na metric that measures order-maintaining quality shows a significantly stronger\ncorrelation. Based on this observation, we propose learning a ranking-based\nmodel that leverages learning to rank techniques to prioritize promising\ndesigns based on their relative scores. We show that the generalization error\non ranking loss can be well bounded. Empirical results across diverse tasks\ndemonstrate the superior performance of our proposed ranking-based models than\ntwenty existing methods.\n","authors":["Rong-Xi Tan","Ke Xue","Shen-Huan Lyu","Haopu Shang","Yao Wang","Yaoyuan Wang","Sheng Fu","Chao Qian"],"pdf_url":"https://arxiv.org/pdf/2410.11502v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2402.06287v2","updated":"2025-03-03T11:28:57Z","published":"2024-02-09T09:54:01Z","title":"AI, Meet Human: Learning Paradigms for Hybrid Decision Making Systems","summary":"  Everyday we increasingly rely on machine learning models to automate and\nsupport high-stake tasks and decisions. This growing presence means that humans\nare now constantly interacting with machine learning-based systems, training\nand using models everyday. Several different techniques in computer science\nliterature account for the human interaction with machine learning systems, but\ntheir classification is sparse and the goals varied. This survey proposes a\ntaxonomy of Hybrid Decision Making Systems, providing both a conceptual and\ntechnical framework for understanding how current computer science literature\nmodels interaction between humans and machines.\n","authors":["Clara Punzi","Roberto Pellungrini","Mattia Setzu","Fosca Giannotti","Dino Pedreschi"],"pdf_url":"https://arxiv.org/pdf/2402.06287v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.18477v4","updated":"2025-03-03T11:25:26Z","published":"2024-02-28T16:58:31Z","title":"Signature Kernel Conditional Independence Tests in Causal Discovery for\n  Stochastic Processes","summary":"  Inferring the causal structure underlying stochastic dynamical systems from\nobservational data holds great promise in domains ranging from science and\nhealth to finance. Such processes can often be accurately modeled via\nstochastic differential equations (SDEs), which naturally imply causal\nrelationships via \"which variables enter the differential of which other\nvariables\". In this paper, we develop conditional independence (CI) constraints\non coordinate processes over selected intervals that are Markov with respect to\nthe acyclic dependence graph (allowing self-loops) induced by a general SDE\nmodel. We then provide a sound and complete causal discovery algorithm, capable\nof handling both fully and partially observed data, and uniquely recovering the\nunderlying or induced ancestral graph by exploiting time directionality\nassuming a CI oracle. Finally, to make our algorithm practically usable, we\nalso propose a flexible, consistent signature kernel-based CI test to infer\nthese constraints from data. We extensively benchmark the CI test in isolation\nand as part of our causal discovery algorithms, outperforming existing\napproaches in SDE models and beyond.\n","authors":["Georg Manten","Cecilia Casolo","Emilio Ferrucci","Søren Wengel Mogensen","Cristopher Salvi","Niki Kilbertus"],"pdf_url":"https://arxiv.org/pdf/2402.18477v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06057v2","updated":"2025-03-03T11:08:15Z","published":"2024-07-08T15:59:44Z","title":"Variational Best-of-N Alignment","summary":"  Best-of-N (BoN) is a popular and effective algorithm for aligning language\nmodels to human preferences. The algorithm works as follows: at inference time,\nN samples are drawn from the language model, and the sample with the highest\nreward, as judged by a reward model, is returned as the output. Despite its\neffectiveness, BoN is computationally expensive; it reduces sampling throughput\nby a factor of N. To make BoN more efficient at inference time, one strategy is\nto fine-tune the language model to mimic what BoN does during inference. To\nachieve this, we derive the distribution induced by the BoN algorithm. We then\npropose to fine-tune the language model to minimize backward KL divergence to\nthe BoN distribution. Our approach is analogous to mean-field variational\ninference and, thus, we term it variational BoN (vBoN). To the extent this\nfine-tuning is successful and we end up with a good approximation, we have\nreduced the inference cost by a factor of N. Our experiments on controlled\ngeneration and summarization tasks show that BoN is the most effective\nalignment method, and our variational approximation to BoN achieves the closest\nperformance to BoN and surpasses models fine-tuned using the standard\nKL-constrained RL objective. In the controlled generation task, vBoN appears\nmore frequently on the Pareto frontier of reward and KL divergence compared to\nother alignment methods. In the summarization task, vBoN achieves high reward\nvalues across various sampling temperatures.\n","authors":["Afra Amini","Tim Vieira","Elliott Ash","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2407.06057v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.05841v2","updated":"2025-03-03T11:00:24Z","published":"2024-11-06T15:06:42Z","title":"FLEXtime: Filterbank learning to explain time series","summary":"  State-of-the-art methods for explaining predictions from time series involve\nlearning an instance-wise saliency mask for each time step; however, many types\nof time series are difficult to interpret in the time domain, due to the\ninherently complex nature of the data. Instead, we propose to view time series\nexplainability as saliency maps over interpretable parts, leaning on\nestablished signal processing methodology on signal decomposition.\nSpecifically, we propose a new method called FLEXtime that uses a bank of\nbandpass filters to split the time series into frequency bands. Then, we learn\nthe combination of these bands that optimally explains the model's prediction.\nOur extensive evaluation shows that, on average, FLEXtime outperforms\nstate-of-the-art explainability methods across a range of datasets. FLEXtime\nfills an important gap in the current time series explainability methodology\nand is a valuable tool for a wide range of time series such as EEG and audio.\nCode will be made available at https://github.com/theabrusch/FLEXtime.\n","authors":["Thea Brüsch","Kristoffer K. Wickstrøm","Mikkel N. Schmidt","Robert Jenssen","Tommy S. Alstrøm"],"pdf_url":"https://arxiv.org/pdf/2411.05841v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18936v4","updated":"2025-03-03T11:00:24Z","published":"2025-01-31T07:41:06Z","title":"Adaptive Prompt: Unlocking the Power of Visual Prompt Tuning","summary":"  Visual Prompt Tuning (VPT) has recently emerged as a powerful method for\nadapting pre-trained vision models to downstream tasks. By introducing\nlearnable prompt tokens as task-specific instructions, VPT effectively guides\npre-trained transformer models with minimal overhead. Despite its empirical\nsuccess, a comprehensive theoretical understanding of VPT remains an active\narea of research. Building on recent insights into the connection between\nmixture of experts and prompt-based approaches, we identify a key limitation in\nVPT: the restricted functional expressiveness in prompt formulation. To address\nthis limitation, we propose Visual Adaptive Prompt Tuning (VAPT), a new\ngeneration of prompts that redefines prompts as adaptive functions of the\ninput. Our theoretical analysis shows that this simple yet intuitive approach\nachieves optimal sample efficiency. Empirical results on VTAB-1K and FGVC\nfurther demonstrate VAPT's effectiveness, with performance gains of 7.34% and\n1.04% over fully fine-tuning baselines, respectively. Notably, VAPT also\nsurpasses VPT by a substantial margin while using fewer parameters. These\nresults highlight both the effectiveness and efficiency of our method and pave\nthe way for future research to explore the potential of adaptive prompts.\n","authors":["Minh Le","Anh Nguyen","Huy Nguyen","Chau Nguyen","Nhat Ho"],"pdf_url":"https://arxiv.org/pdf/2501.18936v4.pdf","comment":"57 pages, 10 figures, 18 tables"},{"id":"http://arxiv.org/abs/2405.20579v3","updated":"2025-03-03T10:57:41Z","published":"2024-05-31T02:17:51Z","title":"HOPE: A Reinforcement Learning-based Hybrid Policy Path Planner for\n  Diverse Parking Scenarios","summary":"  Automated parking stands as a highly anticipated application of autonomous\ndriving technology. However, existing path planning methodologies fall short of\naddressing this need due to their incapability to handle the diverse and\ncomplex parking scenarios in reality. While non-learning methods provide\nreliable planning results, they are vulnerable to intricate occasions, whereas\nlearning-based ones are good at exploration but unstable in converging to\nfeasible solutions. To leverage the strengths of both approaches, we introduce\nHybrid pOlicy Path plannEr (HOPE). This novel solution integrates a\nreinforcement learning agent with Reeds-Shepp curves, enabling effective\nplanning across diverse scenarios. HOPE guides the exploration of the\nreinforcement learning agent by applying an action mask mechanism and employs a\ntransformer to integrate the perceived environmental information with the mask.\nTo facilitate the training and evaluation of the proposed planner, we propose a\ncriterion for categorizing the difficulty level of parking scenarios based on\nspace and obstacle distribution. Experimental results demonstrate that our\napproach outperforms typical rule-based algorithms and traditional\nreinforcement learning methods, showing higher planning success rates and\ngeneralization across various scenarios. We also conduct real-world experiments\nto verify the practicability of HOPE. The code for our solution is openly\navailable on https://github.com/jiamiya/HOPE.\n","authors":["Mingyang Jiang","Yueyuan Li","Songan Zhang","Siyuan Chen","Chunxiang Wang","Ming Yang"],"pdf_url":"https://arxiv.org/pdf/2405.20579v3.pdf","comment":"Accepted by T-ITS. 11 pages, 5 tables, 6 figures, 2 page appendix"},{"id":"http://arxiv.org/abs/2410.02423v2","updated":"2025-03-03T10:44:06Z","published":"2024-10-03T12:13:56Z","title":"PnP-Flow: Plug-and-Play Image Restoration with Flow Matching","summary":"  In this paper, we introduce Plug-and-Play (PnP) Flow Matching, an algorithm\nfor solving imaging inverse problems. PnP methods leverage the strength of\npre-trained denoisers, often deep neural networks, by integrating them in\noptimization schemes. While they achieve state-of-the-art performance on\nvarious inverse problems in imaging, PnP approaches face inherent limitations\non more generative tasks like inpainting. On the other hand, generative models\nsuch as Flow Matching pushed the boundary in image sampling yet lack a clear\nmethod for efficient use in image restoration. We propose to combine the PnP\nframework with Flow Matching (FM) by defining a time-dependent denoiser using a\npre-trained FM model. Our algorithm alternates between gradient descent steps\non the data-fidelity term, reprojections onto the learned FM path, and\ndenoising. Notably, our method is computationally efficient and\nmemory-friendly, as it avoids backpropagation through ODEs and trace\ncomputations. We evaluate its performance on denoising, super-resolution,\ndeblurring, and inpainting tasks, demonstrating superior results compared to\nexisting PnP algorithms and Flow Matching based state-of-the-art methods.\n","authors":["Ségolène Martin","Anne Gagneux","Paul Hagemann","Gabriele Steidl"],"pdf_url":"https://arxiv.org/pdf/2410.02423v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.11542v2","updated":"2025-03-03T10:39:41Z","published":"2024-12-16T08:22:23Z","title":"Meta Curvature-Aware Minimization for Domain Generalization","summary":"  Domain generalization (DG) aims to enhance the ability of models trained on\nsource domains to generalize effectively to unseen domains. Recently,\nSharpness-Aware Minimization (SAM) has shown promise in this area by reducing\nthe sharpness of the loss landscape to obtain more generalized models. However,\nSAM and its variants sometimes fail to guide the model toward a flat minimum,\nand their training processes exhibit limitations, hindering further\nimprovements in model generalization. In this paper, we first propose an\nimproved model training process aimed at encouraging the model to converge to a\nflat minima. To achieve this, we design a curvature metric that has a minimal\neffect when the model is far from convergence but becomes increasingly\ninfluential in indicating the curvature of the minima as the model approaches a\nlocal minimum. Then we derive a novel algorithm from this metric, called Meta\nCurvature-Aware Minimization (MeCAM), to minimize the curvature around the\nlocal minima. Specifically, the optimization objective of MeCAM simultaneously\nminimizes the regular training loss, the surrogate gap of SAM, and the\nsurrogate gap of meta-learning. We provide theoretical analysis on MeCAM's\ngeneralization error and convergence rate, and demonstrate its superiority over\nexisting DG methods through extensive experiments on five benchmark DG\ndatasets, including PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet. Code\nwill be available on GitHub.\n","authors":["Ziyang Chen","Yiwen Ye","Feilong Tang","Yongsheng Pan","Yong Xia"],"pdf_url":"https://arxiv.org/pdf/2412.11542v2.pdf","comment":"22 pages, 5 figures, 17 tables"},{"id":"http://arxiv.org/abs/2502.08005v2","updated":"2025-03-03T10:38:34Z","published":"2025-02-11T23:02:14Z","title":"Towards Training One-Step Diffusion Models Without Distillation","summary":"  Recent advances in one-step generative models typically follow a two-stage\nprocess: first training a teacher diffusion model and then distilling it into a\none-step student model. This distillation process traditionally relies on both\nthe teacher model's score function to compute the distillation loss and its\nweights for student initialization. In this paper, we explore whether one-step\ngenerative models can be trained directly without this distillation process.\nFirst, we show that the teacher's score function is not essential and propose a\nfamily of distillation methods that achieve competitive results without relying\non score estimation. Next, we demonstrate that initialization from teacher\nweights is indispensable in successful training. Surprisingly, we find that\nthis benefit is not due to improved ``input-output\" mapping but rather the\nlearned feature representations, which dominate distillation quality. Our\nfindings provide a better understanding of the role of initialization in\none-step model training and its impact on distillation quality.\n","authors":["Mingtian Zhang","Jiajun He","Wenlin Chen","Zijing Ou","José Miguel Hernández-Lobato","Bernhard Schölkopf","David Barber"],"pdf_url":"https://arxiv.org/pdf/2502.08005v2.pdf","comment":"13 pages, Technical Report"},{"id":"http://arxiv.org/abs/2502.15425v3","updated":"2025-03-03T10:35:14Z","published":"2025-02-21T12:52:16Z","title":"TAG: A Decentralized Framework for Multi-Agent Hierarchical\n  Reinforcement Learning","summary":"  Hierarchical organization is fundamental to biological systems and human\nsocieties, yet artificial intelligence systems often rely on monolithic\narchitectures that limit adaptability and scalability. Current hierarchical\nreinforcement learning (HRL) approaches typically restrict hierarchies to two\nlevels or require centralized training, which limits their practical\napplicability. We introduce TAME Agent Framework (TAG), a framework for\nconstructing fully decentralized hierarchical multi-agent systems.TAG enables\nhierarchies of arbitrary depth through a novel LevelEnv concept, which\nabstracts each hierarchy level as the environment for the agents above it. This\napproach standardizes information flow between levels while preserving loose\ncoupling, allowing for seamless integration of diverse agent types. We\ndemonstrate the effectiveness of TAG by implementing hierarchical architectures\nthat combine different RL agents across multiple levels, achieving improved\nperformance over classical multi-agent RL baselines on standard benchmarks. Our\nresults show that decentralized hierarchical organization enhances both\nlearning speed and final performance, positioning TAG as a promising direction\nfor scalable multi-agent systems.\n","authors":["Giuseppe Paolo","Abdelhakim Benechehab","Hamza Cherkaoui","Albert Thomas","Balázs Kégl"],"pdf_url":"https://arxiv.org/pdf/2502.15425v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.06916v2","updated":"2025-03-03T10:22:24Z","published":"2024-11-11T12:19:28Z","title":"Slowing Down Forgetting in Continual Learning","summary":"  A common challenge in continual learning (CL) is catastrophic forgetting,\nwhere the performance on old tasks drops after new, additional tasks are\nlearned. In this paper, we propose a novel framework called ReCL to slow down\nforgetting in CL. Our framework exploits an implicit bias of gradient-based\nneural networks due to which these converge to margin maximization points. Such\nconvergence points allow us to reconstruct old data from previous tasks, which\nwe then combine with the current training data. Our framework is flexible and\ncan be applied on top of existing, state-of-the-art CL methods. We further\ndemonstrate the performance gain from our framework across a large series of\nexperiments, including two challenging CL scenarios (class incremental and\ndomain incremental learning), different datasets (MNIST, CIFAR10,\nTinyImagenet), and different network architectures. Across all experiments, we\nfind large performance gains through ReCL. To the best of our knowledge, our\nframework is the first to address catastrophic forgetting by leveraging models\nin CL as their own memory buffers.\n","authors":["Pascal Janetzky","Tobias Schlagenhauf","Stefan Feuerriegel"],"pdf_url":"https://arxiv.org/pdf/2411.06916v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.21123v2","updated":"2025-03-03T10:00:03Z","published":"2025-02-28T14:57:33Z","title":"Causality Is Key to Understand and Balance Multiple Goals in Trustworthy\n  ML and Foundation Models","summary":"  Ensuring trustworthiness in machine learning (ML) systems is crucial as they\nbecome increasingly embedded in high-stakes domains. This paper advocates for\nintegrating causal methods into machine learning to navigate the trade-offs\namong key principles of trustworthy ML, including fairness, privacy,\nrobustness, accuracy, and explainability. While these objectives should ideally\nbe satisfied simultaneously, they are often addressed in isolation, leading to\nconflicts and suboptimal solutions. Drawing on existing applications of\ncausality in ML that successfully align goals such as fairness and accuracy or\nprivacy and robustness, this paper argues that a causal approach is essential\nfor balancing multiple competing objectives in both trustworthy ML and\nfoundation models. Beyond highlighting these trade-offs, we examine how\ncausality can be practically integrated into ML and foundation models, offering\nsolutions to enhance their reliability and interpretability. Finally, we\ndiscuss the challenges, limitations, and opportunities in adopting causal\nframeworks, paving the way for more accountable and ethically sound AI systems.\n","authors":["Ruta Binkyte","Ivaxi Sheth","Zhijing Jin","Mohammad Havaei","Bernhard Schölkopf","Mario Fritz"],"pdf_url":"https://arxiv.org/pdf/2502.21123v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02392v2","updated":"2025-03-03T09:50:18Z","published":"2024-10-03T11:13:55Z","title":"MANTRA: The Manifold Triangulations Assemblage","summary":"  The rising interest in leveraging higher-order interactions present in\ncomplex systems has led to a surge in more expressive models exploiting\nhigher-order structures in the data, especially in topological deep learning\n(TDL), which designs neural networks on higher-order domains such as simplicial\ncomplexes. However, progress in this field is hindered by the scarcity of\ndatasets for benchmarking these architectures. To address this gap, we\nintroduce MANTRA, the first large-scale, diverse, and intrinsically\nhigher-order dataset for benchmarking higher-order models, comprising over\n43,000 and 250,000 triangulations of surfaces and three-dimensional manifolds,\nrespectively. With MANTRA, we assess several graph- and simplicial\ncomplex-based models on three topological classification tasks. We demonstrate\nthat while simplicial complex-based neural networks generally outperform their\ngraph-based counterparts in capturing simple topological invariants, they also\nstruggle, suggesting a rethink of TDL. Thus, MANTRA serves as a benchmark for\nassessing and advancing topological methods, leading the way for more effective\nhigher-order models.\n","authors":["Rubén Ballester","Ernst Röell","Daniel Bīn Schmid","Mathieu Alain","Sergio Escalera","Carles Casacuberta","Bastian Rieck"],"pdf_url":"https://arxiv.org/pdf/2410.02392v2.pdf","comment":"Accepted at ICLR 2025 (https://openreview.net/forum?id=X6y5CC44HM)"},{"id":"http://arxiv.org/abs/2402.09154v2","updated":"2025-03-03T09:37:27Z","published":"2024-02-14T13:13:26Z","title":"Attacking Large Language Models with Projected Gradient Descent","summary":"  Current LLM alignment methods are readily broken through specifically crafted\nadversarial prompts. While crafting adversarial prompts using discrete\noptimization is highly effective, such attacks typically use more than 100,000\nLLM calls. This high computational cost makes them unsuitable for, e.g.,\nquantitative analyses and adversarial training. To remedy this, we revisit\nProjected Gradient Descent (PGD) on the continuously relaxed input prompt.\nAlthough previous attempts with ordinary gradient-based attacks largely failed,\nwe show that carefully controlling the error introduced by the continuous\nrelaxation tremendously boosts their efficacy. Our PGD for LLMs is up to one\norder of magnitude faster than state-of-the-art discrete optimization to\nachieve the same devastating attack results.\n","authors":["Simon Geisler","Tom Wollschläger","M. H. I. Abdalla","Johannes Gasteiger","Stephan Günnemann"],"pdf_url":"https://arxiv.org/pdf/2402.09154v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23751v2","updated":"2025-03-03T09:30:42Z","published":"2024-10-31T09:11:56Z","title":"EXACFS -- A CIL Method to mitigate Catastrophic Forgetting","summary":"  Deep neural networks (DNNS) excel at learning from static datasets but\nstruggle with continual learning, where data arrives sequentially. Catastrophic\nforgetting, the phenomenon of forgetting previously learned knowledge, is a\nprimary challenge. This paper introduces EXponentially Averaged Class-wise\nFeature Significance (EXACFS) to mitigate this issue in the class incremental\nlearning (CIL) setting. By estimating the significance of model features for\neach learned class using loss gradients, gradually aging the significance\nthrough the incremental tasks and preserving the significant features through a\ndistillation loss, EXACFS effectively balances remembering old knowledge\n(stability) and learning new knowledge (plasticity). Extensive experiments on\nCIFAR-100 and ImageNet-100 demonstrate EXACFS's superior performance in\npreserving stability while acquiring plasticity.\n","authors":["S Balasubramanian","M Sai Subramaniam","Sai Sriram Talasu","Yedu Krishna P","Manepalli Pranav Phanindra Sai","Ravi Mukkamala","Darshan Gera"],"pdf_url":"https://arxiv.org/pdf/2410.23751v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.00537v2","updated":"2025-03-03T09:26:05Z","published":"2024-11-30T17:05:12Z","title":"Exact Certification of (Graph) Neural Networks Against Label Poisoning","summary":"  Machine learning models are highly vulnerable to label flipping, i.e., the\nadversarial modification (poisoning) of training labels to compromise\nperformance. Thus, deriving robustness certificates is important to guarantee\nthat test predictions remain unaffected and to understand worst-case robustness\nbehavior. However, for Graph Neural Networks (GNNs), the problem of certifying\nlabel flipping has so far been unsolved. We change this by introducing an exact\ncertification method, deriving both sample-wise and collective certificates.\nOur method leverages the Neural Tangent Kernel (NTK) to capture the training\ndynamics of wide networks enabling us to reformulate the bilevel optimization\nproblem representing label flipping into a Mixed-Integer Linear Program (MILP).\nWe apply our method to certify a broad range of GNN architectures in node\nclassification tasks. Thereby, concerning the worst-case robustness to label\nflipping: $(i)$ we establish hierarchies of GNNs on different benchmark graphs;\n$(ii)$ quantify the effect of architectural choices such as activations, depth\nand skip-connections; and surprisingly, $(iii)$ uncover a novel phenomenon of\nthe robustness plateauing for intermediate perturbation budgets across all\ninvestigated datasets and architectures. While we focus on GNNs, our\ncertificates are applicable to sufficiently wide NNs in general through their\nNTK. Thus, our work presents the first exact certificate to a poisoning attack\never derived for neural networks, which could be of independent interest. The\ncode is available at https://github.com/saper0/qpcert.\n","authors":["Mahalakshmi Sabanayagam","Lukas Gosch","Stephan Günnemann","Debarghya Ghoshdastidar"],"pdf_url":"https://arxiv.org/pdf/2412.00537v2.pdf","comment":"Published as a spotlight presentation at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.16890v2","updated":"2025-03-03T08:58:48Z","published":"2025-02-24T06:40:33Z","title":"ReFocus: Reinforcing Mid-Frequency and Key-Frequency Modeling for\n  Multivariate Time Series Forecasting","summary":"  Recent advancements have progressively incorporated frequency-based\ntechniques into deep learning models, leading to notable improvements in\naccuracy and efficiency for time series analysis tasks. However, the\nMid-Frequency Spectrum Gap in the real-world time series, where the energy is\nconcentrated at the low-frequency region while the middle-frequency band is\nnegligible, hinders the ability of existing deep learning models to extract the\ncrucial frequency information. Additionally, the shared Key-Frequency in\nmultivariate time series, where different time series share indistinguishable\nfrequency patterns, is rarely exploited by existing literature. This work\nintroduces a novel module, Adaptive Mid-Frequency Energy Optimizer, based on\nconvolution and residual learning, to emphasize the significance of\nmid-frequency bands. We also propose an Energy-based Key-Frequency Picking\nBlock to capture shared Key-Frequency, which achieves superior inter-series\nmodeling performance with fewer parameters. A novel Key-Frequency Enhanced\nTraining strategy is employed to further enhance Key-Frequency modeling, where\nspectral information from other channels is randomly introduced into each\nchannel. Our approach advanced multivariate time series forecasting on the\nchallenging Traffic, ECL, and Solar benchmarks, reducing MSE by 4%, 6%, and 5%\ncompared to the previous SOTA iTransformer. Code is available at this GitHub\nRepository: https://github.com/Levi-Ackman/ReFocus.\n","authors":["Guoqi Yu","Yaoming Li","Juncheng Wang","Xiaoyu Guo","Angelica I. Aviles-Rivero","Tong Yang","Shujun Wang"],"pdf_url":"https://arxiv.org/pdf/2502.16890v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2502.08679v3","updated":"2025-03-03T08:50:28Z","published":"2025-02-12T08:56:35Z","title":"Deep Learning-Driven Malware Classification with API Call Sequence\n  Analysis and Concept Drift Handling","summary":"  Malware classification in dynamic environments presents a significant\nchallenge due to concept drift, where the statistical properties of malware\ndata evolve over time, complicating detection efforts. To address this issue,\nwe propose a deep learning framework enhanced with a genetic algorithm to\nimprove malware classification accuracy and adaptability. Our approach\nincorporates mutation operations and fitness score evaluations within genetic\nalgorithms to continuously refine the deep learning model, ensuring robustness\nagainst evolving malware threats. Experimental results demonstrate that this\nhybrid method significantly enhances classification performance and\nadaptability, outperforming traditional static models. Our proposed approach\noffers a promising solution for real-time malware classification in\never-changing cybersecurity landscapes.\n","authors":["Bishwajit Prasad Gond","Durga Prasad Mohapatra"],"pdf_url":"https://arxiv.org/pdf/2502.08679v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.03856v4","updated":"2025-03-03T08:48:38Z","published":"2024-07-04T11:42:36Z","title":"Q-Adapter: Customizing Pre-trained LLMs to New Preferences with\n  Forgetting Mitigation","summary":"  Large Language Models (LLMs), trained on a large amount of corpus, have\ndemonstrated remarkable abilities. However, it may not be sufficient to\ndirectly apply open-source LLMs like Llama to certain real-world scenarios,\nsince most of them are trained for \\emph{general} purposes. Thus, the demands\nfor customizing publicly available LLMs emerge, but are currently\nunder-studied. In this work, we consider customizing pre-trained LLMs with new\nhuman preferences. Specifically, the LLM should not only meet the new\npreference but also preserve its original capabilities after customization.\nDrawing inspiration from the observation that human preference can be expressed\nas a reward model, we propose to cast LLM customization as optimizing the sum\nof two reward functions, one of which (denoted as $r_1$) was used to pre-train\nthe LLM while the other (denoted as $r_2$) characterizes the new human\npreference. The obstacle here is that both reward functions are unknown, making\nthe application of modern reinforcement learning methods infeasible. Thanks to\nthe residual Q-learning framework, we can restore the customized LLM with the\npre-trained LLM and the \\emph{residual Q-function} without the reward function\n$r_1$. Moreover, we find that for a fixed pre-trained LLM, the reward function\n$r_2$ can be derived from the residual Q-function, enabling us to directly\nlearn the residual Q-function from the new human preference data upon the\nBradley-Terry model. We name our method Q-Adapter as it introduces an adapter\nmodule to approximate the residual Q-function for customizing the pre-trained\nLLM towards the new preference. Experiments based on the Llama-3.1 model on the\nDSP dataset and HH-RLHF dataset illustrate the superior effectiveness of\nQ-Adapter on both retaining existing knowledge and learning new preferences.\nCode is available at https://github.com/mansicer/Q-Adapter.\n","authors":["Yi-Chen Li","Fuxiang Zhang","Wenjie Qiu","Lei Yuan","Chengxing Jia","Zongzhang Zhang","Yang Yu","Bo An"],"pdf_url":"https://arxiv.org/pdf/2407.03856v4.pdf","comment":"Camera ready version of ICLR 2025"},{"id":"http://arxiv.org/abs/2410.07267v2","updated":"2025-03-03T08:45:31Z","published":"2024-10-09T02:44:53Z","title":"Scintillation pulse characterization with spectrum-inspired temporal\n  neural networks: case studies on particle detector signals","summary":"  Particle detectors based on scintillators are widely used in high-energy\nphysics and astroparticle physics experiments, nuclear medicine imaging,\nindustrial and environmental detection, etc. Precisely extracting scintillation\nsignal characteristics at the event level is important for these applications,\nnot only in respect of understanding the scintillator itself, but also kinds\nand physical property of incident particles. Recent researches demonstrate\ndata-driven neural networks surpass traditional statistical methods, especially\nwhen the analytical form of signals is hard to obtain, or noise is significant.\nHowever, most densely connected or convolution-based networks fail to fully\nexploit the spectral and temporal structure of scintillation signals, leaving\nlarge space for performance improvement. In this paper, we propose a network\narchitecture specially tailored for scintillation pulse characterization based\non previous works on time series analysis. The core insight is that, by\ndirectly applying Fast Fourier Transform on original signals and utilizing\ndifferent frequency components, the proposed network architecture can serve as\na lightweight and enhanced representation learning backbone. We prove our idea\nin two case studies: (a) simulation data generated with the setting of the LUX\ndark matter detector, and (b) experimental electrical signals with fast\nelectronics to emulate scintillation variations for the NICA/MPD calorimeter.\nThe proposed model achieves significantly better results than the reference\nmodel in literature and densely connected models, and demonstrates higher\ncost-efficiency than conventional machine learning methods.\n","authors":["Pengcheng Ai","Xiangming Sun","Zhi Deng","Xinchi Ran"],"pdf_url":"https://arxiv.org/pdf/2410.07267v2.pdf","comment":"29 pages, 14 figures"},{"id":"http://arxiv.org/abs/2502.11167v2","updated":"2025-03-03T08:26:12Z","published":"2025-02-16T15:38:19Z","title":"SURGE: On the Potential of Large Language Models as General-Purpose\n  Surrogate Code Executors","summary":"  Neural surrogate models have emerged as powerful and efficient tools in data\nmining. Meanwhile, large language models (LLMs) have demonstrated remarkable\ncapabilities in code-related tasks. We investigate a novel application: using\nLLMs as surrogate models for code execution prediction. Given LLMs' unique\nability to understand and process diverse programs, they present a promising\ndirection for building general-purpose surrogate models. To systematically\ninvestigate this capability, we introduce SURGE, a comprehensive benchmark with\n$1160$ problems covering $8$ key aspects: multi-language programming tasks,\ncompetition-level programming problems, repository-level code analysis,\nhigh-cost scientific computing, time-complexity-intensive algorithms, buggy\ncode analysis, programs dependent on specific compilers or execution\nenvironments, and formal mathematical proof verification. Through extensive\nempirical analysis of $21$ open-source and proprietary LLMs, we examine scaling\nlaws, data efficiency, and predictive accuracy. Our findings reveal important\ninsights about the feasibility of LLMs as efficient surrogates for\ncomputational processes, with implications for automated software testing,\nprogram analysis, and computational resource optimization in data mining\napplications. Code and dataset are released at\nhttps://github.com/Imbernoulli/SURGE.\n","authors":["Bohan Lyu","Siqiao Huang","Zichen Liang"],"pdf_url":"https://arxiv.org/pdf/2502.11167v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19316v2","updated":"2025-03-03T08:22:25Z","published":"2024-05-29T17:39:48Z","title":"Robust Preference Optimization through Reward Model Distillation","summary":"  Language model (LM) post-training (or alignment) involves maximizing a reward\nfunction that is derived from preference annotations. Direct Preference\nOptimization (DPO) is a popular offline alignment method that trains a policy\ndirectly on preference data without the need to train a reward model or apply\nreinforcement learning. However, the empirical evidence suggests that DPO\ntypically assigns implicit rewards that overfit, and trend towards infinite\nmagnitude. This frequently leads to degenerate policies, sometimes causing even\nthe probabilities of the preferred generations to go to zero. In this work, we\nanalyze this phenomenon and use distillation to get a better proxy for the true\npreference distribution over generation pairs: we train the LM such that its\ninduced implicit reward, i.e., the scaled log-likelihood ratio of the model to\nthe reference model, matches an explicit reward model trained on the preference\ndata. Moreover, to account for uncertainty in the reward model we are\ndistilling from, we optimize against a family of reward models that, as a\nwhole, is likely to include at least one reasonable proxy for the preference\ndistribution. Our results show that distilling from such a family of reward\nmodels leads to improved robustness to distribution shift in preference\nannotations, while preserving the simple supervised nature of DPO.\n","authors":["Adam Fisch","Jacob Eisenstein","Vicky Zayats","Alekh Agarwal","Ahmad Beirami","Chirag Nagpal","Pete Shaw","Jonathan Berant"],"pdf_url":"https://arxiv.org/pdf/2405.19316v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07407v2","updated":"2025-03-03T08:05:53Z","published":"2024-12-10T10:58:47Z","title":"Towards Graph Foundation Models: A Study on the Generalization of\n  Positional and Structural Encodings","summary":"  Recent advances in integrating positional and structural encodings (PSEs)\ninto graph neural networks (GNNs) have significantly enhanced their performance\nacross various graph learning tasks. However, the general applicability of\nthese encodings and their potential to serve as foundational representations\nfor graphs remain uncertain. This paper investigates the fine-tuning\nefficiency, scalability with sample size, and generalization capability of\nlearnable PSEs across diverse graph datasets. Specifically, we evaluate their\npotential as universal pre-trained models that can be easily adapted to new\ntasks with minimal fine-tuning and limited data. Furthermore, we assess the\nexpressivity of the learned representations, particularly, when used to augment\ndownstream GNNs. We demonstrate through extensive benchmarking and empirical\nanalysis that PSEs generally enhance downstream models. However, some datasets\nmay require specific PSE-augmentations to achieve optimal performance.\nNevertheless, our findings highlight their significant potential to become\nintegral components of future graph foundation models. We provide new insights\ninto the strengths and limitations of PSEs, contributing to the broader\ndiscourse on foundation models in graph learning.\n","authors":["Billy Joe Franks","Moshe Eliasof","Semih Cantürk","Guy Wolf","Carola-Bibiane Schönlieb","Sophie Fellenz","Marius Kloft"],"pdf_url":"https://arxiv.org/pdf/2412.07407v2.pdf","comment":"Published at TMLR (https://openreview.net/forum?id=mSoDRZXsqj)"},{"id":"http://arxiv.org/abs/2410.02683v2","updated":"2025-03-03T07:20:54Z","published":"2024-10-03T17:08:52Z","title":"DailyDilemmas: Revealing Value Preferences of LLMs with Quandaries of\n  Daily Life","summary":"  As users increasingly seek guidance from LLMs for decision-making in daily\nlife, many of these decisions are not clear-cut and depend significantly on the\npersonal values and ethical standards of people. We present DailyDilemmas, a\ndataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma\npresents two possible actions, along with affected parties and relevant human\nvalues for each action. Based on these dilemmas, we gather a repository of\nhuman values covering diverse everyday topics, such as interpersonal\nrelationships, workplace, and environmental issues. With DailyDilemmas, we\nevaluate LLMs on these dilemmas to determine what action they will choose and\nthe values represented by these action choices. Then, we analyze values through\nthe lens of five theoretical frameworks inspired by sociology, psychology, and\nphilosophy, including the World Values Survey, Moral Foundations Theory,\nMaslow's Hierarchy of Needs, Aristotle's Virtues, and Plutchik's Wheel of\nEmotions. For instance, we find LLMs are most aligned with self-expression over\nsurvival in World Values Survey and care over loyalty in Moral Foundations\nTheory. Interestingly, we find substantial preference differences in models for\nsome core values. For example, for truthfulness, Mixtral-8x7B neglects it by\n9.7% while GPT-4-turbo selects it by 9.4%. We also study the recent guidance\nreleased by OpenAI (ModelSpec), and Anthropic (Constitutional AI) to understand\nhow their designated principles reflect their models' actual value\nprioritization when facing nuanced moral reasoning in daily-life settings.\nFinally, we find that end users cannot effectively steer such prioritization\nusing system prompts.\n","authors":["Yu Ying Chiu","Liwei Jiang","Yejin Choi"],"pdf_url":"https://arxiv.org/pdf/2410.02683v2.pdf","comment":"Accepted into ICLR 2025 (spotlight)"},{"id":"http://arxiv.org/abs/2501.02497v2","updated":"2025-03-03T07:16:16Z","published":"2025-01-05T10:24:20Z","title":"Test-Time Compute: from System-1 Thinking to System-2 Thinking","summary":"  The remarkable performance of the o1 model in complex reasoning demonstrates\nthat test-time compute scaling can further unlock the model's potential,\nenabling powerful System-2 thinking. However, there is still a lack of\ncomprehensive surveys for test-time compute scaling. We trace the concept of\ntest-time compute back to System-1 models. In System-1 models, test-time\ncompute addresses distribution shifts and improves robustness and\ngeneralization through parameter updating, input modification, representation\nediting, and output calibration. In System-2 models, it enhances the model's\nreasoning ability to solve complex problems through repeated sampling,\nself-correction, and tree search. We organize this survey according to the\ntrend of System-1 to System-2 thinking, highlighting the key role of test-time\ncompute in the transition from System-1 models to weak System-2 models, and\nthen to strong System-2 models. We also point out a few possible future\ndirections.\n","authors":["Yixin Ji","Juntao Li","Hai Ye","Kaixin Wu","Kai Yao","Jia Xu","Linjian Mo","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.02497v2.pdf","comment":"work in progress"},{"id":"http://arxiv.org/abs/2403.03636v3","updated":"2025-03-03T06:56:29Z","published":"2024-03-06T11:48:08Z","title":"SheetAgent: Towards A Generalist Agent for Spreadsheet Reasoning and\n  Manipulation via Large Language Models","summary":"  Spreadsheets are ubiquitous across the World Wide Web, playing a critical\nrole in enhancing work efficiency across various domains. Large language model\n(LLM) has been recently attempted for automatic spreadsheet manipulation but\nhas not yet been investigated in complicated and realistic tasks where\nreasoning challenges exist (e.g., long horizon manipulation with multi-step\nreasoning and ambiguous requirements). To bridge the gap with the real-world\nrequirements, we introduce SheetRM, a benchmark featuring long-horizon and\nmulti-category tasks with reasoning-dependent manipulation caused by real-life\nchallenges. To mitigate the above challenges, we further propose SheetAgent, a\nnovel autonomous agent that utilizes the power of LLMs. SheetAgent consists of\nthree collaborative modules: Planner, Informer, and Retriever, achieving both\nadvanced reasoning and accurate manipulation over spreadsheets without human\ninteraction through iterative task reasoning and reflection. Extensive\nexperiments demonstrate that SheetAgent delivers 20--40\\% pass rate\nimprovements on multiple benchmarks over baselines, achieving enhanced\nprecision in spreadsheet manipulation and demonstrating superior table\nreasoning abilities. More details and visualizations are available at the\nproject website: https://sheetagent.github.io/. The datasets and source code\nare available at https://anonymous.4open.science/r/SheetAgent.\n","authors":["Yibin Chen","Yifu Yuan","Zeyu Zhang","Yan Zheng","Jinyi Liu","Fei Ni","Jianye Hao","Hangyu Mao","Fuzheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2403.03636v3.pdf","comment":"Accepted by International World Wide Web Conference (WWW) 2025 (oral)"},{"id":"http://arxiv.org/abs/2407.04752v2","updated":"2025-03-03T06:46:33Z","published":"2024-07-05T08:37:17Z","title":"SpikeLLM: Scaling up Spiking Neural Network to Large Language Models via\n  Saliency-based Spiking","summary":"  Recent advancements in large language models (LLMs) with billions of\nparameters have improved performance in various applications, but their\ninference processes demand significant energy and computational resources. In\ncontrast, the human brain, with approximately 86 billion neurons, is much more\nenergy-efficient than LLMs with similar parameters. Inspired by this, we\nredesign 7$\\sim$70 billion parameter LLMs using bio-plausible spiking\nmechanisms, emulating the efficient behavior of the human brain. We propose the\nfirst spiking large language model, SpikeLLM. Coupled with the proposed model,\ntwo essential approaches are proposed to improve spike training efficiency:\nGeneralized Integrate-and-Fire (GIF) neurons to compress spike length from $T$\nto $\\frac{T}{L} \\log_2 L$ bits, and an Optimal Brain Spiking framework to\ndivide outlier channels and allocate different $T$ for GIF neurons, which\nfurther compresses spike length to approximate $log_2T$ bits. The necessity of\nspike-driven LLM is proved by comparison with quantized LLMs with similar\noperations. In the OmniQuant pipeline, SpikeLLM reduces 11.01% WikiText2\nperplexity and improves 2.55% accuracy of common scene reasoning on a LLAMA-7B\nW4A4 model. In the GPTQ pipeline, SpikeLLM achieves direct additive in linear\nlayers, significantly exceeding PB-LLMs.\n","authors":["Xingrun Xing","Boyan Gao","Zheng Zhang","David A. Clifton","Shitao Xiao","Li Du","Guoqi Li","Jiajun Zhang"],"pdf_url":"https://arxiv.org/pdf/2407.04752v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12949v2","updated":"2025-03-03T06:42:17Z","published":"2025-02-18T16:00:10Z","title":"Efficient Learning Under Density Shift in Incremental Settings Using\n  Cramér-Rao-Based Regularization","summary":"  The continuous surge in data volume and velocity is often dealt with using\ndata orchestration and distributed processing approaches, abstracting away the\nmachine learning challenges that exist at the algorithmic level. With growing\ninterest in automating the learning loop, training with data that arrive in a\nsequence rather than in the classical in-memory training data form will face a\nmachine learning challenge because of evolving feature distributions across\nbatches of training data biasing the cross-validation step\n(\\cite{sugiyama2012machine}). This work takes a distributed density estimation\nangle to the problem where data are temporally distributed. It processes data\nin batches and allows a neural network to treat a batch as training data. The\nmethod accumulates knowledge about the data density via posterior probability\nabsorption using the Fisher Information Matrix, which contains information\nabout the local optimization gradients for the batch. This is then used as a\nregularizer for the loss in the following batch, and therefore the density\nestimate for the entire dataset constructively gets more robust to the non-iid\ndistribution shift. This needs the presence of a pair of batches in memory at a\ntime, so the space cost is not a function of the size of the complete,\ndistributed dataset. We proposed a novel regularization-based approach\nCovariate Shift Correction $C^{2}A$ that leverages Fisher information and\nKullback-Leibler divergence to adapt to both natural and sequential covariate\nshift caused by dataset fragmentation. $C^{2}A$ achieves $19\\%$ accuracy at\nmaximum against state-of-the-art methods.\n","authors":["Behraj Khan","Behroz Mirza","Nouman Durrani","Tahir Syed"],"pdf_url":"https://arxiv.org/pdf/2502.12949v2.pdf","comment":"It is the older version of our this paper arXiv:2502.15756. So this\n  is the duplicate older version mistakenly uploaded. There are mistakes in the\n  method part of this paper"},{"id":"http://arxiv.org/abs/2412.15598v2","updated":"2025-03-03T06:39:17Z","published":"2024-12-20T06:42:58Z","title":"Long-Term EEG Partitioning for Seizure Onset Detection","summary":"  Deep learning models have recently shown great success in classifying\nepileptic patients using EEG recordings. Unfortunately, classification-based\nmethods lack a sound mechanism to detect the onset of seizure events. In this\nwork, we propose a two-stage framework, SODor, that explicitly models seizure\nonset through a novel task formulation of subsequence clustering. Given an EEG\nsequence, the framework first learns a set of second-level embeddings with\nlabel supervision. It then employs model-based clustering to explicitly capture\nlong-term temporal dependencies in EEG sequences and identify meaningful\nsubsequences. Epochs within a subsequence share a common cluster assignment\n(normal or seizure), with cluster or state transitions representing successful\nonset detections. Extensive experiments on three datasets demonstrate that our\nmethod can correct misclassifications, achieving 5\\%-11\\% classification\nimprovements over other baselines and accurately detecting seizure onsets.\n","authors":["Zheng Chen","Yasuko Matsubara","Yasushi Sakurai","Jimeng Sun"],"pdf_url":"https://arxiv.org/pdf/2412.15598v2.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2309.15531v3","updated":"2025-03-03T06:37:01Z","published":"2023-09-27T09:48:31Z","title":"Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight\n  Quantization of Large Language Models","summary":"  Large Language Models (LLMs) have recently demonstrated remarkable success\nacross various tasks. However, efficiently serving LLMs has been a challenge\ndue to the large memory bottleneck, specifically in small batch inference\nsettings (e.g. mobile devices). Weight-only quantization can be a promising\napproach, but sub-4 bit quantization remains a challenge due to large-magnitude\nactivation outliers. To mitigate the undesirable outlier effect, we first\npropose per-IC quantization, a simple yet effective method that creates\nquantization groups within each input channel (IC) rather than the conventional\nper-output-channel (per-OC). Our method is motivated by the observation that\nactivation outliers affect the input dimension of the weight matrix, so\nsimilarly grouping the weights in the IC direction can isolate outliers within\na group. We also find that activation outliers do not dictate quantization\ndifficulty, and inherent weight sensitivities also exist. With per-IC\nquantization as a new outlier-friendly scheme, we propose Adaptive Dimensions\n(AdaDim), a versatile quantization framework that can adapt to various weight\nsensitivity patterns. We demonstrate the effectiveness of AdaDim by augmenting\nprior methods such as Round-To-Nearest and GPTQ, showing significant\nimprovements across various language modeling benchmarks for both base (up to\n+4.7% on MMLU) and instruction-tuned (up to +10% on HumanEval) LLMs. Code is\navailable at https://github.com/johnheo/adadim-llm\n","authors":["Jung Hwan Heo","Jeonghoon Kim","Beomseok Kwon","Byeongwook Kim","Se Jung Kwon","Dongsoo Lee"],"pdf_url":"https://arxiv.org/pdf/2309.15531v3.pdf","comment":"ICLR 2024"},{"id":"http://arxiv.org/abs/2412.19160v2","updated":"2025-03-03T06:34:25Z","published":"2024-12-26T10:40:15Z","title":"Cross-Spectral Vision Transformer for Biometric Authentication using\n  Forehead Subcutaneous Vein Pattern and Periocular Pattern","summary":"  Traditional biometric systems have encountered significant setbacks due to\nvarious unavoidable factors, for example, face recognition-based biometrics\nfails due to the wearing of face masks and fingerprints create hygiene\nconcerns. This paper proposes a novel lightweight cross-spectral vision\ntransformer (CS-ViT) for biometric authentication using forehead subcutaneous\nvein patterns and periocular patterns, offering a promising alternative to\ntraditional methods, capable of performing well even with the face masks and\nwithout any physical touch. The proposed framework comprises a cross-spectral\ndual-channel architecture designed to handle two distinct biometric traits and\nto capture inter-dependencies in terms of relative spectral patterns. Each\nchannel consists of a Phase-Only Correlation Cross-Spectral Attention (POC-CSA)\nthat captures their individual as well as correlated patterns. The computation\nof cross-spectral attention using POC extracts the phase correlation in the\nspatial features. Therefore, it is robust against the resolution/intensity\nvariations and illumination of the input images, assuming both biometric traits\nare from the same person. The lightweight model is suitable for edge device\ndeployment. The performance of the proposed algorithm was rigorously evaluated\nusing the Forehead Subcutaneous Vein Pattern and Periocular Biometric Pattern\n(FSVP-PBP) database. The results demonstrated the superiority of the algorithm\nover state-of-the-art methods, achieving a remarkable classification accuracy\nof 98.8% with the combined vein and periocular patterns.\n","authors":["Arun K. Sharma","Shubhobrata Bhattacharya","Motahar Reza","Bishakh Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2412.19160v2.pdf","comment":"Submitted to IEEE TPAMI"},{"id":"http://arxiv.org/abs/2410.01746v2","updated":"2025-03-03T06:17:54Z","published":"2024-10-02T17:01:01Z","title":"Leray-Schauder Mappings for Operator Learning","summary":"  We present an algorithm for learning operators between Banach spaces, based\non the use of Leray-Schauder mappings to learn a finite-dimensional\napproximation of compact subspaces. We show that the resulting method is a\nuniversal approximator of (possibly nonlinear) operators. We demonstrate the\nefficiency of the approach on two benchmark datasets showing it achieves\nresults comparable to state of the art models.\n","authors":["Emanuele Zappala"],"pdf_url":"https://arxiv.org/pdf/2410.01746v2.pdf","comment":"13 pages, 2 figures, 1 table. Comments are welcome! v2: Theoretical\n  analysis expanded, several explanations regarding the experiments have been\n  added for improved clarity"},{"id":"http://arxiv.org/abs/2310.01405v4","updated":"2025-03-03T06:14:14Z","published":"2023-10-02T17:59:07Z","title":"Representation Engineering: A Top-Down Approach to AI Transparency","summary":"  In this paper, we identify and characterize the emerging area of\nrepresentation engineering (RepE), an approach to enhancing the transparency of\nAI systems that draws on insights from cognitive neuroscience. RepE places\npopulation-level representations, rather than neurons or circuits, at the\ncenter of analysis, equipping us with novel methods for monitoring and\nmanipulating high-level cognitive phenomena in deep neural networks (DNNs). We\nprovide baselines and an initial analysis of RepE techniques, showing that they\noffer simple yet effective solutions for improving our understanding and\ncontrol of large language models. We showcase how these methods can provide\ntraction on a wide range of safety-relevant problems, including honesty,\nharmlessness, power-seeking, and more, demonstrating the promise of top-down\ntransparency research. We hope that this work catalyzes further exploration of\nRepE and fosters advancements in the transparency and safety of AI systems.\n","authors":["Andy Zou","Long Phan","Sarah Chen","James Campbell","Phillip Guo","Richard Ren","Alexander Pan","Xuwang Yin","Mantas Mazeika","Ann-Kathrin Dombrowski","Shashwat Goel","Nathaniel Li","Michael J. Byun","Zifan Wang","Alex Mallen","Steven Basart","Sanmi Koyejo","Dawn Song","Matt Fredrikson","J. Zico Kolter","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2310.01405v4.pdf","comment":"Code is available at\n  https://github.com/andyzoujm/representation-engineering"},{"id":"http://arxiv.org/abs/2306.14872v4","updated":"2025-03-03T06:05:48Z","published":"2023-06-26T17:38:45Z","title":"Geometry-Aware Approaches for Balancing Performance and Theoretical\n  Guarantees in Linear Bandits","summary":"  This paper is motivated by recent research in the $d$-dimensional stochastic\nlinear bandit literature, which has revealed an unsettling discrepancy:\nalgorithms like Thompson sampling and Greedy demonstrate promising empirical\nperformance, yet this contrasts with their pessimistic theoretical regret\nbounds. The challenge arises from the fact that while these algorithms may\nperform poorly in certain problem instances, they generally excel in typical\ninstances. To address this, we propose a new data-driven technique that tracks\nthe geometric properties of the uncertainty ellipsoid around the main problem\nparameter. This methodology enables us to formulate a data-driven frequentist\nregret bound, which incorporates the geometric information, for a broad class\nof base algorithms, including Greedy, OFUL, and Thompson sampling. This result\nallows us to identify and ``course-correct\" problem instances in which the base\nalgorithms perform poorly. The course-corrected algorithms achieve the minimax\noptimal regret of order $\\tilde{\\mathcal{O}}(d\\sqrt{T})$ for a $T$-period\ndecision-making scenario, effectively maintaining the desirable attributes of\nthe base algorithms, including their empirical efficacy. We present simulation\nresults to validate our findings using synthetic and real data.\n","authors":["Yuwei Luo","Mohsen Bayati"],"pdf_url":"https://arxiv.org/pdf/2306.14872v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02886v2","updated":"2025-03-03T05:49:41Z","published":"2024-11-05T07:56:24Z","title":"TokenSelect: Efficient Long-Context Inference and Length Extrapolation\n  for LLMs via Dynamic Token-Level KV Cache Selection","summary":"  The rapid advancement of Large Language Models (LLMs) has driven growing\ndemand for processing extended context sequences in contemporary applications.\nHowever, this progress faces two major challenges: performance degradation due\nto sequence lengths out-of-distribution, and excessively long inference times\ncaused by the quadratic computational complexity of attention. These issues\nhinder the application of LLMs in long-context scenarios. In this paper, we\npropose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free\nmethod for efficient and accurate long-context inference. TokenSelect builds\nupon the observation of non-contiguous attention sparsity, using Query-Key dot\nproducts to measure per-head KV Cache criticality at token-level. By per-head\nsoft voting mechanism, TokenSelect selectively involves a few critical KV cache\ntokens in attention calculation without sacrificing accuracy. To further\naccelerate TokenSelect, we design the Selection Cache based on observations of\nconsecutive Query similarity and implemented efficient dot product kernel,\nsignificantly reducing the overhead. A comprehensive evaluation of TokenSelect\ndemonstrates up to 23.84x speedup in attention computation and up to 2.28x\nacceleration in end-to-end latency, while providing superior performance\ncompared to state-of-the-art long-context inference methods.\n","authors":["Wei Wu","Zhuoshi Pan","Chao Wang","Liyi Chen","Yunchu Bai","Tianfu Wang","Kun Fu","Zheng Wang","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2411.02886v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04495v4","updated":"2025-03-03T05:38:10Z","published":"2024-07-05T13:35:14Z","title":"Speed-accuracy relations for the diffusion models: Wisdom from\n  nonequilibrium thermodynamics and optimal transport","summary":"  We discuss a connection between a generative model, called the diffusion\nmodel, and nonequilibrium thermodynamics for the Fokker-Planck equation, called\nstochastic thermodynamics. Based on the techniques of stochastic\nthermodynamics, we derive the speed-accuracy relations for the diffusion\nmodels, which are inequalities that relate the accuracy of data generation to\nthe entropy production rate, which can be interpreted as the speed of the\ndiffusion dynamics in the absence of the non-conservative force. From a\nstochastic thermodynamic perspective, our results provide a quantitative\ninsight into how best to generate data in diffusion models. The optimal\nlearning protocol is introduced by the geodesic of space of the 2-Wasserstein\ndistance in optimal transport theory. We numerically illustrate the validity of\nthe speed-accuracy relations for the diffusion models with different noise\nschedules and the different data. We numerically discuss our results for the\noptimal and suboptimal learning protocols. We also show the inaccurate data\ngeneration due to the non-conservative force, and the applicability of our\nresults to data generation from the real-world image datasets.\n","authors":["Kotaro Ikeda","Tomoya Uda","Daisuke Okanohara","Sosuke Ito"],"pdf_url":"https://arxiv.org/pdf/2407.04495v4.pdf","comment":"36 pages, 7 figures"},{"id":"http://arxiv.org/abs/2410.02268v3","updated":"2025-03-03T05:32:47Z","published":"2024-10-03T07:40:14Z","title":"Structural-Entropy-Based Sample Selection for Efficient and Effective\n  Learning","summary":"  Sample selection improves the efficiency and effectiveness of machine\nlearning models by providing informative and representative samples. Typically,\nsamples can be modeled as a sample graph, where nodes are samples and edges\nrepresent their similarities. Most existing methods are based on local\ninformation, such as the training difficulty of samples, thereby overlooking\nglobal information, such as connectivity patterns. This oversight can result in\nsuboptimal selection because global information is crucial for ensuring that\nthe selected samples well represent the structural properties of the graph. To\naddress this issue, we employ structural entropy to quantify global information\nand losslessly decompose it from the whole graph to individual nodes using the\nShapley value. Based on the decomposition, we present\n$\\textbf{S}$tructural-$\\textbf{E}$ntropy-based sample $\\textbf{S}$election\n($\\textbf{SES}$), a method that integrates both global and local information to\nselect informative and representative samples. SES begins by constructing a\n$k$NN-graph among samples based on their similarities. It then measures sample\nimportance by combining structural entropy (global metric) with training\ndifficulty (local metric). Finally, SES applies importance-biased blue noise\nsampling to select a set of diverse and representative samples. Comprehensive\nexperiments on three learning scenarios -- supervised learning, active\nlearning, and continual learning -- clearly demonstrate the effectiveness of\nour method.\n","authors":["Tianchi Xie","Jiangning Zhu","Guozu Ma","Minzhi Lin","Wei Chen","Weikai Yang","Shixia Liu"],"pdf_url":"https://arxiv.org/pdf/2410.02268v3.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.01912v2","updated":"2025-03-03T05:25:43Z","published":"2025-02-04T01:05:12Z","title":"PATCH: a deep learning method to assess heterogeneity of artistic\n  practice in historical paintings","summary":"  The history of art has seen significant shifts in the manner in which\nartworks are created, making understanding of creative processes a central\nquestion in technical art history. In the Renaissance and Early Modern period,\npaintings were largely produced by master painters directing workshops of\napprentices who often contributed to projects. The masters varied significantly\nin artistic and managerial styles, meaning different combinations of artists\nand implements might be seen both between masters and within workshops or even\nindividual canvases. Information on how different workshops were managed and\nthe processes by which artworks were created remains elusive. Machine learning\nmethods have potential to unearth new information about artists' creative\nprocesses by extending the analysis of brushwork to a microscopic scale.\nAnalysis of workshop paintings, however, presents a challenge in that\ndocumentation of the artists and materials involved is sparse, meaning external\nexamples are not available to train networks to recognize their contributions.\nHere we present a novel machine learning approach we call pairwise assignment\ntraining for classifying heterogeneity (PATCH) that is capable of identifying\nindividual artistic practice regimes with no external training data, or \"ground\ntruth.\" The method achieves unsupervised results by supervised means, and\noutperforms both simple statistical procedures and unsupervised machine\nlearning methods. We apply this method to two historical paintings by the\nSpanish Renaissance master, El Greco: The Baptism of Christ and Christ on the\nCross with Landscape, and our findings regarding the former potentially\nchallenge previous work that has assigned the painting to workshop members.\nFurther, the results of our analyses create a measure of heterogeneity of\nartistic practice that can be used to characterize artworks across time and\nspace.\n","authors":["Andrew Van Horn","Lauryn Smith","Mahamad Mahmoud","Michael McMaster","Clara Pinchbeck","Ina Martin","Andrew Lininger","Anthony Ingrisano","Adam Lowe","Carlos Bayod","Elizabeth Bolman","Kenneth Singer","Michael Hinczewski"],"pdf_url":"https://arxiv.org/pdf/2502.01912v2.pdf","comment":"main text: 16 pages, 6 figures; SI: 7 pages, 3 figures; v2: minor\n  typo corrections, higher resolution figures"},{"id":"http://arxiv.org/abs/2405.13937v8","updated":"2025-03-03T05:10:46Z","published":"2024-05-22T19:10:24Z","title":"Node-Time Conditional Prompt Learning In Dynamic Graphs","summary":"  Dynamic graphs capture evolving interactions between entities, such as in\nsocial networks, online learning platforms, and crowdsourcing projects. For\ndynamic graph modeling, dynamic graph neural networks (DGNNs) have emerged as a\nmainstream technique. However, they are generally pre-trained on the link\nprediction task, leaving a significant gap from the objectives of downstream\ntasks such as node classification. To bridge the gap, prompt-based learning has\ngained traction on graphs, but most existing efforts focus on static graphs,\nneglecting the evolution of dynamic graphs. In this paper, we propose\nDYGPROMPT, a novel pre-training and prompt learning framework for dynamic graph\nmodeling. First, we design dual prompts to address the gap in both task\nobjectives and temporal variations across pre-training and downstream tasks.\nSecond, we recognize that node and time features mutually characterize each\nother, and propose dual condition-nets to model the evolving node-time patterns\nin downstream tasks. Finally, we thoroughly evaluate and analyze DYGPROMPT\nthrough extensive experiments on four public datasets.\n","authors":["Xingtong Yu","Zhenghao Liu","Xinming Zhang","Yuan Fang"],"pdf_url":"https://arxiv.org/pdf/2405.13937v8.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2409.07002v2","updated":"2025-03-03T04:32:29Z","published":"2024-09-11T04:30:45Z","title":"AdvLogo: Adversarial Patch Attack against Object Detectors based on\n  Diffusion Models","summary":"  With the rapid development of deep learning, object detectors have\ndemonstrated impressive performance; however, vulnerabilities still exist in\ncertain scenarios. Current research exploring the vulnerabilities using\nadversarial patches often struggles to balance the trade-off between attack\neffectiveness and visual quality. To address this problem, we propose a novel\nframework of patch attack from semantic perspective, which we refer to as\nAdvLogo. Based on the hypothesis that every semantic space contains an\nadversarial subspace where images can cause detectors to fail in recognizing\nobjects, we leverage the semantic understanding of the diffusion denoising\nprocess and drive the process to adversarial subareas by perturbing the latent\nand unconditional embeddings at the last timestep. To mitigate the distribution\nshift that exposes a negative impact on image quality, we apply perturbation to\nthe latent in frequency domain with the Fourier Transform. Experimental results\ndemonstrate that AdvLogo achieves strong attack performance while maintaining\nhigh visual quality.\n","authors":["Boming Miao","Chunxiao Li","Yao Zhu","Weixiang Sun","Zizhe Wang","Xiaoyi Wang","Chuanlong Xie"],"pdf_url":"https://arxiv.org/pdf/2409.07002v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09906v3","updated":"2025-03-03T04:28:49Z","published":"2024-02-15T12:12:19Z","title":"Generative Representational Instruction Tuning","summary":"  All text-based language problems can be reduced to either generation or\nembedding. Current models only perform well at one or the other. We introduce\ngenerative representational instruction tuning (GRIT) whereby a large language\nmodel is trained to handle both generative and embedding tasks by\ndistinguishing between them through instructions. Compared to other open\nmodels, our resulting GritLM 7B sets a new state of the art on the Massive Text\nEmbedding Benchmark (MTEB) and outperforms all models up to its size on a range\nof generative tasks. By scaling up further, GritLM 8x7B outperforms all open\ngenerative language models that we tried while still being among the best\nembedding models. Notably, we find that GRIT matches training on only\ngenerative or embedding data, thus we can unify both at no performance loss.\nAmong other benefits, the unification via GRIT speeds up Retrieval-Augmented\nGeneration (RAG) by > 60% for long documents, by no longer requiring separate\nretrieval and generation models. Models, code, etc. are freely available at\nhttps://github.com/ContextualAI/gritlm.\n","authors":["Niklas Muennighoff","Hongjin Su","Liang Wang","Nan Yang","Furu Wei","Tao Yu","Amanpreet Singh","Douwe Kiela"],"pdf_url":"https://arxiv.org/pdf/2402.09906v3.pdf","comment":"67 pages (16 main), 25 figures, 34 tables"},{"id":"http://arxiv.org/abs/2403.17010v3","updated":"2025-03-03T04:22:19Z","published":"2024-03-25T17:59:59Z","title":"Calib3D: Calibrating Model Preferences for Reliable 3D Scene\n  Understanding","summary":"  Safety-critical 3D scene understanding tasks necessitate not only accurate\nbut also confident predictions from 3D perception models. This study introduces\nCalib3D, a pioneering effort to benchmark and scrutinize the reliability of 3D\nscene understanding models from an uncertainty estimation viewpoint. We\ncomprehensively evaluate 28 state-of-the-art models across 10 diverse 3D\ndatasets, uncovering insightful phenomena that cope with both the aleatoric and\nepistemic uncertainties in 3D scene understanding. We discover that despite\nachieving impressive levels of accuracy, existing models frequently fail to\nprovide reliable uncertainty estimates -- a pitfall that critically undermines\ntheir applicability in safety-sensitive contexts. Through extensive analysis of\nkey factors such as network capacity, LiDAR representations, rasterization\nresolutions, and 3D data augmentation techniques, we correlate these aspects\ndirectly with the model calibration efficacy. Furthermore, we introduce DeptS,\na novel depth-aware scaling approach aimed at enhancing 3D model calibration.\nExtensive experiments across a wide range of configurations validate the\nsuperiority of our method. We hope this work could serve as a cornerstone for\nfostering reliable 3D scene understanding. Code and benchmark toolkit are\npublicly available.\n","authors":["Lingdong Kong","Xiang Xu","Jun Cen","Wenwei Zhang","Liang Pan","Kai Chen","Ziwei Liu"],"pdf_url":"https://arxiv.org/pdf/2403.17010v3.pdf","comment":"WACV 2025 Oral; 26 pages, 8 figures, 12 tables; Code at\n  https://github.com/ldkong1205/Calib3D"},{"id":"http://arxiv.org/abs/2410.08892v2","updated":"2025-03-03T04:14:17Z","published":"2024-10-11T15:10:38Z","title":"Federated Learning in Practice: Reflections and Projections","summary":"  Federated Learning (FL) is a machine learning technique that enables multiple\nentities to collaboratively learn a shared model without exchanging their local\ndata. Over the past decade, FL systems have achieved substantial progress,\nscaling to millions of devices across various learning domains while offering\nmeaningful differential privacy (DP) guarantees. Production systems from\norganizations like Google, Apple, and Meta demonstrate the real-world\napplicability of FL. However, key challenges remain, including verifying\nserver-side DP guarantees and coordinating training across heterogeneous\ndevices, limiting broader adoption. Additionally, emerging trends such as large\n(multi-modal) models and blurred lines between training, inference, and\npersonalization challenge traditional FL frameworks. In response, we propose a\nredefined FL framework that prioritizes privacy principles rather than rigid\ndefinitions. We also chart a path forward by leveraging trusted execution\nenvironments and open-source ecosystems to address these challenges and\nfacilitate future advancements in FL.\n","authors":["Katharine Daly","Hubert Eichner","Peter Kairouz","H. Brendan McMahan","Daniel Ramage","Zheng Xu"],"pdf_url":"https://arxiv.org/pdf/2410.08892v2.pdf","comment":"Published at 2024 IEEE 6th International Conference on Trust, Privacy\n  and Security in Intelligent Systems, and Applications (TPS-ISA)"},{"id":"http://arxiv.org/abs/2411.02728v2","updated":"2025-03-03T04:04:30Z","published":"2024-11-05T01:55:07Z","title":"Compositional simulation-based inference for time series","summary":"  Amortized simulation-based inference (SBI) methods train neural networks on\nsimulated data to perform Bayesian inference. While this strategy avoids the\nneed for tractable likelihoods, it often requires a large number of simulations\nand has been challenging to scale to time series data. Scientific simulators\nfrequently emulate real-world dynamics through thousands of single-state\ntransitions over time. We propose an SBI approach that can exploit such\nMarkovian simulators by locally identifying parameters consistent with\nindividual state transitions. We then compose these local results to obtain a\nposterior over parameters that align with the entire time series observation.\nWe focus on applying this approach to neural posterior score estimation but\nalso show how it can be applied, e.g., to neural likelihood (ratio) estimation.\nWe demonstrate that our approach is more simulation-efficient than directly\nestimating the global posterior on several synthetic benchmark tasks and\nsimulators used in ecology and epidemiology. Finally, we validate scalability\nand simulation efficiency of our approach by applying it to a high-dimensional\nKolmogorov flow simulator with around one million data dimensions.\n","authors":["Manuel Gloeckler","Shoji Toyota","Kenji Fukumizu","Jakob H. Macke"],"pdf_url":"https://arxiv.org/pdf/2411.02728v2.pdf","comment":"To be published in the proceedings of the Thirteenth International\n  Conference on Learning Representations (ICLR 2025), Singapore, 2025"},{"id":"http://arxiv.org/abs/2502.02954v2","updated":"2025-03-03T03:56:38Z","published":"2025-02-05T07:35:15Z","title":"Direct Distributional Optimization for Provable Alignment of Diffusion\n  Models","summary":"  We introduce a novel alignment method for diffusion models from distribution\noptimization perspectives while providing rigorous convergence guarantees. We\nfirst formulate the problem as a generic regularized loss minimization over\nprobability distributions and directly optimize the distribution using the Dual\nAveraging method. Next, we enable sampling from the learned distribution by\napproximating its score function via Doob's $h$-transform technique. The\nproposed framework is supported by rigorous convergence guarantees and an\nend-to-end bound on the sampling error, which imply that when the original\ndistribution's score is known accurately, the complexity of sampling from\nshifted distributions is independent of isoperimetric conditions. This\nframework is broadly applicable to general distribution optimization problems,\nincluding alignment tasks in Reinforcement Learning with Human Feedback (RLHF),\nDirect Preference Optimization (DPO), and Kahneman-Tversky Optimization (KTO).\nWe empirically validate its performance on synthetic and image datasets using\nthe DPO objective.\n","authors":["Ryotaro Kawata","Kazusato Oko","Atsushi Nitanda","Taiji Suzuki"],"pdf_url":"https://arxiv.org/pdf/2502.02954v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.00617v4","updated":"2025-03-03T03:41:11Z","published":"2024-06-30T08:00:34Z","title":"Iterative Nash Policy Optimization: Aligning LLMs with General\n  Preferences via No-Regret Learning","summary":"  Reinforcement Learning with Human Feedback (RLHF) has achieved great success\nin aligning large language models (LLMs) with human preferences. Prevalent RLHF\napproaches are reward-based, following the Bradley-Terry (BT) model assumption,\nwhich may not fully capture the complexity of human preferences. In this paper,\nwe explore RLHF under a general preference framework and approach it from a\ngame-theoretic perspective. Specifically, we formulate the problem as a\ntwo-player game and propose a novel online algorithm, iterative Nash policy\noptimization (INPO). The key idea is to let the policy play against itself via\nno-regret learning, thereby approximating the Nash policy. Unlike previous\nmethods, INPO bypasses the need for estimating the expected win rate for\nindividual responses, which typically incurs high computational or annotation\ncosts. Instead, we introduce a new loss objective that is directly minimized\nover a preference dataset. We provide theoretical analysis for our approach and\ndemonstrate its effectiveness through experiments on various representative\nbenchmarks. With an LLaMA-3-8B-based SFT model, INPO achieves a 42.6%\nlength-controlled win rate on AlpacaEval 2.0 and a 37.8% win rate on\nArena-Hard, showing substantial improvement over the state-of-the-art online\nRLHF algorithms.\n","authors":["Yuheng Zhang","Dian Yu","Baolin Peng","Linfeng Song","Ye Tian","Mingyue Huo","Nan Jiang","Haitao Mi","Dong Yu"],"pdf_url":"https://arxiv.org/pdf/2407.00617v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.04405v3","updated":"2025-03-03T03:39:50Z","published":"2024-07-05T10:41:15Z","title":"Discovering physical laws with parallel combinatorial tree search","summary":"  Symbolic regression plays a crucial role in modern scientific research thanks\nto its capability of discovering concise and interpretable mathematical\nexpressions from data. A grand challenge lies in the arduous search for\nparsimonious and generalizable mathematical formulas, in an infinite search\nspace, while intending to fit the training data. Existing algorithms have faced\na critical bottleneck of accuracy and efficiency over a decade when handling\nproblems of complexity, which essentially hinders the pace of applying symbolic\nregression for scientific exploration across interdisciplinary domains. To this\nend, we introduce a parallel combinatorial tree search (PCTS) model to\nefficiently distill generic mathematical expressions from limited data. Through\na series of extensive experiments, we demonstrate the superior accuracy and\nefficiency of PCTS for equation discovery, which greatly outperforms the\nstate-of-the-art baseline models on over 200 synthetic and experimental\ndatasets (e.g., lifting its performance by up to 99% accuracy improvement and\none-order of magnitude speed up). PCTS represents a key advance in accurate and\nefficient data-driven discovery of symbolic, interpretable models (e.g.,\nunderlying physical laws) and marks a pivotal transition towards scalable\nsymbolic learning.\n","authors":["Kai Ruan","Yilong Xu","Ze-Feng Gao","Yike Guo","Hao Sun","Ji-Rong Wen","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2407.04405v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01117v2","updated":"2025-03-03T03:35:00Z","published":"2025-02-03T07:13:59Z","title":"Learning to Learn Weight Generation via Trajectory Diffusion","summary":"  Diffusion-based algorithms have emerged as promising techniques for weight\ngeneration, particularly in scenarios like multi-task learning that require\nfrequent weight updates. However, existing solutions suffer from limited\ncross-task transferability. In addition, they only utilize optimal weights as\ntraining samples, ignoring the value of other weights in the optimization\nprocess. To address these issues, we propose Lt-Di, which integrates the\ndiffusion algorithm with meta-learning to generate weights for unseen tasks.\nFurthermore, we extend the vanilla diffusion algorithm into a trajectory\ndiffusion algorithm to utilize other weights along the optimization trajectory.\nTrajectory diffusion decomposes the entire diffusion chain into multiple\nshorter ones, improving training and inference efficiency. We analyze the\nconvergence properties of the weight generation paradigm and improve\nconvergence efficiency without additional time overhead. Our experiments\ndemonstrate Lt-Di's higher accuracy while reducing computational overhead\nacross various tasks, including zero-shot and few-shot learning, multi-domain\ngeneralization, and large-scale language model fine-tuning.Our code is released\nat https://anonymous.4open.science/r/Lt-Di-0E51.\n","authors":["Yunchuan Guan","Yu Liu","Ke Zhou","Zhiqi Shen","Serge Belongie","Jenq-Neng Hwang","Lei Li"],"pdf_url":"https://arxiv.org/pdf/2502.01117v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.17674v2","updated":"2025-03-03T03:24:09Z","published":"2024-07-24T23:47:05Z","title":"Struc2mapGAN: improving synthetic cryo-EM density maps with generative\n  adversarial networks","summary":"  Generating synthetic cryogenic electron microscopy 3D density maps from\nmolecular structures has potential important applications in structural\nbiology. Yet existing simulation-based methods cannot mimic all the complex\nfeatures present in experimental maps, such as secondary structure elements. As\nan alternative, we propose struc2mapGAN, a novel data-driven method that\nemploys a generative adversarial network to produce improved experimental-like\ndensity maps from molecular structures. More specifically, struc2mapGAN uses a\nnested U-Net architecture as the generator, with an additional L1 loss term and\nfurther processing of raw training experimental maps to enhance learning\nefficiency. While struc2mapGAN can promptly generate maps after training, we\ndemonstrate that it outperforms existing simulation-based methods for a wide\narray of tested maps and across various evaluation metrics.\n","authors":["Chenwei Zhang","Anne Condon","Khanh Dao Duc"],"pdf_url":"https://arxiv.org/pdf/2407.17674v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13213v2","updated":"2025-03-03T03:20:08Z","published":"2024-10-17T04:37:37Z","title":"LLMOPT: Learning to Define and Solve General Optimization Problems from\n  Scratch","summary":"  Optimization problems are prevalent across various scenarios. Formulating and\nthen solving optimization problems described by natural language often requires\nhighly specialized human expertise, which could block the widespread\napplication of optimization-based decision making. To automate problem\nformulation and solving, leveraging large language models (LLMs) has emerged as\na potential way. However, this kind of approach suffers from the issue of\noptimization generalization. Namely, the accuracy of most current LLM-based\nmethods and the generality of optimization problem types that they can model\nare still limited. In this paper, we propose a unified learning-based framework\ncalled LLMOPT to boost optimization generalization. Starting from the natural\nlanguage descriptions of optimization problems and a pre-trained LLM, LLMOPT\nconstructs the introduced five-element formulation as a universal model for\nlearning to define diverse optimization problem types. Then, LLMOPT employs the\nmulti-instruction tuning to enhance both problem formalization and solver code\ngeneration accuracy and generality. After that, to prevent hallucinations in\nLLMs, such as sacrificing solving accuracy to avoid execution errors, the model\nalignment and self-correction mechanism are adopted in LLMOPT. We evaluate the\noptimization generalization ability of LLMOPT and compared methods across six\nreal-world datasets covering roughly 20 fields such as health, environment,\nenergy and manufacturing, etc. Extensive experiment results show that LLMOPT is\nable to model various optimization problem types such as linear/nonlinear\nprogramming, mixed integer programming, and combinatorial optimization, and\nachieves a notable 11.08% average solving accuracy improvement compared with\nthe state-of-the-art methods. The code is available at\nhttps://github.com/caigaojiang/LLMOPT.\n","authors":["Caigao Jiang","Xiang Shu","Hong Qian","Xingyu Lu","Jun Zhou","Aimin Zhou","Yang Yu"],"pdf_url":"https://arxiv.org/pdf/2410.13213v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00645v2","updated":"2025-03-03T03:19:08Z","published":"2024-10-01T12:58:37Z","title":"TSVD: Bridging Theory and Practice in Continual Learning with\n  Pre-trained Models","summary":"  The goal of continual learning (CL) is to train a model that can solve\nmultiple tasks presented sequentially. Recent CL approaches have achieved\nstrong performance by leveraging large pre-trained models that generalize well\nto downstream tasks. However, such methods lack theoretical guarantees, making\nthem prone to unexpected failures. Conversely, principled CL approaches often\nfail to achieve competitive performance. In this work, we aim to bridge this\ngap between theory and practice by designing a simple CL method that is\ntheoretically sound and highly performant. Specifically, we lift pre-trained\nfeatures into a higher dimensional space and formulate an over-parametrized\nminimum-norm least-squares problem. We find that the lifted features are highly\nill-conditioned, potentially leading to large training errors (numerical\ninstability) and increased generalization errors. We address these challenges\nby continually truncating the singular value decomposition (SVD) of the lifted\nfeatures. Our approach, termed TSVD, is stable with respect to the choice of\nhyperparameters, can handle hundreds of tasks, and outperforms state-of-the-art\nCL methods on multiple datasets. Importantly, our method satisfies a recurrence\nrelation throughout its continual learning process, which allows us to prove it\nmaintains small training and generalization errors by appropriately truncating\na fraction of SVD factors. This results in a stable continual learning method\nwith strong empirical performance and theoretical guarantees. Code available:\nhttps://github.com/liangzu/tsvd.\n","authors":["Liangzu Peng","Juan Elenter","Joshua Agterberg","Alejandro Ribeiro","René Vidal"],"pdf_url":"https://arxiv.org/pdf/2410.00645v2.pdf","comment":"47 pages, 18 figures, 16 tables (v2, accepted to ICLR 2025)"},{"id":"http://arxiv.org/abs/2410.13085v2","updated":"2025-03-03T03:08:28Z","published":"2024-10-16T23:03:27Z","title":"MMed-RAG: Versatile Multimodal RAG System for Medical Vision Language\n  Models","summary":"  Artificial Intelligence (AI) has demonstrated significant potential in\nhealthcare, particularly in disease diagnosis and treatment planning. Recent\nprogress in Medical Large Vision-Language Models (Med-LVLMs) has opened up new\npossibilities for interactive diagnostic tools. However, these models often\nsuffer from factual hallucination, which can lead to incorrect diagnoses.\nFine-tuning and retrieval-augmented generation (RAG) have emerged as methods to\naddress these issues. However, the amount of high-quality data and distribution\nshifts between training data and deployment data limit the application of\nfine-tuning methods. Although RAG is lightweight and effective, existing\nRAG-based approaches are not sufficiently general to different medical domains\nand can potentially cause misalignment issues, both between modalities and\nbetween the model and the ground truth. In this paper, we propose a versatile\nmultimodal RAG system, MMed-RAG, designed to enhance the factuality of\nMed-LVLMs. Our approach introduces a domain-aware retrieval mechanism, an\nadaptive retrieved contexts selection method, and a provable RAG-based\npreference fine-tuning strategy. These innovations make the RAG process\nsufficiently general and reliable, significantly improving alignment when\nintroducing retrieved contexts. Experimental results across five medical\ndatasets (involving radiology, ophthalmology, pathology) on medical VQA and\nreport generation demonstrate that MMed-RAG can achieve an average improvement\nof 43.8% in the factual accuracy of Med-LVLMs. Our data and code are available\nin https://github.com/richard-peng-xia/MMed-RAG.\n","authors":["Peng Xia","Kangyu Zhu","Haoran Li","Tianze Wang","Weijia Shi","Sheng Wang","Linjun Zhang","James Zou","Huaxiu Yao"],"pdf_url":"https://arxiv.org/pdf/2410.13085v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2406.06600v3","updated":"2025-03-03T03:05:30Z","published":"2024-06-06T13:44:57Z","title":"HORAE: A Domain-Agnostic Modeling Language for Automating Multimodal\n  Service Regulation","summary":"  Artificial intelligence is rapidly encroaching on the field of service\nregulation. This work-in-progress article presents the design principles behind\nHORAE, a unified specification language to model multimodal regulation rules\nacross a diverse set of domains. We show how HORAE facilitates an intelligent\nservice regulation pipeline by further exploiting a fine-tuned large language\nmodel named HORAE that automates the HORAE modeling process, thereby yielding\nan end-to-end framework for fully automated intelligent service regulation.\n","authors":["Yutao Sun","Mingshuai Chen","Kangjia Zhao","Jintao Chen"],"pdf_url":"https://arxiv.org/pdf/2406.06600v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00564v3","updated":"2025-03-03T02:59:29Z","published":"2024-10-01T10:25:03Z","title":"Scaling Offline Model-Based RL via Jointly-Optimized World-Action Model\n  Pretraining","summary":"  A significant aspiration of offline reinforcement learning (RL) is to develop\na generalist agent with high capabilities from large and heterogeneous\ndatasets. However, prior approaches that scale offline RL either rely heavily\non expert trajectories or struggle to generalize to diverse unseen tasks.\nInspired by the excellent generalization of world model in conditional video\ngeneration, we explore the potential of image observation-based world model for\nscaling offline RL and enhancing generalization on novel tasks. In this paper,\nwe introduce JOWA: Jointly-Optimized World-Action model, an offline model-based\nRL agent pretrained on multiple Atari games with 6 billion tokens data to learn\ngeneral-purpose representation and decision-making ability. Our method jointly\noptimizes a world-action model through a shared transformer backbone, which\nstabilize temporal difference learning with large models during pretraining.\nMoreover, we propose a provably efficient and parallelizable planning algorithm\nto compensate for the Q-value estimation error and thus search out better\npolicies. Experimental results indicate that our largest agent, with 150\nmillion parameters, achieves 78.9% human-level performance on pretrained games\nusing only 10% subsampled offline data, outperforming existing state-of-the-art\nlarge-scale offline RL baselines by 31.6% on averange. Furthermore, JOWA scales\nfavorably with model capacity and can sample-efficiently transfer to novel\ngames using only 5k offline fine-tuning data (approximately 4 trajectories) per\ngame, demonstrating superior generalization. We will release codes and model\nweights at https://github.com/CJReinforce/JOWA\n","authors":["Jie Cheng","Ruixi Qiao","Yingwei Ma","Binhua Li","Gang Xiong","Qinghai Miao","Yongbin Li","Yisheng Lv"],"pdf_url":"https://arxiv.org/pdf/2410.00564v3.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2410.01337v3","updated":"2025-03-03T02:50:30Z","published":"2024-10-02T08:54:18Z","title":"PhyMPGN: Physics-encoded Message Passing Graph Network for\n  spatiotemporal PDE systems","summary":"  Solving partial differential equations (PDEs) serves as a cornerstone for\nmodeling complex dynamical systems. Recent progresses have demonstrated grand\nbenefits of data-driven neural-based models for predicting spatiotemporal\ndynamics (e.g., tremendous speedup gain compared with classical numerical\nmethods). However, most existing neural models rely on rich training data, have\nlimited extrapolation and generalization abilities, and suffer to produce\nprecise or reliable physical prediction under intricate conditions (e.g.,\nirregular mesh or geometry, complex boundary conditions, diverse PDE\nparameters, etc.). To this end, we propose a new graph learning approach,\nnamely, Physics-encoded Message Passing Graph Network (PhyMPGN), to model\nspatiotemporal PDE systems on irregular meshes given small training datasets.\nSpecifically, we incorporate a GNN into a numerical integrator to approximate\nthe temporal marching of spatiotemporal dynamics for a given PDE system.\nConsidering that many physical phenomena are governed by diffusion processes,\nwe further design a learnable Laplace block, which encodes the discrete\nLaplace-Beltrami operator, to aid and guide the GNN learning in a physically\nfeasible solution space. A boundary condition padding strategy is also designed\nto improve the model convergence and accuracy. Extensive experiments\ndemonstrate that PhyMPGN is capable of accurately predicting various types of\nspatiotemporal dynamics on coarse unstructured meshes, consistently achieves\nthe state-of-the-art results, and outperforms other baselines with considerable\ngains.\n","authors":["Bocheng Zeng","Qi Wang","Mengtao Yan","Yang Liu","Ruizhi Chengze","Yi Zhang","Hongsheng Liu","Zidong Wang","Hao Sun"],"pdf_url":"https://arxiv.org/pdf/2410.01337v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.08109v3","updated":"2025-03-03T02:45:58Z","published":"2024-10-10T16:56:05Z","title":"A Closer Look at Machine Unlearning for Large Language Models","summary":"  Large language models (LLMs) may memorize sensitive or copyrighted content,\nraising privacy and legal concerns. Due to the high cost of retraining from\nscratch, researchers attempt to employ machine unlearning to remove specific\ncontent from LLMs while preserving the overall performance. In this paper, we\ndiscuss several issues in machine unlearning for LLMs and provide our insights\non possible approaches. To address the issue of inadequate evaluation of model\noutputs after unlearning, we introduce three additional metrics to evaluate\ntoken diversity, sentence semantics, and factual correctness. We then\ncategorize unlearning methods into untargeted and targeted, and discuss their\nissues respectively. Specifically, the behavior that untargeted unlearning\nattempts to approximate is unpredictable and may involve hallucinations, and\nexisting regularization is insufficient for targeted unlearning. To alleviate\nthese issues, we propose using the objective of maximizing entropy (ME) for\nuntargeted unlearning and incorporate answer preservation (AP) loss as\nregularization for targeted unlearning. Experimental results across three\nscenarios, i.e., fictitious unlearning, continual unlearning, and real-world\nunlearning, demonstrate the effectiveness of our approaches. The code is\navailable at https://github.com/sail-sg/closer-look-LLM-unlearning.\n","authors":["Xiaojian Yuan","Tianyu Pang","Chao Du","Kejiang Chen","Weiming Zhang","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2410.08109v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2411.18872v2","updated":"2025-03-03T02:41:10Z","published":"2024-11-28T02:50:42Z","title":"A Lean Dataset for International Math Olympiad: Small Steps towards\n  Writing Math Proofs for Hard Problems","summary":"  Using AI to write formal proofs for mathematical problems is a challenging\ntask that has seen some advancements in recent years. Automated systems such as\nLean can verify the correctness of proofs written in formal language, yet\nwriting the proofs in formal language can be challenging for humans and\nmachines. The miniF2F benchmark has 20 IMO problems in its test set, yet formal\nproofs are available only for 6 of these problems (3 of which are only written\nby mathematicians). The model with best accuracy can only prove 2 of these 20\nIMO problems, from 1950s and 60s, while its training set is a secret. In this\nwork, we write complete, original formal proofs for the remaining IMO problems\nin Lean along with 3 extra problems from IMO 2022 and 2023. This effort expands\nthe availability of proof currently in the public domain by creating 5,880\nlines of Lean proof. The goal of the paper is to pave the way for developing AI\nmodels that can automatically write the formal proofs for all the IMO problems\nin miniF2F and beyond by providing an evaluation benchmark. In this pursuit, we\ndevise a method to decompose the proofs of these problems into their building\nblocks, constructing a dataset of 1,329 lemmas with more than 40k lines of Lean\ncode. These lemmas are not trivial, yet they are approachable, providing the\nopportunity to evaluate and diagnose the failures and successes of AI models.\nWe evaluate the ability of the SOTA LLMs on our dataset and analyze their\nsuccess and failure modes from different perspectives. Our dataset and code is\navailable at: https://github.com/roozbeh-yz/IMO-Steps.\n","authors":["Roozbeh Yousefzadeh","Xuenan Cao","Azim Ospanov"],"pdf_url":"https://arxiv.org/pdf/2411.18872v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.21186v2","updated":"2025-03-03T02:33:31Z","published":"2025-02-28T16:02:23Z","title":"Scalable Decision-Making in Stochastic Environments through Learned\n  Temporal Abstraction","summary":"  Sequential decision-making in high-dimensional continuous action spaces,\nparticularly in stochastic environments, faces significant computational\nchallenges. We explore this challenge in the traditional offline RL setting,\nwhere an agent must learn how to make decisions based on data collected through\na stochastic behavior policy. We present Latent Macro Action Planner (L-MAP),\nwhich addresses this challenge by learning a set of temporally extended\nmacro-actions through a state-conditional Vector Quantized Variational\nAutoencoder (VQ-VAE), effectively reducing action dimensionality. L-MAP employs\na (separate) learned prior model that acts as a latent transition model and\nallows efficient sampling of plausible actions. During planning, our approach\naccounts for stochasticity in both the environment and the behavior policy by\nusing Monte Carlo tree search (MCTS). In offline RL settings, including\nstochastic continuous control tasks, L-MAP efficiently searches over discrete\nlatent actions to yield high expected returns. Empirical results demonstrate\nthat L-MAP maintains low decision latency despite increased action\ndimensionality. Notably, across tasks ranging from continuous control with\ninherently stochastic dynamics to high-dimensional robotic hand manipulation,\nL-MAP significantly outperforms existing model-based methods and performs\non-par with strong model-free actor-critic baselines, highlighting the\neffectiveness of the proposed approach in planning in complex and stochastic\nenvironments with high-dimensional action spaces.\n","authors":["Baiting Luo","Ava Pettet","Aron Laszka","Abhishek Dubey","Ayan Mukhopadhyay"],"pdf_url":"https://arxiv.org/pdf/2502.21186v2.pdf","comment":"Accepted by ICLR2025. Code would be available at\n  https://github.com/BaitingLuo/L-MAP.git"},{"id":"http://arxiv.org/abs/2412.01021v2","updated":"2025-03-03T02:13:49Z","published":"2024-12-02T00:41:25Z","title":"On the Feature Learning in Diffusion Models","summary":"  The predominant success of diffusion models in generative modeling has\nspurred significant interest in understanding their theoretical foundations. In\nthis work, we propose a feature learning framework aimed at analyzing and\ncomparing the training dynamics of diffusion models with those of traditional\nclassification models. Our theoretical analysis demonstrates that diffusion\nmodels, due to the denoising objective, are encouraged to learn more balanced\nand comprehensive representations of the data. In contrast, neural networks\nwith a similar architecture trained for classification tend to prioritize\nlearning specific patterns in the data, often focusing on easy-to-learn\ncomponents. To support these theoretical insights, we conduct several\nexperiments on both synthetic and real-world datasets, which empirically\nvalidate our findings and highlight the distinct feature learning dynamics in\ndiffusion models compared to classification.\n","authors":["Andi Han","Wei Huang","Yuan Cao","Difan Zou"],"pdf_url":"https://arxiv.org/pdf/2412.01021v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.19228v3","updated":"2025-03-03T02:06:52Z","published":"2024-04-30T03:15:04Z","title":"Weighted Point Set Embedding for Multimodal Contrastive Learning Toward\n  Optimal Similarity Metric","summary":"  In typical multimodal contrastive learning, such as CLIP, encoders produce\none point in the latent representation space for each input. However, one-point\nrepresentation has difficulty in capturing the relationship and the similarity\nstructure of a huge amount of instances in the real world. For richer classes\nof the similarity, we propose the use of weighted point sets, namely, sets of\npairs of weight and vector, as representations of instances. In this work, we\ntheoretically show the benefit of our proposed method through a new\nunderstanding of the contrastive loss of CLIP, which we call symmetric InfoNCE.\nWe clarify that the optimal similarity that minimizes symmetric InfoNCE is the\npointwise mutual information, and show an upper bound of excess risk on\ndownstream classification tasks of representations that achieve the optimal\nsimilarity. In addition, we show that our proposed similarity based on weighted\npoint sets consistently achieves the optimal similarity. To verify the\neffectiveness of our proposed method, we demonstrate pretraining of text-image\nrepresentation models and classification tasks on common benchmarks.\n","authors":["Toshimitsu Uesaka","Taiji Suzuki","Yuhta Takida","Chieh-Hsin Lai","Naoki Murata","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2404.19228v3.pdf","comment":"ICLR 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2309.13838v2","updated":"2025-03-03T01:47:00Z","published":"2023-09-25T02:50:22Z","title":"Penalized Principal Component Analysis Using Smoothing","summary":"  Principal components computed via PCA (principal component analysis) are\ntraditionally used to reduce dimensionality in genomic data or to correct for\npopulation stratification. In this paper, we explore the penalized eigenvalue\nproblem (PEP) which reformulates the computation of the first eigenvector as an\noptimization problem and adds an $L_1$ penalty constraint to enforce sparseness\nof the solution. The contribution of our article is threefold. First, we extend\nPEP by applying smoothing to the original LASSO-type $L_1$ penalty. This allows\none to compute analytical gradients which enable faster and more efficient\nminimization of the objective function associated with the optimization\nproblem. Second, we demonstrate how higher order eigenvectors can be calculated\nwith PEP using established results from singular value decomposition (SVD).\nThird, we present four experimental studies to demonstrate the usefulness of\nthe smoothed penalized eigenvectors. Using data from the 1000 Genomes Project\ndataset, we empirically demonstrate that our proposed smoothed PEP allows one\nto increase numerical stability and obtain meaningful eigenvectors. We also\nemploy the penalized eigenvector approach in two additional real data\napplications (computation of a polygenic risk score and clustering),\ndemonstrating that exchanging the penalized eigenvectors for their smoothed\ncounterparts can increase prediction accuracy in polygenic risk scores and\nenhance discernibility of clusterings. Moreover, we compare our proposed\nsmoothed PEP to seven state-of-the-art algorithms for sparse PCA and evaluate\nthe accuracy of the obtained eigenvectors, their support recovery, and their\nruntime.\n","authors":["Rebecca M. Hurwitz","Georg Hahn"],"pdf_url":"https://arxiv.org/pdf/2309.13838v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.02060v2","updated":"2025-03-03T01:25:46Z","published":"2024-09-03T17:08:20Z","title":"OLMoE: Open Mixture-of-Experts Language Models","summary":"  We introduce OLMoE, a fully open, state-of-the-art language model leveraging\nsparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but\nuses only 1B per input token. We pretrain it on 5 trillion tokens and further\nadapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available\nmodels with similar active parameters, even surpassing larger ones like\nLlama2-13B-Chat and DeepSeekMoE-16B. We present various experiments on MoE\ntraining, analyze routing in our model showing high specialization, and\nopen-source all aspects of our work: model weights, training data, code, and\nlogs.\n","authors":["Niklas Muennighoff","Luca Soldaini","Dirk Groeneveld","Kyle Lo","Jacob Morrison","Sewon Min","Weijia Shi","Pete Walsh","Oyvind Tafjord","Nathan Lambert","Yuling Gu","Shane Arora","Akshita Bhagia","Dustin Schwenk","David Wadden","Alexander Wettig","Binyuan Hui","Tim Dettmers","Douwe Kiela","Ali Farhadi","Noah A. Smith","Pang Wei Koh","Amanpreet Singh","Hannaneh Hajishirzi"],"pdf_url":"https://arxiv.org/pdf/2409.02060v2.pdf","comment":"63 pages (24 main), 36 figures, 17 tables"},{"id":"http://arxiv.org/abs/2407.10223v2","updated":"2025-03-03T01:21:39Z","published":"2024-07-14T14:26:17Z","title":"On Large Language Model Continual Unlearning","summary":"  While large language models have demonstrated impressive performance across\nvarious domains and tasks, their security issues have become increasingly\nsevere. Machine unlearning has emerged as a representative approach for model\nsafety and security by removing the influence of undesired data on the target\nmodel. However, these methods do not sufficiently consider that unlearning\nrequests in real-world scenarios are continuously emerging, especially in the\ncontext of LLMs, which may lead to accumulated model utility loss that\neventually becomes unacceptable. Moreover, existing LLM unlearning methods\noften ignore previous data access limitations due to privacy concerns and\ncopyright protection. Without previous data, the utility preservation during\nunlearning is much harder. To overcome these challenges, we propose the OOO\nframework that includes an Orthogonal low-rank adapter (LoRA) for continually\nunlearning requested data and an Out-Of-Distribution (OOD) detector to measure\nthe similarity between input and unlearning data. The orthogonal LoRA achieves\nparameter disentanglement among continual unlearning requests. The OOD detector\nis trained with a novel contrastive entropy loss and utilizes a glocal-aware\nscoring mechanism. During inference, our OOO framework can decide whether and\nto what extent to load the unlearning LoRA based on the OOD detector's\npredicted similarity between the input and the unlearned knowledge. Notably,\nOOO's effectiveness does not rely on any retained data. We conducted extensive\nexperiments on OOO and state-of-the-art LLM unlearning methods across three\ntasks and seven datasets. The results indicate that OOO consistently achieves\nthe best unlearning effectiveness and utility preservation, especially when\nfacing continuous unlearning requests. The source codes can be found at\nhttps://github.com/GCYZSL/O3-LLM-UNLEARNING.\n","authors":["Chongyang Gao","Lixu Wang","Kaize Ding","Chenkai Weng","Xiao Wang","Qi Zhu"],"pdf_url":"https://arxiv.org/pdf/2407.10223v2.pdf","comment":"This paper has been accepted by ICLR 2025. The first two authors\n  contribute equally and they are ordered alphabetically"},{"id":"http://arxiv.org/abs/2407.10967v2","updated":"2025-03-03T01:19:23Z","published":"2024-07-15T17:59:23Z","title":"BECAUSE: Bilinear Causal Representation for Generalizable Offline\n  Model-based Reinforcement Learning","summary":"  Offline model-based reinforcement learning (MBRL) enhances data efficiency by\nutilizing pre-collected datasets to learn models and policies, especially in\nscenarios where exploration is costly or infeasible. Nevertheless, its\nperformance often suffers from the objective mismatch between model and policy\nlearning, resulting in inferior performance despite accurate model predictions.\nThis paper first identifies the primary source of this mismatch comes from the\nunderlying confounders present in offline data for MBRL. Subsequently, we\nintroduce \\textbf{B}ilin\\textbf{E}ar \\textbf{CAUS}al\nr\\textbf{E}presentation~(BECAUSE), an algorithm to capture causal\nrepresentation for both states and actions to reduce the influence of the\ndistribution shift, thus mitigating the objective mismatch problem.\nComprehensive evaluations on 18 tasks that vary in data quality and environment\ncontext demonstrate the superior performance of BECAUSE over existing offline\nRL algorithms. We show the generalizability and robustness of BECAUSE under\nfewer samples or larger numbers of confounders. Additionally, we offer\ntheoretical analysis of BECAUSE to prove its error bound and sample efficiency\nwhen integrating causal representation into offline MBRL.\n","authors":["Haohong Lin","Wenhao Ding","Jian Chen","Laixi Shi","Jiacheng Zhu","Bo Li","Ding Zhao"],"pdf_url":"https://arxiv.org/pdf/2407.10967v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14519v2","updated":"2025-03-03T00:51:41Z","published":"2024-09-22T16:25:31Z","title":"RobotFingerPrint: Unified Gripper Coordinate Space for Multi-Gripper\n  Grasp Synthesis and Transfer","summary":"  We introduce a novel grasp representation named the Unified Gripper\nCoordinate Space (UGCS) for grasp synthesis and grasp transfer. Our\nrepresentation leverages spherical coordinates to create a shared coordinate\nspace across different robot grippers, enabling it to synthesize and transfer\ngrasps for both novel objects and previously unseen grippers. The strength of\nthis representation lies in the ability to map palm and fingers of a gripper\nand the unified coordinate space. Grasp synthesis is formulated as predicting\nthe unified spherical coordinates on object surface points via a conditional\nvariational autoencoder. The predicted unified gripper coordinates establish\nexact correspondences between the gripper and object points, which is used to\noptimize grasp pose and joint values. Grasp transfer is facilitated through the\npoint-to-point correspondence between any two (potentially unseen) grippers and\nsolved via a similar optimization. Extensive simulation and real-world\nexperiments showcase the efficacy of the unified grasp representation for grasp\nsynthesis in generating stable and diverse grasps. Similarly, we showcase\nreal-world grasp transfer from human demonstrations across different objects.\n","authors":["Ninad Khargonkar","Luis Felipe Casas","Balakrishnan Prabhakaran","Yu Xiang"],"pdf_url":"https://arxiv.org/pdf/2409.14519v2.pdf","comment":"8 pages, 11 figures, 3 tables. Project page available at\n  https://irvlutd.github.io/RobotFingerPrint"},{"id":"http://arxiv.org/abs/2410.01417v2","updated":"2025-03-03T00:41:36Z","published":"2024-10-02T10:58:54Z","title":"The Labyrinth of Links: Navigating the Associative Maze of Multi-modal\n  LLMs","summary":"  Multi-modal Large Language Models (MLLMs) have exhibited impressive\ncapability. However, recently many deficiencies of MLLMs have been found\ncompared to human intelligence, $\\textit{e.g.}$, hallucination. To drive the\nMLLMs study, the community dedicated efforts to building larger benchmarks with\ncomplex tasks. In this paper, we propose benchmarking an essential but usually\noverlooked intelligence: $\\textbf{association}$, a human's basic capability to\nlink observation and prior practice memory. To comprehensively investigate\nMLLM's performance on the association, we formulate the association task and\ndevise a standard benchmark based on adjective and verb semantic concepts.\nInstead of costly data annotation and curation, we propose a convenient\n$\\textbf{annotation-free}$ construction method transforming the general dataset\nfor our association tasks. Simultaneously, we devise a rigorous data refinement\nprocess to eliminate confusion in the raw dataset. Building on this database,\nwe establish three levels of association tasks: single-step, synchronous, and\nasynchronous associations. Moreover, we conduct a comprehensive investigation\ninto the MLLMs' zero-shot association capabilities, addressing multiple\ndimensions, including three distinct memory strategies, both open-source and\nclosed-source MLLMs, cutting-edge Mixture-of-Experts (MoE) models, and the\ninvolvement of human experts. Our systematic investigation shows that current\nopen-source MLLMs consistently exhibit poor capability in our association\ntasks, even the currently state-of-the-art GPT-4V(vision) also has a\nsignificant gap compared to humans. We believe our benchmark would pave the way\nfor future MLLM studies. $\\textit{Our data and code are available at:}$\nhttps://mvig-rhos.com/llm_inception.\n","authors":["Hong Li","Nanxi Li","Yuanjie Chen","Jianbin Zhu","Qinlu Guo","Cewu Lu","Yong-Lu Li"],"pdf_url":"https://arxiv.org/pdf/2410.01417v2.pdf","comment":"Accepted by ICLR 2025. Project page:\n  https://mvig-rhos.com/llm_inception"},{"id":"http://arxiv.org/abs/2405.02318v2","updated":"2025-03-03T00:38:48Z","published":"2024-04-18T00:20:48Z","title":"NL2FOL: Translating Natural Language to First-Order Logic for Logical\n  Fallacy Detection","summary":"  Translating natural language into formal language such as First-Order Logic\n(FOL) is a foundational challenge in NLP with wide-ranging applications in\nautomated reasoning, misinformation tracking, and knowledge validation. In this\npaper, we introduce Natural Language to First-Order Logic (NL2FOL), a framework\nto autoformalize natural language to FOL step by step using Large Language\nModels (LLMs). Our approach addresses key challenges in this translation\nprocess, including the integration of implicit background knowledge. By\nleveraging structured representations generated by NL2FOL, we use\nSatisfiability Modulo Theory (SMT) solvers to reason about the logical validity\nof natural language statements. We present logical fallacy detection as a case\nstudy to evaluate the efficacy of NL2FOL. Being neurosymbolic, our approach\nalso provides interpretable insights into the reasoning process and\ndemonstrates robustness without requiring model fine-tuning or labeled training\ndata. Our framework achieves strong performance on multiple datasets. On the\nLOGIC dataset, NL2FOL achieves an F1-score of 78%, while generalizing\neffectively to the LOGICCLIMATE dataset with an F1-score of 80%.\n","authors":["Abhinav Lalwani","Tasha Kim","Lovish Chopra","Christopher Hahn","Zhijing Jin","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2405.02318v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22729v2","updated":"2025-03-03T00:23:28Z","published":"2024-10-30T06:28:21Z","title":"Identifying Drift, Diffusion, and Causal Structure from Temporal\n  Snapshots","summary":"  Stochastic differential equations (SDEs) are a fundamental tool for modelling\ndynamic processes, including gene regulatory networks (GRNs), contaminant\ntransport, financial markets, and image generation. However, learning the\nunderlying SDE from data is a challenging task, especially if individual\ntrajectories are not observable. Motivated by burgeoning research in\nsingle-cell datasets, we present the first comprehensive approach for jointly\nidentifying the drift and diffusion of an SDE from its temporal marginals.\nAssuming linear drift and additive diffusion, we prove that these parameters\nare identifiable from marginals if and only if the initial distribution lacks\nany generalized rotational symmetries. We further prove that the causal graph\nof any SDE with additive diffusion can be recovered from the SDE parameters. To\ncomplement this theory, we adapt entropy-regularized optimal transport to\nhandle anisotropic diffusion, and introduce APPEX (Alternating Projection\nParameter Estimation from $X_0$), an iterative algorithm designed to estimate\nthe drift, diffusion, and causal graph of an additive noise SDE, solely from\ntemporal marginals. We show that APPEX iteratively decreases Kullback-Leibler\ndivergence to the true solution, and demonstrate its effectiveness on simulated\ndata from linear additive noise SDEs.\n","authors":["Vincent Guan","Joseph Janssen","Hossein Rahmani","Andrew Warren","Stephen Zhang","Elina Robeva","Geoffrey Schiebinger"],"pdf_url":"https://arxiv.org/pdf/2410.22729v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.16002v2","updated":"2025-03-03T00:11:11Z","published":"2024-05-25T01:44:35Z","title":"Does SGD really happen in tiny subspaces?","summary":"  Understanding the training dynamics of deep neural networks is challenging\ndue to their high-dimensional nature and intricate loss landscapes. Recent\nstudies have revealed that, along the training trajectory, the gradient\napproximately aligns with a low-rank top eigenspace of the training loss\nHessian, referred to as the dominant subspace. Given this alignment, this paper\nexplores whether neural networks can be trained within the dominant subspace,\nwhich, if feasible, could lead to more efficient training methods. Our primary\nobservation is that when the SGD update is projected onto the dominant\nsubspace, the training loss does not decrease further. This suggests that the\nobserved alignment between the gradient and the dominant subspace is spurious.\nSurprisingly, projecting out the dominant subspace proves to be just as\neffective as the original update, despite removing the majority of the original\nupdate component. We observe similar behavior across practical setups,\nincluding the large learning rate regime (also known as Edge of Stability),\nSharpness-Aware Minimization, momentum, and adaptive optimizers. We discuss the\nmain causes and implications of this spurious alignment, shedding light on the\ndynamics of neural network training.\n","authors":["Minhak Song","Kwangjun Ahn","Chulhee Yun"],"pdf_url":"https://arxiv.org/pdf/2405.16002v2.pdf","comment":"Published at ICLR 2025"},{"id":"http://arxiv.org/abs/2401.15262v2","updated":"2025-03-03T00:04:46Z","published":"2024-01-27T01:16:33Z","title":"Asymptotic Behavior of Adversarial Training Estimator under\n  $\\ell_\\infty$-Perturbation","summary":"  Adversarial training has been proposed to protect machine learning models\nagainst adversarial attacks. This paper focuses on adversarial training under\n$\\ell_\\infty$-perturbation, which has recently attracted much research\nattention. The asymptotic behavior of the adversarial training estimator is\ninvestigated in the generalized linear model. The results imply that the\nasymptotic distribution of the adversarial training estimator under\n$\\ell_\\infty$-perturbation could put a positive probability mass at $0$ when\nthe true parameter is $0$, providing a theoretical guarantee of the associated\nsparsity-recovery ability. Alternatively, a two-step procedure is proposed --\nadaptive adversarial training, which could further improve the performance of\nadversarial training under $\\ell_\\infty$-perturbation. Specifically, the\nproposed procedure could achieve asymptotic variable-selection consistency and\nunbiasedness. Numerical experiments are conducted to show the sparsity-recovery\nability of adversarial training under $\\ell_\\infty$-perturbation and to compare\nthe empirical performance between classic adversarial training and adaptive\nadversarial training.\n","authors":["Yiling Xie","Xiaoming Huo"],"pdf_url":"https://arxiv.org/pdf/2401.15262v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2409.18459v2","updated":"2025-03-03T15:04:18Z","published":"2024-09-27T05:43:22Z","title":"FoodMLLM-JP: Leveraging Multimodal Large Language Models for Japanese\n  Recipe Generation","summary":"  Research on food image understanding using recipe data has been a\nlong-standing focus due to the diversity and complexity of the data. Moreover,\nfood is inextricably linked to people's lives, making it a vital research area\nfor practical applications such as dietary management. Recent advancements in\nMultimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities, not only in their vast knowledge but also in their ability to\nhandle languages naturally. While English is predominantly used, they can also\nsupport multiple languages including Japanese. This suggests that MLLMs are\nexpected to significantly improve performance in food image understanding\ntasks. We fine-tuned open MLLMs LLaVA-1.5 and Phi-3 Vision on a Japanese recipe\ndataset and benchmarked their performance against the closed model GPT-4o. We\nthen evaluated the content of generated recipes, including ingredients and\ncooking procedures, using 5,000 evaluation samples that comprehensively cover\nJapanese food culture. Our evaluation demonstrates that the open models trained\non recipe data outperform GPT-4o, the current state-of-the-art model, in\ningredient generation. Our model achieved F1 score of 0.531, surpassing\nGPT-4o's F1 score of 0.481, indicating a higher level of accuracy. Furthermore,\nour model exhibited comparable performance to GPT-4o in generating cooking\nprocedure text.\n","authors":["Yuki Imajuku","Yoko Yamakata","Kiyoharu Aizawa"],"pdf_url":"https://arxiv.org/pdf/2409.18459v2.pdf","comment":"15 pages, 5 figures. We found errors in the calculation of evaluation\n  metrics, which were corrected in this version with\n  $\\color{blue}{\\text{modifications highlighted in blue}}$. Please also see the\n  Appendix"}],"Robotics":[{"id":"http://arxiv.org/abs/2503.02111v1","updated":"2025-03-03T22:53:06Z","published":"2025-03-03T22:53:06Z","title":"NavG: Risk-Aware Navigation in Crowded Environments Based on\n  Reinforcement Learning with Guidance Points","summary":"  Motion planning in navigation systems is highly susceptible to upstream\nperceptual errors, particularly in human detection and tracking. To mitigate\nthis issue, the concept of guidance points--a novel directional cue within a\nreinforcement learning-based framework--is introduced. A structured method for\nidentifying guidance points is developed, consisting of obstacle boundary\nextraction, potential guidance point detection, and redundancy elimination. To\nintegrate guidance points into the navigation pipeline, a\nperception-to-planning mapping strategy is proposed, unifying guidance points\nwith other perceptual inputs and enabling the RL agent to effectively leverage\nthe complementary relationships among raw laser data, human detection and\ntracking, and guidance points. Qualitative and quantitative simulations\ndemonstrate that the proposed approach achieves the highest success rate and\nnear-optimal travel times, greatly improving both safety and efficiency.\nFurthermore, real-world experiments in dynamic corridors and lobbies validate\nthe robot's ability to confidently navigate around obstacles and robustly avoid\npedestrians.\n","authors":["Qianyi Zhang","Wentao Luo","Boyi Liu","Ziyang Zhang","Yaoyuan Wang","Jingtai Liu"],"pdf_url":"https://arxiv.org/pdf/2503.02111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02107v1","updated":"2025-03-03T22:43:52Z","published":"2025-03-03T22:43:52Z","title":"Balancing Act: Trading Off Doppler Odometry and Map Registration for\n  Efficient Lidar Localization","summary":"  Most autonomous vehicles rely on accurate and efficient localization, which\nis achieved by comparing live sensor data to a preexisting map, to navigate\ntheir environment. Balancing the accuracy of localization with computational\nefficiency remains a significant challenge, as high-accuracy methods often come\nwith higher computational costs. In this paper, we present two ways of\nimproving lidar localization efficiency and study their impact on performance.\nFirst, we integrate a lightweight Doppler-based odometry method into a\ntopometric localization pipeline and compare its performance against an\niterative closest point (ICP)-based method. We highlight the trade-offs between\nthese approaches: the Doppler estimator offers faster, lightweight updates,\nwhile ICP provides higher accuracy at the cost of increased computational load.\nSecond, by controlling the frequency of localization updates and leveraging\nodometry estimates between them, we demonstrate that accurate localization can\nbe maintained while optimizing for computational efficiency using either\nodometry method. Our experimental results show that localizing every 10 lidar\nframes strikes a favourable balance, achieving a localization accuracy below\n0.05 meters in translation and below 0.1 degrees in orientation while reducing\ncomputational effort by over 30% in an ICP-based pipeline. We quantify the\ntrade-off of accuracy to computational effort using over 100 kilometers of\nreal-world driving data in different on-road environments.\n","authors":["Katya M. Papais","Daniil Lisus","David J. Yoon","Andrew Lambert","Keith Y. K. Leung","Timothy D. Barfoot"],"pdf_url":"https://arxiv.org/pdf/2503.02107v1.pdf","comment":"8 pages, 3 figures, 2 tables, submitted to IEEE/RSJ International\n  Conference on Intelligent Robots and Systems (IROS) 2025"},{"id":"http://arxiv.org/abs/2503.02106v1","updated":"2025-03-03T22:43:12Z","published":"2025-03-03T22:43:12Z","title":"OVAMOS: A Framework for Open-Vocabulary Multi-Object Search in Unknown\n  Environments","summary":"  Object search is a fundamental task for robots deployed in indoor building\nenvironments, yet challenges arise due to observation instability, especially\nfor open-vocabulary models. While foundation models (LLMs/VLMs) enable\nreasoning about object locations even without direct visibility, the ability to\nrecover from failures and replan remains crucial. The Multi-Object Search (MOS)\nproblem further increases complexity, requiring the tracking multiple objects\nand thorough exploration in novel environments, making observation uncertainty\na significant obstacle. To address these challenges, we propose a framework\nintegrating VLM-based reasoning, frontier-based exploration, and a Partially\nObservable Markov Decision Process (POMDP) framework to solve the MOS problem\nin novel environments. VLM enhances search efficiency by inferring\nobject-environment relationships, frontier-based exploration guides navigation\nin unknown spaces, and POMDP models observation uncertainty, allowing recovery\nfrom failures in occlusion and cluttered environments. We evaluate our\nframework on 120 simulated scenarios across several Habitat-Matterport3D (HM3D)\nscenes and a real-world robot experiment in a 50-square-meter office,\ndemonstrating significant improvements in both efficiency and success rate over\nbaseline methods.\n","authors":["Qianwei Wang","Yifan Xu","Vineet Kamat","Carol Menassa"],"pdf_url":"https://arxiv.org/pdf/2503.02106v1.pdf","comment":"7 pages, 4 Figures"},{"id":"http://arxiv.org/abs/2402.05421v4","updated":"2025-03-03T22:40:19Z","published":"2024-02-08T05:26:40Z","title":"DiffTORI: Differentiable Trajectory Optimization for Deep Reinforcement\n  and Imitation Learning","summary":"  This paper introduces DiffTORI, which utilizes Differentiable Trajectory\nOptimization as the policy representation to generate actions for deep\nReinforcement and Imitation learning. Trajectory optimization is a powerful and\nwidely used algorithm in control, parameterized by a cost and a dynamics\nfunction. The key to our approach is to leverage the recent progress in\ndifferentiable trajectory optimization, which enables computing the gradients\nof the loss with respect to the parameters of trajectory optimization. As a\nresult, the cost and dynamics functions of trajectory optimization can be\nlearned end-to-end. DiffTORI addresses the ``objective mismatch'' issue of\nprior model-based RL algorithms, as the dynamics model in DiffTORI is learned\nto directly maximize task performance by differentiating the policy gradient\nloss through the trajectory optimization process. We further benchmark DiffTORI\nfor imitation learning on standard robotic manipulation task suites with\nhigh-dimensional sensory observations and compare our method to feed-forward\npolicy classes as well as Energy-Based Models (EBM) and Diffusion. Across 15\nmodel-based RL tasks and 35 imitation learning tasks with high-dimensional\nimage and point cloud inputs, DiffTORI outperforms prior state-of-the-art\nmethods in both domains. Our code is available at\nhttps://github.com/wkwan7/DiffTORI.\n","authors":["Weikang Wan","Ziyu Wang","Yufei Wang","Zackory Erickson","David Held"],"pdf_url":"https://arxiv.org/pdf/2402.05421v4.pdf","comment":"NeurIPS 2024 (Spotlight)"},{"id":"http://arxiv.org/abs/2503.02092v1","updated":"2025-03-03T22:23:57Z","published":"2025-03-03T22:23:57Z","title":"Data Augmentation for NeRFs in the Low Data Limit","summary":"  Current methods based on Neural Radiance Fields fail in the low data limit,\nparticularly when training on incomplete scene data. Prior works augment\ntraining data only in next-best-view applications, which lead to hallucinations\nand model collapse with sparse data. In contrast, we propose adding a set of\nviews during training by rejection sampling from a posterior uncertainty\ndistribution, generated by combining a volumetric uncertainty estimator with\nspatial coverage. We validate our results on partially observed scenes; on\naverage, our method performs 39.9% better with 87.5% less variability across\nestablished scene reconstruction benchmarks, as compared to state of the art\nbaselines. We further demonstrate that augmenting the training set by sampling\nfrom any distribution leads to better, more consistent scene reconstruction in\nsparse environments. This work is foundational for robotic tasks where\naugmenting a dataset with informative data is critical in resource-constrained,\na priori unknown environments. Videos and source code are available at\nhttps://murpheylab.github.io/low-data-nerf/.\n","authors":["Ayush Gaggar","Todd D. Murphey"],"pdf_url":"https://arxiv.org/pdf/2503.02092v1.pdf","comment":"To be published in 2025 IEEE International Conference on Robotics and\n  Automation (ICRA 2025)"},{"id":"http://arxiv.org/abs/2503.02087v1","updated":"2025-03-03T22:13:51Z","published":"2025-03-03T22:13:51Z","title":"Uncertainty Representation in a SOTIF-Related Use Case with\n  Dempster-Shafer Theory for LiDAR Sensor-Based Object Detection","summary":"  Uncertainty in LiDAR sensor-based object detection arises from environmental\nvariability and sensor performance limitations. Representing these\nuncertainties is essential for ensuring the Safety of the Intended\nFunctionality (SOTIF), which focuses on preventing hazards in automated driving\nscenarios. This paper presents a systematic approach to identifying,\nclassifying, and representing uncertainties in LiDAR-based object detection\nwithin a SOTIF-related scenario. Dempster-Shafer Theory (DST) is employed to\nconstruct a Frame of Discernment (FoD) to represent detection outcomes.\nConditional Basic Probability Assignments (BPAs) are applied based on\ndependencies among identified uncertainty sources. Yager's Rule of Combination\nis used to resolve conflicting evidence from multiple sources, providing a\nstructured framework to evaluate uncertainties' effects on detection accuracy.\nThe study applies variance-based sensitivity analysis (VBSA) to quantify and\nprioritize uncertainties, detailing their specific impact on detection\nperformance.\n","authors":["Milin Patel","Rolf Jung"],"pdf_url":"https://arxiv.org/pdf/2503.02087v1.pdf","comment":"submitted as extended paper of Vehicle Technology and Intelligent\n  Transport Systems (VEHITS)2024 conference and will be published by Springer\n  in a CCIS Series book later in 2025"},{"id":"http://arxiv.org/abs/2312.06802v2","updated":"2025-03-03T22:03:06Z","published":"2023-12-11T19:26:30Z","title":"On the Feasibility of Fingerprinting Collaborative Robot Traffic","summary":"  This study examines privacy risks in collaborative robotics, focusing on the\npotential for traffic analysis in encrypted robot communications. While\nprevious research has explored low-level command recovery in teleoperation\nsetups, our work investigates high-level motion recovery from script-based\ncontrol interfaces. We evaluate the efficacy of prominent website\nfingerprinting techniques (e.g., Tik-Tok, RF) and their limitations in\naccurately identifying robotic actions due to their inability to capture\ndetailed temporal relationships. To address this, we introduce a traffic\nclassification approach using signal processing techniques, demonstrating high\naccuracy in action identification and highlighting the vulnerability of\nencrypted communications to privacy breaches. Additionally, we explore defenses\nsuch as packet padding and timing manipulation, revealing the challenges in\nbalancing traffic analysis resistance with network efficiency. Our findings\nemphasize the need for continued development of practical defenses in robotic\nprivacy and security.\n","authors":["Cheng Tang","Diogo Barradas","Urs Hengartner","Yue Hu"],"pdf_url":"https://arxiv.org/pdf/2312.06802v2.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2409.09883v2","updated":"2025-03-03T22:00:36Z","published":"2024-09-15T22:28:39Z","title":"Robots that Suggest Safe Alternatives","summary":"  Goal-conditioned policies, such as those learned via imitation learning,\nprovide an easy way for humans to influence what tasks robots accomplish.\nHowever, these robot policies are not guaranteed to execute safely or to\nsucceed when faced with out-of-distribution requests. In this work, we enable\nrobots to know when they can confidently execute a user's desired goal, and\nautomatically suggest safe alternatives when they cannot. Our approach is\ninspired by control-theoretic safety filtering, wherein a safety filter\nminimally adjusts a robot's candidate action to be safe. Our key idea is to\npose alternative suggestion as a safe control problem in goal space, rather\nthan in action space. Offline, we use reachability analysis to compute a\ngoal-parameterized reach-avoid value network which quantifies the safety and\nliveness of the robot's pre-trained policy. Online, our robot uses the\nreach-avoid value network as a safety filter, monitoring the human's given goal\nand actively suggesting alternatives that are similar but meet the safety\nspecification. We demonstrate our Safe ALTernatives (SALT) framework in\nsimulation experiments with indoor navigation and Franka Panda tabletop\nmanipulation, and with both discrete and continuous goal representations. We\nfind that SALT is able to learn to predict successful and failed closed-loop\nexecutions, is a less pessimistic monitor than open-loop uncertainty\nquantification, and proposes alternatives that consistently align with those\npeople find acceptable.\n","authors":["Hyun Joe Jeong","Rosy Chen","Andrea Bajcsy"],"pdf_url":"https://arxiv.org/pdf/2409.09883v2.pdf","comment":"10 pages, 6 figures, 2 tables"},{"id":"http://arxiv.org/abs/2503.02076v1","updated":"2025-03-03T21:57:28Z","published":"2025-03-03T21:57:28Z","title":"CorrA: Leveraging Large Language Models for Dynamic Obstacle Avoidance\n  of Autonomous Vehicles","summary":"  In this paper, we present Corridor-Agent (CorrA), a framework that integrates\nlarge language models (LLMs) with model predictive control (MPC) to address the\nchallenges of dynamic obstacle avoidance in autonomous vehicles. Our approach\nleverages LLM reasoning ability to generate appropriate parameters for\nsigmoid-based boundary functions that define safe corridors around obstacles,\neffectively reducing the state-space of the controlled vehicle. The proposed\nframework adjusts these boundaries dynamically based on real-time vehicle data\nthat guarantees collision-free trajectories while also ensuring both\ncomputational efficiency and trajectory optimality. The problem is formulated\nas an optimal control problem and solved with differential dynamic programming\n(DDP) for constrained optimization, and the proposed approach is embedded\nwithin an MPC framework. Extensive simulation and real-world experiments\ndemonstrate that the proposed framework achieves superior performance in\nmaintaining safety and efficiency in complex, dynamic environments compared to\na baseline MPC approach.\n","authors":["Shanting Wang","Panagiotis Typaldos","Andreas A. Malikopoulos"],"pdf_url":"https://arxiv.org/pdf/2503.02076v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02075v1","updated":"2025-03-03T21:57:08Z","published":"2025-03-03T21:57:08Z","title":"Active Alignments of Lens Systems with Reinforcement Learning","summary":"  Aligning a lens system relative to an imager is a critical challenge in\ncamera manufacturing. While optimal alignment can be mathematically computed\nunder ideal conditions, real-world deviations caused by manufacturing\ntolerances often render this approach impractical. Measuring these tolerances\ncan be costly or even infeasible, and neglecting them may result in suboptimal\nalignments. We propose a reinforcement learning (RL) approach that learns\nexclusively in the pixel space of the sensor output, eliminating the need to\ndevelop expert-designed alignment concepts. We conduct an extensive benchmark\nstudy and show that our approach surpasses other methods in speed, precision,\nand robustness. We further introduce relign, a realistic, freely explorable,\nopen-source simulation utilizing physically based rendering that models optical\nsystems with non-deterministic manufacturing tolerances and noise in robotic\nalignment movement. It provides an interface to popular machine learning\nframeworks, enabling seamless experimentation and development. Our work\nhighlights the potential of RL in a manufacturing environment to enhance\nefficiency of optical alignments while minimizing the need for manual\nintervention.\n","authors":["Matthias Burkhardt","Tobias Schmähling","Michael Layh","Tobias Windisch"],"pdf_url":"https://arxiv.org/pdf/2503.02075v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2404.01618v2","updated":"2025-03-03T21:48:03Z","published":"2024-04-02T03:44:25Z","title":"Coordinated Multi-Robot Navigation with Formation Adaptation","summary":"  Coordinated multi-robot navigation is an essential ability for a team of\nrobots operating in diverse environments. Robot teams often need to maintain\nspecific formations, such as wedge formations, to enhance visibility,\npositioning, and efficiency during fast movement. However, complex environments\nsuch as narrow corridors challenge rigid team formations, which makes effective\nformation control difficult in real-world environments. To address this\nchallenge, we introduce a novel Adaptive Formation with Oscillation Reduction\n(AFOR) approach to improve coordinated multi-robot navigation. We develop AFOR\nunder the theoretical framework of hierarchical learning and integrate a\nspring-damper model with hierarchical learning to enable both team coordination\nand individual robot control. At the upper level, a graph neural network\nfacilitates formation adaptation and information sharing among the robots. At\nthe lower level, reinforcement learning enables each robot to navigate and\navoid obstacles while maintaining the formations. We conducted extensive\nexperiments using Gazebo in the Robot Operating System (ROS), a high-fidelity\nUnity3D simulator with ROS, and real robot teams. Results demonstrate that AFOR\nenables smooth navigation with formation adaptation in complex scenarios and\noutperforms previous methods. More details of this work are provided on the\nproject website: https://hcrlab.gitlab.io/project/afor.\n","authors":["Zihao Deng","Peng Gao","Williard Joshua Jose","Christopher Reardon","Maggie Wigness","John Rogers","Hao Zhang"],"pdf_url":"https://arxiv.org/pdf/2404.01618v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02050v1","updated":"2025-03-03T21:02:31Z","published":"2025-03-03T21:02:31Z","title":"Constraint-Based Modeling of Dynamic Entities in 3D Scene Graphs for\n  Robust SLAM","summary":"  Autonomous robots depend crucially on their ability to perceive and process\ninformation from dynamic, ever-changing environments. Traditional simultaneous\nlocalization and mapping (SLAM) approaches struggle to maintain consistent\nscene representations because of numerous moving objects, often treating\ndynamic elements as outliers rather than explicitly modeling them in the scene\nrepresentation. In this paper, we present a novel hierarchical 3D scene\ngraph-based SLAM framework that addresses the challenge of modeling and\nestimating the pose of dynamic objects and agents. We use fiducial markers to\ndetect dynamic entities and to extract their attributes while improving\nkeyframe selection and implementing new capabilities for dynamic entity\nmapping. We maintain a hierarchical representation where dynamic objects are\nregistered in the SLAM graph and are constrained with robot keyframes and the\nfloor level of the building with our novel entity-keyframe constraints and\nintra-entity constraints. By combining semantic and geometric constraints\nbetween dynamic entities and the environment, our system jointly optimizes the\nSLAM graph to estimate the pose of the robot and various dynamic agents and\nobjects while maintaining an accurate map. Experimental evaluation demonstrates\nthat our approach achieves a 27.57% reduction in pose estimation error compared\nto traditional methods and enables higher-level reasoning about scene dynamics.\n","authors":["Marco Giberna","Muhammad Shaheer","Hriday Bavle","Jose Andres Millan-Romera","Jose Luis Sanchez-Lopez","Holger Voos"],"pdf_url":"https://arxiv.org/pdf/2503.02050v1.pdf","comment":"8 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2503.02048v1","updated":"2025-03-03T20:56:39Z","published":"2025-03-03T20:56:39Z","title":"FRMD: Fast Robot Motion Diffusion with Consistency-Distilled Movement\n  Primitives for Smooth Action Generation","summary":"  We consider the problem of using diffusion models to generate fast, smooth,\nand temporally consistent robot motions. Although diffusion models have\ndemonstrated superior performance in robot learning due to their task\nscalability and multi-modal flexibility, they suffer from two fundamental\nlimitations: (1) they often produce non-smooth, jerky motions due to their\ninability to capture temporally consistent movement dynamics, and (2) their\niterative sampling process incurs prohibitive latency for many robotic tasks.\nInspired by classic robot motion generation methods such as DMPs and ProMPs,\nwhich capture temporally and spatially consistent dynamic of trajectories using\nlow-dimensional vectors -- and by recent advances in diffusion-based image\ngeneration that use consistency models with probability flow ODEs to accelerate\nthe denoising process, we propose Fast Robot Motion Diffusion (FRMD). FRMD\nuniquely integrates Movement Primitives (MPs) with Consistency Models to enable\nefficient, single-step trajectory generation. By leveraging probabilistic flow\nODEs and consistency distillation, our method models trajectory distributions\nwhile learning a compact, time-continuous motion representation within an\nencoder-decoder architecture. This unified approach eliminates the slow,\nmulti-step denoising process of conventional diffusion models, enabling\nefficient one-step inference and smooth robot motion generation. We extensively\nevaluated our FRMD on the well-recognized Meta-World and ManiSkills Benchmarks,\nranging from simple to more complex manipulation tasks, comparing its\nperformance against state-of-the-art baselines. Our results show that FRMD\ngenerates significantly faster, smoother trajectories while achieving higher\nsuccess rates.\n","authors":["Xirui Shi","Jun Jin"],"pdf_url":"https://arxiv.org/pdf/2503.02048v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2406.01586 by other authors"},{"id":"http://arxiv.org/abs/2409.16392v2","updated":"2025-03-03T20:56:33Z","published":"2024-09-24T18:46:50Z","title":"Rao-Blackwellized POMDP Planning","summary":"  Partially Observable Markov Decision Processes (POMDPs) provide a structured\nframework for decision-making under uncertainty, but their application requires\nefficient belief updates. Sequential Importance Resampling Particle Filters\n(SIRPF), also known as Bootstrap Particle Filters, are commonly used as belief\nupdaters in large approximate POMDP solvers, but they face challenges such as\nparticle deprivation and high computational costs as the system's state\ndimension grows. To address these issues, this study introduces\nRao-Blackwellized POMDP (RB-POMDP) approximate solvers and outlines generic\nmethods to apply Rao-Blackwellization in both belief updates and online\nplanning. We compare the performance of SIRPF and Rao-Blackwellized Particle\nFilters (RBPF) in a simulated localization problem where an agent navigates\ntoward a target in a GPS-denied environment using POMCPOW and RB-POMCPOW\nplanners. Our results not only confirm that RBPFs maintain accurate belief\napproximations over time with fewer particles, but, more surprisingly, RBPFs\ncombined with quadrature-based integration improve planning quality\nsignificantly compared to SIRPF-based planning under the same computational\nlimits.\n","authors":["Jiho Lee","Nisar R. Ahmed","Kyle H. Wray","Zachary N. Sunberg"],"pdf_url":"https://arxiv.org/pdf/2409.16392v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02042v1","updated":"2025-03-03T20:36:15Z","published":"2025-03-03T20:36:15Z","title":"Optimizing Robot Programming: Mixed Reality Gripper Control","summary":"  Conventional robot programming methods are complex and time-consuming for\nusers. In recent years, alternative approaches such as mixed reality have been\nexplored to address these challenges and optimize robot programming. While the\nfindings of the mixed reality robot programming methods are convincing, most\nexisting methods rely on gesture interaction for robot programming. Since\ncontroller-based interactions have proven to be more reliable, this paper\nexamines three controller-based programming methods within a mixed reality\nscenario: 1) Classical Jogging, where the user positions the robot's end\neffector using the controller's thumbsticks, 2) Direct Control, where the\ncontroller's position and orientation directly corresponds to the end\neffector's, and 3) Gripper Control, where the controller is enhanced with a\n3D-printed gripper attachment to grasp and release objects. A within-subjects\nstudy (n = 30) was conducted to compare these methods. The findings indicate\nthat the Gripper Control condition outperforms the others in terms of task\ncompletion time, user experience, mental demand, and task performance, while\nalso being the preferred method. Therefore, it demonstrates promising potential\nas an effective and efficient approach for future robot programming. Video\navailable at https://youtu.be/83kWr8zUFIQ.\n","authors":["Maximilian Rettinger","Leander Hacker","Philipp Wolters","Gerhard Rigoll"],"pdf_url":"https://arxiv.org/pdf/2503.02042v1.pdf","comment":"6 pages, 7 figures"},{"id":"http://arxiv.org/abs/2412.01765v2","updated":"2025-03-03T20:13:37Z","published":"2024-12-02T18:03:12Z","title":"Planning and Reasoning with 3D Deformable Objects for Hierarchical\n  Text-to-3D Robotic Shaping","summary":"  Deformable object manipulation remains a key challenge in developing\nautonomous robotic systems that can be successfully deployed in real-world\nscenarios. In this work, we explore the challenges of deformable object\nmanipulation through the task of sculpting clay into 3D shapes. We propose the\nfirst coarse-to-fine autonomous sculpting system in which the sculpting agent\nfirst selects how many and where to place discrete chunks of clay into the\nworkspace to create a coarse shape, and then iteratively refines the shape with\nsequences of deformation actions. We leverage large language models for\nsub-goal generation, and train a point cloud region-based action model to\npredict robot actions from the desired point cloud sub-goals. Additionally, our\nmethod is the first autonomous sculpting system that is a real-world text-to-3D\nshaping pipeline without any explicit 3D goals or sub-goals provided to the\nsystem. We demonstrate our method is able to successfully create a set of\nsimple shapes solely from text-based prompting. Furthermore, we explore\nrigorously how to best quantify success for the text-to-3D sculpting task, and\ncompare existing text-image and text-point cloud similarity metrics to human\nevaluations for this task. For experimental videos, human evaluation details,\nand full prompts, please see our project website:\nhttps://sites.google.com/andrew.cmu.edu/hierarchicalsculpting\n","authors":["Alison Bartsch","Amir Barati Farimani"],"pdf_url":"https://arxiv.org/pdf/2412.01765v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.03640v2","updated":"2025-03-03T19:42:45Z","published":"2025-02-05T21:51:47Z","title":"Discrete GCBF Proximal Policy Optimization for Multi-agent Safe Optimal\n  Control","summary":"  Control policies that can achieve high task performance and satisfy safety\nconstraints are desirable for any system, including multi-agent systems (MAS).\nOne promising technique for ensuring the safety of MAS is distributed control\nbarrier functions (CBF). However, it is difficult to design distributed\nCBF-based policies for MAS that can tackle unknown discrete-time dynamics,\npartial observability, changing neighborhoods, and input constraints,\nespecially when a distributed high-performance nominal policy that can achieve\nthe task is unavailable. To tackle these challenges, we propose DGPPO, a new\nframework that simultaneously learns both a discrete graph CBF which handles\nneighborhood changes and input constraints, and a distributed high-performance\nsafe policy for MAS with unknown discrete-time dynamics. We empirically\nvalidate our claims on a suite of multi-agent tasks spanning three different\nsimulation engines. The results suggest that, compared with existing methods,\nour DGPPO framework obtains policies that achieve high task performance\n(matching baselines that ignore the safety constraints), and high safety rates\n(matching the most conservative baselines), with a constant set of\nhyperparameters across all environments.\n","authors":["Songyuan Zhang","Oswin So","Mitchell Black","Chuchu Fan"],"pdf_url":"https://arxiv.org/pdf/2502.03640v2.pdf","comment":"31 pages, 15 figures; Accepted by the thirteenth International\n  Conference on Learning Representations (ICLR 2025)"},{"id":"http://arxiv.org/abs/2503.02012v1","updated":"2025-03-03T19:41:22Z","published":"2025-03-03T19:41:22Z","title":"Pretrained Embeddings as a Behavior Specification Mechanism","summary":"  We propose an approach to formally specifying the behavioral properties of\nsystems that rely on a perception model for interactions with the physical\nworld. The key idea is to introduce embeddings -- mathematical representations\nof a real-world concept -- as a first-class construct in a specification\nlanguage, where properties are expressed in terms of distances between a pair\nof ideal and observed embeddings. To realize this approach, we propose a new\ntype of temporal logic called Embedding Temporal Logic (ETL), and describe how\nit can be used to express a wider range of properties about AI-enabled systems\nthan previously possible. We demonstrate the applicability of ETL through a\npreliminary evaluation involving planning tasks in robots that are driven by\nfoundation models; the results are promising, showing that embedding-based\nspecifications can be used to steer a system towards desirable behaviors.\n","authors":["Parv Kapoor","Abigail Hammer","Ashish Kapoor","Karen Leung","Eunsuk Kang"],"pdf_url":"https://arxiv.org/pdf/2503.02012v1.pdf","comment":"18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.02010v1","updated":"2025-03-03T19:37:43Z","published":"2025-03-03T19:37:43Z","title":"Minimum-Length Coordinated Motions For Two Convex Centrally-Symmetric\n  Robots","summary":"  We study the problem of determining coordinated motions, of minimum total\nlength, for two arbitrary convex centrally-symmetric (CCS) robots in an\notherwise obstacle-free plane. Using the total path length traced by the two\nrobot centres as a measure of distance, we give an exact characterization of a\n(not necessarily unique) shortest collision-avoiding motion for all initial and\ngoal configurations of the robots. The individual paths are composed of at most\nsix convex pieces, and their total length can be expressed as a simple integral\nwith a closed form solution depending only on the initial and goal\nconfiguration of the robots. The path pieces are either straight segments or\nsegments of the boundary of the Minkowski sum of the two robots (circular arcs,\nin the special case of disc robots). Furthermore, the paths can be\nparameterized in such a way that (i) only one robot is moving at any given time\n(decoupled motion), or (ii) the orientation of the robot configuration changes\nmonotonically.\n","authors":["David Kirkpatrick","Paul Liu"],"pdf_url":"https://arxiv.org/pdf/2503.02010v1.pdf","comment":"31 pages, 32 figures"},{"id":"http://arxiv.org/abs/2503.01842v1","updated":"2025-03-03T18:59:23Z","published":"2025-03-03T18:59:23Z","title":"Discrete-Time Hybrid Automata Learning: Legged Locomotion Meets\n  Skateboarding","summary":"  This paper introduces Discrete-time Hybrid Automata Learning (DHAL), a\nframework using on-policy Reinforcement Learning to identify and execute\nmode-switching without trajectory segmentation or event function learning.\nHybrid dynamical systems, which include continuous flow and discrete mode\nswitching, can model robotics tasks like legged robot locomotion. Model-based\nmethods usually depend on predefined gaits, while model-free approaches lack\nexplicit mode-switching knowledge. Current methods identify discrete modes via\nsegmentation before regressing continuous flow, but learning high-dimensional\ncomplex rigid body dynamics without trajectory labels or segmentation is a\nchallenging open problem. Our approach incorporates a beta policy distribution\nand a multi-critic architecture to model contact-guided motions, exemplified by\na challenging quadrupedal robot skateboard task. We validate our method through\nsimulations and real-world tests, demonstrating robust performance in hybrid\ndynamical systems.\n","authors":["Hang Liu","Sangli Teng","Ben Liu","Wei Zhang","Maani Ghaffari"],"pdf_url":"https://arxiv.org/pdf/2503.01842v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01231v2","updated":"2025-03-03T18:57:13Z","published":"2025-02-03T10:41:44Z","title":"Societal Attitudes Toward Service Robots: Adore, Abhor, Ignore, or\n  Unsure?","summary":"  Societal or population-level attitudes are aggregated patterns of different\nindividual attitudes, representing collective general predispositions. As\nservice robots become ubiquitous, understanding attitudes towards them at the\npopulation (vs. individual) level enables firms to expand robot services to a\nbroad (vs. niche) market. Targeting population-level attitudes would benefit\nservice firms because: (1) they are more persistent, thus, stronger predictors\nof behavioral patterns and (2) this approach is less reliant on personal data,\nwhereas individualized services are vulnerable to AI-related privacy risks. As\nfor service theory, ignoring broad unobserved differences in attitudes produces\nbiased conclusions, and our systematic review of previous research highlights a\npoor understanding of potential heterogeneity in attitudes toward service\nrobots. We present five diverse studies (S1-S5), utilizing multinational and\n\"real world\" data (Ntotal = 89,541; years: 2012-2024). Results reveal a stable\nstructure comprising four distinct attitude profiles (S1-S5): positive\n(\"adore\"), negative (\"abhor\"), indifferent (\"ignore\"), and ambivalent\n(\"unsure\"). The psychological need for interacting with service staff, and for\nautonomy and relatedness in technology use, function as attitude profile\nantecedents (S2). Importantly, the attitude profiles predict differences in\npost-interaction discomfort and anxiety (S3), satisfaction ratings and service\nevaluations (S4), and perceived sociability and uncanniness based on a robot's\nhumanlikeness (S5).\n","authors":["V. Yoganathan","V. -S. Osburg","A. Fronzetti Colladon","V. Charles","W. Toporowski"],"pdf_url":"https://arxiv.org/pdf/2502.01231v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01837v1","updated":"2025-03-03T18:57:08Z","published":"2025-03-03T18:57:08Z","title":"Multi-Stage Manipulation with Demonstration-Augmented Reward, Policy,\n  and World Model Learning","summary":"  Long-horizon tasks in robotic manipulation present significant challenges in\nreinforcement learning (RL) due to the difficulty of designing dense reward\nfunctions and effectively exploring the expansive state-action space. However,\ndespite a lack of dense rewards, these tasks often have a multi-stage\nstructure, which can be leveraged to decompose the overall objective into\nmanageable subgoals. In this work, we propose DEMO3, a framework that exploits\nthis structure for efficient learning from visual inputs. Specifically, our\napproach incorporates multi-stage dense reward learning, a bi-phasic training\nscheme, and world model learning into a carefully designed\ndemonstration-augmented RL framework that strongly mitigates the challenge of\nexploration in long-horizon tasks. Our evaluations demonstrate that our method\nimproves data-efficiency by an average of 40% and by 70% on particularly\ndifficult tasks compared to state-of-the-art approaches. We validate this\nacross 16 sparse-reward tasks spanning four domains, including challenging\nhumanoid visual control tasks using as few as five demonstrations.\n","authors":["Adrià López Escoriza","Nicklas Hansen","Stone Tao","Tongzhou Mu","Hao Su"],"pdf_url":"https://arxiv.org/pdf/2503.01837v1.pdf","comment":"Project page can be found at\n  https://adrialopezescoriza.github.io/demo3/"},{"id":"http://arxiv.org/abs/2409.11764v2","updated":"2025-03-03T18:50:18Z","published":"2024-09-18T07:44:08Z","title":"One Map to Find Them All: Real-time Open-Vocabulary Mapping for\n  Zero-shot Multi-Object Navigation","summary":"  The capability to efficiently search for objects in complex environments is\nfundamental for many real-world robot applications. Recent advances in\nopen-vocabulary vision models have resulted in semantically-informed object\nnavigation methods that allow a robot to search for an arbitrary object without\nprior training. However, these zero-shot methods have so far treated the\nenvironment as unknown for each consecutive query. In this paper we introduce a\nnew benchmark for zero-shot multi-object navigation, allowing the robot to\nleverage information gathered from previous searches to more efficiently find\nnew objects. To address this problem we build a reusable open-vocabulary\nfeature map tailored for real-time object search. We further propose a\nprobabilistic-semantic map update that mitigates common sources of errors in\nsemantic feature extraction and leverage this semantic uncertainty for informed\nmulti-object exploration. We evaluate our method on a set of object navigation\ntasks in both simulation as well as with a real robot, running in real-time on\na Jetson Orin AGX. We demonstrate that it outperforms existing state-of-the-art\napproaches both on single and multi-object navigation tasks. Additional videos,\ncode and the multi-object navigation benchmark will be available on\nhttps://finnbsch.github.io/OneMap.\n","authors":["Finn Lukas Busch","Timon Homberger","Jesús Ortega-Peimbert","Quantao Yang","Olov Andersson"],"pdf_url":"https://arxiv.org/pdf/2409.11764v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01789v1","updated":"2025-03-03T18:21:26Z","published":"2025-03-03T18:21:26Z","title":"TacCap: A Wearable FBG-Based Tactile Sensor for Seamless Human-to-Robot\n  Skill Transfer","summary":"  Tactile sensing is essential for dexterous manipulation, yet large-scale\nhuman demonstration datasets lack tactile feedback, limiting their\neffectiveness in skill transfer to robots. To address this, we introduce\nTacCap, a wearable Fiber Bragg Grating (FBG)-based tactile sensor designed for\nseamless human-to-robot transfer. TacCap is lightweight, durable, and immune to\nelectromagnetic interference, making it ideal for real-world data collection.\nWe detail its design and fabrication, evaluate its sensitivity, repeatability,\nand cross-sensor consistency, and assess its effectiveness through grasp\nstability prediction and ablation studies. Our results demonstrate that TacCap\nenables transferable tactile data collection, bridging the gap between human\ndemonstrations and robotic execution. To support further research and\ndevelopment, we open-source our hardware design and software.\n","authors":["Chengyi Xing","Hao Li","Yi-Lin Wei","Tian-Ao Ren","Tianyu Tu","Yuhao Lin","Elizabeth Schumann","Wei-Shi Zheng","Mark R. Cutkosky"],"pdf_url":"https://arxiv.org/pdf/2503.01789v1.pdf","comment":"7 pages, 8 figures"},{"id":"http://arxiv.org/abs/2501.10513v2","updated":"2025-03-03T18:20:23Z","published":"2025-01-17T19:13:21Z","title":"ConfigBot: Adaptive Resource Allocation for Robot Applications in\n  Dynamic Environments","summary":"  The growing use of service robots in dynamic environments requires flexible\nmanagement of on-board compute resources to optimize the performance of diverse\ntasks such as navigation, localization, and perception. Current robot\ndeployments often rely on static OS configurations and system\nover-provisioning. However, they are suboptimal because they do not account for\nvariations in resource usage. This results in poor system-wide behavior such as\nrobot instability or inefficient resource use. This paper presents ConifgBot, a\nnovel system designed to adaptively reconfigure robot applications to meet a\npredefined performance specification by leveraging \\emph{runtime profiling} and\n\\emph{automated configuration tuning}. Through experiments on multiple real\nrobots, each running a different stack with diverse performance requirements,\nwhich could be \\emph{context}-dependent, we illustrate ConifgBot's efficacy in\nmaintaining system stability and optimizing resource allocation. Our findings\nhighlight the promise of automatic system configuration tuning for robot\ndeployments, including adaptation to dynamic changes.\n","authors":["Rohit Dwivedula","Sadanand Modak","Aditya Akella","Joydeep Biswas","Daehyeok Kim","Christopher J. Rossbach"],"pdf_url":"https://arxiv.org/pdf/2501.10513v2.pdf","comment":"9 pages, 8 figures, 7 tables"},{"id":"http://arxiv.org/abs/2503.01783v1","updated":"2025-03-03T18:15:11Z","published":"2025-03-03T18:15:11Z","title":"vS-Graphs: Integrating Visual SLAM and Situational Graphs through\n  Multi-level Scene Understanding","summary":"  Current Visual Simultaneous Localization and Mapping (VSLAM) systems often\nstruggle to create maps that are both semantically rich and easily\ninterpretable. While incorporating semantic scene knowledge aids in building\nricher maps with contextual associations among mapped objects, representing\nthem in structured formats like scene graphs has not been widely addressed,\nencountering complex map comprehension and limited scalability. This paper\nintroduces visual S-Graphs (vS-Graphs), a novel real-time VSLAM framework that\nintegrates vision-based scene understanding with map reconstruction and\ncomprehensible graph-based representation. The framework infers structural\nelements (i.e., rooms and corridors) from detected building components (i.e.,\nwalls and ground surfaces) and incorporates them into optimizable 3D scene\ngraphs. This solution enhances the reconstructed map's semantic richness,\ncomprehensibility, and localization accuracy. Extensive experiments on standard\nbenchmarks and real-world datasets demonstrate that vS-Graphs outperforms\nstate-of-the-art VSLAM methods, reducing trajectory error by an average of\n3.38% and up to 9.58% on real-world data. Furthermore, the proposed framework\nachieves environment-driven semantic entity detection accuracy comparable to\nprecise LiDAR-based frameworks using only visual features. A web page\ncontaining more media and evaluation outcomes is available on\nhttps://snt-arg.github.io/vsgraphs-results/.\n","authors":["Ali Tourani","Saad Ejaz","Hriday Bavle","David Morilla-Cabello","Jose Luis Sanchez-Lopez","Holger Voos"],"pdf_url":"https://arxiv.org/pdf/2503.01783v1.pdf","comment":"13 pages, 8 figures, 2 tables"},{"id":"http://arxiv.org/abs/2409.15493v2","updated":"2025-03-03T18:04:06Z","published":"2024-09-23T19:25:03Z","title":"Autonomous Exploration and Semantic Updating of Large-Scale Indoor\n  Environments with Mobile Robots","summary":"  We introduce a new robotic system that enables a mobile robot to autonomously\nexplore an unknown environment, build a semantic map of the environment, and\nsubsequently update the semantic map to reflect environment changes, such as\nlocation changes of objects. Our system leverages a LiDAR scanner for 2D\noccupancy grid mapping and an RGB-D camera for object perception. We introduce\na semantic map representation that combines a 2D occupancy grid map for\ngeometry with a topological map for object semantics. This map representation\nenables us to effectively update the semantics by deleting or adding nodes to\nthe topological map. Our system has been tested on a Fetch robot, semantically\nmapping a 93m x 90m and a 9m x 13m indoor environment and updating their\nsemantic maps once objects are moved in the environments\n","authors":["Sai Haneesh Allu","Itay Kadosh","Tyler Summers","Yu Xiang"],"pdf_url":"https://arxiv.org/pdf/2409.15493v2.pdf","comment":"7 pages, 7 figures. Project page is available at\n  https://irvlutd.github.io/SemanticMapping/"},{"id":"http://arxiv.org/abs/2411.07848v2","updated":"2025-03-03T17:33:39Z","published":"2024-11-12T15:01:40Z","title":"Zero-shot Object-Centric Instruction Following: Integrating Foundation\n  Models with Traditional Navigation","summary":"  Large scale scenes such as multifloor homes can be robustly and efficiently\nmapped with a 3D graph of landmarks estimated jointly with robot poses in a\nfactor graph, a technique commonly used in commercial robots such as drones and\nrobot vacuums. In this work, we propose Language-Inferred Factor Graph for\nInstruction Following (LIFGIF), a zero-shot method to ground natural language\ninstructions in such a map. LIFGIF also includes a policy for following natural\nlanguage navigation instructions in a novel environment while the map is\nconstructed, enabling robust navigation performance in the physical world. To\nevaluate LIFGIF, we present a new dataset, Object-Centric VLN (OC-VLN), in\norder to evaluate grounding of object-centric natural language navigation\ninstructions. We compare to two state-of-the-art zero-shot baselines from\nrelated tasks, Object Goal Navigation and Vision Language Navigation, to\ndemonstrate that LIFGIF outperforms them across all our evaluation metrics on\nOCVLN. Finally, we successfully demonstrate the effectiveness of LIFGIF for\nperforming zero-shot object-centric instruction following in the real world on\na Boston Dynamics Spot robot.\n","authors":["Sonia Raychaudhuri","Duy Ta","Katrina Ashton","Angel X. Chang","Jiuguang Wang","Bernadette Bucher"],"pdf_url":"https://arxiv.org/pdf/2411.07848v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15304v3","updated":"2025-03-03T17:24:52Z","published":"2024-06-18T15:15:18Z","title":"Learning Object Compliance via Young's Modulus from Single Grasps using\n  Camera-Based Tactile Sensors","summary":"  Compliance is a useful parametrization of tactile information that humans\noften utilize in manipulation tasks. It can be used to inform low-level\ncontact-rich actions or characterize objects at a high-level. In robotic\nmanipulation, existing approaches to estimate compliance have struggled to\ngeneralize across both object shape and material. Using camera-based tactile\nsensors, proprioception, and force measurements, we present a novel approach to\nestimate object compliance as Young's modulus (E) from parallel grasps. We\nevaluate our method over a novel dataset of 285 common objects, including a\nwide array of shapes and materials with Young's moduli ranging from 5.0 kPa to\n250 GPa. Combining analytical and data-driven approaches, we develop a hybrid\nsystem using a multi-tower neural network to analyze a sequence of tactile\nimages from grasping. This system is shown to estimate the Young's modulus of\nunseen objects within an order of magnitude at 74.2% accuracy across our\ndataset. This is an improvement over purely analytical and data-driven\nbaselines which exhibit 28.9% and 65.0% accuracy respectively. Importantly,\nthis estimation system performs irrespective of object geometry and\ndemonstrates increased robustness across material types.\n","authors":["Michael Burgess","Jialiang Zhao","Laurence Willemet"],"pdf_url":"https://arxiv.org/pdf/2406.15304v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18170v5","updated":"2025-03-03T17:14:38Z","published":"2024-05-28T13:30:53Z","title":"An Open-Source Reproducible Chess Robot for Human-Robot Interaction\n  Research","summary":"  Recent advancements in AI have accelerated the evolution of versatile robot\ndesigns. Chess provides a standardized environment for evaluating the impact of\nrobot behavior on human behavior. This article presents an open-source chess\nrobot for human-robot interaction (HRI) research, specifically focusing on\nverbal and non-verbal interactions. The OpenChessRobot recognizes chess pieces\nusing computer vision, executes moves, and interacts with the human player\nthrough voice and robotic gestures. We detail the software design, provide\nquantitative evaluations of the efficacy of the robot, and offer a guide for\nits reproducibility. An online survey examining people's views of the robot in\nthree possible scenarios was conducted with 597 participants. The robot\nreceived the highest ratings in the robotics education and the chess coach\nscenarios, while the home entertainment scenario received the lowest scores.\nThe code is accessible on GitHub: https://github.com/renchizhhhh/OpenChessRobot\n","authors":["Renchi Zhang","Joost de Winter","Dimitra Dodou","Harleigh Seyffert","Yke Bauke Eisma"],"pdf_url":"https://arxiv.org/pdf/2405.18170v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.06408v2","updated":"2025-03-03T16:55:35Z","published":"2024-11-10T10:08:36Z","title":"Visuotactile-Based Learning for Insertion with Compliant Hands","summary":"  Compared to rigid hands, underactuated compliant hands offer greater\nadaptability to object shapes, provide stable grasps, and are often more\ncost-effective. However, they introduce uncertainties in hand-object\ninteractions due to their inherent compliance and lack of precise finger\nproprioception as in rigid hands. These limitations become particularly\nsignificant when performing contact-rich tasks like insertion. To address these\nchallenges, additional sensing modalities are required to enable robust\ninsertion capabilities. This letter explores the essential sensing requirements\nfor successful insertion tasks with compliant hands, focusing on the role of\nvisuotactile perception (i.e., visual and tactile perception). We propose a\nsimulation-based multimodal policy learning framework that leverages all-around\ntactile sensing and an extrinsic depth camera. A transformer-based policy,\ntrained through a teacher-student distillation process, is successfully\ntransferred to a real-world robotic system without further training. Our\nresults emphasize the crucial role of tactile sensing in conjunction with\nvisual perception for accurate object-socket pose estimation, successful\nsim-to-real transfer and robust task execution.\n","authors":["Osher Azulay","Dhruv Metha Ramesh","Nimrod Curtis","Avishai Sintov"],"pdf_url":"https://arxiv.org/pdf/2411.06408v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01732v1","updated":"2025-03-03T16:51:24Z","published":"2025-03-03T16:51:24Z","title":"No Plan but Everything Under Control: Robustly Solving Sequential Tasks\n  with Dynamically Composed Gradient Descent","summary":"  We introduce a novel gradient-based approach for solving sequential tasks by\ndynamically adjusting the underlying myopic potential field in response to\nfeedback and the world's regularities. This adjustment implicitly considers\nsubgoals encoded in these regularities, enabling the solution of long\nsequential tasks, as demonstrated by solving the traditional planning domain of\nBlocks World - without any planning. Unlike conventional planning methods, our\nfeedback-driven approach adapts to uncertain and dynamic environments, as\ndemonstrated by one hundred real-world trials involving drawer manipulation.\nThese experiments highlight the robustness of our method compared to planning\nand show how interactive perception and error recovery naturally emerge from\ngradient descent without explicitly implementing them. This offers a\ncomputationally efficient alternative to planning for a variety of sequential\ntasks, while aligning with observations on biological problem-solving\nstrategies.\n","authors":["Vito Mengers","Oliver Brock"],"pdf_url":"https://arxiv.org/pdf/2503.01732v1.pdf","comment":"Accepted at ICRA25; 7 pages + 6 figures; Supplementary Material under\n  https://www.tu.berlin/robotics/papers/noplan"},{"id":"http://arxiv.org/abs/2503.01729v1","updated":"2025-03-03T16:49:15Z","published":"2025-03-03T16:49:15Z","title":"FLAME: A Federated Learning Benchmark for Robotic Manipulation","summary":"  Recent progress in robotic manipulation has been fueled by large-scale\ndatasets collected across diverse environments. Training robotic manipulation\npolicies on these datasets is traditionally performed in a centralized manner,\nraising concerns regarding scalability, adaptability, and data privacy. While\nfederated learning enables decentralized, privacy-preserving training, its\napplication to robotic manipulation remains largely unexplored. We introduce\nFLAME (Federated Learning Across Manipulation Environments), the first\nbenchmark designed for federated learning in robotic manipulation. FLAME\nconsists of: (i) a set of large-scale datasets of over 160,000 expert\ndemonstrations of multiple manipulation tasks, collected across a wide range of\nsimulated environments; (ii) a training and evaluation framework for robotic\npolicy learning in a federated setting. We evaluate standard federated learning\nalgorithms in FLAME, showing their potential for distributed policy learning\nand highlighting key challenges. Our benchmark establishes a foundation for\nscalable, adaptive, and privacy-aware robotic learning.\n","authors":["Santiago Bou Betran","Alberta Longhini","Miguel Vasco","Yuchong Zhang","Danica Kragic"],"pdf_url":"https://arxiv.org/pdf/2503.01729v1.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2409.13195v2","updated":"2025-03-03T16:38:51Z","published":"2024-09-20T03:55:47Z","title":"Guaranteed Reach-Avoid for Black-Box Systems through Narrow Gaps via\n  Neural Network Reachability","summary":"  In the classical reach-avoid problem, autonomous mobile robots are tasked to\nreach a goal while avoiding obstacles. However, it is difficult to provide\nguarantees on the robot's performance when the obstacles form a narrow gap and\nthe robot is a black-box (i.e. the dynamics are not known analytically, but\ninteracting with the system is cheap). To address this challenge, this paper\npresents NeuralPARC. The method extends the authors' prior Piecewise Affine\nReach-avoid Computation (PARC) method to systems modeled by rectified linear\nunit (ReLU) neural networks, which are trained to represent parameterized\ntrajectory data demonstrated by the robot. NeuralPARC computes the reachable\nset of the network while accounting for modeling error, and returns a set of\nstates and parameters with which the black-box system is guaranteed to reach\nthe goal and avoid obstacles. NeuralPARC is shown to outperform PARC,\ngenerating provably-safe extreme vehicle drift parking maneuvers in simulations\nand in real life on a model car, as well as enabling safety on an autonomous\nsurface vehicle (ASV) subjected to large disturbances and controlled by a deep\nreinforcement learning (RL) policy.\n","authors":["Long Kiu Chung","Wonsuhk Jung","Srivatsank Pullabhotla","Parth Shinde","Yadu Sunil","Saihari Kota","Luis Felipe Wolf Batista","Cédric Pradalier","Shreyas Kousik"],"pdf_url":"https://arxiv.org/pdf/2409.13195v2.pdf","comment":"This work has been submitted for possible publication"},{"id":"http://arxiv.org/abs/2409.09975v2","updated":"2025-03-03T16:24:36Z","published":"2024-09-16T04:13:22Z","title":"Constrained Bandwidth Observation Sharing for Multi-Robot Navigation in\n  Dynamic Environments via Intelligent Knapsack","summary":"  Multi-robot navigation is increasingly crucial in various domains, including\ndisaster response, autonomous vehicles, and warehouse and manufacturing\nautomation. Robot teams often must operate in highly dynamic environments and\nunder strict bandwidth constraints imposed by communication infrastructure,\nrendering effective observation sharing within the system a challenging\nproblem. This paper presents a novel optimal communication scheme, Intelligent\nKnapsack (iKnap), for multi-robot navigation in dynamic environments under\nbandwidth constraints. We model multi-robot communication as belief propagation\nin a graph of inferential agents. We then formulate the combinatorial\noptimization for observation sharing as a 0/1 knapsack problem, where each\npotential pairwise communication between robots is assigned a decision-making\nutility to be weighed against its bandwidth cost, and the system has some\ncumulative bandwidth limit. We evaluate our approach in a simulated robotic\nwarehouse with human workers using ROS2 and the Open Robotics Middleware\nFramework. Compared to state-of-the-art broadcast-based optimal communication\nschemes, iKnap yields significant improvements in navigation performance with\nrespect to scenario complexity while maintaining a similar runtime.\nFurthermore, iKnap utilizes allocated bandwidth and observational resources\nmore efficiently than existing approaches, especially in very low-resource and\nhigh-uncertainty settings. Based on these results, we claim that the proposed\nmethod enables more robust collaboration for multi-robot teams in real-world\nnavigation problems.\n","authors":["Anirudh Chari","Rui Chen","Han Zheng","Changliu Liu"],"pdf_url":"https://arxiv.org/pdf/2409.09975v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01700v1","updated":"2025-03-03T16:13:41Z","published":"2025-03-03T16:13:41Z","title":"Code-as-Symbolic-Planner: Foundation Model-Based Robot Planning via\n  Symbolic Code Generation","summary":"  Recent works have shown great potentials of Large Language Models (LLMs) in\nrobot task and motion planning (TAMP). Current LLM approaches generate text- or\ncode-based reasoning chains with sub-goals and action plans. However, they do\nnot fully leverage LLMs' symbolic computing and code generation capabilities.\nMany robot TAMP tasks involve complex optimization under multiple constraints,\nwhere pure textual reasoning is insufficient. While augmenting LLMs with\npredefined solvers and planners improves performance, it lacks generalization\nacross tasks. Given LLMs' growing coding proficiency, we enhance their TAMP\ncapabilities by steering them to generate code as symbolic planners for\noptimization and constraint verification. Unlike prior work that uses code to\ninterface with robot action modules, we steer LLMs to generate code as solvers,\nplanners, and checkers for TAMP tasks requiring symbolic computing, while still\nleveraging textual reasoning to incorporate common sense. With a multi-round\nguidance and answer evolution framework, the proposed Code-as-Symbolic-Planner\nimproves success rates by average 24.1\\% over best baseline methods across\nseven typical TAMP tasks and three popular LLMs. Code-as-Symbolic-Planner shows\nstrong effectiveness and generalizability across discrete and continuous\nenvironments, 2D/3D simulations and real-world settings, as well as single- and\nmulti-robot tasks with diverse requirements. See our project website\nhttps://yongchao98.github.io/Code-Symbol-Planner/ for prompts, videos, and\ncode.\n","authors":["Yongchao Chen","Yilun Hao","Yang Zhang","Chuchu Fan"],"pdf_url":"https://arxiv.org/pdf/2503.01700v1.pdf","comment":"7 pages, 7 figures, 3 tables"},{"id":"http://arxiv.org/abs/2409.13607v2","updated":"2025-03-03T16:04:29Z","published":"2024-09-20T16:07:31Z","title":"RECON: Reducing Causal Confusion with Human-Placed Markers","summary":"  Imitation learning enables robots to learn new tasks from human examples. One\nfundamental limitation while learning from humans is causal confusion. Causal\nconfusion occurs when the robot's observations include both task-relevant and\nextraneous information: for instance, a robot's camera might see not only the\nintended goal, but also clutter and changes in lighting within its environment.\nBecause the robot does not know which aspects of its observations are important\na priori, it often misinterprets the human's examples and fails to learn the\ndesired task. To address this issue, we highlight that -- while the robot\nlearner may not know what to focus on -- the human teacher does. In this paper\nwe propose that the human proactively marks key parts of their task with small,\nlightweight beacons. Under our framework (RECON) the human attaches these\nbeacons to task-relevant objects before providing demonstrations: as the human\nshows examples of the task, beacons track the position of marked objects. We\nthen harness this offline beacon data to train a task-relevant state embedding.\nSpecifically, we embed the robot's observations to a latent state that is\ncorrelated with the measured beacon readings: in practice, this causes the\nrobot to autonomously filter out extraneous observations and make decisions\nbased on features learned from the beacon data. Our simulations and a real\nrobot experiment suggest that this framework for human-placed beacons mitigates\ncausal confusion. Indeed, we find that using RECON significantly reduces the\nnumber of demonstrations needed to convey the task, lowering the overall time\nrequired for human teaching. See videos here: https://youtu.be/oy85xJvtLSU\n","authors":["Robert Ramirez Sanchez","Heramb Nemlekar","Shahabedin Sagheb","Cara M. Nunez","Dylan P. Losey"],"pdf_url":"https://arxiv.org/pdf/2409.13607v2.pdf","comment":"7 pages, 5 figures"},{"id":"http://arxiv.org/abs/2403.17622v2","updated":"2025-03-03T15:57:00Z","published":"2024-03-26T11:51:58Z","title":"Online Tree Reconstruction and Forest Inventory on a Mobile Robotic\n  System","summary":"  Terrestrial laser scanning (TLS) is the standard technique used to create\naccurate point clouds for digital forest inventories. However, the measurement\nprocess is demanding, requiring up to two days per hectare for data collection,\nsignificant data storage, as well as resource-heavy post-processing of 3D data.\nIn this work, we present a real-time mapping and analysis system that enables\nonline generation of forest inventories using mobile laser scanners that can be\nmounted e.g. on mobile robots. Given incrementally created and locally accurate\nsubmaps-data payloads-our approach extracts tree candidates using a custom,\nVoronoi-inspired clustering algorithm. Tree candidates are reconstructed using\nan adapted Hough algorithm, which enables robust modeling of the tree stem.\nFurther, we explicitly incorporate the incremental nature of the data\ncollection by consistently updating the database using a pose graph LiDAR SLAM\nsystem. This enables us to refine our estimates of the tree traits if an area\nis revisited later during a mission. We demonstrate competitive accuracy to TLS\nor manual measurements using laser scanners that we mounted on backpacks or\nmobile robots operating in conifer, broad-leaf and mixed forests. Our results\nachieve RMSE of 1.93 cm, a bias of 0.65 cm and a standard deviation of 1.81 cm\n(averaged across these sequences)-with no post-processing required after the\nmission is complete.\n","authors":["Leonard Freißmuth","Matias Mattamala","Nived Chebrolu","Simon Schaefer","Stefan Leutenegger","Maurice Fallon"],"pdf_url":"https://arxiv.org/pdf/2403.17622v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01650v1","updated":"2025-03-03T15:27:11Z","published":"2025-03-03T15:27:11Z","title":"CAPS: Context-Aware Priority Sampling for Enhanced Imitation Learning in\n  Autonomous Driving","summary":"  In this paper, we introduce CAPS (Context-Aware Priority Sampling), a novel\nmethod designed to enhance data efficiency in learning-based autonomous driving\nsystems. CAPS addresses the challenge of imbalanced training datasets in\nimitation learning by leveraging Vector Quantized Variational Autoencoders\n(VQ-VAEs). The use of VQ-VAE provides a structured and interpretable data\nrepresentation, which helps reveal meaningful patterns in the data. These\npatterns are used to group the data into clusters, with each sample being\nassigned a cluster ID. The cluster IDs are then used to re-balance the dataset,\nensuring that rare yet valuable samples receive higher priority during\ntraining. By ensuring a more diverse and informative training set, CAPS\nimproves the generalization of the trained planner across a wide range of\ndriving scenarios. We evaluate our method through closed-loop simulations in\nthe CARLA environment. The results on Bench2Drive scenarios demonstrate that\nour framework outperforms state-of-the-art methods, leading to notable\nimprovements in model performance.\n","authors":["Hamidreza Mirkhani","Behzad Khamidehi","Ehsan Ahmadi","Fazel Arasteh","Mohammed Elmahgiubi","Weize Zhang","Umar Rajguru","Kasra Rezaee"],"pdf_url":"https://arxiv.org/pdf/2503.01650v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.09524v2","updated":"2025-03-03T15:10:42Z","published":"2024-11-14T15:40:16Z","title":"FlowNav: Combining Flow Matching and Depth Priors for Efficient\n  Navigation","summary":"  Effective robot navigation in unseen environments is a challenging task that\nrequires precise control actions at high frequencies. Recent advances have\nframed it as an image-goal-conditioned control problem, where the robot\ngenerates navigation actions using frontal RGB images. Current state-of-the-art\nmethods in this area use diffusion policies to generate these control actions.\nDespite their promising results, these models are computationally expensive and\nsuffer from weak perception. To address these limitations, we present FlowNav,\na novel approach that uses a combination of Conditional Flow Matching (CFM) and\ndepth priors from off-the-shelf foundation models to learn action policies for\nrobot navigation. FlowNav is significantly more accurate at navigation and\nexploration than state-of-the-art methods. We validate our contributions using\nreal robot experiments in multiple unseen environments, demonstrating improved\nnavigation reliability and accuracy. We make the code and trained models\npublicly available.\n","authors":["Samiran Gode","Abhijeet Nayak","Débora N. P. Oliveira","Michael Krawez","Cordelia Schmid","Wolfram Burgard"],"pdf_url":"https://arxiv.org/pdf/2411.09524v2.pdf","comment":"Submitted to IROS'25. Previous version accepted at CoRL 2024 workshop\n  on Learning Effective Abstractions for Planning (LEAP) and workshop on\n  Differentiable Optimization Everywhere: Simulation, Estimation, Learning, and\n  Control"},{"id":"http://arxiv.org/abs/2503.01626v1","updated":"2025-03-03T15:00:55Z","published":"2025-03-03T15:00:55Z","title":"A Note on the Time Complexity of Using Subdivision Methods for the\n  Approximation of Fibers","summary":"  Subdivision methods such as quadtrees, octrees, and higher-dimensional\northrees are standard practice in different domains of computer science. We can\nuse these methods to represent given geometries, such as curves, meshes, or\nsurfaces. This representation is achieved by splitting some bounding voxel\nrecursively while further splitting only sub-voxels that intersect with the\ngiven geometry. It is fairly known that subdivision methods are more efficient\nthan traversing a fine-grained voxel grid. In this short note, we propose\nanother outlook on analyzing the construction time complexity of orthrees to\nrepresent implicitly defined geometries that are fibers (preimages) of some\nfunction. This complexity is indeed asymptotically better than traversing dense\nvoxel grids, under certain conditions, which we specify in the note. In fact,\nthe complexity is output sensitive, and is closely related to the Hausdorff\nmeasure and Hausdorff dimension of the resulting geometry.\n","authors":["Michael M. Bilevich","Dan Halperin"],"pdf_url":"https://arxiv.org/pdf/2503.01626v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18038v2","updated":"2025-03-03T14:52:51Z","published":"2024-09-26T16:42:53Z","title":"MMDVS-LF: Multi-Modal Dynamic Vision Sensor and Eye-Tracking Dataset for\n  Line Following","summary":"  Dynamic Vision Sensors (DVS) offer a unique advantage in control applications\ndue to their high temporal resolution and asynchronous event-based data. Still,\ntheir adoption in machine learning algorithms remains limited. To address this\ngap and promote the development of models that leverage the specific\ncharacteristics of DVS data, we introduce the MMDVS-LF: Multi-Modal Dynamic\nVision Sensor and Eye-Tracking Dataset for Line Following. This comprehensive\ndataset is the first to integrate multiple sensor modalities, including DVS\nrecordings and eye-tracking data from a small-scale standardized vehicle.\nAdditionally, the dataset includes RGB video, odometry, Inertial Measurement\nUnit (IMU) data, and demographic data of drivers performing a Line Following.\nWith its diverse range of data, MMDVS-LF opens new opportunities for developing\nevent-based deep learning algorithms just like the MNIST dataset did for\nConvolutional Neural Networks.\n","authors":["Felix Resch","Mónika Farsang","Radu Grosu"],"pdf_url":"https://arxiv.org/pdf/2409.18038v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01616v1","updated":"2025-03-03T14:49:52Z","published":"2025-03-03T14:49:52Z","title":"RoboDexVLM: Visual Language Model-Enabled Task Planning and Motion\n  Control for Dexterous Robot Manipulation","summary":"  This paper introduces RoboDexVLM, an innovative framework for robot task\nplanning and grasp detection tailored for a collaborative manipulator equipped\nwith a dexterous hand. Previous methods focus on simplified and limited\nmanipulation tasks, which often neglect the complexities associated with\ngrasping a diverse array of objects in a long-horizon manner. In contrast, our\nproposed framework utilizes a dexterous hand capable of grasping objects of\nvarying shapes and sizes while executing tasks based on natural language\ncommands. The proposed approach has the following core components: First, a\nrobust task planner with a task-level recovery mechanism that leverages\nvision-language models (VLMs) is designed, which enables the system to\ninterpret and execute open-vocabulary commands for long sequence tasks. Second,\na language-guided dexterous grasp perception algorithm is presented based on\nrobot kinematics and formal methods, tailored for zero-shot dexterous\nmanipulation with diverse objects and commands. Comprehensive experimental\nresults validate the effectiveness, adaptability, and robustness of RoboDexVLM\nin handling long-horizon scenarios and performing dexterous grasping. These\nresults highlight the framework's ability to operate in complex environments,\nshowcasing its potential for open-vocabulary dexterous manipulation. Our\nopen-source project page can be found at\nhttps://henryhcliu.github.io/robodexvlm.\n","authors":["Haichao Liu","Sikai Guo","Pengfei Mai","Jiahang Cao","Haoang Li","Jun Ma"],"pdf_url":"https://arxiv.org/pdf/2503.01616v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10071v4","updated":"2025-03-03T14:47:34Z","published":"2024-09-16T08:21:22Z","title":"Towards Physically Realizable Adversarial Attacks in Embodied Vision\n  Navigation","summary":"  The significant advancements in embodied vision navigation have raised\nconcerns about its susceptibility to adversarial attacks exploiting deep neural\nnetworks. Investigating the adversarial robustness of embodied vision\nnavigation is crucial, especially given the threat of 3D physical attacks that\ncould pose risks to human safety. However, existing attack methods for embodied\nvision navigation often lack physical feasibility due to challenges in\ntransferring digital perturbations into the physical world. Moreover, current\nphysical attacks for object detection struggle to achieve both multi-view\neffectiveness and visual naturalness in navigation scenarios. To address this,\nwe propose a practical attack method for embodied navigation by attaching\nadversarial patches to objects, where both opacity and textures are learnable.\nSpecifically, to ensure effectiveness across varying viewpoints, we employ a\nmulti-view optimization strategy based on object-aware sampling, which\noptimizes the patch's texture based on feedback from the vision-based\nperception model used in navigation. To make the patch inconspicuous to human\nobservers, we introduce a two-stage opacity optimization mechanism, in which\nopacity is fine-tuned after texture optimization. Experimental results\ndemonstrate that our adversarial patches decrease the navigation success rate\nby an average of 22.39%, outperforming previous methods in practicality,\neffectiveness, and naturalness. Code is available at:\nhttps://github.com/chen37058/Physical-Attacks-in-Embodied-Nav\n","authors":["Meng Chen","Jiawei Tu","Chao Qi","Yonghao Dang","Feng Zhou","Wei Wei","Jianqin Yin"],"pdf_url":"https://arxiv.org/pdf/2409.10071v4.pdf","comment":"7 pages, 7 figures, submitted to IEEE/RSJ International Conference on\n  Intelligent Robots and Systems (IROS) 2025"},{"id":"http://arxiv.org/abs/2409.13533v2","updated":"2025-03-03T14:40:34Z","published":"2024-09-20T14:23:05Z","title":"Using High-Level Patterns to Estimate How Humans Predict a Robot will\n  Behave","summary":"  Humans interacting with robots often form predictions of what the robot will\ndo next. For instance, based on the recent behavior of an autonomous car, a\nnearby human driver might predict that the car is going to remain in the same\nlane. It is important for the robot to understand the human's prediction for\nsafe and seamless interaction: e.g., if the autonomous car knows the human\nthinks it is not merging -- but the autonomous car actually intends to merge --\nthen the car can adjust its behavior to prevent an accident. Prior works\ntypically assume that humans make precise predictions of robot behavior.\nHowever, recent research on human-human prediction suggests the opposite:\nhumans tend to approximate other agents by predicting their high-level\nbehaviors. We apply this finding to develop a second-order theory of mind\napproach that enables robots to estimate how humans predict they will behave.\nTo extract these high-level predictions directly from data, we embed the recent\nhuman and robot trajectories into a discrete latent space. Each element of this\nlatent space captures a different type of behavior (e.g., merging in front of\nthe human, remaining in the same lane) and decodes into a vector field across\nthe state space that is consistent with the underlying behavior type. We\nhypothesize that our resulting high-level and course predictions of robot\nbehavior will correspond to actual human predictions. We provide initial\nevidence in support of this hypothesis through proof-of-concept simulations,\ntesting our method's predictions against those of real users, and experiments\non a real-world interactive driving dataset.\n","authors":["Sagar Parekh","Lauren Bramblett","Nicola Bezzo","Dylan P. Losey"],"pdf_url":"https://arxiv.org/pdf/2409.13533v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.04123v2","updated":"2025-03-03T14:29:40Z","published":"2024-04-05T14:22:56Z","title":"Designing Robots to Help Women","summary":"  Robots are being designed to help people in an increasing variety of\nsettings--but seemingly little attention has been given so far to the specific\nneeds of women, who represent roughly half of the world's population but are\nhighly underrepresented in robotics. Here we used a speculative prototyping\napproach to explore this expansive design space: First, we identified some\npotential challenges of interest, including crimes and illnesses that\ndisproportionately affect women, as well as potential opportunities for\ndesigners, which were visualized in five sketches. Then, one of the sketched\nscenarios was further explored by developing a prototype, of a robotic helper\ndrone equipped with computer vision to detect hidden cameras that could be used\nto spy on women. While object detection introduced some errors, hidden cameras\nwere identified with a reasonable accuracy of 80% (Intersection over Union\n(IoU) score: 0.40). Our aim is that the identified challenges and opportunities\ncould help spark discussion and inspire designers, toward realizing a safer,\nmore inclusive future through responsible use of technology.\n","authors":["Martin Cooney","Lena Klasén","Fernando Alonso-Fernandez"],"pdf_url":"https://arxiv.org/pdf/2404.04123v2.pdf","comment":"10 pages, Accepted Version of a Published Conference Paper in 14th\n  Scandinavian Conference on Artificial Intelligence (SCAI 2024): AI for a\n  better society, June 10-11, 2024, J\\\"onk\\\"oping, Sweden, Link\\\"oping\n  Electronic Conference Proceedings (ECP) ISSN: 1650-3740 (CC BY 4.0 License)"},{"id":"http://arxiv.org/abs/2503.01585v1","updated":"2025-03-03T14:26:19Z","published":"2025-03-03T14:26:19Z","title":"Soft Everting Prosthetic Hand and Comparison with Existing Body-Powered\n  Terminal Devices","summary":"  In this paper, we explore the use of a soft gripper, specifically a soft\ninverting-everting toroidal hydrostat, as a prosthetic hand. We present a\ndesign of the gripper integrated into a body-powered elbow-driven system and\nevaluate its performance compared to similar body-powered terminal devices: the\nKwawu 3D-printed hand and the Hosmer hook. Our experiments highlight advantages\nof the Everting hand, such as low required cable tension for operation (1.6 N\nfor Everting, 30.0 N for Kwawu, 28.1 N for Hosmer), limited restriction on the\nelbow angle range, and secure grasping capability (peak pulling force required\nto remove an object: 15.8 N for Everting, 6.9 N for Kwawu, 4.0 N for Hosmer).\nIn our pilot user study, six able-bodied participants performed standardized\nhand dexterity tests. With the Everting hand compared to the Kwawu hand, users\ntransferred more blocks in one minute and completed three tasks (moving small\ncommon objects, simulated feeding with a spoon, and moving large empty cans)\nfaster (p~$\\leq$~0.05). With the Everting hand compared to the Hosmer hook,\nusers moved large empty cans faster (p~$\\leq$~0.05) and achieved similar\nperformance on all other tasks. Overall, user preference leaned toward the\nEverting hand for its adaptable grip and ease of use, although its abilities\ncould be improved in tasks requiring high precision such as writing with a pen,\nand in handling heavier objects such as large heavy cans.\n","authors":["Gayoung Park","Katalin Schäffer","Margaret M. Coad"],"pdf_url":"https://arxiv.org/pdf/2503.01585v1.pdf","comment":"The paper was accepted to the 8th IEEE-RAS International Conference\n  on Soft Robotics (RoboSoft 2025). The corresponding video attachment is\n  available at: https://youtu.be/zO_Some_HxY"},{"id":"http://arxiv.org/abs/2410.13586v2","updated":"2025-03-03T14:24:45Z","published":"2024-10-17T14:21:32Z","title":"Preference Aligned Diffusion Planner for Quadrupedal Locomotion Control","summary":"  Diffusion models demonstrate superior performance in capturing complex\ndistributions from large-scale datasets, providing a promising solution for\nquadrupedal locomotion control. However, the robustness of the diffusion\nplanner is inherently dependent on the diversity of the pre-collected datasets.\nTo mitigate this issue, we propose a two-stage learning framework to enhance\nthe capability of the diffusion planner under limited dataset\n(reward-agnostic). Through the offline stage, the diffusion planner learns the\njoint distribution of state-action sequences from expert datasets without using\nreward labels. Subsequently, we perform the online interaction in the\nsimulation environment based on the trained offline planner, which\nsignificantly diversified the original behavior and thus improves the\nrobustness. Specifically, we propose a novel weak preference labeling method\nwithout the ground-truth reward or human preferences. The proposed method\nexhibits superior stability and velocity tracking accuracy in pacing, trotting,\nand bounding gait under different speeds and can perform a zero-shot transfer\nto the real Unitree Go1 robots. The project website for this paper is at\nhttps://shangjaven.github.io/preference-aligned-diffusion-legged.\n","authors":["Xinyi Yuan","Zhiwei Shang","Zifan Wang","Chenkai Wang","Zhao Shan","Meixin Zhu","Chenjia Bai","Xuelong Li","Weiwei Wan","Kensuke Harada"],"pdf_url":"https://arxiv.org/pdf/2410.13586v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01571v1","updated":"2025-03-03T14:12:47Z","published":"2025-03-03T14:12:47Z","title":"MLINE-VINS: Robust Monocular Visual-Inertial SLAM With Flow Manhattan\n  and Line Features","summary":"  In this paper we introduce MLINE-VINS, a novel monocular visual-inertial\nodometry (VIO) system that leverages line features and Manhattan Word\nassumption. Specifically, for line matching process, we propose a novel\ngeometric line optical flow algorithm that efficiently tracks line features\nwith varying lengths, whitch is do not require detections and descriptors in\nevery frame. To address the instability of Manhattan estimation from line\nfeatures, we propose a tracking-by-detection module that consistently tracks\nand optimizes Manhattan framse in consecutive images. By aligning the Manhattan\nWorld with the VIO world frame, the tracking could restart using the latest\npose from back-end, simplifying the coordinate transformations within the\nsystem. Furthermore, we implement a mechanism to validate Manhattan frames and\na novel global structural constraints back-end optimization. Extensive\nexperiments results on vairous datasets, including benchmark and self-collected\ndatasets, show that the proposed approach outperforms existing methods in terms\nof accuracy and long-range robustness. The source code of our method is\navailable at: https://github.com/LiHaoy-ux/MLINE-VINS.\n","authors":["Chao Ye","Haoyuan Li","Weiyang Lin","Xianqiang Yang"],"pdf_url":"https://arxiv.org/pdf/2503.01571v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01562v1","updated":"2025-03-03T14:07:20Z","published":"2025-03-03T14:07:20Z","title":"VF-Plan: Bridging the Art Gallery Problem and Static LiDAR Scanning with\n  Visibility Field Optimization","summary":"  Viewpoint planning is crucial for 3D data collection and autonomous\nnavigation, yet existing methods often miss key optimization objectives for\nstatic LiDAR, resulting in suboptimal network designs. The Viewpoint Planning\nProblem (VPP), which builds upon the Art Gallery Problem (AGP), requires not\nonly full coverage but also robust registrability and connectivity under\nlimited sensor views. We introduce a greedy optimization algorithm that tackles\nthese VPP and AGP challenges through a novel Visibility Field (VF) approach.\nThe VF captures visibility characteristics unique to static LiDAR, enabling a\nreduction from 2D to 1D by focusing on medial axis and joints. This leads to a\nminimal, fully connected viewpoint network with comprehensive coverage and\nminimal redundancy. Experiments across diverse environments show that our\nmethod achieves high efficiency and scalability, matching or surpassing expert\ndesigns. Compared to state-of-the-art methods, our approach achieves comparable\nviewpoint counts (VC) while reducing Weighted Average Path Length (WAPL) by\napproximately 95\\%, indicating a much more compact and connected network.\nDataset and source code will be released upon acceptance.\n","authors":["Biao Xionga","Longjun Zhanga","Ruiqi Huanga","Junwei Zhoua","Bojian Wub","Fashuai Lic"],"pdf_url":"https://arxiv.org/pdf/2503.01562v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07487v2","updated":"2025-03-03T14:04:23Z","published":"2024-12-10T13:12:32Z","title":"Stereo Hand-Object Reconstruction for Human-to-Robot Handover","summary":"  Jointly estimating hand and object shape facilitates the grasping task in\nhuman-to-robot handovers. However, relying on hand-crafted prior knowledge\nabout the geometric structure of the object fails when generalising to unseen\nobjects, and depth sensors fail to detect transparent objects such as drinking\nglasses. In this work, we propose a stereo-based method for hand-object\nreconstruction that combines single-view reconstructions probabilistically to\nform a coherent stereo reconstruction. We learn 3D shape priors from a large\nsynthetic hand-object dataset to ensure that our method is generalisable, and\nuse RGB inputs to better capture transparent objects. We show that our method\nreduces the object Chamfer distance compared to existing RGB based hand-object\nreconstruction methods on single view and stereo settings. We process the\nreconstructed hand-object shape with a projection-based outlier removal step\nand use the output to guide a human-to-robot handover pipeline with\nwide-baseline stereo RGB cameras. Our hand-object reconstruction enables a\nrobot to successfully receive a diverse range of household objects from the\nhuman.\n","authors":["Yik Lung Pang","Alessio Xompero","Changjae Oh","Andrea Cavallaro"],"pdf_url":"https://arxiv.org/pdf/2412.07487v2.pdf","comment":"8 pages, 9 figures, 1 table"},{"id":"http://arxiv.org/abs/2503.01548v1","updated":"2025-03-03T13:54:56Z","published":"2025-03-03T13:54:56Z","title":"MapExRL: Human-Inspired Indoor Exploration with Predicted Environment\n  Context and Reinforcement Learning","summary":"  Path planning for robotic exploration is challenging, requiring reasoning\nover unknown spaces and anticipating future observations. Efficient exploration\nrequires selecting budget-constrained paths that maximize information gain.\nDespite advances in autonomous exploration, existing algorithms still fall\nshort of human performance, particularly in structured environments where\npredictive cues exist but are underutilized. Guided by insights from our user\nstudy, we introduce MapExRL, which improves robot exploration efficiency in\nstructured indoor environments by enabling longer-horizon planning through\nreinforcement learning (RL) and global map predictions. Unlike many RL-based\nexploration methods that use motion primitives as the action space, our\napproach leverages frontiers for more efficient model learning and longer\nhorizon reasoning. Our framework generates global map predictions from the\nobserved map, which our policy utilizes, along with the prediction uncertainty,\nestimated sensor coverage, frontier distance, and remaining distance budget, to\nassess the strategic long-term value of frontiers. By leveraging multiple\nfrontier scoring methods and additional context, our policy makes more informed\ndecisions at each stage of the exploration. We evaluate our framework on a\nreal-world indoor map dataset, achieving up to an 18.8% improvement over the\nstrongest state-of-the-art baseline, with even greater gains compared to\nconventional frontier-based algorithms.\n","authors":["Narek Harutyunyan","Brady Moon","Seungchan Kim","Cherie Ho","Adam Hung","Sebastian Scherer"],"pdf_url":"https://arxiv.org/pdf/2503.01548v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.01543v1","updated":"2025-03-03T13:52:41Z","published":"2025-03-03T13:52:41Z","title":"Exo-ViHa: A Cross-Platform Exoskeleton System with Visual and Haptic\n  Feedback for Efficient Dexterous Skill Learning","summary":"  Imitation learning has emerged as a powerful paradigm for robot skills\nlearning. However, traditional data collection systems for dexterous\nmanipulation face challenges, including a lack of balance between acquisition\nefficiency, consistency, and accuracy. To address these issues, we introduce\nExo-ViHa, an innovative 3D-printed exoskeleton system that enables users to\ncollect data from a first-person perspective while providing real-time haptic\nfeedback. This system combines a 3D-printed modular structure with a slam\ncamera, a motion capture glove, and a wrist-mounted camera. Various dexterous\nhands can be installed at the end, enabling it to simultaneously collect the\nposture of the end effector, hand movements, and visual data. By leveraging the\nfirst-person perspective and direct interaction, the exoskeleton enhances the\ntask realism and haptic feedback, improving the consistency between\ndemonstrations and actual robot deployments. In addition, it has cross-platform\ncompatibility with various robotic arms and dexterous hands. Experiments show\nthat the system can significantly improve the success rate and efficiency of\ndata collection for dexterous manipulation tasks.\n","authors":["Xintao Chao","Shilong Mu","Yushan Liu","Shoujie Li","Chuqiao Lyu","Xiao-Ping Zhang","Wenbo Ding"],"pdf_url":"https://arxiv.org/pdf/2503.01543v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2409.15517v2","updated":"2025-03-03T13:22:14Z","published":"2024-09-23T20:09:43Z","title":"MATCH POLICY: A Simple Pipeline from Point Cloud Registration to\n  Manipulation Policies","summary":"  Many manipulation tasks require the robot to rearrange objects relative to\none another. Such tasks can be described as a sequence of relative poses\nbetween parts of a set of rigid bodies. In this work, we propose MATCH POLICY,\na simple but novel pipeline for solving high-precision pick and place tasks.\nInstead of predicting actions directly, our method registers the pick and place\ntargets to the stored demonstrations. This transfers action inference into a\npoint cloud registration task and enables us to realize nontrivial manipulation\npolicies without any training. MATCH POLICY is designed to solve high-precision\ntasks with a key-frame setting. By leveraging the geometric interaction and the\nsymmetries of the task, it achieves extremely high sample efficiency and\ngeneralizability to unseen configurations. We demonstrate its state-of-the-art\nperformance across various tasks on RLBench benchmark compared with several\nstrong baselines and test it on a real robot with six tasks.\n","authors":["Haojie Huang","Haotian Liu","Dian Wang","Robin Walters","Robert Platt"],"pdf_url":"https://arxiv.org/pdf/2409.15517v2.pdf","comment":"project url: https://haojhuang.github.io/match_page/"},{"id":"http://arxiv.org/abs/2410.07554v3","updated":"2025-03-03T12:41:06Z","published":"2024-10-10T02:50:04Z","title":"ForceMimic: Force-Centric Imitation Learning with Force-Motion Capture\n  System for Contact-Rich Manipulation","summary":"  In most contact-rich manipulation tasks, humans apply time-varying forces to\nthe target object, compensating for inaccuracies in the vision-guided hand\ntrajectory. However, current robot learning algorithms primarily focus on\ntrajectory-based policy, with limited attention given to learning force-related\nskills. To address this limitation, we introduce ForceMimic, a force-centric\nrobot learning system, providing a natural, force-aware and robot-free robotic\ndemonstration collection system, along with a hybrid force-motion imitation\nlearning algorithm for robust contact-rich manipulation. Using the proposed\nForceCapture system, an operator can peel a zucchini in 5 minutes, while\nforce-feedback teleoperation takes over 13 minutes and struggles with task\ncompletion. With the collected data, we propose HybridIL to train a\nforce-centric imitation learning model, equipped with hybrid force-position\ncontrol primitive to fit the predicted wrench-position parameters during robot\nexecution. Experiments demonstrate that our approach enables the model to learn\na more robust policy under the contact-rich task of vegetable peeling,\nincreasing the success rates by 54.5% relatively compared to state-ofthe-art\npure-vision-based imitation learning. Hardware, code, data and more results can\nbe found on the project website at https://forcemimic.github.io.\n","authors":["Wenhai Liu","Junbo Wang","Yiming Wang","Weiming Wang","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2410.07554v3.pdf","comment":"8 pages, 7 figures, accepted by 2025 IEEE International Conference on\n  Robotics and Automation (ICRA 2025), the first three authors contribute\n  equally, project website at https://forcemimic.github.io"},{"id":"http://arxiv.org/abs/2503.01481v1","updated":"2025-03-03T12:40:17Z","published":"2025-03-03T12:40:17Z","title":"Origami-Inspired Soft Gripper with Tunable Constant Force Output","summary":"  Soft robotic grippers gently and safely manipulate delicate objects due to\ntheir inherent adaptability and softness. Limited by insufficient stiffness and\nimprecise force control, conventional soft grippers are not suitable for\napplications that require stable grasping force. In this work, we propose a\nsoft gripper that utilizes an origami-inspired structure to achieve tunable\nconstant force output over a wide strain range. The geometry of each taper\npanel is established to provide necessary parameters such as protrusion\ndistance, taper angle, and crease thickness required for 3D modeling and FEA\nanalysis. Simulations and experiments show that by optimizing these parameters,\nour design can achieve a tunable constant force output. Moreover, the\norigami-inspired soft gripper dynamically adapts to different shapes while\npreventing excessive forces, with potential applications in logistics,\nmanufacturing, and other industrial settings that require stable and adaptive\noperations\n","authors":["Zhenwei Ni","Chang Xu","Zhihang Qin","Ceng Zhang","Zhiqiang Tang","Peiyi Wang","Cecilia Laschi"],"pdf_url":"https://arxiv.org/pdf/2503.01481v1.pdf","comment":"7 pages, 8 figures, conference"},{"id":"http://arxiv.org/abs/2503.01476v1","updated":"2025-03-03T12:33:50Z","published":"2025-03-03T12:33:50Z","title":"Trajectory Planning with Signal Temporal Logic Costs using Deterministic\n  Path Integral Optimization","summary":"  Formulating the intended behavior of a dynamic system can be challenging.\nSignal temporal logic (STL) is frequently used for this purpose due to its\nsuitability in formalizing comprehensible, modular, and versatile\nspatiotemporal specifications. Due to scaling issues with respect to the\ncomplexity of the specifications and the potential occurrence of\nnon-differentiable terms, classical optimization methods often solve STL-based\nproblems inefficiently. Smoothing and approximation techniques can alleviate\nthese issues but require changing the optimization problem. This paper proposes\na novel sampling-based method based on model predictive path integral control\nto solve optimal control problems with STL cost functions. We demonstrate the\neffectiveness of our method on benchmark motion planning problems and compare\nits performance with state-of-the-art methods. The results show that our method\nefficiently solves optimal control problems with STL costs.\n","authors":["Patrick Halder","Hannes Homburger","Lothar Kiltz","Johannes Reuter","Matthias Althoff"],"pdf_url":"https://arxiv.org/pdf/2503.01476v1.pdf","comment":"6+2 pages, 3 figures, P. Halder and H. Homburger contributed equally\n  to the paper, accepted to the 2025 IEEE International Conference on Robotics\n  & Automation (ICRA25)"},{"id":"http://arxiv.org/abs/2503.01474v1","updated":"2025-03-03T12:29:48Z","published":"2025-03-03T12:29:48Z","title":"Interactive Navigation for Legged Manipulators with Learned Arm-Pushing\n  Controller","summary":"  Interactive navigation is crucial in scenarios where proactively interacting\nwith objects can yield shorter paths, thus significantly improving traversal\nefficiency. Existing methods primarily focus on using the robot body to\nrelocate large obstacles (which could be comparable to the size of a robot).\nHowever, they prove ineffective in narrow or constrained spaces where the\nrobot's dimensions restrict its manipulation capabilities. This paper\nintroduces a novel interactive navigation framework for legged manipulators,\nfeaturing an active arm-pushing mechanism that enables the robot to reposition\nmovable obstacles in space-constrained environments. To this end, we develop a\nreinforcement learning-based arm-pushing controller with a two-stage reward\nstrategy for large-object manipulation. Specifically, this strategy first\ndirects the manipulator to a designated pushing zone to achieve a kinematically\nfeasible contact configuration. Then, the end effector is guided to maintain\nits position at appropriate contact points for stable object displacement while\npreventing toppling. The simulations validate the robustness of the arm-pushing\ncontroller, showing that the two-stage reward strategy improves policy\nconvergence and long-term performance. Real-world experiments further\ndemonstrate the effectiveness of the proposed navigation framework, which\nachieves shorter paths and reduced traversal time. The open-source project can\nbe found at\nhttps://github.com/Zhihaibi/Interactive-Navigation-for-legged-manipulator.git.\n","authors":["Zhihai Bi","Kai Chen","Chunxin Zheng","Yulin Li","Haoang Li","Jun Ma"],"pdf_url":"https://arxiv.org/pdf/2503.01474v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01471v1","updated":"2025-03-03T12:25:23Z","published":"2025-03-03T12:25:23Z","title":"Aerial Gym Simulator: A Framework for Highly Parallelized Simulation of\n  Aerial Robots","summary":"  This paper contributes the Aerial Gym Simulator, a highly parallelized,\nmodular framework for simulation and rendering of arbitrary multirotor\nplatforms based on NVIDIA Isaac Gym. Aerial Gym supports the simulation of\nunder-, fully- and over-actuated multirotors offering parallelized geometric\ncontrollers, alongside a custom GPU-accelerated rendering framework for\nray-casting capable of capturing depth, segmentation and vertex-level\nannotations from the environment. Multiple examples for key tasks, such as\ndepth-based navigation through reinforcement learning are provided. The\ncomprehensive set of tools developed within the framework makes it a powerful\nresource for research on learning for control, planning, and navigation using\nstate information as well as exteroceptive sensor observations. Extensive\nsimulation studies are conducted and successful sim2real transfer of trained\npolicies is demonstrated. The Aerial Gym Simulator is open-sourced at:\nhttps://github.com/ntnu-arl/aerial_gym_simulator.\n","authors":["Mihir Kulkarni","Welf Rehberg","Kostas Alexis"],"pdf_url":"https://arxiv.org/pdf/2503.01471v1.pdf","comment":"Accepted for publication in IEEE Robotics and Automation Letters\n  (RA-L)"},{"id":"http://arxiv.org/abs/2503.01439v1","updated":"2025-03-03T11:44:55Z","published":"2025-03-03T11:44:55Z","title":"AVR: Active Vision-Driven Robotic Precision Manipulation with Viewpoint\n  and Focal Length Optimization","summary":"  Robotic manipulation within dynamic environments presents challenges to\nprecise control and adaptability. Traditional fixed-view camera systems face\nchallenges adapting to change viewpoints and scale variations, limiting\nperception and manipulation precision. To tackle these issues, we propose the\nActive Vision-driven Robotic (AVR) framework, a teleoperation hardware solution\nthat supports dynamic viewpoint and dynamic focal length adjustments to\ncontinuously center targets and maintain optimal scale, accompanied by a\ncorresponding algorithm that effectively enhances the success rates of various\noperational tasks. Using the RoboTwin platform with a real-time image\nprocessing plugin, AVR framework improves task success rates by 5%-16% on five\nmanipulation tasks. Physical deployment on a dual-arm system demonstrates in\ncollaborative tasks and 36% precision in screwdriver insertion, outperforming\nbaselines by over 25%. Experimental results confirm that AVR framework enhances\nenvironmental perception, manipulation repeatability (40% $\\le $1 cm error),\nand robustness in complex scenarios, paving the way for future robotic\nprecision manipulation methods in the pursuit of human-level robot dexterity\nand precision.\n","authors":["Yushan Liu","Shilong Mu","Xintao Chao","Zizhen Li","Yao Mu","Tianxing Chen","Shoujie Li","Chuqiao Lyu","Xiao-ping Zhang","Wenbo Ding"],"pdf_url":"https://arxiv.org/pdf/2503.01439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01438v1","updated":"2025-03-03T11:44:49Z","published":"2025-03-03T11:44:49Z","title":"CAO-RONet: A Robust 4D Radar Odometry with Exploring More Information\n  from Low-Quality Points","summary":"  Recently, 4D millimetre-wave radar exhibits more stable perception ability\nthan LiDAR and camera under adverse conditions (e.g. rain and fog). However,\nlow-quality radar points hinder its application, especially the odometry task\nthat requires a dense and accurate matching. To fully explore the potential of\n4D radar, we introduce a learning-based odometry framework, enabling robust\nego-motion estimation from finite and uncertain geometry information. First,\nfor sparse radar points, we propose a local completion to supplement missing\nstructures and provide denser guideline for aligning two frames. Then, a\ncontext-aware association with a hierarchical structure flexibly matches points\nof different scales aided by feature similarity, and improves local matching\nconsistency through correlation balancing. Finally, we present a window-based\noptimizer that uses historical priors to establish a coupling state estimation\nand correct errors of inter-frame matching. The superiority of our algorithm is\nconfirmed on View-of-Delft dataset, achieving around a 50% performance\nimprovement over previous approaches and delivering accuracy on par with LiDAR\nodometry. Our code will be available.\n","authors":["Zhiheng Li","Yubo Cui","Ningyuan Huang","Chenglin Pang","Zheng Fang"],"pdf_url":"https://arxiv.org/pdf/2503.01438v1.pdf","comment":"7 pages, 7 figures"},{"id":"http://arxiv.org/abs/2502.18696v2","updated":"2025-03-03T11:39:04Z","published":"2025-02-25T23:18:20Z","title":"Interpretable Data-Driven Ship Dynamics Model: Enhancing Physics-Based\n  Motion Prediction with Parameter Optimization","summary":"  The deployment of autonomous navigation systems on ships necessitates\naccurate motion prediction models tailored to individual vessels. Traditional\nphysics-based models, while grounded in hydrodynamic principles, often fail to\naccount for ship-specific behaviors under real-world conditions. Conversely,\npurely data-driven models offer specificity but lack interpretability and\nrobustness in edge cases. This study proposes a data-driven physics-based model\nthat integrates physics-based equations with data-driven parameter\noptimization, leveraging the strengths of both approaches to ensure\ninterpretability and adaptability. The model incorporates physics-based\ncomponents such as 3-DoF dynamics, rudder, and propeller forces, while\nparameters such as resistance curve and rudder coefficients are optimized using\nsynthetic data. By embedding domain knowledge into the parameter optimization\nprocess, the fitted model maintains physical consistency. Validation of the\napproach is realized with two container ships by comparing, both qualitatively\nand quantitatively, predictions against ground-truth trajectories. The results\ndemonstrate significant improvements, in predictive accuracy and reliability,\nof the data-driven physics-based models over baseline physics-based models\ntuned with traditional marine engineering practices. The fitted models capture\nship-specific behaviors in diverse conditions with their predictions being,\n51.6% (ship A) and 57.8% (ship B) more accurate, 72.36% (ship A) and 89.67%\n(ship B) more consistent.\n","authors":["Christos Papandreou","Michail Mathioudakis","Theodoros Stouraitis","Petros Iatropoulos","Antonios Nikitakis","Stavros Paschalakis","Konstantinos Kyriakopoulos"],"pdf_url":"https://arxiv.org/pdf/2502.18696v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01434v1","updated":"2025-03-03T11:38:08Z","published":"2025-03-03T11:38:08Z","title":"RUSSO: Robust Underwater SLAM with Sonar Optimization against Visual\n  Degradation","summary":"  Visual degradation in underwater environments poses unique and significant\nchallenges, which distinguishes underwater SLAM from popular vision-based SLAM\non the ground. In this paper, we propose RUSSO, a robust underwater SLAM system\nwhich fuses stereo camera, inertial measurement unit (IMU), and imaging sonar\nto achieve robust and accurate localization in challenging underwater\nenvironments for 6 degrees of freedom (DoF) estimation. During visual\ndegradation, the system is reduced to a sonar-inertial system estimating 3-DoF\nposes. The sonar pose estimation serves as a strong prior for IMU propagation,\nthereby enhancing the reliability of pose estimation with IMU propagation.\nAdditionally, we propose a SLAM initialization method that leverages the\nimaging sonar to counteract the lack of visual features during the\ninitialization stage of SLAM. We extensively validate RUSSO through experiments\nin simulator, pool, and sea scenarios. The results demonstrate that RUSSO\nachieves better robustness and localization accuracy compared to the\nstate-of-the-art visual-inertial SLAM systems, especially in visually\nchallenging scenarios. To the best of our knowledge, this is the first time\nfusing stereo camera, IMU, and imaging sonar to realize robust underwater SLAM\nagainst visual degradation.\n","authors":["Shu Pan","Ziyang Hong","Zhangrui Hu","Xiandong Xu","Wenjie Lu","Liang Hu"],"pdf_url":"https://arxiv.org/pdf/2503.01434v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13452v2","updated":"2025-03-03T11:16:49Z","published":"2025-02-19T05:58:30Z","title":"Ephemerality meets LiDAR-based Lifelong Mapping","summary":"  Lifelong mapping is crucial for the long-term deployment of robots in dynamic\nenvironments. In this paper, we present ELite, an ephemerality-aided\nLiDAR-based lifelong mapping framework which can seamlessly align multiple\nsession data, remove dynamic objects, and update maps in an end-to-end fashion.\nMap elements are typically classified as static or dynamic, but cases like\nparked cars indicate the need for more detailed categories than binary. Central\nto our approach is the probabilistic modeling of the world into two-stage\n$\\textit{ephemerality}$, which represent the transiency of points in the map\nwithin two different time scales. By leveraging the spatiotemporal context\nencoded in ephemeralities, ELite can accurately infer transient map elements,\nmaintain a reliable up-to-date static map, and improve robustness in aligning\nthe new data in a more fine-grained manner. Extensive real-world experiments on\nlong-term datasets demonstrate the robustness and effectiveness of our system.\nThe source code is publicly available for the robotics community:\nhttps://github.com/dongjae0107/ELite.\n","authors":["Hyeonjae Gil","Dongjae Lee","Giseop Kim","Ayoung Kim"],"pdf_url":"https://arxiv.org/pdf/2502.13452v2.pdf","comment":"6+2 pages, 11 figures, accepted at ICRA 2025"},{"id":"http://arxiv.org/abs/2408.07384v2","updated":"2025-03-03T11:14:06Z","published":"2024-08-14T09:00:49Z","title":"Enhanced Optimization Strategies to Design an Underactuated Hand\n  Exoskeleton","summary":"  Exoskeletons can boost human strength and provide assistance to individuals\nwith physical disabilities. However, ensuring safety and optimal performance in\ntheir design poses substantial challenges. This study presents the design\nprocess for an underactuated hand exoskeleton (U-HEx), first including a single\nobjective (maximizing force transmission), then expanding into multi objective\n(also minimizing torque variance and actuator displacement). The optimization\nrelies on a Genetic Algorithm, the Big Bang-Big Crunch Algorithm, and their\nversions for multi-objective optimization. Analyses revealed that using Big\nBang-Big Crunch provides high and more consistent results in terms of\noptimality with lower convergence time. In addition, adding more objectives\noffers a variety of trade-off solutions to the designers, who might later set\npriorities for the objectives without repeating the process - at the cost of\ncomplicating the optimization algorithm and computational burden. These\nfindings underline the importance of performing proper optimization while\ndesigning exoskeletons, as well as providing a significant improvement to this\nspecific robotic design.\n","authors":["Baris Akbas","Huseyin Taner Yuksel","Aleyna Soylemez","Mine Sarac","Fabio Stroppa"],"pdf_url":"https://arxiv.org/pdf/2408.07384v2.pdf","comment":"14 pages, 8 figures, 9 talbes"},{"id":"http://arxiv.org/abs/2405.20579v3","updated":"2025-03-03T10:57:41Z","published":"2024-05-31T02:17:51Z","title":"HOPE: A Reinforcement Learning-based Hybrid Policy Path Planner for\n  Diverse Parking Scenarios","summary":"  Automated parking stands as a highly anticipated application of autonomous\ndriving technology. However, existing path planning methodologies fall short of\naddressing this need due to their incapability to handle the diverse and\ncomplex parking scenarios in reality. While non-learning methods provide\nreliable planning results, they are vulnerable to intricate occasions, whereas\nlearning-based ones are good at exploration but unstable in converging to\nfeasible solutions. To leverage the strengths of both approaches, we introduce\nHybrid pOlicy Path plannEr (HOPE). This novel solution integrates a\nreinforcement learning agent with Reeds-Shepp curves, enabling effective\nplanning across diverse scenarios. HOPE guides the exploration of the\nreinforcement learning agent by applying an action mask mechanism and employs a\ntransformer to integrate the perceived environmental information with the mask.\nTo facilitate the training and evaluation of the proposed planner, we propose a\ncriterion for categorizing the difficulty level of parking scenarios based on\nspace and obstacle distribution. Experimental results demonstrate that our\napproach outperforms typical rule-based algorithms and traditional\nreinforcement learning methods, showing higher planning success rates and\ngeneralization across various scenarios. We also conduct real-world experiments\nto verify the practicability of HOPE. The code for our solution is openly\navailable on https://github.com/jiamiya/HOPE.\n","authors":["Mingyang Jiang","Yueyuan Li","Songan Zhang","Siyuan Chen","Chunxiang Wang","Ming Yang"],"pdf_url":"https://arxiv.org/pdf/2405.20579v3.pdf","comment":"Accepted by T-ITS. 11 pages, 5 tables, 6 figures, 2 page appendix"},{"id":"http://arxiv.org/abs/2503.01378v1","updated":"2025-03-03T10:21:36Z","published":"2025-03-03T10:21:36Z","title":"CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time\n  Cognitive Task Solving and Reasoning in UAVs","summary":"  This paper introduces CognitiveDrone, a novel Vision-Language-Action (VLA)\nmodel tailored for complex Unmanned Aerial Vehicles (UAVs) tasks that demand\nadvanced cognitive abilities. Trained on a dataset comprising over 8,000\nsimulated flight trajectories across three key categories-Human Recognition,\nSymbol Understanding, and Reasoning-the model generates real-time 4D action\ncommands based on first-person visual inputs and textual instructions. To\nfurther enhance performance in intricate scenarios, we propose\nCognitiveDrone-R1, which integrates an additional Vision-Language Model (VLM)\nreasoning module to simplify task directives prior to high-frequency control.\nExperimental evaluations using our open-source benchmark, CognitiveDroneBench,\nreveal that while a racing-oriented model (RaceVLA) achieves an overall success\nrate of 31.3%, the base CognitiveDrone model reaches 59.6%, and\nCognitiveDrone-R1 attains a success rate of 77.2%. These results demonstrate\nimprovements of up to 30% in critical cognitive tasks, underscoring the\neffectiveness of incorporating advanced reasoning capabilities into UAV control\nsystems. Our contributions include the development of a state-of-the-art VLA\nmodel for UAV control and the introduction of the first dedicated benchmark for\nassessing cognitive tasks in drone operations. The complete repository is\navailable at cognitivedrone.github.io\n","authors":["Artem Lykov","Valerii Serpiva","Muhammad Haris Khan","Oleg Sautenkov","Artyom Myshlyaev","Grik Tadevosyan","Yasheerah Yaqoot","Dzmitry Tsetserukou"],"pdf_url":"https://arxiv.org/pdf/2503.01378v1.pdf","comment":"Paper submitted to the IEEE conference"}]},"2025-03-02T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2502.12361v3","updated":"2025-03-02T22:19:39Z","published":"2025-02-17T22:56:42Z","title":"ConFit v2: Improving Resume-Job Matching using Hypothetical Resume\n  Embedding and Runner-Up Hard-Negative Mining","summary":"  A reliable resume-job matching system helps a company recommend suitable\ncandidates from a pool of resumes and helps a job seeker find relevant jobs\nfrom a list of job posts. However, since job seekers apply only to a few jobs,\ninteraction labels in resume-job datasets are sparse. We introduce ConFit v2,\nan improvement over ConFit to tackle this sparsity problem. We propose two\ntechniques to enhance the encoder's contrastive training process: augmenting\njob data with hypothetical reference resume generated by a large language\nmodel; and creating high-quality hard negatives from unlabeled resume/job pairs\nusing a novel hard-negative mining strategy. We evaluate ConFit v2 on two\nreal-world datasets and demonstrate that it outperforms ConFit and prior\nmethods (including BM25 and OpenAI text-embedding-003), achieving an average\nabsolute improvement of 13.8% in recall and 17.5% in nDCG across job-ranking\nand resume-ranking tasks.\n","authors":["Xiao Yu","Ruize Xu","Chengyuan Xue","Jinzhong Zhang","Xu Ma","Zhou Yu"],"pdf_url":"https://arxiv.org/pdf/2502.12361v3.pdf","comment":"arXiv admin note: text overlap with arXiv:2401.16349"},{"id":"http://arxiv.org/abs/2402.10767v2","updated":"2025-03-02T20:33:20Z","published":"2024-02-16T15:41:23Z","title":"Inference to the Best Explanation in Large Language Models","summary":"  While Large Language Models (LLMs) have found success in real-world\napplications, their underlying explanatory process is still poorly understood.\nThis paper proposes IBE-Eval, a framework inspired by philosophical accounts on\nInference to the Best Explanation (IBE) to advance the interpretation and\nevaluation of LLMs' explanations. IBE-Eval estimates the plausibility of\nnatural language explanations through a combination of explicit logical and\nlinguistic features including: consistency, parsimony, coherence, and\nuncertainty. Extensive experiments are conducted on Causal Question Answering\n(CQA), where \\textit{IBE-Eval} is tasked to select the most plausible causal\nexplanation amongst competing ones generated by LLMs (i.e., GPT 3.5 and Llama\n2). The experiments reveal that IBE-Eval can successfully identify the best\nexplanation with up to 77\\% accuracy ($\\approx 27\\%$ above random), improving\nupon a GPT 3.5-as-a-Judge baseline ($\\approx+17\\%$) while being intrinsically\nmore efficient and interpretable. Additional analyses suggest that, despite\nmodel-specific variances, LLM-generated explanations tend to conform to IBE\ncriteria and that IBE-Eval is significantly correlated with human judgment,\nopening up opportunities for future development of automated explanation\nverification tools.\n","authors":["Dhairya Dalal","Marco Valentino","André Freitas","Paul Buitelaar"],"pdf_url":"https://arxiv.org/pdf/2402.10767v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.04343v2","updated":"2025-03-02T19:44:37Z","published":"2024-10-06T03:42:15Z","title":"Inference Scaling for Long-Context Retrieval Augmented Generation","summary":"  The scaling of inference computation has unlocked the potential of\nlong-context large language models (LLMs) across diverse settings. For\nknowledge-intensive tasks, the increased compute is often allocated to\nincorporate more external knowledge. However, without effectively utilizing\nsuch knowledge, solely expanding context does not always enhance performance.\nIn this work, we investigate inference scaling for retrieval augmented\ngeneration (RAG), exploring the combination of multiple strategies beyond\nsimply increasing the quantity of knowledge, including in-context learning and\niterative prompting. These strategies provide additional flexibility to scale\ntest-time computation (e.g., by increasing retrieved documents or generation\nsteps), thereby enhancing LLMs' ability to effectively acquire and utilize\ncontextual information. We address two key questions: (1) How does RAG\nperformance benefit from the scaling of inference computation when optimally\nconfigured? (2) Can we predict the optimal test-time compute allocation for a\ngiven budget by modeling the relationship between RAG performance and inference\nparameters? Our observations reveal that increasing inference computation leads\nto nearly linear gains in RAG performance when optimally allocated, a\nrelationship we describe as the inference scaling laws for RAG. Building on\nthis, we further develop the computation allocation model to estimate RAG\nperformance across different inference configurations. The model predicts\noptimal inference parameters under various computation constraints, which align\nclosely with the experimental results. By applying these optimal\nconfigurations, we demonstrate that scaling inference compute on long-context\nLLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.\n","authors":["Zhenrui Yue","Honglei Zhuang","Aijun Bai","Kai Hui","Rolf Jagerman","Hansi Zeng","Zhen Qin","Dong Wang","Xuanhui Wang","Michael Bendersky"],"pdf_url":"https://arxiv.org/pdf/2410.04343v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2405.14105v4","updated":"2025-03-02T18:24:29Z","published":"2024-05-23T02:14:17Z","title":"Distributed Speculative Inference (DSI): Speculation Parallelism for\n  Provably Faster Lossless Language Model Inference","summary":"  This paper introduces distributed speculative inference (DSI), a novel\ninference algorithm that is provably faster than speculative inference (SI)\n[leviathan2023, chen2023, miao2024, sun2025, timor2025] and standard\nautoregressive inference (non-SI). Like other SI algorithms, DSI operates on\nfrozen language models (LMs), requiring no training or architectural\nmodifications, and it preserves the target distribution. Prior studies on SI\nhave demonstrated empirical speedups over non-SI--but rely on sufficiently fast\nand accurate drafters, which are often unavailable in practice. We identify a\ngap where SI can be slower than non-SI if drafters are too slow or inaccurate.\nWe close this gap by proving that DSI is faster than both SI and non-SI--given\nany drafters. DSI is therefore not only faster than SI, but also unlocks the\nacceleration of LMs for which SI fails. DSI leverages speculation parallelism\n(SP), a novel type of task parallelism, to orchestrate target and drafter\ninstances that overlap in time, establishing a new foundational tradeoff\nbetween computational resources and latency. Our simulations show that DSI is\n1.29-1.92x faster than SI in single-node setups for various off-the-shelf LMs\nand tasks. We open-source all our code.\n","authors":["Nadav Timor","Jonathan Mamou","Daniel Korat","Moshe Berchansky","Oren Pereg","Moshe Wasserblat","Tomer Galanti","Michal Gordon","David Harel"],"pdf_url":"https://arxiv.org/pdf/2405.14105v4.pdf","comment":"Published at ICLR 2025. (Link:\n  https://openreview.net/forum?id=cJd1BgZ9CS)"},{"id":"http://arxiv.org/abs/2403.08743v2","updated":"2025-03-02T17:33:03Z","published":"2024-03-13T17:46:28Z","title":"Prompting Fairness: Integrating Causality to Debias Large Language\n  Models","summary":"  Large language models (LLMs), despite their remarkable capabilities, are\nsusceptible to generating biased and discriminatory responses. As LLMs\nincreasingly influence high-stakes decision-making (e.g., hiring and\nhealthcare), mitigating these biases becomes critical. In this work, we propose\na causality-guided debiasing framework to tackle social biases, aiming to\nreduce the objectionable dependence between LLMs' decisions and the social\ninformation in the input. Our framework introduces a novel perspective to\nidentify how social information can affect an LLM's decision through different\ncausal pathways. Leveraging these causal insights, we outline principled\nprompting strategies that regulate these pathways through selection mechanisms.\nThis framework not only unifies existing prompting-based debiasing techniques,\nbut also opens up new directions for reducing bias by encouraging the model to\nprioritize fact-based reasoning over reliance on biased social cues. We\nvalidate our framework through extensive experiments on real-world datasets\nacross multiple domains, demonstrating its effectiveness in debiasing LLM\ndecisions, even with only black-box access to the model.\n","authors":["Jingling Li","Zeyu Tang","Xiaoyu Liu","Peter Spirtes","Kun Zhang","Liu Leqi","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.08743v2.pdf","comment":"24 pages, 10 figures"},{"id":"http://arxiv.org/abs/2502.11882v3","updated":"2025-03-02T17:15:11Z","published":"2025-02-17T15:09:45Z","title":"Leveraging Dual Process Theory in Language Agent Framework for Real-time\n  Simultaneous Human-AI Collaboration","summary":"  Agents built on large language models (LLMs) have excelled in turn-by-turn\nhuman-AI collaboration but struggle with simultaneous tasks requiring real-time\ninteraction. Latency issues and the challenge of inferring variable human\nstrategies hinder their ability to make autonomous decisions without explicit\ninstructions. Through experiments with current independent System 1 and System\n2 methods, we validate the necessity of using Dual Process Theory (DPT) in\nreal-time tasks. We propose DPT-Agent, a novel language agent framework that\nintegrates System 1 and System 2 for efficient real-time simultaneous human-AI\ncollaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and\ncode-as-policy for fast, intuitive, and controllable decision-making.\nDPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous\nreflection to infer human intentions and perform reasoning-based autonomous\ndecisions. We demonstrate the effectiveness of DPT-Agent through further\nexperiments with rule-based agents and human collaborators, showing significant\nimprovements over mainstream LLM-based frameworks. DPT-Agent can effectively\nhelp LLMs convert correct slow thinking and reasoning into executable actions,\nthereby improving performance. To the best of our knowledge, DPT-Agent is the\nfirst language agent framework that achieves successful real-time simultaneous\nhuman-AI collaboration autonomously. Code of DPT-Agent can be found in\nhttps://github.com/sjtu-marl/DPT-Agent.\n","authors":["Shao Zhang","Xihuai Wang","Wenhao Zhang","Chaoran Li","Junru Song","Tingyu Li","Lin Qiu","Xuezhi Cao","Xunliang Cai","Wen Yao","Weinan Zhang","Xinbing Wang","Ying Wen"],"pdf_url":"https://arxiv.org/pdf/2502.11882v3.pdf","comment":"Preprint under review. Update the experimental results of the\n  DeepSeek-R1 series models, o3-mini-high and o3-mini-medium"},{"id":"http://arxiv.org/abs/2502.18036v2","updated":"2025-03-02T16:56:04Z","published":"2025-02-25T09:48:53Z","title":"Harnessing Multiple Large Language Models: A Survey on LLM Ensemble","summary":"  LLM Ensemble -- which involves the comprehensive use of multiple large\nlanguage models (LLMs), each aimed at handling user queries during downstream\ninference, to benefit from their individual strengths -- has gained substantial\nattention recently. The widespread availability of LLMs, coupled with their\nvarying strengths and out-of-the-box usability, has profoundly advanced the\nfield of LLM Ensemble. This paper presents the first systematic review of\nrecent developments in LLM Ensemble. First, we introduce our taxonomy of LLM\nEnsemble and discuss several related research problems. Then, we provide a more\nin-depth classification of the methods under the broad categories of\n\"ensemble-before-inference, ensemble-during-inference,\nensemble-after-inference'', and review all relevant methods. Finally, we\nintroduce related benchmarks and applications, summarize existing studies, and\nsuggest several future research directions. A curated list of papers on LLM\nEnsemble is available at https://github.com/junchenzhi/Awesome-LLM-Ensemble.\n","authors":["Zhijun Chen","Jingzheng Li","Pengpeng Chen","Zhuoran Li","Kai Sun","Yuankai Luo","Qianren Mao","Dingqi Yang","Hailong Sun","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2502.18036v2.pdf","comment":"9 pages, 2 figures, codebase:\n  https://github.com/junchenzhi/Awesome-LLM-Ensemble"},{"id":"http://arxiv.org/abs/2502.06563v2","updated":"2025-03-02T16:38:28Z","published":"2025-02-10T15:31:54Z","title":"Large Language Models Meet Symbolic Provers for Logical Reasoning\n  Evaluation","summary":"  First-order logic (FOL) reasoning, which involves sequential deduction, is\npivotal for intelligent systems and serves as a valuable task for evaluating\nreasoning capabilities, particularly in chain-of-thought (CoT) contexts.\nExisting benchmarks often rely on extensive human annotation or handcrafted\ntemplates, making it difficult to achieve the necessary complexity,\nscalability, and diversity for robust evaluation. To address these limitations,\nwe propose a novel framework called ProverGen that synergizes the generative\nstrengths of Large Language Models (LLMs) with the rigor and precision of\nsymbolic provers, enabling the creation of a scalable, diverse, and\nhigh-quality FOL reasoning dataset, ProverQA. ProverQA is also distinguished by\nits inclusion of accessible and logically coherent intermediate reasoning steps\nfor each problem. Our evaluation shows that state-of-the-art LLMs struggle to\nsolve ProverQA problems, even with CoT prompting, highlighting the dataset's\nchallenging nature. We also finetune Llama3.1-8B-Instruct on a separate\ntraining set generated by our framework. The finetuned model demonstrates\nconsistent improvements on both in-distribution and out-of-distribution test\nsets, suggesting the value of our proposed data generation framework. Code\navailable at: https://github.com/opendatalab/ProverGen\n","authors":["Chengwen Qi","Ren Ma","Bowen Li","He Du","Binyuan Hui","Jinwang Wu","Yuanjun Laili","Conghui He"],"pdf_url":"https://arxiv.org/pdf/2502.06563v2.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2410.00812v2","updated":"2025-03-02T16:32:24Z","published":"2024-10-01T15:57:48Z","title":"Generative causal testing to bridge data-driven models and scientific\n  theories in language neuroscience","summary":"  Representations from large language models are highly effective at predicting\nBOLD fMRI responses to language stimuli. However, these representations are\nlargely opaque: it is unclear what features of the language stimulus drive the\nresponse in each brain area. We present generative causal testing (GCT), a\nframework for generating concise explanations of language selectivity in the\nbrain from predictive models and then testing those explanations in follow-up\nexperiments using LLM-generated stimuli.This approach is successful at\nexplaining selectivity both in individual voxels and cortical regions of\ninterest (ROIs), including newly identified microROIs in prefrontal cortex. We\nshow that explanatory accuracy is closely related to the predictive power and\nstability of the underlying predictive models. Finally, we show that GCT can\ndissect fine-grained differences between brain areas with similar functional\nselectivity. These results demonstrate that LLMs can be used to bridge the\nwidening gap between data-driven models and formal scientific theories.\n","authors":["Richard Antonello","Chandan Singh","Shailee Jain","Aliyah Hsu","Sihang Guo","Jianfeng Gao","Bin Yu","Alexander Huth"],"pdf_url":"https://arxiv.org/pdf/2410.00812v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19412v2","updated":"2025-03-02T16:16:39Z","published":"2025-02-26T18:56:38Z","title":"The Mighty ToRR: A Benchmark for Table Reasoning and Robustness","summary":"  Despite its real-world significance, model performance on tabular data\nremains underexplored, leaving uncertainty about which model to rely on and\nwhich prompt configuration to adopt. To address this gap, we create ToRR, a\nbenchmark for Table Reasoning and Robustness, measuring model performance and\nrobustness on table-related tasks. The benchmark includes 10 datasets that\ncover different types of table reasoning capabilities across varied domains.\nToRR goes beyond model performance rankings, and is designed to reflect whether\nmodels can handle tabular data consistently and robustly, across a variety of\ncommon table representation formats. We present a leaderboard as well as\ncomprehensive analyses of the results of leading models over ToRR. Our results\nreveal a striking pattern of brittle model behavior, where even strong models\nare unable to perform robustly on tabular data tasks. Although no specific\ntable format leads to consistently better performance, we show that testing\nover multiple formats is crucial for reliably estimating model capabilities.\nMoreover, we show that the reliability boost from testing multiple prompts can\nbe equivalent to adding more test examples. Overall, our findings show that\ntable understanding and reasoning tasks remain a significant challenge.\n","authors":["Shir Ashury-Tahan","Yifan Mai","Rajmohan C","Ariel Gera","Yotam Perlitz","Asaf Yehudai","Elron Bandel","Leshem Choshen","Eyal Shnarch","Percy Liang","Michal Shmueli-Scheuer"],"pdf_url":"https://arxiv.org/pdf/2502.19412v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.03895v2","updated":"2025-03-02T15:55:07Z","published":"2025-01-07T16:03:14Z","title":"LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One\n  Vision Token","summary":"  The advent of real-time large multimodal models (LMMs) like GPT-4o has\nsparked considerable interest in efficient LMMs. LMM frameworks typically\nencode visual inputs into vision tokens (continuous representations) and\nintegrate them and textual instructions into the context of large language\nmodels (LLMs), where large-scale parameters and numerous context tokens\n(predominantly vision tokens) result in substantial computational overhead.\nPrevious efforts towards efficient LMMs always focus on replacing the LLM\nbackbone with smaller models, while neglecting the crucial issue of token\nquantity. In this paper, we introduce LLaVA-Mini, an efficient LMM with minimal\nvision tokens. To achieve a high compression ratio of vision tokens while\npreserving visual information, we first analyze how LMMs understand vision\ntokens and find that most vision tokens only play a crucial role in the early\nlayers of LLM backbone, where they mainly fuse visual information into text\ntokens. Building on this finding, LLaVA-Mini introduces modality pre-fusion to\nfuse visual information into text tokens in advance, thereby facilitating the\nextreme compression of vision tokens fed to LLM backbone into one token.\nLLaVA-Mini is a unified large multimodal model that can support the\nunderstanding of images, high-resolution images, and videos in an efficient\nmanner. Experiments across 11 image-based and 7 video-based benchmarks\ndemonstrate that LLaVA-Mini outperforms LLaVA-v1.5 with just 1 vision token\ninstead of 576. Efficiency analyses reveal that LLaVA-Mini can reduce FLOPs by\n77%, deliver low-latency responses within 40 milliseconds, and process over\n10,000 frames of video on the GPU hardware with 24GB of memory.\n","authors":["Shaolei Zhang","Qingkai Fang","Zhe Yang","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2501.03895v2.pdf","comment":"Accepted to ICLR 2025. Code: https://github.com/ictnlp/LLaVA-Mini\n  Model: https://huggingface.co/ICTNLP/llava-mini-llama-3.1-8b"},{"id":"http://arxiv.org/abs/2410.03524v2","updated":"2025-03-02T15:54:11Z","published":"2024-10-04T15:44:47Z","title":"Steering Large Language Models between Code Execution and Textual\n  Reasoning","summary":"  While a lot of recent research focuses on enhancing the textual reasoning\ncapabilities of Large Language Models (LLMs) by optimizing the multi-agent\nframework or reasoning chains, several benchmark tasks can be solved with 100\\%\nsuccess through direct coding, which is more scalable and avoids the\ncomputational overhead associated with textual iterating and searching. Textual\nreasoning has inherent limitations in solving tasks with challenges in math,\nlogics, optimization, and searching, which is unlikely to be solved by simply\nscaling up the model and data size. The recently released OpenAI GPT Code\nInterpreter and multi-agent frameworks such as AutoGen have demonstrated\nremarkable proficiency of integrating code generation and execution to solve\ncomplex tasks using LLMs. However, based on our experiments on 7 existing\npopular methods for steering code/text generation in both single- and\nmulti-turn settings with 14 tasks and 6 types of LLMs (including the new\nO1-preview), currently there is no optimal method to correctly steer LLMs to\nwrite code when needed. We discover some interesting patterns on when models\nuse code vs. textual reasoning with the evolution to task complexity and model\nsizes, which even result in an astonishingly inverse scaling behavior. We also\ndiscover that results from LLM written code are not always better than using\ntextual reasoning, even if the task could be solved through code. To mitigate\nthe above issues, we propose three methods to better steer LLM code/text\ngeneration and achieve a notable improvement. The costs of token lengths and\nruntime are thoroughly discussed for all the methods. We believe the problem of\nsteering LLM code/text generation is critical for future research and has much\nspace for further improvement. Project Page, Datasets, and Codes are available\nat https://yongchao98.github.io/CodeSteer/.\n","authors":["Yongchao Chen","Harsh Jhamtani","Srinagesh Sharma","Chuchu Fan","Chi Wang"],"pdf_url":"https://arxiv.org/pdf/2410.03524v2.pdf","comment":"32 pages, 12 figures, 12 tables"},{"id":"http://arxiv.org/abs/2410.10781v2","updated":"2025-03-02T14:37:53Z","published":"2024-10-14T17:50:28Z","title":"When Attention Sink Emerges in Language Models: An Empirical View","summary":"  Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink.\n","authors":["Xiangming Gu","Tianyu Pang","Chao Du","Qian Liu","Fengzhuo Zhang","Cunxiao Du","Ye Wang","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2410.10781v2.pdf","comment":"ICLR 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2409.13555v2","updated":"2025-03-02T14:36:29Z","published":"2024-09-20T14:56:33Z","title":"Generating Visual Stories with Grounded and Coreferent Characters","summary":"  Characters are important in narratives. They move the plot forward, create\nemotional connections, and embody the story's themes. Visual storytelling\nmethods focus more on the plot and events relating to it, without building the\nnarrative around specific characters. As a result, the generated stories feel\ngeneric, with character mentions being absent, vague, or incorrect. To mitigate\nthese issues, we introduce the new task of character-centric story generation\nand present the first model capable of predicting visual stories with\nconsistently grounded and coreferent character mentions. Our model is finetuned\non a new dataset which we build on top of the widely used VIST benchmark.\nSpecifically, we develop an automated pipeline to enrich VIST with visual and\ntextual character coreference chains. We also propose new evaluation metrics to\nmeasure the richness of characters and coreference in stories. Experimental\nresults show that our model generates stories with recurring characters which\nare consistent and coreferent to larger extent compared to baselines and\nstate-of-the-art systems.\n","authors":["Danyang Liu","Mirella Lapata","Frank Keller"],"pdf_url":"https://arxiv.org/pdf/2409.13555v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07137v2","updated":"2025-03-02T14:28:33Z","published":"2024-10-09T17:53:06Z","title":"Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates","summary":"  Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and\nMT-Bench, have become popular for evaluating language models due to their\ncost-effectiveness and scalability compared to human evaluation. Achieving high\nwin rates on these benchmarks can significantly boost the promotional impact of\nnewly released language models. This promotional benefit may motivate tricks,\nsuch as manipulating model output length or style to game win rates, even\nthough several mechanisms have been developed to control length and disentangle\nstyle to reduce gameability. Nonetheless, we show that even a \"null model\" that\nalways outputs a constant response (irrelevant to input instructions) can cheat\nautomatic benchmarks and achieve top-ranked win rates: an 86.5% LC win rate on\nAlpacaEval 2.0; an 83.0 score on Arena-Hard-Auto; and a 9.55 score on MT-Bench.\nMoreover, the crafted cheating outputs are transferable because we assume that\nthe instructions of these benchmarks (e.g., 805 samples of AlpacaEval 2.0) are\nprivate and cannot be accessed. While our experiments are primarily\nproof-of-concept, an adversary could use LLMs to generate more imperceptible\ncheating responses, unethically benefiting from high win rates and promotional\nimpact. Our findings call for the development of anti-cheating mechanisms for\nreliable automatic benchmarks. The code is available at\nhttps://github.com/sail-sg/Cheating-LLM-Benchmarks.\n","authors":["Xiaosen Zheng","Tianyu Pang","Chao Du","Qian Liu","Jing Jiang","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2410.07137v2.pdf","comment":"ICLR 2025 (Oral)"},{"id":"http://arxiv.org/abs/2411.03962v4","updated":"2025-03-02T13:52:25Z","published":"2024-11-06T14:51:02Z","title":"How Does A Text Preprocessing Pipeline Affect Ontology Syntactic\n  Matching?","summary":"  The generic text preprocessing pipeline, comprising Tokenisation,\nNormalisation, Stop Words Removal, and Stemming/Lemmatisation, has been\nimplemented in many systems for syntactic ontology matching (OM). However, the\nlack of standardisation in text preprocessing creates diversity in mapping\nresults. In this paper, we investigate the effect of the text preprocessing\npipeline on syntactic OM in 8 Ontology Alignment Evaluation Initiative (OAEI)\ntracks with 49 distinct alignments. We find that Phase 1 text preprocessing\n(Tokenisation and Normalisation) is currently more effective than Phase 2 text\npreprocessing (Stop Words Removal and Stemming/Lemmatisation). To repair the\nless effective Phase 2 text preprocessing caused by unwanted false mappings, we\npropose a novel context-based pipeline repair approach that employs an ad hoc\ncheck to find common words that cause false mappings. These words are stored in\na reserved word set and applied in text preprocessing. The experimental results\nshow that our approach improves the matching correctness and the overall\nmatching performance. We also discuss the integration of the classical text\npreprocessing pipeline with modern large language models (LLMs). We recommend\nthat LLMs inject the text preprocessing pipeline via function calling to avoid\nthe tendency towards unstable true mappings produced by prompt-based LLM\napproaches, and use LLMs to repair false mappings generated by the text\npreprocessing pipeline.\n","authors":["Zhangcheng Qiang","Kerry Taylor","Weiqing Wang"],"pdf_url":"https://arxiv.org/pdf/2411.03962v4.pdf","comment":"13 pages, 11 figures, 4 tables"},{"id":"http://arxiv.org/abs/2406.15812v2","updated":"2025-03-02T12:28:24Z","published":"2024-06-22T10:36:04Z","title":"Intrinsic Dimension Correlation: uncovering nonlinear connections in\n  multimodal representations","summary":"  To gain insight into the mechanisms behind machine learning methods, it is\ncrucial to establish connections among the features describing data points.\nHowever, these correlations often exhibit a high-dimensional and strongly\nnonlinear nature, which makes them challenging to detect using standard\nmethods. This paper exploits the entanglement between intrinsic dimensionality\nand correlation to propose a metric that quantifies the (potentially nonlinear)\ncorrelation between high-dimensional manifolds. We first validate our method on\nsynthetic data in controlled environments, showcasing its advantages and\ndrawbacks compared to existing techniques. Subsequently, we extend our analysis\nto large-scale applications in neural network representations. Specifically, we\nfocus on latent representations of multimodal data, uncovering clear\ncorrelations between paired visual and textual embeddings, whereas existing\nmethods struggle significantly in detecting similarity. Our results indicate\nthe presence of highly nonlinear correlation patterns between latent manifolds.\n","authors":["Lorenzo Basile","Santiago Acevedo","Luca Bortolussi","Fabio Anselmi","Alex Rodriguez"],"pdf_url":"https://arxiv.org/pdf/2406.15812v2.pdf","comment":"Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2405.01229v2","updated":"2025-03-02T12:27:07Z","published":"2024-05-02T12:18:14Z","title":"Boosting Jailbreak Attack with Momentum","summary":"  Large Language Models (LLMs) have achieved remarkable success across diverse\ntasks, yet they remain vulnerable to adversarial attacks, notably the\nwell-known jailbreak attack. In particular, the Greedy Coordinate Gradient\n(GCG) attack has demonstrated efficacy in exploiting this vulnerability by\noptimizing adversarial prompts through a combination of gradient heuristics and\ngreedy search. However, the efficiency of this attack has become a bottleneck\nin the attacking process. To mitigate this limitation, in this paper we rethink\nthe generation of the adversarial prompts through an optimization lens, aiming\nto stabilize the optimization process and harness more heuristic insights from\nprevious optimization iterations. Specifically, we propose the\n\\textbf{M}omentum \\textbf{A}ccelerated G\\textbf{C}G (\\textbf{MAC}) attack,\nwhich integrates a momentum term into the gradient heuristic to boost and\nstabilize the random search for tokens in adversarial prompts. Experimental\nresults showcase the notable enhancement achieved by MAC over baselines in\nterms of attack success rate and optimization efficiency. Moreover, we\ndemonstrate that MAC can still exhibit superior performance for transfer\nattacks and models under defense mechanisms. Our code is available at\nhttps://github.com/weizeming/momentum-attack-llm.\n","authors":["Yihao Zhang","Zeming Wei"],"pdf_url":"https://arxiv.org/pdf/2405.01229v2.pdf","comment":"Accepted by ICASSP 2025"},{"id":"http://arxiv.org/abs/2310.17953v4","updated":"2025-03-02T12:17:06Z","published":"2023-10-27T08:01:55Z","title":"Developing a Multilingual Dataset and Evaluation Metrics for\n  Code-Switching: A Focus on Hong Kong's Polylingual Dynamics","summary":"  The existing audio datasets are predominantly tailored towards single\nlanguages, overlooking the complex linguistic behaviors of multilingual\ncommunities that engage in code-switching. This practice, where individuals\nfrequently mix two or more languages in their daily interactions, is\nparticularly prevalent in multilingual regions such as Hong Kong, China. To\nbridge this gap, we have developed a 34.8-hour dataset of Mixed Cantonese and\nEnglish (MCE) audio using our Multi-Agent Data Generation Framework (MADGF). We\nfine-tuned the open-source multilingual Automatic Speech Recognition (ASR)\nmodel, Whisper, with the MCE dataset, leading to impressive zero-shot\nperformance. The traditional metrics overlook important factors such as latency\nin real-world applications and code-switching scenarios. We have introduced a\nnovel evaluation metric called Fidelity to the Original Audio, Accuracy, and\nLatency (FAL). This metric aims to overcome the limitations of traditional\nmetrics used to assess ASR systems.\n","authors":["Peng Xie","Kani Chen"],"pdf_url":"https://arxiv.org/pdf/2310.17953v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.13940v3","updated":"2025-03-02T12:11:13Z","published":"2024-08-25T21:20:17Z","title":"Derailer-Rerailer: Adaptive Verification for Efficient and Reliable\n  Language Model Reasoning","summary":"  Large Language Models (LLMs) have shown impressive reasoning capabilities,\nyet existing prompting methods face a critical trade-off: simple approaches\noften struggle with complex tasks and reasoning stability, while more\nsophisticated methods require multiple inferences and substantial computational\nresources, limiting their practical deployment. To address this challenge, we\npropose Derailer-Rerailer, a novel framework that adaptively balances reasoning\naccuracy and computational efficiency. At its core, our framework employs a\nlightweight Derailer mechanism to assess reasoning stability and selectively\ntriggers an advanced Rerailer verification process only when necessary, thereby\noptimizing computational resource usage. Extensive evaluation across both open\nand closed-source models on more than 20 categories of mathematical, symbolic,\nand commonsense reasoning tasks demonstrates our framework's effectiveness:\nDerailer-Rerailer achieves significant accuracy improvements (8-11\\% across\nvarious reasoning tasks) while maintaining 2-3 times better efficiency than\nexisting verification methods, with particularly strong performance in\nmathematical and symbolic reasoning, offering a practical solution for\nenhancing LLM reasoning reliability while significantly reducing computational\noverhead.\n","authors":["Guangya Wan","Yuqi Wu","Hao Wang","Shengming Zhao","Jie Chen","Sheng Li"],"pdf_url":"https://arxiv.org/pdf/2408.13940v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19649v2","updated":"2025-03-02T11:23:58Z","published":"2025-02-27T00:40:01Z","title":"Taxonomy, Opportunities, and Challenges of Representation Engineering\n  for Large Language Models","summary":"  Representation Engineering (RepE) is a novel paradigm for controlling the\nbehavior of LLMs. Unlike traditional approaches that modify inputs or fine-tune\nthe model, RepE directly manipulates the model's internal representations. As a\nresult, it may offer more effective, interpretable, data-efficient, and\nflexible control over models' behavior. We present the first comprehensive\nsurvey of RepE for LLMs, reviewing the rapidly growing literature to address\nkey questions: What RepE methods exist and how do they differ? For what\nconcepts and problems has RepE been applied? What are the strengths and\nweaknesses of RepE compared to other methods? To answer these, we propose a\nunified framework describing RepE as a pipeline comprising representation\nidentification, operationalization, and control. We posit that while RepE\nmethods offer significant potential, challenges remain, including managing\nmultiple concepts, ensuring reliability, and preserving models' performance.\nTowards improving RepE, we identify opportunities for experimental and\nmethodological improvements and construct a guide for best practices.\n","authors":["Jan Wehner","Sahar Abdelnabi","Daniel Tan","David Krueger","Mario Fritz"],"pdf_url":"https://arxiv.org/pdf/2502.19649v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.14666v2","updated":"2025-03-02T10:38:32Z","published":"2024-10-18T17:56:11Z","title":"DiscoGraMS: Enhancing Movie Screen-Play Summarization using Movie\n  Character-Aware Discourse Graph","summary":"  Summarizing movie screenplays presents a unique set of challenges compared to\nstandard document summarization. Screenplays are not only lengthy, but also\nfeature a complex interplay of characters, dialogues, and scenes, with numerous\ndirect and subtle relationships and contextual nuances that are difficult for\nmachine learning models to accurately capture and comprehend. Recent attempts\nat screenplay summarization focus on fine-tuning transformer-based pre-trained\nmodels, but these models often fall short in capturing long-term dependencies\nand latent relationships, and frequently encounter the \"lost in the middle\"\nissue. To address these challenges, we introduce DiscoGraMS, a novel resource\nthat represents movie scripts as a movie character-aware discourse graph (CaD\nGraph). This approach is well-suited for various downstream tasks, such as\nsummarization, question-answering, and salience detection. The model aims to\npreserve all salient information, offering a more comprehensive and faithful\nrepresentation of the screenplay's content. We further explore a baseline\nmethod that combines the CaD Graph with the corresponding movie script through\na late fusion of graph and text modalities, and we present very initial\npromising results.\n","authors":["Maitreya Prafulla Chitale","Uday Bindal","Rajakrishnan Rajkumar","Rahul Mishra"],"pdf_url":"https://arxiv.org/pdf/2410.14666v2.pdf","comment":"Accepted at NAACL 2025 (Main)"},{"id":"http://arxiv.org/abs/2502.14897v2","updated":"2025-03-02T10:18:09Z","published":"2025-02-17T21:35:18Z","title":"Market-Derived Financial Sentiment Analysis: Context-Aware Language\n  Models for Crypto Forecasting","summary":"  Financial Sentiment Analysis (FSA) traditionally relies on human-annotated\nsentiment labels to infer investor sentiment and forecast market movements.\nHowever, inferring the potential market impact of words based on their\nhuman-perceived intentions is inherently challenging. We hypothesize that the\nhistorical market reactions to words, offer a more reliable indicator of their\npotential impact on markets than subjective sentiment interpretations by human\nannotators. To test this hypothesis, a market-derived labeling approach is\nproposed to assign tweet labels based on ensuing short-term price trends,\nenabling the language model to capture the relationship between textual signals\nand market dynamics directly. A domain-specific language model was fine-tuned\non these labels, achieving up to an 11% improvement in short-term trend\nprediction accuracy over traditional sentiment-based benchmarks. Moreover, by\nincorporating market and temporal context through prompt-tuning, the proposed\ncontext-aware language model demonstrated an accuracy of 89.6% on a curated\ndataset of 227 impactful Bitcoin-related news events with significant market\nimpacts. Aggregating daily tweet predictions into trading signals, our method\noutperformed traditional fusion models (which combine sentiment-based and\nprice-based predictions). It challenged the assumption that sentiment-based\nsignals are inferior to price-based predictions in forecasting market\nmovements. Backtesting these signals across three distinct market regimes\nyielded robust Sharpe ratios of up to 5.07 in trending markets and 3.73 in\nneutral markets. Our findings demonstrate that language models can serve as\neffective short-term market predictors. This paradigm shift underscores the\nuntapped capabilities of language models in financial decision-making and opens\nnew avenues for market prediction applications.\n","authors":["Hamid Moradi-Kamali","Mohammad-Hossein Rajabi-Ghozlou","Mahdi Ghazavi","Ali Soltani","Amirreza Sattarzadeh","Reza Entezari-Maleki"],"pdf_url":"https://arxiv.org/pdf/2502.14897v2.pdf","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2408.10557v2","updated":"2025-03-02T09:59:36Z","published":"2024-08-20T05:45:04Z","title":"Speech Representation Learning Revisited: The Necessity of Separate\n  Learnable Parameters and Robust Data Augmentation","summary":"  Speech modeling methods learn one embedding for a fixed segment of speech,\ntypically in between 10-25 ms. The information present in speech can be divided\ninto two categories: \"what is being said\" (content) and \"how it is expressed\"\n(other) and these two are orthogonal in nature causing the optimization\nalgorithm to find a sub-optimal solution if forced to optimize together. This\nleads to sub-optimal performance in one or all downstream tasks as shown by\nprevious studies. Current self-supervised learning (SSL) methods such as HuBERT\nare very good at modeling the content information present in speech. Data\naugmentation improves the performance on tasks which require effective modeling\nof other information but this leads to a divided capacity of the model. In this\nwork, we conduct a preliminary study to understand the importance of modeling\nother information using separate learnable parameters. We propose a modified\nversion of HuBERT, termed Other HuBERT (O-HuBERT), to test our hypothesis. Our\nfindings are twofold: first, the O-HuBERT method is able to utilize all layers\nto build complex features to encode other information; second, a robust data\naugmentation strategy is essential for learning the information required by\ntasks that depend on other information and to achieve state-of-the-art (SOTA)\nperformance on the SUPERB benchmark with a similarly sized model (100 million\nparameters) and pre-training data (960 hours).\n","authors":["Hemant Yadav","Sunayana Sitaram","Rajiv Ratn Shah"],"pdf_url":"https://arxiv.org/pdf/2408.10557v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04236v3","updated":"2025-03-02T09:39:57Z","published":"2024-02-06T18:43:48Z","title":"CogCoM: A Visual Language Model with Chain-of-Manipulations Reasoning","summary":"  Vision-Language Models (VLMs) have demonstrated their broad effectiveness\nthanks to extensive training in aligning visual instructions to responses.\nHowever, such training of conclusive alignment leads models to ignore essential\nvisual reasoning, further resulting in failures in meticulous visual problems\nand unfaithful responses. Drawing inspiration from human cognition in solving\nvisual problems (e.g., marking, zoom in), this paper introduces Chain of\nManipulations, a mechanism that enables VLMs to solve problems step-by-step\nwith evidence. After training, models can solve various visual problems by\neliciting intrinsic manipulations (e.g., grounding, zoom in) with results\n(e.g., boxes, image) actively without involving external tools, while also\nallowing users to trace error causes. We study the roadmap to implement this\nmechanism, including (1) a flexible design of manipulations upon extensive\nanalysis, (2) an efficient automated data generation pipeline, (3) a compatible\nVLM architecture capable of multi-turn multi-image, and (4) a model training\nprocess for versatile capabilities. With the design, we also manually annotate\n6K high-quality samples for the challenging graphical mathematical problems.\nOur trained model, \\textbf{CogCoM}, equipped with this mechanism with 17B\nparameters achieves state-of-the-art performance across 9 benchmarks from 4\ncategories, demonstrating the effectiveness while preserving the\ninterpretability. Our code, model weights, and collected data are publicly\navailable at https://github.com/THUDM/CogCoM.\n","authors":["Ji Qi","Ming Ding","Weihan Wang","Yushi Bai","Qingsong Lv","Wenyi Hong","Bin Xu","Lei Hou","Juanzi Li","Yuxiao Dong","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2402.04236v3.pdf","comment":"21 pages, 10 figures"},{"id":"http://arxiv.org/abs/2412.02956v2","updated":"2025-03-02T09:35:28Z","published":"2024-12-04T02:05:21Z","title":"Curriculum-style Data Augmentation for LLM-based Metaphor Detection","summary":"  Recently, utilizing large language models (LLMs) for metaphor detection has\nachieved promising results. However, these methods heavily rely on the\ncapabilities of closed-source LLMs, which come with relatively high inference\ncosts and latency. To address this, we propose a method for metaphor detection\nby fine-tuning open-source LLMs, effectively reducing inference costs and\nlatency with a single inference step. Furthermore, metaphor detection suffers\nfrom a severe data scarcity problem, which hinders effective fine-tuning of\nLLMs. To tackle this, we introduce Curriculum-style Data Augmentation (CDA).\nSpecifically, before fine-tuning, we evaluate the training data to identify\ncorrectly predicted instances for fine-tuning, while incorrectly predicted\ninstances are used as seed data for data augmentation. This approach enables\nthe model to quickly learn simpler knowledge and progressively acquire more\ncomplex knowledge, thereby improving performance incrementally. Experimental\nresults demonstrate that our method achieves state-of-the-art performance\nacross all baselines. Additionally, we provide detailed ablation studies to\nvalidate the effectiveness of CDA.\n","authors":["Kaidi Jia","Yanxia Wu","Ming Liu","Rongsheng Li"],"pdf_url":"https://arxiv.org/pdf/2412.02956v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.23771v2","updated":"2025-03-02T09:23:18Z","published":"2024-10-31T09:39:28Z","title":"What is Wrong with Perplexity for Long-context Language Modeling?","summary":"  Handling long-context inputs is crucial for large language models (LLMs) in\ntasks such as extended conversations, document summarization, and many-shot\nin-context learning. While recent approaches have extended the context windows\nof LLMs and employed perplexity (PPL) as a standard evaluation metric, PPL has\nproven unreliable for assessing long-context capabilities. The underlying cause\nof this limitation has remained unclear. In this work, we provide a\ncomprehensive explanation for this issue. We find that PPL overlooks key\ntokens, which are essential for long-context understanding, by averaging across\nall tokens and thereby obscuring the true performance of models in long-context\nscenarios. To address this, we propose \\textbf{LongPPL}, a novel metric that\nfocuses on key tokens by employing a long-short context contrastive method to\nidentify them. Our experiments demonstrate that LongPPL strongly correlates\nwith performance on various long-context benchmarks (e.g., Pearson correlation\nof -0.96), significantly outperforming traditional PPL in predictive accuracy.\nAdditionally, we introduce \\textbf{LongCE} (Long-context Cross-Entropy) loss, a\nre-weighting strategy for fine-tuning that prioritizes key tokens, leading to\nconsistent improvements across diverse benchmarks. In summary, these\ncontributions offer deeper insights into the limitations of PPL and present\neffective solutions for accurately evaluating and enhancing the long-context\ncapabilities of LLMs. Code is available at https://github.com/PKU-ML/LongPPL.\n","authors":["Lizhe Fang","Yifei Wang","Zhaoyang Liu","Chenheng Zhang","Stefanie Jegelka","Jinyang Gao","Bolin Ding","Yisen Wang"],"pdf_url":"https://arxiv.org/pdf/2410.23771v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.07168v2","updated":"2025-03-02T09:16:05Z","published":"2024-10-09T17:59:04Z","title":"Sylber: Syllabic Embedding Representation of Speech from Raw Audio","summary":"  Syllables are compositional units of spoken language that efficiently\nstructure human speech perception and production. However, current neural\nspeech representations lack such structure, resulting in dense token sequences\nthat are costly to process. To bridge this gap, we propose a new model, Sylber,\nthat produces speech representations with clean and robust syllabic structure.\nSpecifically, we propose a self-supervised learning (SSL) framework that\nbootstraps syllabic embeddings by distilling from its own initial unsupervised\nsyllabic segmentation. This results in a highly structured representation of\nspeech features, offering three key benefits: 1) a fast, linear-time syllable\nsegmentation algorithm, 2) efficient syllabic tokenization with an average of\n4.27 tokens per second, and 3) novel phonological units suited for efficient\nspoken language modeling. Our proposed segmentation method is highly robust and\ngeneralizes to out-of-domain data and unseen languages without any tuning. By\ntraining token-to-speech generative models, fully intelligible speech can be\nreconstructed from Sylber tokens with a significantly lower bitrate than\nbaseline SSL tokens. This suggests that our model effectively compresses speech\ninto a compact sequence of tokens with minimal information loss. Lastly, we\ndemonstrate that categorical perception-a linguistic phenomenon in speech\nperception-emerges naturally in Sylber, making the embedding space more\ncategorical and sparse than previous speech features and thus supporting the\nhigh efficiency of our tokenization. Together, we present a novel SSL approach\nfor representing speech as syllables, with significant potential for efficient\nspeech tokenization and spoken language modeling.\n","authors":["Cheol Jun Cho","Nicholas Lee","Akshat Gupta","Dhruv Agarwal","Ethan Chen","Alan W Black","Gopala K. Anumanchipalli"],"pdf_url":"https://arxiv.org/pdf/2410.07168v2.pdf","comment":"Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2409.01281v2","updated":"2025-03-02T09:13:56Z","published":"2024-08-25T01:45:53Z","title":"Path-Consistency: Prefix Enhancement for Efficient Inference in LLM","summary":"  To enhance the reasoning capabilities of large language models (LLMs),\nself-consistency has gained significant popularity by combining multiple\nsampling with majority voting. However, the state-of-the-art self-consistency\napproaches consume substantial computational resources and lead to significant\nadditional time costs due to the multiple sampling. This prevents its full\npotential from being realized in scenarios where computational resources are\ncritical. To improve the inference efficiency, this paper introduces\n\\textit{path-consistency}, a method that leverages the confidence of answers\ngenerated in earlier branches to identify the prefix of the most promising\npath. By dynamically guiding the generation of subsequent branches based on\nthis prefix, the \\textit{path-consistency} mitigates both the errors and\nredundancies from random or less useful sampling in self-consistency. As a\nresult, it can significantly accelerate the inference process by reducing the\nnumber of tokens generated. Our extensive empirical evaluation shows that the\n\\textit{path-consistency} achieves significant acceleration in inference\nlatency ranging from $7.8\\%$ to $40.5\\%$, while maintaining or even improving\ntask accuracy across different datasets, including mathematical reasoning,\ncommon sense reasoning, symbolic reasoning, and code generation.\n","authors":["Jiace Zhu","Yingtao Shen","Jie Zhao","An Zou"],"pdf_url":"https://arxiv.org/pdf/2409.01281v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06751v2","updated":"2025-03-02T09:10:13Z","published":"2025-01-12T08:36:38Z","title":"Padding Tone: A Mechanistic Analysis of Padding Tokens in T2I Models","summary":"  Text-to-image (T2I) diffusion models rely on encoded prompts to guide the\nimage generation process. Typically, these prompts are extended to a fixed\nlength by adding padding tokens before text encoding. Despite being a default\npractice, the influence of padding tokens on the image generation process has\nnot been investigated. In this work, we conduct the first in-depth analysis of\nthe role padding tokens play in T2I models. We develop two causal techniques to\nanalyze how information is encoded in the representation of tokens across\ndifferent components of the T2I pipeline. Using these techniques, we\ninvestigate when and how padding tokens impact the image generation process.\nOur findings reveal three distinct scenarios: padding tokens may affect the\nmodel's output during text encoding, during the diffusion process, or be\neffectively ignored. Moreover, we identify key relationships between these\nscenarios and the model's architecture (cross or self-attention) and its\ntraining process (frozen or trained text encoder). These insights contribute to\na deeper understanding of the mechanisms of padding tokens, potentially\ninforming future model design and training practices in T2I systems.\n","authors":["Michael Toker","Ido Galil","Hadas Orgad","Rinon Gal","Yoad Tewel","Gal Chechik","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2501.06751v2.pdf","comment":"Published in: NAACL 2025. Project webpage:\n  https://padding-tone.github.io/"},{"id":"http://arxiv.org/abs/2408.02976v3","updated":"2025-03-02T08:30:58Z","published":"2024-08-06T06:16:00Z","title":"Empathy Level Alignment via Reinforcement Learning for Empathetic\n  Response Generation","summary":"  Empathetic response generation, aiming to understand the user's situation and\nfeelings and respond empathically, is crucial in building human-like dialogue\nsystems. Traditional approaches typically employ maximum likelihood estimation\nas the optimization objective during training, yet fail to align the empathy\nlevels between generated and target responses. To this end, we propose an\nempathetic response generation framework using reinforcement learning (EmpRL).\nThe framework develops an effective empathy reward function and generates\nempathetic responses by maximizing the expected reward through reinforcement\nlearning. EmpRL utilizes the pre-trained T5 model as the generator and further\nfine-tunes it to initialize the policy. To align the empathy levels between\ngenerated and target responses within a given context, an empathy reward\nfunction containing three empathy communication mechanisms -- emotional\nreaction, interpretation, and exploration -- is constructed using pre-designed\nand pre-trained empathy identifiers. During reinforcement learning training,\nthe proximal policy optimization algorithm is used to fine-tune the policy,\nenabling the generation of empathetic responses. Both automatic and human\nevaluations demonstrate that the proposed EmpRL framework significantly\nimproves the quality of generated responses, enhances the similarity in empathy\nlevels between generated and target responses, and produces empathetic\nresponses covering both affective and cognitive aspects.\n","authors":["Hui Ma","Bo Zhang","Bo Xu","Jian Wang","Hongfei Lin","Xiao Sun"],"pdf_url":"https://arxiv.org/pdf/2408.02976v3.pdf","comment":"Accepted by IEEE Transactions on Affective Computing"},{"id":"http://arxiv.org/abs/2407.00886v3","updated":"2025-03-02T08:26:23Z","published":"2024-07-01T01:12:20Z","title":"Efficient Automated Circuit Discovery in Transformers using Contextual\n  Decomposition","summary":"  Automated mechanistic interpretation research has attracted great interest\ndue to its potential to scale explanations of neural network internals to large\nmodels. Existing automated circuit discovery work relies on activation patching\nor its approximations to identify subgraphs in models for specific tasks\n(circuits). They often suffer from slow runtime, approximation errors, and\nspecific requirements of metrics, such as non-zero gradients. In this work, we\nintroduce contextual decomposition for transformers (CD-T) to build\ninterpretable circuits in large language models. CD-T can produce circuits of\narbitrary level of abstraction, and is the first able to produce circuits as\nfine-grained as attention heads at specific sequence positions efficiently.\nCD-T consists of a set of mathematical equations to isolate contribution of\nmodel features. Through recursively computing contribution of all nodes in a\ncomputational graph of a model using CD-T followed by pruning, we are able to\nreduce circuit discovery runtime from hours to seconds compared to\nstate-of-the-art baselines. On three standard circuit evaluation datasets\n(indirect object identification, greater-than comparisons, and docstring\ncompletion), we demonstrate that CD-T outperforms ACDC and EAP by better\nrecovering the manual circuits with an average of 97% ROC AUC under low\nruntimes. In addition, we provide evidence that faithfulness of CD-T circuits\nis not due to random chance by showing our circuits are 80% more faithful than\nrandom circuits of up to 60% of the original model size. Finally, we show CD-T\ncircuits are able to perfectly replicate original models' behavior\n(faithfulness $ = 1$) using fewer nodes than the baselines for all tasks. Our\nresults underscore the great promise of CD-T for efficient automated\nmechanistic interpretability, paving the way for new insights into the workings\nof large language models.\n","authors":["Aliyah R. Hsu","Georgia Zhou","Yeshwanth Cherapanamjeri","Yaxuan Huang","Anobel Y. Odisho","Peter R. Carroll","Bin Yu"],"pdf_url":"https://arxiv.org/pdf/2407.00886v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09097v2","updated":"2025-03-02T07:58:08Z","published":"2025-02-13T09:13:23Z","title":"A Hybrid Transformer Model for Fake News Detection: Leveraging Bayesian\n  Optimization and Bidirectional Recurrent Unit","summary":"  In this paper, we propose an optimized Transformer model that integrates\nBayesian algorithms with a Bidirectional Gated Recurrent Unit (BiGRU), and\napply it to fake news classification for the first time. First, we employ the\nTF-IDF method to extract features from news texts and transform them into\nnumeric representations to facilitate subsequent machine learning tasks. Two\nsets of experiments are then conducted for fake news detection and\nclassification: one using a Transformer model optimized only with BiGRU, and\nthe other incorporating Bayesian algorithms into the BiGRU-based Transformer.\nExperimental results show that the BiGRU-optimized Transformer achieves 100%\naccuracy on the training set and 99.67% on the test set, while the addition of\nthe Bayesian algorithm maintains 100% accuracy on the training set and slightly\nimproves test-set accuracy to 99.73%. This indicates that the Bayesian\nalgorithm boosts model accuracy by 0.06%, further enhancing the detection\ncapability for fake news. Moreover, the proposed algorithm converges rapidly at\naround the 10th training epoch with accuracy nearing 100%, demonstrating both\nits effectiveness and its fast classification ability. Overall, the optimized\nTransformer model, enhanced by the Bayesian algorithm and BiGRU, exhibits\nexcellent continuous learning and detection performance, offering a robust\ntechnical means to combat the spread of fake news in the current era of\ninformation overload.\n","authors":["Tianyi Huang","Zeqiu Xu","Peiyang Yu","Jingyuan Yi","Xiaochuan Xu"],"pdf_url":"https://arxiv.org/pdf/2502.09097v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.13757v2","updated":"2025-03-02T07:34:35Z","published":"2024-10-17T16:53:50Z","title":"MobA: Multifaceted Memory-Enhanced Adaptive Planning for Efficient\n  Mobile Task Automation","summary":"  Existing Multimodal Large Language Model (MLLM)-based agents face significant\nchallenges in handling complex GUI (Graphical User Interface) interactions on\ndevices. These challenges arise from the dynamic and structured nature of GUI\nenvironments, which integrate text, images, and spatial relationships, as well\nas the variability in action spaces across different pages and tasks. To\naddress these limitations, we propose MobA, a novel MLLM-based mobile assistant\nsystem. MobA introduces an adaptive planning module that incorporates a\nreflection mechanism for error recovery and dynamically adjusts plans to align\nwith the real environment contexts and action module's execution capacity.\nAdditionally, a multifaceted memory module provides comprehensive memory\nsupport to enhance adaptability and efficiency. We also present MobBench, a\ndataset designed for complex mobile interactions. Experimental results on\nMobBench and AndroidArena demonstrate MobA's ability to handle dynamic GUI\nenvironments and perform complex mobile task.\n","authors":["Zichen Zhu","Hao Tang","Yansi Li","Dingye Liu","Hongshen Xu","Kunyao Lan","Danyang Zhang","Yixuan Jiang","Hao Zhou","Chenrun Wang","Situo Zhang","Liangtai Sun","Yixiao Wang","Yuheng Sun","Lu Chen","Kai Yu"],"pdf_url":"https://arxiv.org/pdf/2410.13757v2.pdf","comment":"NAACL 2025 Demo Track"},{"id":"http://arxiv.org/abs/2501.14294v3","updated":"2025-03-02T06:49:21Z","published":"2025-01-24T07:24:23Z","title":"Examining Alignment of Large Language Models through Representative\n  Heuristics: The Case of Political Stereotypes","summary":"  Examining the alignment of large language models (LLMs) has become\nincreasingly important, e.g., when LLMs fail to operate as intended. This study\nexamines the alignment of LLMs with human values for the domain of politics.\nPrior research has shown that LLM-generated outputs can include political\nleanings and mimic the stances of political parties on various issues. However,\nthe extent and conditions under which LLMs deviate from empirical positions are\ninsufficiently examined. To address this gap, we analyze the factors that\ncontribute to LLMs' deviations from empirical positions on political issues,\naiming to quantify these deviations and identify the conditions that cause\nthem.\n  Drawing on findings from cognitive science about representativeness\nheuristics, i.e., situations where humans lean on representative attributes of\na target group in a way that leads to exaggerated beliefs, we scrutinize LLM\nresponses through this heuristics' lens. We conduct experiments to determine\nhow LLMs inflate predictions about political parties, which results in\nstereotyping. We find that while LLMs can mimic certain political parties'\npositions, they often exaggerate these positions more than human survey\nrespondents do. Also, LLMs tend to overemphasize representativeness more than\nhumans. This study highlights the susceptibility of LLMs to representativeness\nheuristics, suggesting a potential vulnerability of LLMs that facilitates\npolitical stereotyping. We also test prompt-based mitigation strategies,\nfinding that strategies that can mitigate representative heuristics in humans\nare also effective in reducing the influence of representativeness on\nLLM-generated responses.\n","authors":["Sullam Jeoung","Yubin Ge","Haohan Wang","Jana Diesner"],"pdf_url":"https://arxiv.org/pdf/2501.14294v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.17924v2","updated":"2025-03-02T06:46:48Z","published":"2025-02-25T07:44:22Z","title":"FACT-AUDIT: An Adaptive Multi-Agent Framework for Dynamic Fact-Checking\n  Evaluation of Large Language Models","summary":"  Large Language Models (LLMs) have significantly advanced the fact-checking\nstudies. However, existing automated fact-checking evaluation methods rely on\nstatic datasets and classification metrics, which fail to automatically\nevaluate the justification production and uncover the nuanced limitations of\nLLMs in fact-checking. In this work, we introduce FACT-AUDIT, an agent-driven\nframework that adaptively and dynamically assesses LLMs' fact-checking\ncapabilities. Leveraging importance sampling principles and multi-agent\ncollaboration, FACT-AUDIT generates adaptive and scalable datasets, performs\niterative model-centric evaluations, and updates assessments based on\nmodel-specific responses. By incorporating justification production alongside\nverdict prediction, this framework provides a comprehensive and evolving audit\nof LLMs' factual reasoning capabilities, to investigate their trustworthiness.\nExtensive experiments demonstrate that FACT-AUDIT effectively differentiates\namong state-of-the-art LLMs, providing valuable insights into model strengths\nand limitations in model-centric fact-checking analysis.\n","authors":["Hongzhan Lin","Yang Deng","Yuxuan Gu","Wenxuan Zhang","Jing Ma","See-Kiong Ng","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2502.17924v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.19425v3","updated":"2025-03-02T06:36:57Z","published":"2024-05-29T18:08:37Z","title":"Adaptive In-conversation Team Building for Language Model Agents","summary":"  Leveraging multiple large language model (LLM) agents has shown to be a\npromising approach for tackling complex tasks, while the effective design of\nmultiple agents for a particular application remains an art. It is thus\nintriguing to answer a critical question: Given a task, how can we build a team\nof LLM agents to solve it effectively? Our new adaptive team-building paradigm\noffers a flexible solution, realized through a novel agent design named Captain\nAgent. It dynamically forms and manages teams for each step of a task-solving\nprocess, utilizing nested group conversations and reflection to ensure diverse\nexpertise and prevent stereotypical outputs, allowing for a flexible yet\nstructured approach to problem-solving. A comprehensive evaluation across six\nreal-world scenarios demonstrates that Captain Agent significantly outperforms\nexisting multi-agent methods with 21.94% improvement in average accuracy,\nproviding outstanding performance without requiring task-specific prompt\nengineering. Our exploration of different backbone LLM and cost analysis\nfurther shows that Captain Agent can improve the conversation quality of weak\nLLM and achieve competitive performance with extremely low cost, which\nilluminates the application of multi-agent systems.\n","authors":["Linxin Song","Jiale Liu","Jieyu Zhang","Shaokun Zhang","Ao Luo","Shijian Wang","Qingyun Wu","Chi Wang"],"pdf_url":"https://arxiv.org/pdf/2405.19425v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01902v2","updated":"2025-03-02T06:28:59Z","published":"2024-07-02T02:58:29Z","title":"SeqAR: Jailbreak LLMs with Sequential Auto-Generated Characters","summary":"  The widespread applications of large language models (LLMs) have brought\nabout concerns regarding their potential misuse. Although aligned with human\npreference data before release, LLMs remain vulnerable to various malicious\nattacks. In this paper, we adopt a red-teaming strategy to enhance LLM safety\nand introduce SeqAR, a simple yet effective framework to design jailbreak\nprompts automatically. The SeqAR framework generates and optimizes multiple\njailbreak characters and then applies sequential jailbreak characters in a\nsingle query to bypass the guardrails of the target LLM. Different from\nprevious work which relies on proprietary LLMs or seed jailbreak templates\ncrafted by human expertise, SeqAR can generate and optimize the jailbreak\nprompt in a cold-start scenario using open-sourced LLMs without any seed\njailbreak templates. Experimental results show that SeqAR achieves attack\nsuccess rates of 88% and 60% in bypassing the safety alignment of GPT-3.5-1106\nand GPT-4, respectively. Furthermore, we extensively evaluate the\ntransferability of the generated templates across different LLMs and held-out\nmalicious requests, while also exploring defense strategies against the\njailbreak attack designed by SeqAR.\n","authors":["Yan Yang","Zeguan Xiao","Xin Lu","Hongru Wang","Xuetao Wei","Hailiang Huang","Guanhua Chen","Yun Chen"],"pdf_url":"https://arxiv.org/pdf/2407.01902v2.pdf","comment":"Accepted by NAACL 2025"},{"id":"http://arxiv.org/abs/2410.07672v2","updated":"2025-03-02T06:25:14Z","published":"2024-10-10T07:29:35Z","title":"MACPO: Weak-to-Strong Alignment via Multi-Agent Contrastive Preference\n  Optimization","summary":"  As large language models (LLMs) are rapidly advancing and achieving\nnear-human capabilities on specific tasks, aligning them with human values is\nbecoming more urgent. In scenarios where LLMs outperform humans, we face a\nweak-to-strong alignment problem where we need to effectively align strong\nstudent LLMs through weak supervision generated by weak teachers. Existing\nalignment methods mainly focus on strong-to-weak alignment and self-alignment\nsettings, and it is impractical to adapt them to the much harder weak-to-strong\nalignment setting. To fill this gap, we propose a multi-agent contrastive\npreference optimization (MACPO) framework. MACPO facilitates weak teachers and\nstrong students to learn from each other by iteratively reinforcing unfamiliar\npositive behaviors while penalizing familiar negative ones. To get this, we\ndevise a mutual positive behavior augmentation strategy to encourage weak\nteachers and strong students to learn from each other's positive behavior and\nfurther provide higher quality positive behavior for the next iteration.\nAdditionally, we propose a hard negative behavior construction strategy to\ninduce weak teachers and strong students to generate familiar negative behavior\nby fine-tuning on negative behavioral data. Experimental results on the HH-RLHF\nand PKU-SafeRLHF datasets, evaluated using both automatic metrics and human\njudgments, demonstrate that MACPO simultaneously improves the alignment\nperformance of strong students and weak teachers. Moreover, as the number of\nweak teachers increases, MACPO achieves better weak-to-strong alignment\nperformance through more iteration optimization rounds.\n","authors":["Yougang Lyu","Lingyong Yan","Zihan Wang","Dawei Yin","Pengjie Ren","Maarten de Rijke","Zhaochun Ren"],"pdf_url":"https://arxiv.org/pdf/2410.07672v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2410.03115v2","updated":"2025-03-02T05:16:38Z","published":"2024-10-04T03:17:27Z","title":"X-ALMA: Plug & Play Modules and Adaptive Rejection for Quality\n  Translation at Scale","summary":"  Large language models (LLMs) have achieved remarkable success across various\nNLP tasks with a focus on English due to English-centric pre-training and\nlimited multilingual data. In this work, we focus on the problem of\ntranslation, and while some multilingual LLMs claim to support for hundreds of\nlanguages, models often fail to provide high-quality responses for mid- and\nlow-resource languages, leading to imbalanced performance heavily skewed in\nfavor of high-resource languages. We introduce **X-ALMA**, a model designed to\nensure top-tier performance across 50 diverse languages, regardless of their\nresource levels. X-ALMA surpasses state-of-the-art open-source multilingual\nLLMs, such as Aya-101 and Aya-23, in every single translation direction on the\nFLORES-200 and WMT'23 test datasets according to COMET-22. This is achieved by\nplug-and-play language-specific module architecture to prevent language\nconflicts during training and a carefully designed training regimen with novel\noptimization methods to maximize the translation performance. After the final\nstage of training regimen, our proposed **A**daptive **R**ejection\n**P**reference **O**ptimization (**ARPO**) surpasses existing preference\noptimization methods in translation tasks.\n","authors":["Haoran Xu","Kenton Murray","Philipp Koehn","Hieu Hoang","Akiko Eriguchi","Huda Khayrallah"],"pdf_url":"https://arxiv.org/pdf/2410.03115v2.pdf","comment":"Published as a conference paper at ICLR 2025 (spotlight)"},{"id":"http://arxiv.org/abs/2406.09044v3","updated":"2025-03-02T04:45:56Z","published":"2024-06-13T12:30:02Z","title":"MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient LLM\n  Finetuning","summary":"  Efficient finetuning of large language models (LLMs) aims to adapt the LLMs\nwith reduced computational and memory cost. Previous LoRA-based approaches\ninitialize the low-rank matrices with Gaussian distribution and zero values\nwhile keeping the original weight matrices frozen. However, the trainable model\nparameters optimized in an unguided subspace might interfere with the\nwell-learned subspace of the pretrained weight matrices. In this paper, we\npropose MiLoRA, a simple yet effective LLM finetuning approach that only\nupdates the minor singular components of the weight matrix while keeping the\nprincipal singular components frozen. It is observed that the minor matrix\ncorresponds to the noisy or long-tail information, while the principal matrix\ncontains important knowledge. The MiLoRA initializes the low-rank matrices\nwithin a subspace that is orthogonal to the principal matrix, thus the\npretrained knowledge is expected to be well preserved. During finetuning,\nMiLoRA makes the most use of the less-optimized subspace for learning the\nlabeled dataset. Extensive experiments on commonsense reasoning, math\nreasoning, instruction following and visual instruction following benchmarks\npresent the superior performance of our method.\n","authors":["Hanqing Wang","Yixia Li","Shuo Wang","Guanhua Chen","Yun Chen"],"pdf_url":"https://arxiv.org/pdf/2406.09044v3.pdf","comment":"This paper has been accepted at NAACL 2025. Code is available at:\n  https://github.com/sufenlp/MiLoRA"},{"id":"http://arxiv.org/abs/2410.21533v2","updated":"2025-03-02T04:39:42Z","published":"2024-10-28T21:02:13Z","title":"L3Ms -- Lagrange Large Language Models","summary":"  Supervised fine-tuning (SFT) and alignment of large language models (LLMs)\nare key steps in providing a good user experience. However, the concept of an\nappropriate alignment is inherently application-dependent, and current methods\noften rely on heuristic choices to drive optimization. In this work, we\nformulate SFT and alignment as a constrained optimization problem: the LLM is\nfine-tuned on a task while being required to meet application-specific\nrequirements, without resorting to heuristics. To solve this, we propose\nLagrange Large Language Models (L3Ms), which employ logarithmic barriers to\nenforce the constraints. This approach allows for the customization of L3Ms\nacross diverse applications while avoiding heuristic-driven processes. We\nexperimentally demonstrate the versatility and efficacy of L3Ms in achieving\ntailored alignments for various applications.\n","authors":["Guneet S. Dhillon","Xingjian Shi","Yee Whye Teh","Alex Smola"],"pdf_url":"https://arxiv.org/pdf/2410.21533v2.pdf","comment":"International Conference on Learning Representations (ICLR), 2025"},{"id":"http://arxiv.org/abs/2502.10709v2","updated":"2025-03-02T04:37:08Z","published":"2025-02-15T07:45:20Z","title":"An Empirical Analysis of Uncertainty in Large Language Model Evaluations","summary":"  As LLM-as-a-Judge emerges as a new paradigm for assessing large language\nmodels (LLMs), concerns have been raised regarding the alignment, bias, and\nstability of LLM evaluators. While substantial work has focused on alignment\nand bias, little research has concentrated on the stability of LLM evaluators.\nIn this paper, we conduct extensive experiments involving 9 widely used LLM\nevaluators across 2 different evaluation settings to investigate the\nuncertainty in model-based LLM evaluations. We pinpoint that LLM evaluators\nexhibit varying uncertainty based on model families and sizes. With careful\ncomparative analyses, we find that employing special prompting strategies,\nwhether during inference or post-training, can alleviate evaluation uncertainty\nto some extent. By utilizing uncertainty to enhance LLM's reliability and\ndetection capability in Out-Of-Distribution (OOD) data, we further fine-tune an\nuncertainty-aware LLM evaluator named ConfiLM using a human-annotated\nfine-tuning set and assess ConfiLM's OOD evaluation ability on a manually\ndesigned test set sourced from the 2024 Olympics. Experimental results\ndemonstrate that incorporating uncertainty as additional information during the\nfine-tuning phase can largely improve the model's evaluation performance in OOD\nscenarios. The code and data are released at:\nhttps://github.com/hasakiXie123/LLM-Evaluator-Uncertainty.\n","authors":["Qiujie Xie","Qingqiu Li","Zhuohao Yu","Yuejie Zhang","Yue Zhang","Linyi Yang"],"pdf_url":"https://arxiv.org/pdf/2502.10709v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2308.11432v7","updated":"2025-03-02T04:04:03Z","published":"2023-08-22T13:30:37Z","title":"A Survey on Large Language Model based Autonomous Agents","summary":"  Autonomous agents have long been a prominent research focus in both academic\nand industry communities. Previous research in this field often focuses on\ntraining agents with limited knowledge within isolated environments, which\ndiverges significantly from human learning processes, and thus makes the agents\nhard to achieve human-like decisions. Recently, through the acquisition of vast\namounts of web knowledge, large language models (LLMs) have demonstrated\nremarkable potential in achieving human-level intelligence. This has sparked an\nupsurge in studies investigating LLM-based autonomous agents. In this paper, we\npresent a comprehensive survey of these studies, delivering a systematic review\nof the field of LLM-based autonomous agents from a holistic perspective. More\nspecifically, we first discuss the construction of LLM-based autonomous agents,\nfor which we propose a unified framework that encompasses a majority of the\nprevious work. Then, we present a comprehensive overview of the diverse\napplications of LLM-based autonomous agents in the fields of social science,\nnatural science, and engineering. Finally, we delve into the evaluation\nstrategies commonly used for LLM-based autonomous agents. Based on the previous\nstudies, we also present several challenges and future directions in this\nfield. To keep track of this field and continuously update our survey, we\nmaintain a repository of relevant references at\nhttps://github.com/Paitesanshi/LLM-Agent-Survey.\n","authors":["Lei Wang","Chen Ma","Xueyang Feng","Zeyu Zhang","Hao Yang","Jingsen Zhang","Zhiyuan Chen","Jiakai Tang","Xu Chen","Yankai Lin","Wayne Xin Zhao","Zhewei Wei","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2308.11432v7.pdf","comment":"Correcting several typos, 35 pages, 5 figures, 3 tables"},{"id":"http://arxiv.org/abs/2407.14985v5","updated":"2025-03-02T03:27:58Z","published":"2024-07-20T21:24:40Z","title":"Generalization v.s. Memorization: Tracing Language Models' Capabilities\n  Back to Pretraining Data","summary":"  The impressive capabilities of large language models (LLMs) have sparked\ndebate over whether these models genuinely generalize to unseen tasks or\npredominantly rely on memorizing vast amounts of pretraining data. To explore\nthis issue, we introduce an extended concept of memorization, distributional\nmemorization, which measures the correlation between the LLM output\nprobabilities and the pretraining data frequency. To effectively capture\ntask-specific pretraining data frequency, we propose a novel task-gram language\nmodel, which is built by counting the co-occurrence of semantically related\n$n$-gram pairs from task inputs and outputs in the pretraining corpus. Using\nthe Pythia models trained on the Pile dataset, we evaluate four distinct tasks:\nmachine translation, factual question answering, world knowledge understanding,\nand math reasoning. Our findings reveal varying levels of memorization, with\nthe strongest effect observed in factual question answering. Furthermore, while\nmodel performance improves across all tasks as LLM size increases, only factual\nquestion answering shows an increase in memorization, whereas machine\ntranslation and reasoning tasks exhibit greater generalization, producing more\nnovel outputs. This study demonstrates that memorization plays a larger role in\nsimpler, knowledge-intensive tasks, while generalization is the key for harder,\nreasoning-based tasks, providing a scalable method for analyzing large\npretraining corpora in greater depth.\n","authors":["Xinyi Wang","Antonis Antoniades","Yanai Elazar","Alfonso Amayuelas","Alon Albalak","Kexun Zhang","William Yang Wang"],"pdf_url":"https://arxiv.org/pdf/2407.14985v5.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2401.06603v2","updated":"2025-03-02T01:46:57Z","published":"2024-01-12T14:35:57Z","title":"Mutual Enhancement of Large Language and Reinforcement Learning Models\n  through Bi-Directional Feedback Mechanisms: A Planning Case Study","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities for\nreinforcement learning (RL) models, such as planning and reasoning\ncapabilities. However, the problems of LLMs and RL model collaboration still\nneed to be solved. In this study, we employ a teacher-student learning\nframework to tackle these problems, specifically by offering feedback for LLMs\nusing RL models and providing high-level information for RL models with LLMs in\na cooperative multi-agent setting. Within this framework, the LLM acts as a\nteacher, while the RL model acts as a student. The two agents cooperatively\nassist each other through a process of recursive help, such as \"I help you help\nI help.\" The LLM agent supplies abstract information to the RL agent, enabling\nefficient exploration and policy improvement. In turn, the RL agent offers\nfeedback to the LLM agent, providing valuable, real-time information that helps\ngenerate more useful tokens. This bi-directional feedback loop promotes\noptimization, exploration, and mutual improvement for both agents, enabling\nthem to accomplish increasingly challenging tasks. Remarkably, we propose a\npractical algorithm to address the problem and conduct empirical experiments to\nevaluate the effectiveness of our method.\n","authors":["Shangding Gu"],"pdf_url":"https://arxiv.org/pdf/2401.06603v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10594v2","updated":"2025-03-02T01:19:51Z","published":"2024-10-14T15:04:18Z","title":"VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality\n  Documents","summary":"  Retrieval-augmented generation (RAG) is an effective technique that enables\nlarge language models (LLMs) to utilize external knowledge sources for\ngeneration. However, current RAG systems are solely based on text, rendering it\nimpossible to utilize vision information like layout and images that play\ncrucial roles in real-world multi-modality documents. In this paper, we\nintroduce VisRAG, which tackles this issue by establishing a vision-language\nmodel (VLM)-based RAG pipeline. In this pipeline, instead of first parsing the\ndocument to obtain text, the document is directly embedded using a VLM as an\nimage and then retrieved to enhance the generation of a VLM. Compared to\ntraditional text-based RAG, VisRAG maximizes the retention and utilization of\nthe data information in the original documents, eliminating the information\nloss introduced during the parsing process. We collect both open-source and\nsynthetic data to train the retriever in VisRAG and explore a variety of\ngeneration methods. Experiments demonstrate that VisRAG outperforms traditional\nRAG in both the retrieval and generation stages, achieving a 20--40% end-to-end\nperformance gain over traditional text-based RAG pipeline. Further analysis\nreveals that VisRAG is efficient in utilizing training data and demonstrates\nstrong generalization capability, positioning it as a promising solution for\nRAG on multi-modality documents. Our code and data are available at\nhttps://github.com/openbmb/visrag.\n","authors":["Shi Yu","Chaoyue Tang","Bokai Xu","Junbo Cui","Junhao Ran","Yukun Yan","Zhenghao Liu","Shuo Wang","Xu Han","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2410.10594v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.13629v3","updated":"2025-03-02T00:46:31Z","published":"2024-06-19T15:25:29Z","title":"InstructRAG: Instructing Retrieval-Augmented Generation via\n  Self-Synthesized Rationales","summary":"  Retrieval-augmented generation (RAG) has shown promising potential to enhance\nthe accuracy and factuality of language models (LMs). However, imperfect\nretrievers or noisy corpora can introduce misleading or even erroneous\ninformation to the retrieved contents, posing a significant challenge to the\ngeneration quality. Existing RAG methods typically address this challenge by\ndirectly predicting final answers despite potentially noisy inputs, resulting\nin an implicit denoising process that is difficult to interpret and verify. On\nthe other hand, the acquisition of explicit denoising supervision is often\ncostly, involving significant human efforts. In this work, we propose\nInstructRAG, where LMs explicitly learn the denoising process through\nself-synthesized rationales -- First, we instruct the LM to explain how the\nground-truth answer is derived from retrieved documents. Then, these rationales\ncan be used either as demonstrations for in-context learning of explicit\ndenoising or as supervised fine-tuning data to train the model. Compared to\nstandard RAG approaches, InstructRAG requires no additional supervision, allows\nfor easier verification of the predicted answers, and effectively improves\ngeneration accuracy. Experiments show InstructRAG consistently outperforms\nexisting RAG methods in both training-free and trainable scenarios, achieving a\nrelative improvement of 8.3% over the best baseline method on average across\nfive knowledge-intensive benchmarks. Extensive analysis indicates that\nInstructRAG scales well with increased numbers of retrieved documents and\nconsistently exhibits robust denoising ability even in out-of-domain datasets,\ndemonstrating strong generalizability.\n","authors":["Zhepei Wei","Wei-Lin Chen","Yu Meng"],"pdf_url":"https://arxiv.org/pdf/2406.13629v3.pdf","comment":"ICLR 2025. Code: https://github.com/weizhepei/InstructRAG"}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2310.07887v4","updated":"2025-03-02T23:48:32Z","published":"2023-10-11T20:48:20Z","title":"Unsupervised Denoising for Signal-Dependent and Row-Correlated Imaging\n  Noise","summary":"  Accurate analysis of microscopy images is hindered by the presence of noise.\nThis noise is usually signal-dependent and often additionally correlated along\nrows or columns of pixels. Current self- and unsupervised denoisers can address\nsignal-dependent noise, but none can reliably remove noise that is also row- or\ncolumn-correlated. Here, we present the first fully unsupervised deep\nlearning-based denoiser capable of handling imaging noise that is\nrow-correlated as well as signal-dependent. Our approach uses a Variational\nAutoencoder (VAE) with a specially designed autoregressive decoder. This\ndecoder is capable of modeling row-correlated and signal-dependent noise but is\nincapable of independently modeling underlying clean signal. The VAE therefore\nproduces latent variables containing only clean signal information, and these\nare mapped back into image space using a proposed second decoder network. Our\nmethod does not require a pre-trained noise model and can be trained from\nscratch using unpaired noisy data. We benchmark our approach on microscopy\ndatatsets from a range of imaging modalities and sensor types, each with row-\nor column-correlated, signal-dependent noise, and show that it outperforms\nexisting self- and unsupervised denoisers.\n","authors":["Benjamin Salmon","Alexander Krull"],"pdf_url":"https://arxiv.org/pdf/2310.07887v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15998v2","updated":"2025-03-02T23:41:37Z","published":"2024-08-28T17:59:31Z","title":"Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of\n  Encoders","summary":"  The ability to accurately interpret complex visual information is a crucial\ntopic of multimodal large language models (MLLMs). Recent work indicates that\nenhanced visual perception significantly reduces hallucinations and improves\nperformance on resolution-sensitive tasks, such as optical character\nrecognition and document analysis. A number of recent MLLMs achieve this goal\nusing a mixture of vision encoders. Despite their success, there is a lack of\nsystematic comparisons and detailed ablation studies addressing critical\naspects, such as expert selection and the integration of multiple vision\nexperts. This study provides an extensive exploration of the design space for\nMLLMs using a mixture of vision encoders and resolutions. Our findings reveal\nseveral underlying principles common to various existing strategies, leading to\na streamlined yet effective design approach. We discover that simply\nconcatenating visual tokens from a set of complementary vision encoders is as\neffective as more complex mixing architectures or strategies. We additionally\nintroduce Pre-Alignment to bridge the gap between vision-focused encoders and\nlanguage tokens, enhancing model coherence. The resulting family of MLLMs,\nEagle, surpasses other leading open-source models on major MLLM benchmarks.\n","authors":["Min Shi","Fuxiao Liu","Shihao Wang","Shijia Liao","Subhashree Radhakrishnan","Yilin Zhao","De-An Huang","Hongxu Yin","Karan Sapra","Yaser Yacoob","Humphrey Shi","Bryan Catanzaro","Andrew Tao","Jan Kautz","Zhiding Yu","Guilin Liu"],"pdf_url":"https://arxiv.org/pdf/2408.15998v2.pdf","comment":"Github: https://github.com/NVlabs/Eagle, HuggingFace:\n  https://huggingface.co/NVEagle"},{"id":"http://arxiv.org/abs/2411.01106v2","updated":"2025-03-02T22:41:37Z","published":"2024-11-02T02:09:01Z","title":"SV-RAG: LoRA-Contextualizing Adaptation of MLLMs for Long Document\n  Understanding","summary":"  Multimodal large language models (MLLMs) have recently shown great progress\nin text-rich image understanding, yet they still struggle with complex,\nmulti-page visually-rich documents. Traditional methods using document parsers\nfor retrieval-augmented generation suffer from performance and efficiency\nlimitations, while directly presenting all pages to MLLMs leads to\ninefficiencies, especially with lengthy ones. In this work, we present a novel\nframework named **S**elf-**V**isual **R**etrieval-**A**ugmented **G**eneration\n(SV-RAG), which can broaden horizons of any MLLM to support long-document\nunderstanding. We demonstrate that **MLLMs themselves can be an effective\nmultimodal retriever** to fetch relevant pages and then answer user questions\nbased on these pages. SV-RAG is implemented with two specific MLLM adapters,\none for evidence page retrieval and the other for question answering. Empirical\nresults show state-of-the-art performance on public benchmarks, demonstrating\nthe effectiveness of SV-RAG.\n","authors":["Jian Chen","Ruiyi Zhang","Yufan Zhou","Tong Yu","Franck Dernoncourt","Jiuxiang Gu","Ryan A. Rossi","Changyou Chen","Tong Sun"],"pdf_url":"https://arxiv.org/pdf/2411.01106v2.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2502.00156v2","updated":"2025-03-02T20:53:26Z","published":"2025-01-31T20:47:06Z","title":"ALBAR: Adversarial Learning approach to mitigate Biases in Action\n  Recognition","summary":"  Bias in machine learning models can lead to unfair decision making, and while\nit has been well-studied in the image and text domains, it remains\nunderexplored in action recognition. Action recognition models often suffer\nfrom background bias (i.e., inferring actions based on background cues) and\nforeground bias (i.e., relying on subject appearance), which can be detrimental\nto real-life applications such as autonomous vehicles or assisted living\nmonitoring. While prior approaches have mainly focused on mitigating background\nbias using specialized augmentations, we thoroughly study both foreground and\nbackground bias. We propose ALBAR, a novel adversarial training method that\nmitigates foreground and background biases without requiring specialized\nknowledge of the bias attributes. Our framework applies an adversarial\ncross-entropy loss to the sampled static clip (where all the frames are the\nsame) and aims to make its class probabilities uniform using a proposed entropy\nmaximization loss. Additionally, we introduce a gradient penalty loss for\nregularization against the debiasing process. We evaluate our method on\nestablished background and foreground bias protocols, setting a new\nstate-of-the-art and strongly improving combined debiasing performance by over\n12% absolute on HMDB51. Furthermore, we identify an issue of background leakage\nin the existing UCF101 protocol for bias evaluation which provides a shortcut\nto predict actions and does not provide an accurate measure of the debiasing\ncapability of a model. We address this issue by proposing more fine-grained\nsegmentation boundaries for the actor, where our method also outperforms\nexisting approaches. Project Page:\nhttps://joefioresi718.github.io/ALBAR_webpage/\n","authors":["Joseph Fioresi","Ishan Rajendrakumar Dave","Mubarak Shah"],"pdf_url":"https://arxiv.org/pdf/2502.00156v2.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2312.15289v3","updated":"2025-03-02T18:36:56Z","published":"2023-12-23T16:10:53Z","title":"Fréchet Wavelet Distance: A Domain-Agnostic Metric for Image\n  Generation","summary":"  Modern metrics for generative learning like Fr\\'echet Inception Distance\n(FID) and DINOv2-Fr\\'echet Distance (FD-DINOv2) demonstrate impressive\nperformance. However, they suffer from various shortcomings, like a bias\ntowards specific generators and datasets. To address this problem, we propose\nthe Fr\\'echet Wavelet Distance (FWD) as a domain-agnostic metric based on the\nWavelet Packet Transform ($W_p$). FWD provides a sight across a broad spectrum\nof frequencies in images with a high resolution, preserving both spatial and\ntextural aspects. Specifically, we use $W_p$ to project generated and real\nimages to the packet coefficient space. We then compute the Fr\\'echet distance\nwith the resultant coefficients to evaluate the quality of a generator. This\nmetric is general-purpose and dataset-domain agnostic, as it does not rely on\nany pre-trained network, while being more interpretable due to its ability to\ncompute Fr\\'echet distance per packet, enhancing transparency. We conclude with\nan extensive evaluation of a wide variety of generators across various datasets\nthat the proposed FWD can generalize and improve robustness to domain shifts\nand various corruptions compared to other metrics.\n","authors":["Lokesh Veeramacheneni","Moritz Wolter","Hildegard Kuehne","Juergen Gall"],"pdf_url":"https://arxiv.org/pdf/2312.15289v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.10509v2","updated":"2025-03-02T18:17:14Z","published":"2024-11-15T15:39:04Z","title":"TESGNN: Temporal Equivariant Scene Graph Neural Networks for Efficient\n  and Robust Multi-View 3D Scene Understanding","summary":"  Scene graphs have proven to be highly effective for various scene\nunderstanding tasks due to their compact and explicit representation of\nrelational information. However, current methods often overlook the critical\nimportance of preserving symmetry when generating scene graphs from 3D point\nclouds, which can lead to reduced accuracy and robustness, particularly when\ndealing with noisy, multi-view data. Furthermore, a major limitation of prior\napproaches is the lack of temporal modeling to capture time-dependent\nrelationships among dynamically evolving entities in a scene. To address these\nchallenges, we propose Temporal Equivariant Scene Graph Neural Network\n(TESGNN), consisting of two key components: (1) an Equivariant Scene Graph\nNeural Network (ESGNN), which extracts information from 3D point clouds to\ngenerate scene graph while preserving crucial symmetry properties, and (2) a\nTemporal Graph Matching Network, which fuses scene graphs generated by ESGNN\nacross multiple time sequences into a unified global representation using an\napproximate graph-matching algorithm. Our combined architecture TESGNN\noutperforms current state-of-the-art methods in scene graph generation,\nachieving higher accuracy and faster training convergence. Moreover, we show\nthat leveraging the symmetry-preserving property produces a more stable and\naccurate global scene representation compared to existing approaches. Last but\nnot least, it is computationally efficient and easily implementable using\nexisting frameworks, making it well-suited for real-time applications in\nrobotics and computer vision. This approach paves the way for more robust and\nscalable solutions to complex multi-view scene understanding challenges. Our\nsource code is publicly available at: https://github.com/HySonLab/TESGraph\n","authors":["Quang P. M. Pham","Khoi T. N. Nguyen","Lan C. Ngo","Truong Do","Dezhen Song","Truong-Son Hy"],"pdf_url":"https://arxiv.org/pdf/2411.10509v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2407.00609"},{"id":"http://arxiv.org/abs/2411.02372v2","updated":"2025-03-02T17:34:53Z","published":"2024-11-04T18:40:46Z","title":"Learning General-Purpose Biomedical Volume Representations using\n  Randomized Synthesis","summary":"  Current volumetric biomedical foundation models struggle to generalize as\npublic 3D datasets are small and do not cover the broad diversity of medical\nprocedures, conditions, anatomical regions, and imaging protocols. We address\nthis by creating a representation learning method that instead anticipates\nstrong domain shifts at training time itself. We first propose a data engine\nthat synthesizes highly variable training samples that would enable\ngeneralization to new biomedical contexts. To then train a single 3D network\nfor any voxel-level task, we develop a contrastive learning method that\npretrains the network to be stable against nuisance imaging variation simulated\nby the data engine, a key inductive bias for generalization. This network's\nfeatures can be used as robust representations of input images for downstream\ntasks and its weights provide a strong, dataset-agnostic initialization for\nfinetuning on new datasets. As a result, we set new standards across both\nmultimodality registration and few-shot segmentation, a first for any 3D\nbiomedical vision model, all without (pre-)training on any existing dataset of\nreal images.\n","authors":["Neel Dey","Benjamin Billot","Hallee E. Wong","Clinton J. Wang","Mengwei Ren","P. Ellen Grant","Adrian V. Dalca","Polina Golland"],"pdf_url":"https://arxiv.org/pdf/2411.02372v2.pdf","comment":"ICLR 2025: International Conference on Learning Representations. Code\n  and model weights available at https://github.com/neel-dey/anatomix.\n  Keywords: synthetic data, representation learning, medical image analysis,\n  image registration, image segmentation"},{"id":"http://arxiv.org/abs/2409.14876v3","updated":"2025-03-02T17:27:04Z","published":"2024-09-23T10:17:13Z","title":"Tri-Clustering: A Multi-views Tri-level Information Fusion Context\n  Clustering Framework for Localization and Classification in Mammography","summary":"  Breast cancer is a significant global health issue, and the diagnosis of\nbreast imaging has always been challenging. Mammography images typically have\nextremely high resolution, with lesions occupying only a very small area.\nDown-sampling in neural networks can easily lead to the loss of\nmicrocalcifications or subtle structures, making it difficult for traditional\nneural network architectures to address these issues. To tackle these\nchallenges, we propose a Context Clustering Network with triple information\nfusion. Firstly, compared to CNNs or transformers, we find that Context\nclustering methods (1) are more computationally efficient and (2) can more\neasily associate structural or pathological features, making them suitable for\nthe clinical tasks of mammography. Secondly, we propose a triple information\nfusion mechanism that integrates global information, feature-based local\ninformation, and patch-based local information. The proposed approach is\nrigorously evaluated on two public datasets, Vindr-Mammo and CBIS-DDSM, using\nfive independent splits to ensure statistical robustness. Our method achieves\nan AUC of 0.828 on Vindr-Mammo and 0.805 on CBIS-DDSM, outperforming the next\nbest method by 3.1% and 2.4%, respectively. These improvements are\nstatistically significant (p<0.05), underscoring the benefits of Context\nClustering Network with triple information fusion. Overall, our Context\nClustering framework demonstrates strong potential as a scalable and\ncost-effective solution for large-scale mammography screening, enabling more\nefficient and accurate breast cancer detection. Access to our method is\navailable at https://github.com/Sohyu1/Mammo_Clustering.\n","authors":["Shilong Yang","Chulong Zhang","Qi Zang","Juan Yu","Liang Zeng","Xiao Luo","Yexuan Xing","Xin Pan","Qi Li","Xiaokun Liang","Yaoqin Xie"],"pdf_url":"https://arxiv.org/pdf/2409.14876v3.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2410.04810v2","updated":"2025-03-02T17:18:04Z","published":"2024-10-07T07:45:18Z","title":"FedBiP: Heterogeneous One-Shot Federated Learning with Personalized\n  Latent Diffusion Models","summary":"  One-Shot Federated Learning (OSFL), a special decentralized machine learning\nparadigm, has recently gained significant attention. OSFL requires only a\nsingle round of client data or model upload, which reduces communication costs\nand mitigates privacy threats compared to traditional FL. Despite these\npromising prospects, existing methods face challenges due to client data\nheterogeneity and limited data quantity when applied to real-world OSFL\nsystems. Recently, Latent Diffusion Models (LDM) have shown remarkable\nadvancements in synthesizing high-quality images through pretraining on\nlarge-scale datasets, thereby presenting a potential solution to overcome these\nissues. However, directly applying pretrained LDM to heterogeneous OSFL results\nin significant distribution shifts in synthetic data, leading to performance\ndegradation in classification models trained on such data. This issue is\nparticularly pronounced in rare domains, such as medical imaging, which are\nunderrepresented in LDM's pretraining data. To address this challenge, we\npropose Federated Bi-Level Personalization (FedBiP), which personalizes the\npretrained LDM at both instance-level and concept-level. Hereby, FedBiP\nsynthesizes images following the client's local data distribution without\ncompromising the privacy regulations. FedBiP is also the first approach to\nsimultaneously address feature space heterogeneity and client data scarcity in\nOSFL. Our method is validated through extensive experiments on three OSFL\nbenchmarks with feature space heterogeneity, as well as on challenging medical\nand satellite image datasets with label heterogeneity. The results demonstrate\nthe effectiveness of FedBiP, which substantially outperforms other OSFL\nmethods.\n","authors":["Haokun Chen","Hang Li","Yao Zhang","Jinhe Bi","Gengyuan Zhang","Yueqi Zhang","Philip Torr","Jindong Gu","Denis Krompass","Volker Tresp"],"pdf_url":"https://arxiv.org/pdf/2410.04810v2.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2308.09036v2","updated":"2025-03-02T17:15:30Z","published":"2023-08-17T15:17:49Z","title":"Synthesizing Physically Plausible Human Motions in 3D Scenes","summary":"  We present a physics-based character control framework for synthesizing\nhuman-scene interactions. Recent advances adopt physics simulation to mitigate\nartifacts produced by data-driven kinematic approaches. However, existing\nphysics-based methods mainly focus on single-object environments, resulting in\nlimited applicability in realistic 3D scenes with multi-objects. To address\nsuch challenges, we propose a framework that enables physically simulated\ncharacters to perform long-term interaction tasks in diverse, cluttered, and\nunseen 3D scenes. The key idea is to decouple human-scene interactions into two\nfundamental processes, Interacting and Navigating, which motivates us to\nconstruct two reusable Controllers, namely InterCon and NavCon. Specifically,\nInterCon uses two complementary policies to enable characters to enter or leave\nthe interacting state with a particular object (e.g., sitting on a chair or\ngetting up). To realize navigation in cluttered environments, we introduce\nNavCon, where a trajectory following policy enables characters to track\npre-planned collision-free paths. Benefiting from the divide and conquer\nstrategy, we can train all policies in simple environments and directly apply\nthem in complex multi-object scenes through coordination from a rule-based\nscheduler. Video and code are available at\nhttps://github.com/liangpan99/InterScene.\n","authors":["Liang Pan","Jingbo Wang","Buzhen Huang","Junyu Zhang","Haofan Wang","Xu Tang","Yangang Wang"],"pdf_url":"https://arxiv.org/pdf/2308.09036v2.pdf","comment":"3DV 2024 version"},{"id":"http://arxiv.org/abs/2410.15744v2","updated":"2025-03-02T16:58:17Z","published":"2024-10-21T08:01:58Z","title":"Unleashing the Potential of Vision-Language Pre-Training for 3D\n  Zero-Shot Lesion Segmentation via Mask-Attribute Alignment","summary":"  Recent advancements in medical vision-language pre-training models have\ndriven significant progress in zero-shot disease recognition. However,\ntransferring image-level knowledge to pixel-level tasks, such as lesion\nsegmentation in 3D CT scans, remains a critical challenge. Due to the\ncomplexity and variability of pathological visual characteristics, existing\nmethods struggle to align fine-grained lesion features not encountered during\ntraining with disease-related textual representations. In this paper, we\npresent Malenia, a novel multi-scale lesion-level mask-attribute alignment\nframework, specifically designed for 3D zero-shot lesion segmentation. Malenia\nimproves the compatibility between mask representations and their associated\nelemental attributes, explicitly linking the visual features of unseen lesions\nwith the extensible knowledge learned from previously seen ones. Furthermore,\nwe design a Cross-Modal Knowledge Injection module to enhance both visual and\ntextual features with mutually beneficial information, effectively guiding the\ngeneration of segmentation results. Comprehensive experiments across three\ndatasets and 12 lesion categories validate the superior performance of Malenia.\n","authors":["Yankai Jiang","Wenhui Lei","Xiaofan Zhang","Shaoting Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.15744v2.pdf","comment":"Accepted as ICLR 2025 conference paper"},{"id":"http://arxiv.org/abs/2403.18035v4","updated":"2025-03-02T16:41:49Z","published":"2024-03-26T18:40:36Z","title":"Bidirectional Consistency Models","summary":"  Diffusion models (DMs) are capable of generating remarkably high-quality\nsamples by iteratively denoising a random vector, a process that corresponds to\nmoving along the probability flow ordinary differential equation (PF ODE).\nInterestingly, DMs can also invert an input image to noise by moving backward\nalong the PF ODE, a key operation for downstream tasks such as interpolation\nand image editing. However, the iterative nature of this process restricts its\nspeed, hindering its broader application. Recently, Consistency Models (CMs)\nhave emerged to address this challenge by approximating the integral of the PF\nODE, largely reducing the number of iterations. Yet, the absence of an explicit\nODE solver complicates the inversion process. To resolve this, we introduce\nBidirectional Consistency Model (BCM), which learns a single neural network\nthat enables both forward and backward traversal along the PF ODE, efficiently\nunifying generation and inversion tasks within one framework. We can train BCM\nfrom scratch or tune it using a pretrained consistency model, which reduces the\ntraining cost and increases scalability. We demonstrate that BCM enables\none-step generation and inversion while also allowing the use of additional\nsteps to enhance generation quality or reduce reconstruction error. We further\nshowcase BCM's capability in downstream tasks, such as interpolation and\ninpainting. Our code and weights are available at\nhttps://github.com/Mosasaur5526/BCM-iCT-torch.\n","authors":["Liangchen Li","Jiajun He"],"pdf_url":"https://arxiv.org/pdf/2403.18035v4.pdf","comment":"39 pages, 27 figures; a shorter version of this paper was acceppted\n  at the ICML 2024 Workshop on Structured Probabilistic Inference & Generative\n  Modeling"},{"id":"http://arxiv.org/abs/2408.11915v2","updated":"2025-03-02T15:55:14Z","published":"2024-08-21T18:06:15Z","title":"Video-Foley: Two-Stage Video-To-Sound Generation via Temporal Event\n  Condition For Foley Sound","summary":"  Foley sound synthesis is crucial for multimedia production, enhancing user\nexperience by synchronizing audio and video both temporally and semantically.\nRecent studies on automating this labor-intensive process through\nvideo-to-sound generation face significant challenges. Systems lacking explicit\ntemporal features suffer from poor alignment and controllability, while\ntimestamp-based models require costly and subjective human annotation. We\npropose Video-Foley, a video-to-sound system using Root Mean Square (RMS) as an\nintuitive condition with semantic timbre prompts (audio or text). RMS, a\nframe-level intensity envelope closely related to audio semantics, acts as a\ntemporal event feature to guide audio generation from video. The\nannotation-free self-supervised learning framework consists of two stages,\nVideo2RMS and RMS2Sound, incorporating novel ideas including RMS discretization\nand RMS-ControlNet with a pretrained text-to-audio model. Our extensive\nevaluation shows that Video-Foley achieves state-of-the-art performance in\naudio-visual alignment and controllability for sound timing, intensity, timbre,\nand nuance. Source code, model weights and demos are available on our companion\nwebsite. (https://jnwnlee.github.io/video-foley-demo)\n","authors":["Junwon Lee","Jaekwon Im","Dabin Kim","Juhan Nam"],"pdf_url":"https://arxiv.org/pdf/2408.11915v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.03895v2","updated":"2025-03-02T15:55:07Z","published":"2025-01-07T16:03:14Z","title":"LLaVA-Mini: Efficient Image and Video Large Multimodal Models with One\n  Vision Token","summary":"  The advent of real-time large multimodal models (LMMs) like GPT-4o has\nsparked considerable interest in efficient LMMs. LMM frameworks typically\nencode visual inputs into vision tokens (continuous representations) and\nintegrate them and textual instructions into the context of large language\nmodels (LLMs), where large-scale parameters and numerous context tokens\n(predominantly vision tokens) result in substantial computational overhead.\nPrevious efforts towards efficient LMMs always focus on replacing the LLM\nbackbone with smaller models, while neglecting the crucial issue of token\nquantity. In this paper, we introduce LLaVA-Mini, an efficient LMM with minimal\nvision tokens. To achieve a high compression ratio of vision tokens while\npreserving visual information, we first analyze how LMMs understand vision\ntokens and find that most vision tokens only play a crucial role in the early\nlayers of LLM backbone, where they mainly fuse visual information into text\ntokens. Building on this finding, LLaVA-Mini introduces modality pre-fusion to\nfuse visual information into text tokens in advance, thereby facilitating the\nextreme compression of vision tokens fed to LLM backbone into one token.\nLLaVA-Mini is a unified large multimodal model that can support the\nunderstanding of images, high-resolution images, and videos in an efficient\nmanner. Experiments across 11 image-based and 7 video-based benchmarks\ndemonstrate that LLaVA-Mini outperforms LLaVA-v1.5 with just 1 vision token\ninstead of 576. Efficiency analyses reveal that LLaVA-Mini can reduce FLOPs by\n77%, deliver low-latency responses within 40 milliseconds, and process over\n10,000 frames of video on the GPU hardware with 24GB of memory.\n","authors":["Shaolei Zhang","Qingkai Fang","Zhe Yang","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2501.03895v2.pdf","comment":"Accepted to ICLR 2025. Code: https://github.com/ictnlp/LLaVA-Mini\n  Model: https://huggingface.co/ICTNLP/llava-mini-llama-3.1-8b"},{"id":"http://arxiv.org/abs/2501.18672v3","updated":"2025-03-02T15:43:39Z","published":"2025-01-30T18:51:54Z","title":"Drag Your Gaussian: Effective Drag-Based Editing with Score Distillation\n  for 3D Gaussian Splatting","summary":"  Recent advancements in 3D scene editing have been propelled by the rapid\ndevelopment of generative models. Existing methods typically utilize generative\nmodels to perform text-guided editing on 3D representations, such as 3D\nGaussian Splatting (3DGS). However, these methods are often limited to texture\nmodifications and fail when addressing geometric changes, such as editing a\ncharacter's head to turn around. Moreover, such methods lack accurate control\nover the spatial position of editing results, as language struggles to\nprecisely describe the extent of edits. To overcome these limitations, we\nintroduce DYG, an effective 3D drag-based editing method for 3D Gaussian\nSplatting. It enables users to conveniently specify the desired editing region\nand the desired dragging direction through the input of 3D masks and pairs of\ncontrol points, thereby enabling precise control over the extent of editing.\nDYG integrates the strengths of the implicit triplane representation to\nestablish the geometric scaffold of the editing results, effectively overcoming\nsuboptimal editing outcomes caused by the sparsity of 3DGS in the desired\nediting regions. Additionally, we incorporate a drag-based Latent Diffusion\nModel into our method through the proposed Drag-SDS loss function, enabling\nflexible, multi-view consistent, and fine-grained editing. Extensive\nexperiments demonstrate that DYG conducts effective drag-based editing guided\nby control point prompts, surpassing other baselines in terms of editing effect\nand quality, both qualitatively and quantitatively. Visit our project page at\nhttps://quyans.github.io/Drag-Your-Gaussian.\n","authors":["Yansong Qu","Dian Chen","Xinyang Li","Xiaofan Li","Shengchuan Zhang","Liujuan Cao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2501.18672v3.pdf","comment":"Visit our project page at https://quyans.github.io/Drag-Your-Gaussian"},{"id":"http://arxiv.org/abs/2310.18709v4","updated":"2025-03-02T15:37:39Z","published":"2023-10-28T13:37:52Z","title":"Audio-Visual Instance Segmentation","summary":"  In this paper, we propose a new multi-modal task, termed audio-visual\ninstance segmentation (AVIS), which aims to simultaneously identify, segment\nand track individual sounding object instances in audible videos. To facilitate\nthis research, we introduce a high-quality benchmark named AVISeg, containing\nover 90K instance masks from 26 semantic categories in 926 long videos.\nAdditionally, we propose a strong baseline model for this task. Our model first\nlocalizes sound source within each frame, and condenses object-specific\ncontexts into concise tokens. Then it builds long-range audio-visual\ndependencies between these tokens using window-based attention, and tracks\nsounding objects among the entire video sequences. Extensive experiments reveal\nthat our method performs best on AVISeg, surpassing the existing methods from\nrelated tasks. We further conduct the evaluation on several multi-modal large\nmodels. Unfortunately, they exhibits subpar performance on instance-level sound\nsource localization and temporal perception. We expect that AVIS will inspire\nthe community towards a more comprehensive multi-modal understanding. Dataset\nand code is available at https://github.com/ruohaoguo/avis.\n","authors":["Ruohao Guo","Xianghua Ying","Yaru Chen","Dantong Niu","Guangyao Li","Liao Qu","Yanyu Qi","Jinxing Zhou","Bowei Xing","Wenzhen Yue","Ji Shi","Qixun Wang","Peiliang Zhang","Buwen Liang"],"pdf_url":"https://arxiv.org/pdf/2310.18709v4.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2409.03114v2","updated":"2025-03-02T15:30:06Z","published":"2024-09-04T22:39:02Z","title":"Evaluating Low-Resource Lane Following Algorithms for\n  Compute-Constrained Automated Vehicles","summary":"  Reliable lane-following is essential for automated and assisted driving, yet\nexisting solutions often rely on models that require extensive computational\nresources, limiting their deployment in compute-constrained vehicles. We\nevaluate five low-resource lane-following algorithms designed for real-time\noperation on vehicles with limited computing resources. Performance was\nassessed through simulation and deployment on real drive-by-wire electric\nvehicles, with evaluation metrics including reliability, comfort, speed, and\nadaptability. The top-performing methods used unsupervised learning to detect\nand separate lane lines with processing time under 10 ms per frame,\noutperforming compute-intensive and poor generalizing deep learning approaches.\nThese approaches demonstrated robustness across lighting conditions, road\ntextures, and lane geometries. The findings highlight the potential for\nefficient lane detection approaches to enhance the accessibility and\nreliability of autonomous vehicle technologies. Reducing computing requirements\nenables lane keeping to be widely deployed in vehicles as part of lower-level\nautomation, including active safety systems.\n","authors":["Beñat Froemming-Aldanondo","Tatiana Rastoskueva","Michael Evans","Marcial Machado","Anna Vadella","Rickey Johnson","Luis Escamilla","Milan Jostes","Devson Butani","Ryan Kaddis","Chan-Jin Chung","Joshua Siegel"],"pdf_url":"https://arxiv.org/pdf/2409.03114v2.pdf","comment":"Supported by the National Science Foundation under Grants No. 2150292\n  and 2150096"},{"id":"http://arxiv.org/abs/2410.03878v2","updated":"2025-03-02T15:22:12Z","published":"2024-10-04T19:22:20Z","title":"SPARTUN3D: Situated Spatial Understanding of 3D World in Large Language\n  Models","summary":"  Integrating the 3D world into large language models (3D-based LLMs) has been\na promising research direction for 3D scene understanding. However, current\n3D-based LLMs fall short in situated understanding due to two key limitations:\n1) existing 3D datasets are constructed from a global perspective of the 3D\nscenes and lack situated context. 2) the architectures of existing 3D-based\nLLMs lack explicit alignment between the spatial representations of 3D scenes\nand natural language, limiting their performance in tasks requiring precise\nspatial reasoning. We address these issues by introducing a scalable situated\n3D dataset, named Spartun3D, that incorporates various situated spatial\nreasoning tasks. Furthermore, we propose Spartun3D-LLM, built on an existing\n3D-based LLM but integrated with a novel situated spatial alignment module,\naiming to enhance the alignment between 3D visual representations and their\ncorresponding textual descriptions. Experimental results demonstrate that both\nour proposed dataset and alignment module significantly enhance the situated\nspatial understanding of 3D-based LLMs.\n","authors":["Yue Zhang","Zhiyang Xu","Ying Shen","Parisa Kordjamshidi","Lifu Huang"],"pdf_url":"https://arxiv.org/pdf/2410.03878v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13426v2","updated":"2025-03-02T15:06:51Z","published":"2024-09-20T11:46:48Z","title":"HMD^2: Environment-aware Motion Generation from Single Egocentric\n  Head-Mounted Device","summary":"  This paper investigates the generation of realistic full-body human motion\nusing a single head-mounted device with an outward-facing color camera and the\nability to perform visual SLAM. To address the ambiguity of this setup, we\npresent HMD^2, a novel system that balances motion reconstruction and\ngeneration. From a reconstruction standpoint, it aims to maximally utilize the\ncamera streams to produce both analytical and learned features, including head\nmotion, SLAM point cloud, and image embeddings. On the generative front, HMD^2\nemploys a multi-modal conditional motion diffusion model with a Transformer\nbackbone to maintain temporal coherence of generated motions, and utilizes\nautoregressive inpainting to facilitate online motion inference with minimal\nlatency (0.17 seconds). We show that our system provides an effective and\nrobust solution that scales to a diverse dataset of over 200 hours of motion in\ncomplex indoor and outdoor environments.\n","authors":["Vladimir Guzov","Yifeng Jiang","Fangzhou Hong","Gerard Pons-Moll","Richard Newcombe","C. Karen Liu","Yuting Ye","Lingni Ma"],"pdf_url":"https://arxiv.org/pdf/2409.13426v2.pdf","comment":"International Conference on 3D Vision 2025 (3DV 2025)"},{"id":"http://arxiv.org/abs/2502.11858v3","updated":"2025-03-02T14:14:07Z","published":"2025-02-17T14:50:34Z","title":"Rethinking Audio-Visual Adversarial Vulnerability from Temporal and\n  Modality Perspectives","summary":"  While audio-visual learning equips models with a richer understanding of the\nreal world by leveraging multiple sensory modalities, this integration also\nintroduces new vulnerabilities to adversarial attacks.\n  In this paper, we present a comprehensive study of the adversarial robustness\nof audio-visual models, considering both temporal and modality-specific\nvulnerabilities. We propose two powerful adversarial attacks: 1) a temporal\ninvariance attack that exploits the inherent temporal redundancy across\nconsecutive time segments and 2) a modality misalignment attack that introduces\nincongruence between the audio and visual modalities. These attacks are\ndesigned to thoroughly assess the robustness of audio-visual models against\ndiverse threats. Furthermore, to defend against such attacks, we introduce a\nnovel audio-visual adversarial training framework. This framework addresses key\nchallenges in vanilla adversarial training by incorporating efficient\nadversarial perturbation crafting tailored to multi-modal data and an\nadversarial curriculum strategy. Extensive experiments in the Kinetics-Sounds\ndataset demonstrate that our proposed temporal and modality-based attacks in\ndegrading model performance can achieve state-of-the-art performance, while our\nadversarial training defense largely improves the adversarial robustness as\nwell as the adversarial training efficiency.\n","authors":["Zeliang Zhang","Susan Liang","Daiki Shimada","Chenliang Xu"],"pdf_url":"https://arxiv.org/pdf/2502.11858v3.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2410.02094v3","updated":"2025-03-02T14:04:22Z","published":"2024-10-02T23:30:05Z","title":"Tracking objects that change in appearance with phase synchrony","summary":"  Objects we encounter often change appearance as we interact with them.\nChanges in illumination (shadows), object pose, or the movement of non-rigid\nobjects can drastically alter available image features. How do biological\nvisual systems track objects as they change? One plausible mechanism involves\nattentional mechanisms for reasoning about the locations of objects\nindependently of their appearances -- a capability that prominent neuroscience\ntheories have associated with computing through neural synchrony. Here, we\ndescribe a novel deep learning circuit that can learn to precisely control\nattention to features separately from their location in the world through\nneural synchrony: the complex-valued recurrent neural network (CV-RNN). Next,\nwe compare object tracking in humans, the CV-RNN, and other deep neural\nnetworks (DNNs), using FeatureTracker: a large-scale challenge that asks\nobservers to track objects as their locations and appearances change in\nprecisely controlled ways. While humans effortlessly solved FeatureTracker,\nstate-of-the-art DNNs did not. In contrast, our CV-RNN behaved similarly to\nhumans on the challenge, providing a computational proof-of-concept for the\nrole of phase synchronization as a neural substrate for tracking\nappearance-morphing objects as they move about.\n","authors":["Sabine Muzellec","Drew Linsley","Alekh K. Ashok","Ennio Mingolla","Girik Malik","Rufin VanRullen","Thomas Serre"],"pdf_url":"https://arxiv.org/pdf/2410.02094v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19047v2","updated":"2025-03-02T13:52:23Z","published":"2025-02-26T11:01:43Z","title":"A Dual-Purpose Framework for Backdoor Defense and Backdoor Amplification\n  in Diffusion Models","summary":"  Diffusion models have emerged as state-of-the-art generative frameworks,\nexcelling in producing high-quality multi-modal samples. However, recent\nstudies have revealed their vulnerability to backdoor attacks, where backdoored\nmodels generate specific, undesirable outputs called backdoor target (e.g.,\nharmful images) when a pre-defined trigger is embedded to their inputs. In this\npaper, we propose PureDiffusion, a dual-purpose framework that simultaneously\nserves two contrasting roles: backdoor defense and backdoor attack\namplification. For defense, we introduce two novel loss functions to invert\nbackdoor triggers embedded in diffusion models. The first leverages\ntrigger-induced distribution shifts across multiple timesteps of the diffusion\nprocess, while the second exploits the denoising consistency effect when a\nbackdoor is activated. Once an accurate trigger inversion is achieved, we\ndevelop a backdoor detection method that analyzes both the inverted trigger and\nthe generated backdoor targets to identify backdoor attacks. In terms of attack\namplification with the role of an attacker, we describe how our trigger\ninversion algorithm can be used to reinforce the original trigger embedded in\nthe backdoored diffusion model. This significantly boosts attack performance\nwhile reducing the required backdoor training time. Experimental results\ndemonstrate that PureDiffusion achieves near-perfect detection accuracy,\noutperforming existing defenses by a large margin, particularly against complex\ntrigger patterns. Additionally, in an attack scenario, our attack amplification\napproach elevates the attack success rate (ASR) of existing backdoor attacks to\nnearly 100\\% while reducing training time by up to 20x.\n","authors":["Vu Tuan Truong","Long Bao Le"],"pdf_url":"https://arxiv.org/pdf/2502.19047v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.15161v2","updated":"2025-03-02T13:49:21Z","published":"2024-04-23T16:01:33Z","title":"Test-Time Adaptation for Combating Missing Modalities in Egocentric\n  Videos","summary":"  Understanding videos that contain multiple modalities is crucial, especially\nin egocentric videos, where combining various sensory inputs significantly\nimproves tasks like action recognition and moment localization. However,\nreal-world applications often face challenges with incomplete modalities due to\nprivacy concerns, efficiency needs, or hardware issues. Current methods, while\neffective, often necessitate retraining the model entirely to handle missing\nmodalities, making them computationally intensive, particularly with large\ntraining datasets. In this study, we propose a novel approach to address this\nissue at test time without requiring retraining. We frame the problem as a\ntest-time adaptation task, where the model adjusts to the available unlabeled\ndata at test time. Our method, MiDl~(Mutual information with\nself-Distillation), encourages the model to be insensitive to the specific\nmodality source present during testing by minimizing the mutual information\nbetween the prediction and the available modality. Additionally, we incorporate\nself-distillation to maintain the model's original performance when both\nmodalities are available. MiDl represents the first self-supervised, online\nsolution for handling missing modalities exclusively at test time. Through\nexperiments with various pretrained models and datasets, MiDl demonstrates\nsubstantial performance improvement without the need for retraining.\n","authors":["Merey Ramazanova","Alejandro Pardo","Bernard Ghanem","Motasem Alfarra"],"pdf_url":"https://arxiv.org/pdf/2404.15161v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20209v2","updated":"2025-03-02T13:36:57Z","published":"2025-02-27T15:50:21Z","title":"DIPSER: A Dataset for In-Person Student Engagement Recognition in the\n  Wild","summary":"  In this paper, a novel dataset is introduced, designed to assess student\nattention within in-person classroom settings. This dataset encompasses RGB\ncamera data, featuring multiple cameras per student to capture both posture and\nfacial expressions, in addition to smartwatch sensor data for each individual.\nThis dataset allows machine learning algorithms to be trained to predict\nattention and correlate it with emotion. A comprehensive suite of attention and\nemotion labels for each student is provided, generated through self-reporting\nas well as evaluations by four different experts. Our dataset uniquely combines\nfacial and environmental camera data, smartwatch metrics, and includes\nunderrepresented ethnicities in similar datasets, all within in-the-wild,\nin-person settings, making it the most comprehensive dataset of its kind\ncurrently available.\n  The dataset presented offers an extensive and diverse collection of data\npertaining to student interactions across different educational contexts,\naugmented with additional metadata from other tools. This initiative addresses\nexisting deficiencies by offering a valuable resource for the analysis of\nstudent attention and emotion in face-to-face lessons.\n","authors":["Luis Marquez-Carpintero","Sergio Suescun-Ferrandiz","Carolina Lorenzo Álvarez","Jorge Fernandez-Herrero","Diego Viejo","Rosabel Roig-Vila","Miguel Cazorla"],"pdf_url":"https://arxiv.org/pdf/2502.20209v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2208.02820v2","updated":"2025-03-02T13:14:11Z","published":"2022-08-04T02:22:29Z","title":"MOVE: Effective and Harmless Ownership Verification via Embedded\n  External Features","summary":"  Currently, deep neural networks (DNNs) are widely adopted in different\napplications. Despite its commercial values, training a well-performing DNN is\nresource-consuming. Accordingly, the well-trained model is valuable\nintellectual property for its owner. However, recent studies revealed the\nthreats of model stealing, where the adversaries can obtain a function-similar\ncopy of the victim model, even when they can only query the model. In this\npaper, we propose an effective and harmless model ownership verification (MOVE)\nto defend against different types of model stealing simultaneously, without\nintroducing new security risks. In general, we conduct the ownership\nverification by verifying whether a suspicious model contains the knowledge of\ndefender-specified external features. Specifically, we embed the external\nfeatures by modifying a few training samples with style transfer. We then train\na meta-classifier to determine whether a model is stolen from the victim. This\napproach is inspired by the understanding that the stolen models should contain\nthe knowledge of features learned by the victim model. In particular,\n\\revision{we develop our MOVE method under both white-box and black-box\nsettings and analyze its theoretical foundation to provide comprehensive model\nprotection.} Extensive experiments on benchmark datasets verify the\neffectiveness of our method and its resistance to potential adaptive attacks.\nThe codes for reproducing the main experiments of our method are available at\nhttps://github.com/THUYimingLi/MOVE.\n","authors":["Yiming Li","Linghui Zhu","Xiaojun Jia","Yang Bai","Yong Jiang","Shu-Tao Xia","Xiaochun Cao","Kui Ren"],"pdf_url":"https://arxiv.org/pdf/2208.02820v2.pdf","comment":"This paper has been accepted by IEEE TPAMI 2025. It is the journal\n  extension of our conference paper in AAAI 2022\n  (https://ojs.aaai.org/index.php/AAAI/article/view/20036). 18 pages"},{"id":"http://arxiv.org/abs/2502.18461v2","updated":"2025-03-02T12:44:06Z","published":"2025-02-25T18:59:12Z","title":"K-LoRA: Unlocking Training-Free Fusion of Any Subject and Style LoRAs","summary":"  Recent studies have explored combining different LoRAs to jointly generate\nlearned style and content. However, existing methods either fail to effectively\npreserve both the original subject and style simultaneously or require\nadditional training. In this paper, we argue that the intrinsic properties of\nLoRA can effectively guide diffusion models in merging learned subject and\nstyle. Building on this insight, we propose K-LoRA, a simple yet effective\ntraining-free LoRA fusion approach. In each attention layer, K-LoRA compares\nthe Top-K elements in each LoRA to be fused, determining which LoRA to select\nfor optimal fusion. This selection mechanism ensures that the most\nrepresentative features of both subject and style are retained during the\nfusion process, effectively balancing their contributions. Experimental results\ndemonstrate that the proposed method effectively integrates the subject and\nstyle information learned by the original LoRAs, outperforming state-of-the-art\ntraining-based approaches in both qualitative and quantitative results.\n","authors":["Ziheng Ouyang","Zhen Li","Qibin Hou"],"pdf_url":"https://arxiv.org/pdf/2502.18461v2.pdf","comment":"CVPR 2025, Project page: https://k-lora.github.io/K-LoRA.io/"},{"id":"http://arxiv.org/abs/2406.15812v2","updated":"2025-03-02T12:28:24Z","published":"2024-06-22T10:36:04Z","title":"Intrinsic Dimension Correlation: uncovering nonlinear connections in\n  multimodal representations","summary":"  To gain insight into the mechanisms behind machine learning methods, it is\ncrucial to establish connections among the features describing data points.\nHowever, these correlations often exhibit a high-dimensional and strongly\nnonlinear nature, which makes them challenging to detect using standard\nmethods. This paper exploits the entanglement between intrinsic dimensionality\nand correlation to propose a metric that quantifies the (potentially nonlinear)\ncorrelation between high-dimensional manifolds. We first validate our method on\nsynthetic data in controlled environments, showcasing its advantages and\ndrawbacks compared to existing techniques. Subsequently, we extend our analysis\nto large-scale applications in neural network representations. Specifically, we\nfocus on latent representations of multimodal data, uncovering clear\ncorrelations between paired visual and textual embeddings, whereas existing\nmethods struggle significantly in detecting similarity. Our results indicate\nthe presence of highly nonlinear correlation patterns between latent manifolds.\n","authors":["Lorenzo Basile","Santiago Acevedo","Luca Bortolussi","Fabio Anselmi","Alex Rodriguez"],"pdf_url":"https://arxiv.org/pdf/2406.15812v2.pdf","comment":"Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2409.20063v2","updated":"2025-03-02T12:17:51Z","published":"2024-09-30T08:05:00Z","title":"Q-Bench-Video: Benchmarking the Video Quality Understanding of LMMs","summary":"  With the rising interest in research on Large Multi-modal Models (LMMs) for\nvideo understanding, many studies have emphasized general video comprehension\ncapabilities, neglecting the systematic exploration into video quality\nunderstanding. To address this oversight, we introduce Q-Bench-Video in this\npaper, a new benchmark specifically designed to evaluate LMMs' proficiency in\ndiscerning video quality. a) To ensure video source diversity, Q-Bench-Video\nencompasses videos from natural scenes, AI-generated Content (AIGC), and\nComputer Graphics (CG). b) Building on the traditional multiple-choice\nquestions format with the Yes-or-No and What-How categories, we include\nOpen-ended questions to better evaluate complex scenarios. Additionally, we\nincorporate the video pair quality comparison question to enhance\ncomprehensiveness. c) Beyond the traditional Technical, Aesthetic, and Temporal\ndistortions, we have expanded our evaluation aspects to include the dimension\nof AIGC distortions, which addresses the increasing demand for video\ngeneration. Finally, we collect a total of 2,378 question-answer pairs and test\nthem on 12 open-source & 5 proprietary LMMs. Our findings indicate that while\nLMMs have a foundational understanding of video quality, their performance\nremains incomplete and imprecise, with a notable discrepancy compared to human\nperformance. Through Q-Bench-Video, we seek to catalyze community interest,\nstimulate further research, and unlock the untapped potential of LMMs to close\nthe gap in video quality understanding.\n","authors":["Zicheng Zhang","Ziheng Jia","Haoning Wu","Chunyi Li","Zijian Chen","Yingjie Zhou","Wei Sun","Xiaohong Liu","Xiongkuo Min","Weisi Lin","Guangtao Zhai"],"pdf_url":"https://arxiv.org/pdf/2409.20063v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.08470v2","updated":"2025-03-02T11:52:31Z","published":"2024-11-13T09:42:12Z","title":"HyperFace: Generating Synthetic Face Recognition Datasets by Exploring\n  Face Embedding Hypersphere","summary":"  Face recognition datasets are often collected by crawling Internet and\nwithout individuals' consents, raising ethical and privacy concerns. Generating\nsynthetic datasets for training face recognition models has emerged as a\npromising alternative. However, the generation of synthetic datasets remains\nchallenging as it entails adequate inter-class and intra-class variations.\nWhile advances in generative models have made it easier to increase intra-class\nvariations in face datasets (such as pose, illumination, etc.), generating\nsufficient inter-class variation is still a difficult task. In this paper, we\nformulate the dataset generation as a packing problem on the embedding space\n(represented on a hypersphere) of a face recognition model and propose a new\nsynthetic dataset generation approach, called HyperFace. We formalize our\npacking problem as an optimization problem and solve it with a gradient\ndescent-based approach. Then, we use a conditional face generator model to\nsynthesize face images from the optimized embeddings. We use our generated\ndatasets to train face recognition models and evaluate the trained models on\nseveral benchmarking real datasets. Our experimental results show that models\ntrained with HyperFace achieve state-of-the-art performance in training face\nrecognition using synthetic datasets.\n","authors":["Hatef Otroshi Shahreza","Sébastien Marcel"],"pdf_url":"https://arxiv.org/pdf/2411.08470v2.pdf","comment":"Accepted in ICLR 2025"},{"id":"http://arxiv.org/abs/2408.09886v3","updated":"2025-03-02T11:32:04Z","published":"2024-08-19T11:01:00Z","title":"Improved Baselines with Synchronized Encoding for Universal Medical\n  Image Segmentation","summary":"  Large foundation models, known for their strong zero-shot generalization\ncapabilities, can be applied to a wide range of downstream tasks. However,\ndeveloping foundation models for medical image segmentation poses a significant\nchallenge due to the domain gap between natural and medical images. While\nfine-tuning techniques based on the Segment Anything Model (SAM) have been\nexplored, they primarily focus on scaling up data or refining inference\nstrategies without incorporating domain-specific architectural designs,\nlimiting their zero-shot performance. To optimize segmentation performance\nunder standard inference settings and provide a strong baseline for future\nresearch, we introduce SyncSAM, which employs a synchronized dual-branch\nencoder that integrates convolution and Transformer features in a synchronized\nmanner to enhance medical image encoding, and a multi-scale dual-branch decoder\nto preserve image details. SyncSAM is trained on two of the largest medical\nimage segmentation datasets, SA-Med2D-20M and IMed-361M, resulting in a series\nof pre-trained models for universal medical image segmentation. Experimental\nresults demonstrate that SyncSAM not only achieves state-of-the-art performance\non test sets but also exhibits strong zero-shot capabilities on unseen\ndatasets. The code and model weights are available at\nhttps://github.com/Hhankyangg/SyncSAM.\n","authors":["Sihan Yang","Xuande Mi","Jiadong Feng","Haixia Bi","Hai Zhang","Jian Sun"],"pdf_url":"https://arxiv.org/pdf/2408.09886v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15445v2","updated":"2025-03-02T11:16:08Z","published":"2025-01-26T08:22:44Z","title":"StochSync: Stochastic Diffusion Synchronization for Image Generation in\n  Arbitrary Spaces","summary":"  We propose a zero-shot method for generating images in arbitrary spaces\n(e.g., a sphere for 360{\\deg} panoramas and a mesh surface for texture) using a\npretrained image diffusion model. The zero-shot generation of various visual\ncontent using a pretrained image diffusion model has been explored mainly in\ntwo directions. First, Diffusion Synchronization-performing reverse diffusion\nprocesses jointly across different projected spaces while synchronizing them in\nthe target space-generates high-quality outputs when enough conditioning is\nprovided, but it struggles in its absence. Second, Score Distillation\nSampling-gradually updating the target space data through gradient\ndescent-results in better coherence but often lacks detail. In this paper, we\nreveal for the first time the interconnection between these two methods while\nhighlighting their differences. To this end, we propose StochSync, a novel\napproach that combines the strengths of both, enabling effective performance\nwith weak conditioning. Our experiments demonstrate that StochSync provides the\nbest performance in 360{\\deg} panorama generation (where image conditioning is\nnot given), outperforming previous finetuning-based methods, and also delivers\ncomparable results in 3D mesh texturing (where depth conditioning is provided)\nwith previous methods.\n","authors":["Kyeongmin Yeo","Jaihoon Kim","Minhyuk Sung"],"pdf_url":"https://arxiv.org/pdf/2501.15445v2.pdf","comment":"Project page: https://stochsync.github.io/ (ICLR 2025)"},{"id":"http://arxiv.org/abs/2410.05260v2","updated":"2025-03-02T10:58:06Z","published":"2024-10-07T17:58:22Z","title":"DartControl: A Diffusion-Based Autoregressive Motion Model for Real-Time\n  Text-Driven Motion Control","summary":"  Text-conditioned human motion generation, which allows for user interaction\nthrough natural language, has become increasingly popular. Existing methods\ntypically generate short, isolated motions based on a single input sentence.\nHowever, human motions are continuous and can extend over long periods,\ncarrying rich semantics. Creating long, complex motions that precisely respond\nto streams of text descriptions, particularly in an online and real-time\nsetting, remains a significant challenge. Furthermore, incorporating spatial\nconstraints into text-conditioned motion generation presents additional\nchallenges, as it requires aligning the motion semantics specified by text\ndescriptions with geometric information, such as goal locations and 3D scene\ngeometry. To address these limitations, we propose DartControl, in short DART,\na Diffusion-based Autoregressive motion primitive model for Real-time\nText-driven motion control. Our model effectively learns a compact motion\nprimitive space jointly conditioned on motion history and text inputs using\nlatent diffusion models. By autoregressively generating motion primitives based\non the preceding history and current text input, DART enables real-time,\nsequential motion generation driven by natural language descriptions.\nAdditionally, the learned motion primitive space allows for precise spatial\nmotion control, which we formulate either as a latent noise optimization\nproblem or as a Markov decision process addressed through reinforcement\nlearning. We present effective algorithms for both approaches, demonstrating\nour model's versatility and superior performance in various motion synthesis\ntasks. Experiments show our method outperforms existing baselines in motion\nrealism, efficiency, and controllability. Video results are available on the\nproject page: https://zkf1997.github.io/DART/.\n","authors":["Kaifeng Zhao","Gen Li","Siyu Tang"],"pdf_url":"https://arxiv.org/pdf/2410.05260v2.pdf","comment":"Updated ICLR camera ready version"},{"id":"http://arxiv.org/abs/2402.04236v3","updated":"2025-03-02T09:39:57Z","published":"2024-02-06T18:43:48Z","title":"CogCoM: A Visual Language Model with Chain-of-Manipulations Reasoning","summary":"  Vision-Language Models (VLMs) have demonstrated their broad effectiveness\nthanks to extensive training in aligning visual instructions to responses.\nHowever, such training of conclusive alignment leads models to ignore essential\nvisual reasoning, further resulting in failures in meticulous visual problems\nand unfaithful responses. Drawing inspiration from human cognition in solving\nvisual problems (e.g., marking, zoom in), this paper introduces Chain of\nManipulations, a mechanism that enables VLMs to solve problems step-by-step\nwith evidence. After training, models can solve various visual problems by\neliciting intrinsic manipulations (e.g., grounding, zoom in) with results\n(e.g., boxes, image) actively without involving external tools, while also\nallowing users to trace error causes. We study the roadmap to implement this\nmechanism, including (1) a flexible design of manipulations upon extensive\nanalysis, (2) an efficient automated data generation pipeline, (3) a compatible\nVLM architecture capable of multi-turn multi-image, and (4) a model training\nprocess for versatile capabilities. With the design, we also manually annotate\n6K high-quality samples for the challenging graphical mathematical problems.\nOur trained model, \\textbf{CogCoM}, equipped with this mechanism with 17B\nparameters achieves state-of-the-art performance across 9 benchmarks from 4\ncategories, demonstrating the effectiveness while preserving the\ninterpretability. Our code, model weights, and collected data are publicly\navailable at https://github.com/THUDM/CogCoM.\n","authors":["Ji Qi","Ming Ding","Weihan Wang","Yushi Bai","Qingsong Lv","Wenyi Hong","Bin Xu","Lei Hou","Juanzi Li","Yuxiao Dong","Jie Tang"],"pdf_url":"https://arxiv.org/pdf/2402.04236v3.pdf","comment":"21 pages, 10 figures"},{"id":"http://arxiv.org/abs/2502.18176v2","updated":"2025-03-02T09:22:47Z","published":"2025-02-25T13:09:34Z","title":"CLIPure: Purification in Latent Space via CLIP for Adversarially Robust\n  Zero-Shot Classification","summary":"  In this paper, we aim to build an adversarially robust zero-shot image\nclassifier. We ground our work on CLIP, a vision-language pre-trained encoder\nmodel that can perform zero-shot classification by matching an image with text\nprompts ``a photo of a <class-name>.''. Purification is the path we choose\nsince it does not require adversarial training on specific attack types and\nthus can cope with any foreseen attacks. We then formulate purification risk as\nthe KL divergence between the joint distributions of the purification process\nof denoising the adversarial samples and the attack process of adding\nperturbations to benign samples, through bidirectional Stochastic Differential\nEquations (SDEs). The final derived results inspire us to explore purification\nin the multi-modal latent space of CLIP. We propose two variants for our\nCLIPure approach: CLIPure-Diff which models the likelihood of images' latent\nvectors with the DiffusionPrior module in DaLLE-2 (modeling the generation\nprocess of CLIP's latent vectors), and CLIPure-Cos which models the likelihood\nwith the cosine similarity between the embeddings of an image and ``a photo of\na.''. As far as we know, CLIPure is the first purification method in\nmulti-modal latent space and CLIPure-Cos is the first purification method that\nis not based on generative models, which substantially improves defense\nefficiency. We conducted extensive experiments on CIFAR-10, ImageNet, and 13\ndatasets that previous CLIP-based defense methods used for evaluating zero-shot\nclassification robustness. Results show that CLIPure boosts the SOTA robustness\nby a large margin, e.g., from 71.7% to 91.1% on CIFAR10, from 59.6% to 72.6% on\nImageNet, and 108% relative improvements of average robustness on the 13\ndatasets over previous SOTA. The code is available at\nhttps://github.com/TMLResearchGroup-CAS/CLIPure.\n","authors":["Mingkun Zhang","Keping Bi","Wei Chen","Jiafeng Guo","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.18176v2.pdf","comment":"accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2412.09765v2","updated":"2025-03-02T09:21:27Z","published":"2024-12-12T23:57:01Z","title":"L-WISE: Boosting Human Visual Category Learning Through Model-Based\n  Image Selection And Enhancement","summary":"  The currently leading artificial neural network models of the visual ventral\nstream - which are derived from a combination of performance optimization and\nrobustification methods - have demonstrated a remarkable degree of behavioral\nalignment with humans on visual categorization tasks. We show that image\nperturbations generated by these models can enhance the ability of humans to\naccurately report the ground truth class. Furthermore, we find that the same\nmodels can also be used out-of-the-box to predict the proportion of correct\nhuman responses to individual images, providing a simple, human-aligned\nestimator of the relative difficulty of each image. Motivated by these\nobservations, we propose to augment visual learning in humans in a way that\nimproves human categorization accuracy at test time. Our learning augmentation\napproach consists of (i) selecting images based on their model-estimated\nrecognition difficulty, and (ii) applying image perturbations that aid\nrecognition for novice learners. We find that combining these model-based\nstrategies leads to categorization accuracy gains of 33-72% relative to control\nsubjects without these interventions, on unmodified, randomly selected held-out\ntest images. Beyond the accuracy gain, the training time for the augmented\nlearning group was also shortened by 20-23%, despite both groups completing the\nsame number of training trials. We demonstrate the efficacy of our approach in\na fine-grained categorization task with natural images, as well as two tasks in\nclinically relevant image domains - histology and dermoscopy - where visual\nlearning is notoriously challenging. To the best of our knowledge, our work is\nthe first application of artificial neural networks to increase visual learning\nperformance in humans by enhancing category-specific image features.\n","authors":["Morgan B. Talbot","Gabriel Kreiman","James J. DiCarlo","Guy Gaziv"],"pdf_url":"https://arxiv.org/pdf/2412.09765v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.06751v2","updated":"2025-03-02T09:10:13Z","published":"2025-01-12T08:36:38Z","title":"Padding Tone: A Mechanistic Analysis of Padding Tokens in T2I Models","summary":"  Text-to-image (T2I) diffusion models rely on encoded prompts to guide the\nimage generation process. Typically, these prompts are extended to a fixed\nlength by adding padding tokens before text encoding. Despite being a default\npractice, the influence of padding tokens on the image generation process has\nnot been investigated. In this work, we conduct the first in-depth analysis of\nthe role padding tokens play in T2I models. We develop two causal techniques to\nanalyze how information is encoded in the representation of tokens across\ndifferent components of the T2I pipeline. Using these techniques, we\ninvestigate when and how padding tokens impact the image generation process.\nOur findings reveal three distinct scenarios: padding tokens may affect the\nmodel's output during text encoding, during the diffusion process, or be\neffectively ignored. Moreover, we identify key relationships between these\nscenarios and the model's architecture (cross or self-attention) and its\ntraining process (frozen or trained text encoder). These insights contribute to\na deeper understanding of the mechanisms of padding tokens, potentially\ninforming future model design and training practices in T2I systems.\n","authors":["Michael Toker","Ido Galil","Hadas Orgad","Rinon Gal","Yoad Tewel","Gal Chechik","Yonatan Belinkov"],"pdf_url":"https://arxiv.org/pdf/2501.06751v2.pdf","comment":"Published in: NAACL 2025. Project webpage:\n  https://padding-tone.github.io/"},{"id":"http://arxiv.org/abs/2410.06614v2","updated":"2025-03-02T08:59:29Z","published":"2024-10-09T07:09:46Z","title":"Pair-VPR: Place-Aware Pre-training and Contrastive Pair Classification\n  for Visual Place Recognition with Vision Transformers","summary":"  In this work we propose a novel joint training method for Visual Place\nRecognition (VPR), which simultaneously learns a global descriptor and a pair\nclassifier for re-ranking. The pair classifier can predict whether a given pair\nof images are from the same place or not. The network only comprises Vision\nTransformer components for both the encoder and the pair classifier, and both\ncomponents are trained using their respective class tokens. In existing VPR\nmethods, typically the network is initialized using pre-trained weights from a\ngeneric image dataset such as ImageNet. In this work we propose an alternative\npre-training strategy, by using Siamese Masked Image Modelling as a\npre-training task. We propose a Place-aware image sampling procedure from a\ncollection of large VPR datasets for pre-training our model, to learn visual\nfeatures tuned specifically for VPR. By re-using the Mask Image Modelling\nencoder and decoder weights in the second stage of training, Pair-VPR can\nachieve state-of-the-art VPR performance across five benchmark datasets with a\nViT-B encoder, along with further improvements in localization recall with\nlarger encoders. The Pair-VPR website is:\nhttps://csiro-robotics.github.io/Pair-VPR.\n","authors":["Stephen Hausler","Peyman Moghadam"],"pdf_url":"https://arxiv.org/pdf/2410.06614v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20092v2","updated":"2025-03-02T08:56:15Z","published":"2025-02-27T13:51:56Z","title":"WalnutData: A UAV Remote Sensing Dataset of Green Walnuts and Model\n  Evaluation","summary":"  The UAV technology is gradually maturing and can provide extremely powerful\nsupport for smart agriculture and precise monitoring. Currently, there is no\ndataset related to green walnuts in the field of agricultural computer vision.\nThus, in order to promote the algorithm design in the field of agricultural\ncomputer vision, we used UAV to collect remote-sensing data from 8 walnut\nsample plots. Considering that green walnuts are subject to various lighting\nconditions and occlusion, we constructed a large-scale dataset with a\nhigher-granularity of target features - WalnutData. This dataset contains a\ntotal of 30,240 images and 706,208 instances, and there are 4 target\ncategories: being illuminated by frontal light and unoccluded (A1), being\nbacklit and unoccluded (A2), being illuminated by frontal light and occluded\n(B1), and being backlit and occluded (B2). Subsequently, we evaluated many\nmainstream algorithms on WalnutData and used these evaluation results as the\nbaseline standard. The dataset and all evaluation results can be obtained at\nhttps://github.com/1wuming/WalnutData.\n","authors":["Mingjie Wu","Chenggui Yang","Huihua Wang","Chen Xue","Yibo Wang","Haoyu Wang","Yansong Wang","Can Peng","Yuqi Han","Ruoyu Li","Lijun Yun","Zaiqing Chen","Songfan Shi","Luhao Fang","Shuyi Wan","Tingfeng Li","Shuangyao Liu","Haotian Feng"],"pdf_url":"https://arxiv.org/pdf/2502.20092v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.14808v2","updated":"2025-03-02T08:53:47Z","published":"2024-11-22T09:08:58Z","title":"High-Resolution Image Synthesis via Next-Token Prediction","summary":"  Recently, autoregressive models have demonstrated remarkable performance in\nclass-conditional image generation. However, the application of next-token\nprediction to high-resolution text-to-image generation remains largely\nunexplored. In this paper, we introduce \\textbf{D-JEPA$\\cdot$T2I}, an\nautoregressive model based on continuous tokens that incorporates innovations\nin both architecture and training strategy to generate high-quality,\nphotorealistic images at arbitrary resolutions, up to 4K. Architecturally, we\nadopt the denoising joint embedding predictive architecture (D-JEPA) while\nleveraging a multimodal visual transformer to effectively integrate textual and\nvisual features. Additionally, we introduce flow matching loss alongside the\nproposed Visual Rotary Positional Embedding (VoPE) to enable continuous\nresolution learning. In terms of training strategy, we propose a data feedback\nmechanism that dynamically adjusts the sampling procedure based on statistical\nanalysis and an online learning critic model. This encourages the model to move\nbeyond its comfort zone, reducing redundant training on well-mastered scenarios\nand compelling it to address more challenging cases with suboptimal generation\nquality. For the first time, we achieve state-of-the-art high-resolution image\nsynthesis via next-token prediction.\n","authors":["Dengsheng Chen","Jie Hu","Tiezhu Yue","Xiaoming Wei","Enhua Wu"],"pdf_url":"https://arxiv.org/pdf/2411.14808v2.pdf","comment":"31 pages"},{"id":"http://arxiv.org/abs/2411.03990v2","updated":"2025-03-02T08:11:34Z","published":"2024-11-06T15:30:42Z","title":"ET-SEED: Efficient Trajectory-Level SE(3) Equivariant Diffusion Policy","summary":"  Imitation learning, e.g., diffusion policy, has been proven effective in\nvarious robotic manipulation tasks. However, extensive demonstrations are\nrequired for policy robustness and generalization. To reduce the demonstration\nreliance, we leverage spatial symmetry and propose ET-SEED, an efficient\ntrajectory-level SE(3) equivariant diffusion model for generating action\nsequences in complex robot manipulation tasks. Further, previous equivariant\ndiffusion models require the per-step equivariance in the Markov process,\nmaking it difficult to learn policy under such strong constraints. We\ntheoretically extend equivariant Markov kernels and simplify the condition of\nequivariant diffusion process, thereby significantly improving training\nefficiency for trajectory-level SE(3) equivariant diffusion policy in an\nend-to-end manner. We evaluate ET-SEED on representative robotic manipulation\ntasks, involving rigid body, articulated and deformable object. Experiments\ndemonstrate superior data efficiency and manipulation proficiency of our\nproposed method, as well as its ability to generalize to unseen configurations\nwith only a few demonstrations. Website: https://et-seed.github.io/\n","authors":["Chenrui Tie","Yue Chen","Ruihai Wu","Boxuan Dong","Zeyi Li","Chongkai Gao","Hao Dong"],"pdf_url":"https://arxiv.org/pdf/2411.03990v2.pdf","comment":"Accept to ICLR 2025"},{"id":"http://arxiv.org/abs/2412.14169v2","updated":"2025-03-02T08:09:39Z","published":"2024-12-18T18:59:53Z","title":"Autoregressive Video Generation without Vector Quantization","summary":"  This paper presents a novel approach that enables autoregressive video\ngeneration with high efficiency. We propose to reformulate the video generation\nproblem as a non-quantized autoregressive modeling of temporal frame-by-frame\nprediction and spatial set-by-set prediction. Unlike raster-scan prediction in\nprior autoregressive models or joint distribution modeling of fixed-length\ntokens in diffusion models, our approach maintains the causal property of\nGPT-style models for flexible in-context capabilities, while leveraging\nbidirectional modeling within individual frames for efficiency. With the\nproposed approach, we train a novel video autoregressive model without vector\nquantization, termed NOVA. Our results demonstrate that NOVA surpasses prior\nautoregressive video models in data efficiency, inference speed, visual\nfidelity, and video fluency, even with a much smaller model capacity, i.e.,\n0.6B parameters. NOVA also outperforms state-of-the-art image diffusion models\nin text-to-image generation tasks, with a significantly lower training cost.\nAdditionally, NOVA generalizes well across extended video durations and enables\ndiverse zero-shot applications in one unified model. Code and models are\npublicly available at https://github.com/baaivision/NOVA.\n","authors":["Haoge Deng","Ting Pan","Haiwen Diao","Zhengxiong Luo","Yufeng Cui","Huchuan Lu","Shiguang Shan","Yonggang Qi","Xinlong Wang"],"pdf_url":"https://arxiv.org/pdf/2412.14169v2.pdf","comment":"Accepted to ICLR 2025. Project page at\n  https://github.com/baaivision/NOVA"},{"id":"http://arxiv.org/abs/2404.14396v2","updated":"2025-03-02T07:53:44Z","published":"2024-04-22T17:56:09Z","title":"SEED-X: Multimodal Models with Unified Multi-granularity Comprehension\n  and Generation","summary":"  The rapid evolution of multimodal foundation model has demonstrated\nsignificant progresses in vision-language understanding and generation, e.g.,\nour previous work SEED-LLaMA. However, there remains a gap between its\ncapability and the real-world applicability, primarily due to the model's\nlimited capacity to effectively respond to various user instructions and\ninteract with diverse visual data. In this work, we focus on bridging this gap\nthrough integrating two enhanced features: (1) comprehending images of\narbitrary sizes and ratios, and (2) enabling multi-granularity image\ngeneration. We present a unified and versatile foundation model, namely,\nSEED-X, which is able to model multi-granularity visual semantics for\ncomprehension and generation tasks. Besides the competitive results on public\nbenchmarks, SEED-X demonstrates its effectiveness in handling real-world\napplications across various domains after instruction tuning. We hope that our\nwork will inspire future research into what can be achieved by versatile\nmultimodal foundation models in real-world applications. The models, codes, and\ndatasets are released in https://github.com/AILab-CVC/SEED-X.\n","authors":["Yuying Ge","Sijie Zhao","Jinguo Zhu","Yixiao Ge","Kun Yi","Lin Song","Chen Li","Xiaohan Ding","Ying Shan"],"pdf_url":"https://arxiv.org/pdf/2404.14396v2.pdf","comment":"We added benchmark results (without updating models) and ablation\n  study in this version. Project released at:\n  https://github.com/AILab-CVC/SEED-X"},{"id":"http://arxiv.org/abs/2405.20986v2","updated":"2025-03-02T07:46:05Z","published":"2024-05-31T16:32:46Z","title":"Predictive Uncertainty Quantification for Bird's Eye View Segmentation:\n  A Benchmark and Novel Loss Function","summary":"  The fusion of raw sensor data to create a Bird's Eye View (BEV)\nrepresentation is critical for autonomous vehicle planning and control. Despite\nthe growing interest in using deep learning models for BEV semantic\nsegmentation, anticipating segmentation errors and enhancing the explainability\nof these models remain underexplored. This paper introduces a comprehensive\nbenchmark for predictive uncertainty quantification in BEV segmentation,\nevaluating multiple uncertainty quantification methods across three popular\ndatasets with three representative network architectures. Our study focuses on\nthe effectiveness of quantified uncertainty in detecting misclassified and\nout-of-distribution (OOD) pixels while also improving model calibration.\nThrough empirical analysis, we uncover challenges in existing uncertainty\nquantification methods and demonstrate the potential of evidential deep\nlearning techniques, which capture both aleatoric and epistemic uncertainty. To\naddress these challenges, we propose a novel loss function,\nUncertainty-Focal-Cross-Entropy (UFCE), specifically designed for highly\nimbalanced data, along with a simple uncertainty-scaling regularization term\nthat improves both uncertainty quantification and model calibration for BEV\nsegmentation.\n","authors":["Linlin Yu","Bowen Yang","Tianhao Wang","Kangshuo Li","Feng Chen"],"pdf_url":"https://arxiv.org/pdf/2405.20986v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2410.03355v3","updated":"2025-03-02T07:45:09Z","published":"2024-10-04T12:21:03Z","title":"LANTERN: Accelerating Visual Autoregressive Models with Relaxed\n  Speculative Decoding","summary":"  Auto-Regressive (AR) models have recently gained prominence in image\ngeneration, often matching or even surpassing the performance of diffusion\nmodels. However, one major limitation of AR models is their sequential nature,\nwhich processes tokens one at a time, slowing down generation compared to\nmodels like GANs or diffusion-based methods that operate more efficiently.\nWhile speculative decoding has proven effective for accelerating LLMs by\ngenerating multiple tokens in a single forward, its application in visual AR\nmodels remains largely unexplored. In this work, we identify a challenge in\nthis setting, which we term \\textit{token selection ambiguity}, wherein visual\nAR models frequently assign uniformly low probabilities to tokens, hampering\nthe performance of speculative decoding. To overcome this challenge, we propose\na relaxed acceptance condition referred to as LANTERN that leverages the\ninterchangeability of tokens in latent space. This relaxation restores the\neffectiveness of speculative decoding in visual AR models by enabling more\nflexible use of candidate tokens that would otherwise be prematurely rejected.\nFurthermore, by incorporating a total variation distance bound, we ensure that\nthese speed gains are achieved without significantly compromising image quality\nor semantic coherence. Experimental results demonstrate the efficacy of our\nmethod in providing a substantial speed-up over speculative decoding. In\nspecific, compared to a na\\\"ive application of the state-of-the-art speculative\ndecoding, LANTERN increases speed-ups by $\\mathbf{1.75}\\times$ and\n$\\mathbf{1.82}\\times$, as compared to greedy decoding and random sampling,\nrespectively, when applied to LlamaGen, a contemporary visual AR model. The\ncode is publicly available at https://github.com/jadohu/LANTERN.\n","authors":["Doohyuk Jang","Sihwan Park","June Yong Yang","Yeonsung Jung","Jihun Yun","Souvik Kundu","Sung-Yub Kim","Eunho Yang"],"pdf_url":"https://arxiv.org/pdf/2410.03355v3.pdf","comment":"30 pages, 13 figures, Accepted to ICLR 2025 (poster)"},{"id":"http://arxiv.org/abs/2410.10010v3","updated":"2025-03-02T07:42:20Z","published":"2024-10-13T21:11:04Z","title":"InterMask: 3D Human Interaction Generation via Collaborative Masked\n  Modeling","summary":"  Generating realistic 3D human-human interactions from textual descriptions\nremains a challenging task. Existing approaches, typically based on diffusion\nmodels, often produce results lacking realism and fidelity. In this work, we\nintroduce InterMask, a novel framework for generating human interactions using\ncollaborative masked modeling in discrete space. InterMask first employs a\nVQ-VAE to transform each motion sequence into a 2D discrete motion token map.\nUnlike traditional 1D VQ token maps, it better preserves fine-grained\nspatio-temporal details and promotes spatial awareness within each token.\nBuilding on this representation, InterMask utilizes a generative masked\nmodeling framework to collaboratively model the tokens of two interacting\nindividuals. This is achieved by employing a transformer architecture\nspecifically designed to capture complex spatio-temporal inter-dependencies.\nDuring training, it randomly masks the motion tokens of both individuals and\nlearns to predict them. For inference, starting from fully masked sequences, it\nprogressively fills in the tokens for both individuals. With its enhanced\nmotion representation, dedicated architecture, and effective learning strategy,\nInterMask achieves state-of-the-art results, producing high-fidelity and\ndiverse human interactions. It outperforms previous methods, achieving an FID\nof $5.154$ (vs $5.535$ of in2IN) on the InterHuman dataset and $0.399$ (vs\n$5.207$ of InterGen) on the InterX dataset. Additionally, InterMask seamlessly\nsupports reaction generation without the need for model redesign or\nfine-tuning.\n","authors":["Muhammad Gohar Javed","Chuan Guo","Li Cheng","Xingyu Li"],"pdf_url":"https://arxiv.org/pdf/2410.10010v3.pdf","comment":"Project webpage: https://gohar-malik.github.io/intermask"},{"id":"http://arxiv.org/abs/2409.19835v2","updated":"2025-03-02T07:32:50Z","published":"2024-09-30T00:17:00Z","title":"MoCoLSK: Modality Conditioned High-Resolution Downscaling for Land\n  Surface Temperature","summary":"  Land Surface Temperature (LST) is a critical parameter for environmental\nstudies, but directly obtaining high spatial resolution LST data remains\nchallenging due to the spatio-temporal trade-off in satellite remote sensing.\nGuided LST downscaling has emerged as an alternative solution to overcome these\nlimitations, but current methods often neglect spatial non-stationarity, and\nthere is a lack of an open-source ecosystem for deep learning methods. In this\npaper, we propose the Modality-Conditional Large Selective Kernel (MoCoLSK)\nNetwork, a novel architecture that dynamically fuses multi-modal data through\nmodality-conditioned projections. MoCoLSK achieves a confluence of dynamic\nreceptive field adjustment and multi-modal feature fusion, leading to enhanced\nLST prediction accuracy. Furthermore, we establish the GrokLST project, a\ncomprehensive open-source ecosystem featuring the GrokLST dataset, a\nhigh-resolution benchmark, and the GrokLST toolkit, an open-source\nPyTorch-based toolkit encapsulating MoCoLSK alongside 40+ state-of-the-art\napproaches. Extensive experimental results validate MoCoLSK's effectiveness in\ncapturing complex dependencies and subtle variations within multispectral data,\noutperforming existing methods in LST downscaling. Our code, dataset, and\ntoolkit are available at https://github.com/GrokCV/GrokLST.\n","authors":["Qun Dai","Chunyang Yuan","Yimian Dai","Yuxuan Li","Xiang Li","Kang Ni","Jianhui Xu","Xiangbo Shu","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2409.19835v2.pdf","comment":"Accepted by IEEE TGRS"},{"id":"http://arxiv.org/abs/2502.10982v3","updated":"2025-03-02T07:31:57Z","published":"2025-02-16T04:00:06Z","title":"TEASER: Token Enhanced Spatial Modeling for Expressions Reconstruction","summary":"  3D facial reconstruction from a single in-the-wild image is a crucial task in\nhuman-centered computer vision tasks. While existing methods can recover\naccurate facial shapes, there remains significant space for improvement in\nfine-grained expression capture. Current approaches struggle with irregular\nmouth shapes, exaggerated expressions, and asymmetrical facial movements. We\npresent TEASER (Token EnhAnced Spatial modeling for Expressions\nReconstruction), which addresses these challenges and enhances 3D facial\ngeometry performance. TEASER tackles two main limitations of existing methods:\ninsufficient photometric loss for self-reconstruction and inaccurate\nlocalization of subtle expressions. We introduce a multi-scale tokenizer to\nextract facial appearance information. Combined with a neural renderer, these\ntokens provide precise geometric guidance for expression reconstruction.\nFurthermore, TEASER incorporates a pose-dependent landmark loss to further\nimprove geometric performances. Our approach not only significantly enhances\nexpression reconstruction quality but also offers interpretable tokens suitable\nfor various downstream applications, such as photorealistic facial video\ndriving, expression transfer, and identity swapping. Quantitative and\nqualitative experimental results across multiple datasets demonstrate that\nTEASER achieves state-of-the-art performance in precise expression\nreconstruction.\n","authors":["Yunfei Liu","Lei Zhu","Lijian Lin","Ye Zhu","Ailing Zhang","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2502.10982v3.pdf","comment":"Accepted by ICLR 2025, code and demos are available at\n  https://tinyurl.com/TEASER-project"},{"id":"http://arxiv.org/abs/2501.19069v2","updated":"2025-03-02T07:22:57Z","published":"2025-01-31T11:55:17Z","title":"Improving vision-language alignment with graph spiking hybrid Networks","summary":"  To bridge the semantic gap between vision and language (VL), it is necessary\nto develop a good alignment strategy, which includes handling semantic\ndiversity, abstract representation of visual information, and generalization\nability of models. Recent works use detector-based bounding boxes or patches\nwith regular partitions to represent visual semantics. While current paradigms\nhave made strides, they are still insufficient for fully capturing the nuanced\ncontextual relations among various objects. This paper proposes a comprehensive\nvisual semantic representation module, necessitating the utilization of\npanoptic segmentation to generate coherent fine-grained semantic features.\nFurthermore, we propose a novel Graph Spiking Hybrid Network (GSHN) that\nintegrates the complementary advantages of Spiking Neural Networks (SNNs) and\nGraph Attention Networks (GATs) to encode visual semantic information.\nIntriguingly, the model not only encodes the discrete and continuous latent\nvariables of instances but also adeptly captures both local and global\ncontextual features, thereby significantly enhancing the richness and diversity\nof semantic representations. Leveraging the spatiotemporal properties inherent\nin SNNs, we employ contrastive learning (CL) to enhance the similarity-based\nrepresentation of embeddings. This strategy alleviates the computational\noverhead of the model and enriches meaningful visual representations by\nconstructing positive and negative sample pairs. We design an innovative\npre-training method, Spiked Text Learning (STL), which uses text features to\nimprove the encoding ability of discrete semantics. Experiments show that the\nproposed GSHN exhibits promising results on multiple VL downstream tasks.\n","authors":["Siyu Zhang","Wenzhe Liu","Yeming Chen","Yiming Wu","Heming Zheng","Cheng Cheng"],"pdf_url":"https://arxiv.org/pdf/2501.19069v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11817v2","updated":"2025-03-02T07:05:19Z","published":"2024-10-15T17:46:31Z","title":"Improving Long-Text Alignment for Text-to-Image Diffusion Models","summary":"  The rapid advancement of text-to-image (T2I) diffusion models has enabled\nthem to generate unprecedented results from given texts. However, as text\ninputs become longer, existing encoding methods like CLIP face limitations, and\naligning the generated images with long texts becomes challenging. To tackle\nthese issues, we propose LongAlign, which includes a segment-level encoding\nmethod for processing long texts and a decomposed preference optimization\nmethod for effective alignment training. For segment-level encoding, long texts\nare divided into multiple segments and processed separately. This method\novercomes the maximum input length limits of pretrained encoding models. For\npreference optimization, we provide decomposed CLIP-based preference models to\nfine-tune diffusion models. Specifically, to utilize CLIP-based preference\nmodels for T2I alignment, we delve into their scoring mechanisms and find that\nthe preference scores can be decomposed into two components: a text-relevant\npart that measures T2I alignment and a text-irrelevant part that assesses other\nvisual aspects of human preference. Additionally, we find that the\ntext-irrelevant part contributes to a common overfitting problem during\nfine-tuning. To address this, we propose a reweighting strategy that assigns\ndifferent weights to these two components, thereby reducing overfitting and\nenhancing alignment. After fine-tuning $512 \\times 512$ Stable Diffusion (SD)\nv1.5 for about 20 hours using our method, the fine-tuned SD outperforms\nstronger foundation models in T2I alignment, such as PixArt-$\\alpha$ and\nKandinsky v2.2. The code is available at\nhttps://github.com/luping-liu/LongAlign.\n","authors":["Luping Liu","Chao Du","Tianyu Pang","Zehan Wang","Chongxuan Li","Dong Xu"],"pdf_url":"https://arxiv.org/pdf/2410.11817v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.03836v3","updated":"2025-03-02T06:41:56Z","published":"2025-01-07T14:45:39Z","title":"SCC-YOLO: An Improved Object Detector for Assisting in Brain Tumor\n  Diagnosis","summary":"  Brain tumors can lead to neurological dysfunction, cognitive and\npsychological changes, increased intracranial pressure, and seizures, posing\nsignificant risks to health. The You Only Look Once (YOLO) series has shown\nsuperior accuracy in medical imaging object detection. This paper presents a\nnovel SCC-YOLO architecture that integrates the SCConv module into YOLOv9. The\nSCConv module optimizes convolutional efficiency by reducing spatial and\nchannel redundancy, enhancing image feature learning. We examine the effects of\ndifferent attention mechanisms with YOLOv9 for brain tumor detection using the\nBr35H dataset and our custom dataset (Brain_Tumor_Dataset). Results indicate\nthat SCC-YOLO improved mAP50 by 0.3% on the Br35H dataset and by 0.5% on our\ncustom dataset compared to YOLOv9. SCC-YOLO achieves state-of-the-art\nperformance in brain tumor detection.\n","authors":["Runci Bai","Guibao Xu","Yanze Shi"],"pdf_url":"https://arxiv.org/pdf/2501.03836v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.03173v2","updated":"2025-03-02T06:24:05Z","published":"2024-12-04T09:53:09Z","title":"IRisPath: Enhancing Costmap for Off-Road Navigation with Robust IR-RGB\n  Fusion for Improved Day and Night Traversability","summary":"  Autonomous off-road navigation is required for applications in agriculture,\nconstruction, search and rescue and defence. Traditional on-road autonomous\nmethods struggle with dynamic terrains, leading to poor vehicle control in\noff-road conditions. Recent deep-learning models have used perception sensors\nalong with kinesthetic feedback for navigation on such terrains. However, this\napproach has out-of-domain uncertainty. Factors like change in time of day and\nweather impacts the performance of the model. We propose a multi modal fusion\nnetwork \"IRisPath\" capable of using Thermal and RGB images to provide\nrobustness against dynamic weather and light conditions. To aid further works\nin this domain, we also open-source a day-night dataset with Thermal and RGB\nimages along with pseudo-labels for traversability. In order to co-register for\nfusion model we also develop a novel method for targetless extrinsic\ncalibration of Thermal, LiDAR and RGB cameras with translation accuracy of\n+/-1.7cm and rotation accuracy of +/-0.827degrees.\n","authors":["Saksham Sharma","Akshit Raizada","Suresh Sundaram"],"pdf_url":"https://arxiv.org/pdf/2412.03173v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2306.12033v2","updated":"2025-03-02T06:19:58Z","published":"2023-06-21T05:48:51Z","title":"End-to-End Augmentation Hyperparameter Tuning for Self-Supervised\n  Anomaly Detection","summary":"  Self-supervised learning (SSL) has emerged as a promising paradigm that\npresents supervisory signals to real-world problems, bypassing the extensive\ncost of manual labeling. Consequently, self-supervised anomaly detection (SSAD)\nhas seen a recent surge of interest, since SSL is especially attractive for\nunsupervised tasks. However, recent works have reported that the choice of a\ndata augmentation function has significant impact on the accuracy of SSAD,\nposing augmentation search as an essential but nontrivial problem with the lack\nof labeled validation data. In this paper, we introduce ST-SSAD, the first\nsystematic approach for rigorous augmentation tuning on SSAD. To this end, our\nwork presents two key contributions. The first is a new unsupervised validation\nloss that quantifies the alignment between augmented training data and\nunlabeled validation data. The second is new differentiable augmentation\nfunctions, allowing data augmentation hyperparameter(s) to be tuned in an\nend-to-end manner. Experiments on two testbeds with semantic class anomalies\nand subtle industrial defects show that ST-SSAD gives significant performance\ngains over existing works.\n","authors":["Jaemin Yoo","Lingxiao Zhao","Leman Akoglu"],"pdf_url":"https://arxiv.org/pdf/2306.12033v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03051v3","updated":"2025-03-02T06:17:12Z","published":"2024-10-04T00:13:54Z","title":"AuroraCap: Efficient, Performant Video Detailed Captioning and a New\n  Benchmark","summary":"  Video detailed captioning is a key task which aims to generate comprehensive\nand coherent textual descriptions of video content, benefiting both video\nunderstanding and generation. In this paper, we propose AuroraCap, a video\ncaptioner based on a large multimodal model. We follow the simplest\narchitecture design without additional parameters for temporal modeling. To\naddress the overhead caused by lengthy video sequences, we implement the token\nmerging strategy, reducing the number of input visual tokens. Surprisingly, we\nfound that this strategy results in little performance loss. AuroraCap shows\nsuperior performance on various video and image captioning benchmarks, for\nexample, obtaining a CIDEr of 88.9 on Flickr30k, beating GPT-4V (55.3) and\nGemini-1.5 Pro (82.2). However, existing video caption benchmarks only include\nsimple descriptions, consisting of a few dozen words, which limits research in\nthis field. Therefore, we develop VDC, a video detailed captioning benchmark\nwith over one thousand carefully annotated structured captions. In addition, we\npropose a new LLM-assisted metric VDCscore for bettering evaluation, which\nadopts a divide-and-conquer strategy to transform long caption evaluation into\nmultiple short question-answer pairs. With the help of human Elo ranking, our\nexperiments show that this benchmark better correlates with human judgments of\nvideo detailed captioning quality.\n","authors":["Wenhao Chai","Enxin Song","Yilun Du","Chenlin Meng","Vashisht Madhavan","Omer Bar-Tal","Jenq-Neng Hwang","Saining Xie","Christopher D. Manning"],"pdf_url":"https://arxiv.org/pdf/2410.03051v3.pdf","comment":"Accepted to ICLR 2025. Code, docs, weight, benchmark and training\n  data are all avaliable at https://rese1f.github.io/aurora-web/"},{"id":"http://arxiv.org/abs/2502.19260v2","updated":"2025-03-02T06:08:34Z","published":"2025-02-26T16:06:35Z","title":"EMT: A Visual Multi-Task Benchmark Dataset for Autonomous Driving in the\n  Arab Gulf Region","summary":"  This paper introduces the Emirates Multi-Task (EMT) dataset - the first\npublicly available dataset for autonomous driving collected in the Arab Gulf\nregion. The EMT dataset captures the unique road topology, high traffic\ncongestion, and distinctive characteristics of the Gulf region, including\nvariations in pedestrian clothing and weather conditions. It contains over\n30,000 frames from a dash-camera perspective, along with 570,000 annotated\nbounding boxes, covering approximately 150 kilometers of driving routes. The\nEMT dataset supports three primary tasks: tracking, trajectory forecasting and\nintention prediction. Each benchmark dataset is complemented with corresponding\nevaluations: (1) multi-agent tracking experiments, focusing on multi-class\nscenarios and occlusion handling; (2) trajectory forecasting evaluation using\ndeep sequential and interaction-aware models; and (3) intention benchmark\nexperiments conducted for predicting agents intentions from observed\ntrajectories. The dataset is publicly available at avlab.io/emt-dataset, and\npre-processing scripts along with evaluation models can be accessed at\ngithub.com/AV-Lab/emt-dataset.\n","authors":["Nadya Abdel Madjid","Murad Mebrahtu","Abdelmoamen Nasser","Bilal Hassan","Naoufel Werghi","Jorge Dias","Majid Khonji"],"pdf_url":"https://arxiv.org/pdf/2502.19260v2.pdf","comment":"19 pages, 6 figures"},{"id":"http://arxiv.org/abs/2502.08813v2","updated":"2025-03-02T05:50:08Z","published":"2025-02-12T21:55:26Z","title":"Measuring Anxiety Levels with Head Motion Patterns in Severe Depression\n  Population","summary":"  Depression and anxiety are prevalent mental health disorders that frequently\ncooccur, with anxiety significantly influencing both the manifestation and\ntreatment of depression. An accurate assessment of anxiety levels in\nindividuals with depression is crucial to develop effective and personalized\ntreatment plans. This study proposes a new noninvasive method for quantifying\nanxiety severity by analyzing head movements -- specifically speed,\nacceleration, and angular displacement -- during video-recorded interviews with\npatients suffering from severe depression. Using data from a new CALYPSO\nDepression Dataset, we extracted head motion characteristics and applied\nregression analysis to predict clinically evaluated anxiety levels. Our results\ndemonstrate a high level of precision, achieving a mean absolute error (MAE) of\n0.35 in predicting the severity of psychological anxiety based on head movement\npatterns. This indicates that our approach can enhance the understanding of\nanxiety's role in depression and assist psychiatrists in refining treatment\nstrategies for individuals.\n","authors":["Fouad Boutaleb","Emery Pierson","Nicolas Doudeau","Clémence Nineuil","Ali Amad","Mohamed Daoudi"],"pdf_url":"https://arxiv.org/pdf/2502.08813v2.pdf","comment":"19th IEEE International Conference on Automatic Face and Gesture\n  Recognition (FG), 2025"},{"id":"http://arxiv.org/abs/2411.01099v2","updated":"2025-03-02T05:33:33Z","published":"2024-11-02T01:31:47Z","title":"Few-Class Arena: A Benchmark for Efficient Selection of Vision Models\n  and Dataset Difficulty Measurement","summary":"  We propose Few-Class Arena (FCA), as a unified benchmark with focus on\ntesting efficient image classification models for few classes. A wide variety\nof benchmark datasets with many classes (80-1000) have been created to assist\nComputer Vision architectural evolution. An increasing number of vision models\nare evaluated with these many-class datasets. However, real-world applications\noften involve substantially fewer classes of interest (2-10). This gap between\nmany and few classes makes it difficult to predict performance of the few-class\napplications using models trained on the available many-class datasets. To\ndate, little has been offered to evaluate models in this Few-Class Regime. We\nconduct a systematic evaluation of the ResNet family trained on ImageNet\nsubsets from 2 to 1000 classes, and test a wide spectrum of Convolutional\nNeural Networks and Transformer architectures over ten datasets by using our\nnewly proposed FCA tool. Furthermore, to aid an up-front assessment of dataset\ndifficulty and a more efficient selection of models, we incorporate a\ndifficulty measure as a function of class similarity. FCA offers a new tool for\nefficient machine learning in the Few-Class Regime, with goals ranging from a\nnew efficient class similarity proposal, to lightweight model architecture\ndesign, to a new scaling law. FCA is user-friendly and can be easily extended\nto new models and datasets, facilitating future research work. Our benchmark is\navailable at https://github.com/bryanbocao/fca.\n","authors":["Bryan Bo Cao","Lawrence O'Gorman","Michael Coss","Shubham Jain"],"pdf_url":"https://arxiv.org/pdf/2411.01099v2.pdf","comment":"10 pages, 32 pages including References and Appendix, 19 figures, 8\n  tables"},{"id":"http://arxiv.org/abs/2410.03816v2","updated":"2025-03-02T05:29:42Z","published":"2024-10-04T18:47:49Z","title":"Modeling and Analysis of Spatial and Temporal Land Clutter Statistics in\n  SAR Imaging Based on MSTAR Data","summary":"  The statistical analysis of land clutter for Synthetic Aperture Radar (SAR)\nimaging has become an increasingly important subject for research and\ninvestigation. It is also absolutely necessary for designing robust algorithms\ncapable of performing the task of target detection in the background clutter.\nAny attempt to extract the energy of the desired targets from the land clutter\nrequires complete knowledge of the statistical properties of the background\nclutter. In this paper, the spatial as well as the temporal characteristics of\nthe land clutter are studied. Since the data for each image has been collected\nbased on a different aspect angle; therefore, the temporal analysis contains\nvariation in the aspect angle. Consequently, the temporal analysis includes the\ncharacteristics of the radar cross section with respect to the aspect angle\nbased on which the data has been collected. In order to perform the statistical\nanalysis, several well-known and relevant distributions, namely, Weibull,\nLog-normal, Gamma, and Rayleigh are considered as prime candidates to model the\nland clutter. The goodness-of-fit test is based on the Kullback-Leibler (KL)\nDivergence metric. The detailed analysis presented in this paper demonstrates\nthat the Weibull distribution is a more accurate fit for the\ntemporal-aspect-angle statistical analysis while the Rayleigh distribution\nmodels the spatial characteristics of the background clutter with higher\naccuracy. Finally, based on the aforementioned statistical analyses and by\nutilizing the Constant False Alarm Rate (CFAR) algorithm, we perform target\ndetection in land clutter. The overall verification of the analysis is\nperformed by exploiting the Moving and Stationary Target Acquisition and\nRecognition (MSTAR) data-set, which has been collected in spotlight mode at\nX-band, and the results are presented.\n","authors":["Shahrokh Hamidi"],"pdf_url":"https://arxiv.org/pdf/2410.03816v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2409.02155"},{"id":"http://arxiv.org/abs/2408.08258v3","updated":"2025-03-02T04:25:12Z","published":"2024-08-15T16:59:15Z","title":"Snuffy: Efficient Whole Slide Image Classifier","summary":"  Whole Slide Image (WSI) classification with multiple instance learning (MIL)\nin digital pathology faces significant computational challenges. Current\nmethods mostly rely on extensive self-supervised learning (SSL) for\nsatisfactory performance, requiring long training periods and considerable\ncomputational resources. At the same time, no pre-training affects performance\ndue to domain shifts from natural images to WSIs. We introduce Snuffy\narchitecture, a novel MIL-pooling method based on sparse transformers that\nmitigates performance loss with limited pre-training and enables continual\nfew-shot pre-training as a competitive option. Our sparsity pattern is tailored\nfor pathology and is theoretically proven to be a universal approximator with\nthe tightest probabilistic sharp bound on the number of layers for sparse\ntransformers, to date. We demonstrate Snuffy's effectiveness on CAMELYON16 and\nTCGA Lung cancer datasets, achieving superior WSI and patch-level accuracies.\nThe code is available on https://github.com/jafarinia/snuffy.\n","authors":["Hossein Jafarinia","Alireza Alipanah","Danial Hamdi","Saeed Razavi","Nahal Mirzaie","Mohammad Hossein Rohban"],"pdf_url":"https://arxiv.org/pdf/2408.08258v3.pdf","comment":"Accepted for ECCV 2024"},{"id":"http://arxiv.org/abs/2405.16071v2","updated":"2025-03-02T04:18:55Z","published":"2024-05-25T05:44:55Z","title":"DynRefer: Delving into Region-level Multimodal Tasks via Dynamic\n  Resolution","summary":"  One fundamental task of multimodal models is to translate referred image\nregions to human preferred language descriptions. Existing methods, however,\nignore the resolution adaptability needs of different tasks, which hinders them\nto find out precise language descriptions. In this study, we propose a DynRefer\napproach, to pursue high-accuracy region-level referring through mimicking the\nresolution adaptability of human visual cognition. During training, DynRefer\nstochastically aligns language descriptions of multimodal tasks with images of\nmultiple resolutions, which are constructed by nesting a set of random views\naround the referred region. During inference, DynRefer performs selectively\nmultimodal referring by sampling proper region representations for tasks from\nthe nested views based on image and task priors. This allows the visual\ninformation for referring to better match human preferences, thereby improving\nthe representational adaptability of region-level multimodal models.\nExperiments show that DynRefer brings mutual improvement upon broad tasks\nincluding region-level captioning, open-vocabulary region recognition and\nattribute detection. Furthermore, DynRefer achieves state-of-the-art results on\nmultiple region-level multimodal tasks using a single model. Code is available\nat https://github.com/callsys/DynRefer.\n","authors":["Yuzhong Zhao","Feng Liu","Yue Liu","Mingxiang Liao","Chen Gong","Qixiang Ye","Fang Wan"],"pdf_url":"https://arxiv.org/pdf/2405.16071v2.pdf","comment":"Accepted in CVPR 2025. Code is available at\n  https://github.com/callsys/DynRefer"},{"id":"http://arxiv.org/abs/2411.18018v2","updated":"2025-03-02T04:05:24Z","published":"2024-11-27T03:21:57Z","title":"Neural Finite-State Machines for Surgical Phase Recognition","summary":"  Surgical phase recognition (SPR) is crucial for applications in workflow\noptimization, performance evaluation, and real-time intervention guidance.\nHowever, current deep learning models often struggle with fragmented\npredictions, failing to capture the sequential nature of surgical workflows. We\npropose the Neural Finite-State Machine (NFSM), a novel approach that enforces\ntemporal coherence by integrating classical state-transition priors with modern\nneural networks. NFSM leverages learnable global state embeddings as unique\nphase identifiers and dynamic transition tables to model phase-to-phase\nprogressions. Additionally, a future phase forecasting mechanism employs\nrepeated frame padding to anticipate upcoming transitions. Implemented as a\nplug-and-play module, NFSM can be integrated into existing SPR pipelines\nwithout changing their core architectures. We demonstrate state-of-the-art\nperformance across multiple benchmarks, including a significant improvement on\nthe BernBypass70 dataset - raising video-level accuracy by 0.9 points and\nphase-level precision, recall, F1-score, and mAP by 3.8, 3.1, 3.3, and 4.1,\nrespectively. Ablation studies confirm each component's effectiveness and the\nmodule's adaptability to various architectures. By unifying finite-state\nprinciples with deep learning, NFSM offers a robust path toward consistent,\nlong-term surgical video analysis.\n","authors":["Hao Ding","Zhongpai Gao","Benjamin Planche","Tianyu Luan","Abhishek Sharma","Meng Zheng","Ange Lou","Terrence Chen","Mathias Unberath","Ziyan Wu"],"pdf_url":"https://arxiv.org/pdf/2411.18018v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.11069v3","updated":"2025-03-02T03:01:19Z","published":"2025-01-19T15:05:15Z","title":"Refinement Module based on Parse Graph of Feature Map for Human Pose\n  Estimation","summary":"  Parse graphs of the human body can be obtained in the human brain to help\nhumans complete the human Pose Estimation better (HPE). It contains a\nhierarchical structure, like a tree structure, and context relations among\nnodes. To equip models with such capabilities, many researchers predefine the\nparse graph of body structure to design HPE frameworks. However, these\nframeworks struggle to adapt to instances that deviate from the predefined\nparse graph and are often parameter-heavy. Unlike them, we view the feature map\nholistically, much like the human body. It can be optimized using parse graphs,\nwhere each node's feature is an implicit expression rather than a fixed one.\nThis allows it to adapt to more instances, unconstrained by rigid structural\nfeatures. In this paper, we design the Refinement Module based on the Parse\nGraph of feature map (RMPG), which includes two stages: top-down decomposition\nand bottom-up combination. In the first stage, the feature map is decomposed\ninto multiple sub-feature maps along the channel. In the second stage, the\ncontext relations of sub-feature maps are calculated to obtain their respective\ncontext information and the sub-feature maps with context information are\nconcatenated along channels to obtain the refined feature map. Additionally, we\ndesign a hierarchical network with fewer parameters using multiple RMPG modules\nto model the context relations and hierarchies in the parse graph of body\nstructure for HPE, some of which are supervised to obtain context relations\namong body parts. Our network achieves excellent results on multiple mainstream\nhuman pose datasets. More importantly, the effectiveness of RMPG is proven on\ndifferent methods. The code of RMPG will be open.\n","authors":["Shibang Liu","Xuemei Xie","Guangming Shi"],"pdf_url":"https://arxiv.org/pdf/2501.11069v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20026v2","updated":"2025-03-02T02:45:56Z","published":"2024-10-26T00:49:06Z","title":"Towards Robust Algorithms for Surgical Phase Recognition via Digital\n  Twin Representation","summary":"  Surgical phase recognition (SPR) is an integral component of surgical data\nscience, enabling high-level surgical analysis. End-to-end trained neural\nnetworks that predict surgical phase directly from videos have shown excellent\nperformance on benchmarks. However, these models struggle with robustness due\nto non-causal associations in the training set. Our goal is to improve model\nrobustness to variations in the surgical videos by leveraging the digital twin\n(DT) paradigm -- an intermediary layer to separate high-level analysis (SPR)\nfrom low-level processing. As a proof of concept, we present a DT\nrepresentation-based framework for SPR from videos. The framework employs\nvision foundation models with reliable low-level scene understanding to craft\nDT representation. We embed the DT representation in place of raw video inputs\nin the state-of-the-art SPR model. The framework is trained on the Cholec80\ndataset and evaluated on out-of-distribution (OOD) and corrupted test samples.\nContrary to the vulnerability of the baseline model, our framework demonstrates\nstrong robustness on both OOD and corrupted samples, with a video-level\naccuracy of 80.3 on a highly corrupted Cholec80 test set, 67.9 on the\nchallenging CRCD dataset, and 99.8 on an internal robotic surgery dataset,\noutperforming the baseline by 3.9, 16.8, and 90.9 respectively. We also find\nthat using DT representation as an augmentation to the raw input can\nsignificantly improve model robustness. Our findings lend support to the thesis\nthat DT representations are effective in enhancing model robustness. Future\nwork will seek to improve the feature informativeness and incorporate\ninterpretability for a more comprehensive framework.\n","authors":["Hao Ding","Yuqian Zhang","Wenzheng Cheng","Xinyu Wang","Xu Lian","Chenhao Yu","Hongchao Shu","Ji Woong Kim","Axel Krieger","Mathias Unberath"],"pdf_url":"https://arxiv.org/pdf/2410.20026v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.04364v4","updated":"2025-03-02T02:32:25Z","published":"2024-01-09T05:32:22Z","title":"SoK: Systematization and Benchmarking of Deepfake Detectors in a Unified\n  Framework","summary":"  Deepfakes have rapidly emerged as a serious threat to society due to their\nease of creation and dissemination, triggering the accelerated development of\ndetection technologies. However, many existing detectors rely on labgenerated\ndatasets for validation, which may not prepare them for novel, real-world\ndeepfakes. This paper extensively reviews and analyzes state-of-the-art\ndeepfake detectors, evaluating them against several critical criteria. These\ncriteria categorize detectors into 4 high-level groups and 13 finegrained\nsub-groups, aligned with a unified conceptual framework we propose. This\nclassification offers practical insights into the factors affecting detector\nefficacy. We evaluate the generalizability of 16 leading detectors across\ncomprehensive attack scenarios, including black-box, white-box, and graybox\nsettings. Our systematized analysis and experiments provide a deeper\nunderstanding of deepfake detectors and their generalizability, paving the way\nfor future research and the development of more proactive defenses against\ndeepfakes.\n","authors":["Binh M. Le","Jiwon Kim","Simon S. Woo","Kristen Moore","Alsharif Abuadbba","Shahroz Tariq"],"pdf_url":"https://arxiv.org/pdf/2401.04364v4.pdf","comment":"20 pages, 6 figures, 7 table, Accepted at IEEE European Symposium on\n  security and privacy 2025 (EuroS&P '25)"},{"id":"http://arxiv.org/abs/2410.05470v2","updated":"2025-03-02T02:07:21Z","published":"2024-10-07T20:04:29Z","title":"Image Watermarks are Removable Using Controllable Regeneration from\n  Clean Noise","summary":"  Image watermark techniques provide an effective way to assert ownership,\ndeter misuse, and trace content sources, which has become increasingly\nessential in the era of large generative models. A critical attribute of\nwatermark techniques is their robustness against various manipulations. In this\npaper, we introduce a watermark removal approach capable of effectively\nnullifying state-of-the-art watermarking techniques. Our primary insight\ninvolves regenerating the watermarked image starting from a clean Gaussian\nnoise via a controllable diffusion model, utilizing the extracted semantic and\nspatial features from the watermarked image. The semantic control adapter and\nthe spatial control network are specifically trained to control the denoising\nprocess towards ensuring image quality and enhancing consistency between the\ncleaned image and the original watermarked image. To achieve a smooth trade-off\nbetween watermark removal performance and image consistency, we further propose\nan adjustable and controllable regeneration scheme. This scheme adds varying\nnumbers of noise steps to the latent representation of the watermarked image,\nfollowed by a controlled denoising process starting from this noisy latent\nrepresentation. As the number of noise steps increases, the latent\nrepresentation progressively approaches clean Gaussian noise, facilitating the\ndesired trade-off. We apply our watermark removal methods across various\nwatermarking techniques, and the results demonstrate that our methods offer\nsuperior visual consistency/quality and enhanced watermark removal performance\ncompared to existing regeneration approaches. Our code is available at\nhttps://github.com/yepengliu/CtrlRegen.\n","authors":["Yepeng Liu","Yiren Song","Hai Ci","Yu Zhang","Haofan Wang","Mike Zheng Shou","Yuheng Bu"],"pdf_url":"https://arxiv.org/pdf/2410.05470v2.pdf","comment":"ICLR2025"},{"id":"http://arxiv.org/abs/2502.10603v2","updated":"2025-03-02T01:50:22Z","published":"2025-02-14T23:18:54Z","title":"Adaptive Neural Networks for Intelligent Data-Driven Development","summary":"  Advances in machine learning methods for computer vision tasks have led to\ntheir consideration for safety-critical applications like autonomous driving.\nHowever, effectively integrating these methods into the automotive development\nlifecycle remains challenging. Since the performance of machine learning\nalgorithms relies heavily on the training data provided, the data and model\ndevelopment lifecycle play a key role in successfully integrating these\ncomponents into the product development lifecycle. Existing models frequently\nencounter difficulties recognizing or adapting to novel instances not present\nin the original training dataset. This poses a significant risk for reliable\ndeployment in dynamic environments. To address this challenge, we propose an\nadaptive neural network architecture and an iterative development framework\nthat enables users to efficiently incorporate previously unknown objects into\nthe current perception system. Our approach builds on continuous learning,\nemphasizing the necessity of dynamic updates to reflect real-world deployment\nconditions. Specifically, we introduce a pipeline with three key components:\n(1) a scalable network extension strategy to integrate new classes while\npreserving existing performance, (2) a dynamic OoD detection component that\nrequires no additional retraining for newly added classes, and (3) a\nretrieval-based data augmentation process tailored for safety-critical\ndeployments. The integration of these components establishes a pragmatic and\nadaptive pipeline for the continuous evolution of perception systems in the\ncontext of autonomous driving.\n","authors":["Youssef Shoeb","Azarm Nowzad","Hanno Gottschalk"],"pdf_url":"https://arxiv.org/pdf/2502.10603v2.pdf","comment":"8 pages, 3 figures, and 3 tables"},{"id":"http://arxiv.org/abs/2412.05707v3","updated":"2025-03-02T01:46:15Z","published":"2024-12-07T17:40:20Z","title":"Segment-Level Road Obstacle Detection Using Visual Foundation Model\n  Priors and Likelihood Ratios","summary":"  Detecting road obstacles is essential for autonomous vehicles to navigate\ndynamic and complex traffic environments safely. Current road obstacle\ndetection methods typically assign a score to each pixel and apply a threshold\nto generate final predictions. However, selecting an appropriate threshold is\nchallenging, and the per-pixel classification approach often leads to\nfragmented predictions with numerous false positives. In this work, we propose\na novel method that leverages segment-level features from visual foundation\nmodels and likelihood ratios to predict road obstacles directly. By focusing on\nsegments rather than individual pixels, our approach enhances detection\naccuracy, reduces false positives, and offers increased robustness to scene\nvariability. We benchmark our approach against existing methods on the\nRoadObstacle and LostAndFound datasets, achieving state-of-the-art performance\nwithout needing a predefined threshold.\n","authors":["Youssef Shoeb","Nazir Nayal","Azarm Nowzad","Fatma Güney","Hanno Gottschalk"],"pdf_url":"https://arxiv.org/pdf/2412.05707v3.pdf","comment":"10 pages, 4 figures, and 1 table, to be published in VISAPP 2025"},{"id":"http://arxiv.org/abs/2410.10594v2","updated":"2025-03-02T01:19:51Z","published":"2024-10-14T15:04:18Z","title":"VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality\n  Documents","summary":"  Retrieval-augmented generation (RAG) is an effective technique that enables\nlarge language models (LLMs) to utilize external knowledge sources for\ngeneration. However, current RAG systems are solely based on text, rendering it\nimpossible to utilize vision information like layout and images that play\ncrucial roles in real-world multi-modality documents. In this paper, we\nintroduce VisRAG, which tackles this issue by establishing a vision-language\nmodel (VLM)-based RAG pipeline. In this pipeline, instead of first parsing the\ndocument to obtain text, the document is directly embedded using a VLM as an\nimage and then retrieved to enhance the generation of a VLM. Compared to\ntraditional text-based RAG, VisRAG maximizes the retention and utilization of\nthe data information in the original documents, eliminating the information\nloss introduced during the parsing process. We collect both open-source and\nsynthetic data to train the retriever in VisRAG and explore a variety of\ngeneration methods. Experiments demonstrate that VisRAG outperforms traditional\nRAG in both the retrieval and generation stages, achieving a 20--40% end-to-end\nperformance gain over traditional text-based RAG pipeline. Further analysis\nreveals that VisRAG is efficient in utilizing training data and demonstrates\nstrong generalization capability, positioning it as a promising solution for\nRAG on multi-modality documents. Our code and data are available at\nhttps://github.com/openbmb/visrag.\n","authors":["Shi Yu","Chaoyue Tang","Bokai Xu","Junbo Cui","Junhao Ran","Yukun Yan","Zhenghao Liu","Shuo Wang","Xu Han","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2410.10594v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11219v3","updated":"2025-03-02T01:07:41Z","published":"2024-09-17T14:12:50Z","title":"Score Forgetting Distillation: A Swift, Data-Free Method for Machine\n  Unlearning in Diffusion Models","summary":"  The machine learning community is increasingly recognizing the importance of\nfostering trust and safety in modern generative AI (GenAI) models. We posit\nmachine unlearning (MU) as a crucial foundation for developing safe, secure,\nand trustworthy GenAI models. Traditional MU methods often rely on stringent\nassumptions and require access to real data. This paper introduces Score\nForgetting Distillation (SFD), an innovative MU approach that promotes the\nforgetting of undesirable information in diffusion models by aligning the\nconditional scores of \"unsafe\" classes or concepts with those of \"safe\" ones.\nTo eliminate the need for real data, our SFD framework incorporates a\nscore-based MU loss into the score distillation objective of a pretrained\ndiffusion model. This serves as a regularization term that preserves desired\ngeneration capabilities while enabling the production of synthetic data through\na one-step generator. Our experiments on pretrained label-conditional and\ntext-to-image diffusion models demonstrate that our method effectively\naccelerates the forgetting of target classes or concepts during generation,\nwhile preserving the quality of other classes or concepts. This unlearned and\ndistilled diffusion not only pioneers a novel concept in MU but also\naccelerates the generation speed of diffusion models. Our experiments and\nstudies on a range of diffusion models and datasets confirm that our approach\nis generalizable, effective, and advantageous for MU in diffusion models. Code\nis available at https://github.com/tqch/score-forgetting-distillation.\n($\\textbf{Warning:}$ This paper contains sexually explicit imagery, discussions\nof pornography, racially-charged terminology, and other content that some\nreaders may find disturbing, distressing, and/or offensive.)\n","authors":["Tianqi Chen","Shujian Zhang","Mingyuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2409.11219v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.02283v3","updated":"2025-03-02T00:25:45Z","published":"2025-02-04T12:50:16Z","title":"GP-GS: Gaussian Processes for Enhanced Gaussian Splatting","summary":"  3D Gaussian Splatting has emerged as an efficient photorealistic novel view\nsynthesis method. However, its reliance on sparse Structure-from-Motion (SfM)\npoint clouds consistently compromises the scene reconstruction quality. To\naddress these limitations, this paper proposes a novel 3D reconstruction\nframework Gaussian Processes Gaussian Splatting (GP-GS), where a multi-output\nGaussian Process model is developed to achieve adaptive and uncertainty-guided\ndensification of sparse SfM point clouds. Specifically, we propose a dynamic\nsampling and filtering pipeline that adaptively expands the SfM point clouds by\nleveraging GP-based predictions to infer new candidate points from the input 2D\npixels and depth maps. The pipeline utilizes uncertainty estimates to guide the\npruning of high-variance predictions, ensuring geometric consistency and\nenabling the generation of dense point clouds. The densified point clouds\nprovide high-quality initial 3D Gaussians to enhance reconstruction\nperformance. Extensive experiments conducted on synthetic and real-world\ndatasets across various scales validate the effectiveness and practicality of\nthe proposed framework.\n","authors":["Zhihao Guo","Jingxuan Su","Shenglin Wang","Jinlong Fan","Jing Zhang","Liangxiu Han","Peng Wang"],"pdf_url":"https://arxiv.org/pdf/2502.02283v3.pdf","comment":"14 pages,11 figures"},{"id":"http://arxiv.org/abs/2411.18810v4","updated":"2025-03-02T00:15:11Z","published":"2024-11-27T23:32:54Z","title":"All Seeds Are Not Equal: Enhancing Compositional Text-to-Image\n  Generation with Reliable Random Seeds","summary":"  Text-to-image diffusion models have demonstrated remarkable capability in\ngenerating realistic images from arbitrary text prompts. However, they often\nproduce inconsistent results for compositional prompts such as \"two dogs\" or \"a\npenguin on the right of a bowl\". Understanding these inconsistencies is crucial\nfor reliable image generation. In this paper, we highlight the significant role\nof initial noise in these inconsistencies, where certain noise patterns are\nmore reliable for compositional prompts than others. Our analyses reveal that\ndifferent initial random seeds tend to guide the model to place objects in\ndistinct image areas, potentially adhering to specific patterns of camera\nangles and image composition associated with the seed. To improve the model's\ncompositional ability, we propose a method for mining these reliable cases,\nresulting in a curated training set of generated images without requiring any\nmanual annotation. By fine-tuning text-to-image models on these generated\nimages, we significantly enhance their compositional capabilities. For\nnumerical composition, we observe relative increases of 29.3% and 19.5% for\nStable Diffusion and PixArt-{\\alpha}, respectively. Spatial composition sees\neven larger gains, with 60.7% for Stable Diffusion and 21.1% for\nPixArt-{\\alpha}.\n","authors":["Shuangqi Li","Hieu Le","Jingyi Xu","Mathieu Salzmann"],"pdf_url":"https://arxiv.org/pdf/2411.18810v4.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2410.10594v2","updated":"2025-03-02T01:19:51Z","published":"2024-10-14T15:04:18Z","title":"VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality\n  Documents","summary":"  Retrieval-augmented generation (RAG) is an effective technique that enables\nlarge language models (LLMs) to utilize external knowledge sources for\ngeneration. However, current RAG systems are solely based on text, rendering it\nimpossible to utilize vision information like layout and images that play\ncrucial roles in real-world multi-modality documents. In this paper, we\nintroduce VisRAG, which tackles this issue by establishing a vision-language\nmodel (VLM)-based RAG pipeline. In this pipeline, instead of first parsing the\ndocument to obtain text, the document is directly embedded using a VLM as an\nimage and then retrieved to enhance the generation of a VLM. Compared to\ntraditional text-based RAG, VisRAG maximizes the retention and utilization of\nthe data information in the original documents, eliminating the information\nloss introduced during the parsing process. We collect both open-source and\nsynthetic data to train the retriever in VisRAG and explore a variety of\ngeneration methods. Experiments demonstrate that VisRAG outperforms traditional\nRAG in both the retrieval and generation stages, achieving a 20--40% end-to-end\nperformance gain over traditional text-based RAG pipeline. Further analysis\nreveals that VisRAG is efficient in utilizing training data and demonstrates\nstrong generalization capability, positioning it as a promising solution for\nRAG on multi-modality documents. Our code and data are available at\nhttps://github.com/openbmb/visrag.\n","authors":["Shi Yu","Chaoyue Tang","Bokai Xu","Junbo Cui","Junhao Ran","Yukun Yan","Zhenghao Liu","Shuo Wang","Xu Han","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2410.10594v2.pdf","comment":null}],"Machine Learning":[{"id":"http://arxiv.org/abs/2408.15998v2","updated":"2025-03-02T23:41:37Z","published":"2024-08-28T17:59:31Z","title":"Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of\n  Encoders","summary":"  The ability to accurately interpret complex visual information is a crucial\ntopic of multimodal large language models (MLLMs). Recent work indicates that\nenhanced visual perception significantly reduces hallucinations and improves\nperformance on resolution-sensitive tasks, such as optical character\nrecognition and document analysis. A number of recent MLLMs achieve this goal\nusing a mixture of vision encoders. Despite their success, there is a lack of\nsystematic comparisons and detailed ablation studies addressing critical\naspects, such as expert selection and the integration of multiple vision\nexperts. This study provides an extensive exploration of the design space for\nMLLMs using a mixture of vision encoders and resolutions. Our findings reveal\nseveral underlying principles common to various existing strategies, leading to\na streamlined yet effective design approach. We discover that simply\nconcatenating visual tokens from a set of complementary vision encoders is as\neffective as more complex mixing architectures or strategies. We additionally\nintroduce Pre-Alignment to bridge the gap between vision-focused encoders and\nlanguage tokens, enhancing model coherence. The resulting family of MLLMs,\nEagle, surpasses other leading open-source models on major MLLM benchmarks.\n","authors":["Min Shi","Fuxiao Liu","Shihao Wang","Shijia Liao","Subhashree Radhakrishnan","Yilin Zhao","De-An Huang","Hongxu Yin","Karan Sapra","Yaser Yacoob","Humphrey Shi","Bryan Catanzaro","Andrew Tao","Jan Kautz","Zhiding Yu","Guilin Liu"],"pdf_url":"https://arxiv.org/pdf/2408.15998v2.pdf","comment":"Github: https://github.com/NVlabs/Eagle, HuggingFace:\n  https://huggingface.co/NVEagle"},{"id":"http://arxiv.org/abs/2411.09851v3","updated":"2025-03-02T23:29:50Z","published":"2024-11-15T00:09:37Z","title":"SymbolFit: Automatic Parametric Modeling with Symbolic Regression","summary":"  We introduce SymbolFit, a framework that automates parametric modeling by\nusing symbolic regression to perform a machine-search for functions that fit\nthe data while simultaneously providing uncertainty estimates in a single run.\nTraditionally, constructing a parametric model to accurately describe binned\ndata has been a manual and iterative process, requiring an adequate functional\nform to be determined before the fit can be performed. The main challenge\narises when the appropriate functional forms cannot be derived from first\nprinciples, especially when there is no underlying true closed-form function\nfor the distribution. In this work, we develop a framework that automates and\nstreamlines the process by utilizing symbolic regression, a machine learning\ntechnique that explores a vast space of candidate functions without requiring a\npredefined functional form because the functional form itself is treated as a\ntrainable parameter, making the process far more efficient and effortless than\ntraditional regression methods. We demonstrate the framework in high-energy\nphysics experiments at the CERN Large Hadron Collider (LHC) using five real\nproton-proton collision datasets from new physics searches, including\nbackground modeling in resonance searches for high-mass dijet, trijet,\npaired-dijet, diphoton, and dimuon events. We show that our framework can\nflexibly and efficiently generate a wide range of candidate functions that fit\na nontrivial distribution well using a simple fit configuration that varies\nonly by random seed, and that the same fit configuration, which defines a vast\nfunction space, can also be applied to distributions of different shapes,\nwhereas achieving a comparable result with traditional methods would have\nrequired extensive manual effort.\n","authors":["Ho Fung Tsoi","Dylan Rankin","Cecile Caillol","Miles Cranmer","Sridhara Dasu","Javier Duarte","Philip Harris","Elliot Lipeles","Vladimir Loncar"],"pdf_url":"https://arxiv.org/pdf/2411.09851v3.pdf","comment":"50 pages, 35 figures. Under review. The API can be used\n  out-of-the-box and is available at https://github.com/hftsoi/symbolfit"},{"id":"http://arxiv.org/abs/2401.17116v2","updated":"2025-03-02T23:04:57Z","published":"2024-01-30T15:50:06Z","title":"Quantum time dynamics mediated by the Yang-Baxter equation and\n  artificial neural networks","summary":"  Quantum computing shows great potential, but errors pose a significant\nchallenge. This study explores new strategies for mitigating quantum errors\nusing artificial neural networks (ANN) and the Yang-Baxter equation (YBE).\nUnlike traditional error mitigation methods, which are computationally\nintensive, we investigate artificial error mitigation. We developed a novel\nmethod that combines ANN for noise mitigation combined with the YBE to generate\nnoisy data. This approach effectively reduces noise in quantum simulations,\nenhancing the accuracy of the results. The YBE rigorously preserves quantum\ncorrelations and symmetries in spin chain simulations in certain classes of\nintegrable lattice models, enabling effective compression of quantum circuits\nwhile retaining linear scalability with the number of qubits. This compression\nfacilitates both full and partial implementations, allowing the generation of\nnoisy quantum data on hardware alongside noiseless simulations using classical\nplatforms. By introducing controlled noise through the YBE, we enhance the\ndataset for error mitigation. We train an ANN model on partial data from\nquantum simulations, demonstrating its effectiveness in mitigating errors in\ntime-evolving quantum states, providing a scalable framework to enhance quantum\ncomputation fidelity, particularly in noisy intermediate-scale quantum (NISQ)\nsystems. We demonstrate the efficacy of this approach by performing quantum\ntime dynamics simulations using the Heisenberg XY Hamiltonian on real quantum\ndevices.\n","authors":["Sahil Gulania","Yuri Alexeev","Stephen K. Gray","Bo Peng","Niranjan Govind"],"pdf_url":"https://arxiv.org/pdf/2401.17116v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.13496v2","updated":"2025-03-02T22:34:01Z","published":"2024-02-21T03:14:45Z","title":"Heterogeneous Graph Neural Network on Semantic Tree","summary":"  The recent past has seen an increasing interest in Heterogeneous Graph Neural\nNetworks (HGNNs), since many real-world graphs are heterogeneous in nature,\nfrom citation graphs to email graphs. However, existing methods ignore a tree\nhierarchy among metapaths, naturally constituted by different node types and\nrelation types. In this paper, we present HetTree, a novel HGNN that models\nboth the graph structure and heterogeneous aspects in a scalable and effective\nmanner. Specifically, HetTree builds a semantic tree data structure to capture\nthe hierarchy among metapaths. To effectively encode the semantic tree, HetTree\nuses a novel subtree attention mechanism to emphasize metapaths that are more\nhelpful in encoding parent-child relationships. Moreover, HetTree proposes\ncarefully matching pre-computed features and labels correspondingly,\nconstituting a complete metapath representation. Our evaluation of HetTree on a\nvariety of real-world datasets demonstrates that it outperforms all existing\nbaselines on open benchmarks and efficiently scales to large real-world graphs\nwith millions of nodes and edges.\n","authors":["Mingyu Guan","Jack W. Stokes","Qinlong Luo","Fuchen Liu","Purvanshi Mehta","Elnaz Nouri","Taesoo Kim"],"pdf_url":"https://arxiv.org/pdf/2402.13496v2.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2407.11249v3","updated":"2025-03-02T22:12:01Z","published":"2024-07-15T21:32:58Z","title":"Disentangling Representations through Multi-task Learning","summary":"  Intelligent perception and interaction with the world hinges on internal\nrepresentations that capture its underlying structure (''disentangled'' or\n''abstract'' representations). Disentangled representations serve as world\nmodels, isolating latent factors of variation in the world along approximately\northogonal directions, thus facilitating feature-based generalization. We\nprovide experimental and theoretical results guaranteeing the emergence of\ndisentangled representations in agents that optimally solve multi-task evidence\naccumulation classification tasks, canonical in the neuroscience literature.\nThe key conceptual finding is that, by producing accurate multi-task\nclassification estimates, a system implicitly represents a set of coordinates\nspecifying a disentangled representation of the underlying latent state of the\ndata it receives. The theory provides conditions for the emergence of these\nrepresentations in terms of noise, number of tasks, and evidence accumulation\ntime. We experimentally validate these predictions in RNNs trained to\nmulti-task, which learn disentangled representations in the form of continuous\nattractors, leading to zero-shot out-of-distribution (OOD) generalization in\npredicting latent factors. We demonstrate the robustness of our framework\nacross autoregressive architectures, decision boundary geometries and in tasks\nrequiring classification confidence estimation. We find that transformers are\nparticularly suited for disentangling representations, which might explain\ntheir unique world understanding abilities. Overall, our framework establishes\na formal link between competence at multiple tasks and the formation of\ndisentangled, interpretable world models in both biological and artificial\nsystems, and helps explain why ANNs often arrive at human-interpretable\nconcepts, and how they both may acquire exceptional zero-shot generalization\ncapabilities.\n","authors":["Pantelis Vafidis","Aman Bhargava","Antonio Rangel"],"pdf_url":"https://arxiv.org/pdf/2407.11249v3.pdf","comment":"43 pages, 17 figures"},{"id":"http://arxiv.org/abs/2406.10279v3","updated":"2025-03-02T21:03:52Z","published":"2024-06-12T03:29:06Z","title":"We Have a Package for You! A Comprehensive Analysis of Package\n  Hallucinations by Code Generating LLMs","summary":"  The reliance of popular programming languages such as Python and JavaScript\non centralized package repositories and open-source software, combined with the\nemergence of code-generating Large Language Models (LLMs), has created a new\ntype of threat to the software supply chain: package hallucinations. These\nhallucinations, which arise from fact-conflicting errors when generating code\nusing LLMs, represent a novel form of package confusion attack that poses a\ncritical threat to the integrity of the software supply chain. This paper\nconducts a rigorous and comprehensive evaluation of package hallucinations\nacross different programming languages, settings, and parameters, exploring how\na diverse set of models and configurations affect the likelihood of generating\nerroneous package recommendations and identifying the root causes of this\nphenomenon. Using 16 popular LLMs for code generation and two unique prompt\ndatasets, we generate 576,000 code samples in two programming languages that we\nanalyze for package hallucinations. Our findings reveal that that the average\npercentage of hallucinated packages is at least 5.2% for commercial models and\n21.7% for open-source models, including a staggering 205,474 unique examples of\nhallucinated package names, further underscoring the severity and pervasiveness\nof this threat. To overcome this problem, we implement several hallucination\nmitigation strategies and show that they are able to significantly reduce the\nnumber of package hallucinations while maintaining code quality. Our\nexperiments and findings highlight package hallucinations as a persistent and\nsystemic phenomenon while using state-of-the-art LLMs for code generation, and\na significant challenge which deserves the research community's urgent\nattention.\n","authors":["Joseph Spracklen","Raveen Wijewickrama","A H M Nazmus Sakib","Anindya Maiti","Bimal Viswanath","Murtuza Jadliwala"],"pdf_url":"https://arxiv.org/pdf/2406.10279v3.pdf","comment":"To appear in the 2025 USENIX Security Symposium. 22 pages, 14\n  figures, 8 tables. Edited from original version for submission to a different\n  conference. No change to original results or findings"},{"id":"http://arxiv.org/abs/2410.06232v3","updated":"2025-03-02T20:40:21Z","published":"2024-10-08T17:41:37Z","title":"Range, not Independence, Drives Modularity in Biologically Inspired\n  Representations","summary":"  Why do biological and artificial neurons sometimes modularise, each encoding\na single meaningful variable, and sometimes entangle their representation of\nmany variables? In this work, we develop a theory of when biologically inspired\nnetworks -- those that are nonnegative and energy efficient -- modularise their\nrepresentation of source variables (sources). We derive necessary and\nsufficient conditions on a sample of sources that determine whether the neurons\nin an optimal biologically-inspired linear autoencoder modularise. Our theory\napplies to any dataset, extending far beyond the case of statistical\nindependence studied in previous work. Rather we show that sources modularise\nif their support is ``sufficiently spread''. From this theory, we extract and\nvalidate predictions in a variety of empirical studies on how data distribution\naffects modularisation in nonlinear feedforward and recurrent neural networks\ntrained on supervised and unsupervised tasks. Furthermore, we apply these ideas\nto neuroscience data, showing that range independence can be used to understand\nthe mixing or modularising of spatial and reward information in entorhinal\nrecordings in seemingly conflicting experiments. Further, we use these results\nto suggest alternate origins of mixed-selectivity, beyond the predominant\ntheory of flexible nonlinear classification. In sum, our theory prescribes\nprecise conditions on when neural activities modularise, providing tools for\ninducing and elucidating modular representations in brains and machines.\n","authors":["Will Dorrell","Kyle Hsu","Luke Hollingsworth","Jin Hwa Lee","Jiajun Wu","Chelsea Finn","Peter E Latham","Tim EJ Behrens","James CR Whittington"],"pdf_url":"https://arxiv.org/pdf/2410.06232v3.pdf","comment":"47 pages, 17 figures. WD and KH contributed equally; LH and JHL\n  contributed equally"},{"id":"http://arxiv.org/abs/2408.15905v2","updated":"2025-03-02T20:30:28Z","published":"2024-08-28T16:19:35Z","title":"MetaGFN: Exploring Distant Modes with Adapted Metadynamics for\n  Continuous GFlowNets","summary":"  Generative Flow Networks (GFlowNets) are a class of generative models that\nsample objects in proportion to a specified reward function through a learned\npolicy. They can be trained either on-policy or off-policy, needing a balance\nbetween exploration and exploitation for fast convergence to a target\ndistribution. While exploration strategies for discrete GFlowNets have been\nstudied, exploration in the continuous case remains to be investigated, despite\nthe potential for novel exploration algorithms due to the local connectedness\nof continuous domains. Here, we introduce Adapted Metadynamics, a variant of\nmetadynamics that can be applied to arbitrary black-box reward functions on\ncontinuous domains. We use Adapted Metadynamics as an exploration strategy for\ncontinuous GFlowNets. We show several continuous domains where the resulting\nalgorithm, MetaGFN, accelerates convergence to the target distribution and\ndiscovers more distant reward modes than previous off-policy exploration\nstrategies used for GFlowNets.\n","authors":["Dominic Phillips","Flaviu Cipcigan"],"pdf_url":"https://arxiv.org/pdf/2408.15905v2.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2502.12381v3","updated":"2025-03-02T20:17:56Z","published":"2025-02-17T23:40:27Z","title":"Linear Diffusion Networks","summary":"  Diffusion kernels capture global dependencies. We present Linear Diffusion\nNetworks (LDNs), a novel architecture that reinterprets sequential data\nprocessing as a unified diffusion process. Our model integrates adaptive\ndiffusion modules with localized nonlinear updates and a diffusion-inspired\nattention mechanism. This design enables efficient global information\npropagation while preserving fine-grained temporal details. LDN overcomes the\nlimitations of conventional recurrent and transformer models by allowing full\nparallelization across time steps and supporting robust multi-scale temporal\nrepresentations. Experiments on benchmark sequence modeling tasks demonstrate\nthat LDN delivers competitive performance across ImageNet and GLUE tasks.\n","authors":["Jacob Fein-Ashley"],"pdf_url":"https://arxiv.org/pdf/2502.12381v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05967v2","updated":"2025-03-02T20:16:43Z","published":"2025-02-09T17:31:09Z","title":"$μ$nit Scaling: Simple and Scalable FP8 LLM Training","summary":"  Large Language Model training with 8-bit floating point (FP8) formats\npromises significant efficiency improvements, but reduced numerical precision\nmakes training challenging. It is currently possible to train in FP8 only if\none is willing to tune various hyperparameters, reduce model scale, or accept\nthe overhead of computing dynamic scale factors. We demonstrate simple,\nscalable FP8 training that requires no dynamic scaling factors or special\nhyperparameters, even at large model sizes. Our method, $\\mu$nit Scaling\n($\\mu$S), also enables simple hyperparameter transfer across model widths,\nmatched numerics across training and inference, and other desirable properties.\n$\\mu$nit Scaling is straightforward to implement, consisting of a set of\nminimal interventions based on a first-principles analysis of common\ntransformer operations. We validate our method by training models from 1B to\n13B parameters, performing all hidden linear layer computations in FP8. We\nachieve quality equal to higher precision baselines while also training up to\n33% faster.\n","authors":["Saaketh Narayan","Abhay Gupta","Mansheej Paul","Davis Blalock"],"pdf_url":"https://arxiv.org/pdf/2502.05967v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.12534v2","updated":"2025-03-02T20:13:11Z","published":"2024-04-18T22:54:08Z","title":"Lean Copilot: Large Language Models as Copilots for Theorem Proving in\n  Lean","summary":"  Neural theorem proving combines large language models (LLMs) with proof\nassistants such as Lean, where the correctness of formal proofs can be\nrigorously verified, leaving no room for hallucination. With existing neural\ntheorem provers pretrained on a fixed collection of data and offering valuable\nsuggestions at times, it is challenging for them to continually prove novel\ntheorems in a fully autonomous mode, where human insights may be critical. In\nthis paper, we explore LLMs as copilots that assist humans in proving theorems.\nWe introduce Lean Copilot, an general framework for running LLM inference\nnatively in Lean. It enables programmers to build various LLM-based proof\nautomation tools that integrate seamlessly into the workflow of Lean users.\nLean users can use our pretrained models or bring their own ones that run\neither locally (with or without GPUs) or on the cloud. Using Lean Copilot, we\nbuild LLM-based tools that suggest proof steps, complete proof goals, and\nselect relevant premises. Experimental results on the Mathematics in Lean\ntextbook demonstrate the effectiveness of our method compared to existing\nrule-based proof automation in Lean (aesop). When assisting humans, Lean\nCopilot requires only 2.08 manually-entered proof steps on average (3.86\nrequired by aesop); when automating the theorem proving process, Lean Copilot\nautomates 74.2% proof steps on average, 85% better than aesop (40.1%). We open\nsource all code and artifacts under a permissive MIT license to facilitate\nfurther research.\n","authors":["Peiyang Song","Kaiyu Yang","Anima Anandkumar"],"pdf_url":"https://arxiv.org/pdf/2404.12534v2.pdf","comment":"All code and artifacts open-sourced at\n  https://github.com/lean-dojo/LeanCopilot"},{"id":"http://arxiv.org/abs/2405.09660v3","updated":"2025-03-02T19:51:43Z","published":"2024-05-15T19:03:08Z","title":"Fast Two-Time-Scale Stochastic Gradient Method with Applications in\n  Reinforcement Learning","summary":"  Two-time-scale optimization is a framework introduced in Zeng et al. (2024)\nthat abstracts a range of policy evaluation and policy optimization problems in\nreinforcement learning (RL). Akin to bi-level optimization under a particular\ntype of stochastic oracle, the two-time-scale optimization framework has an\nupper level objective whose gradient evaluation depends on the solution of a\nlower level problem, which is to find the root of a strongly monotone operator.\nIn this work, we propose a new method for solving two-time-scale optimization\nthat achieves significantly faster convergence than the prior arts. The key\nidea of our approach is to leverage an averaging step to improve the estimates\nof the operators in both lower and upper levels before using them to update the\ndecision variables. These additional averaging steps eliminate the direct\ncoupling between the main variables, enabling the accelerated performance of\nour algorithm. We characterize the finite-time convergence rates of the\nproposed algorithm under various conditions of the underlying objective\nfunction, including strong convexity, Polyak-Lojasiewicz condition, and general\nnon-convexity. These rates significantly improve over the best-known complexity\nof the standard two-time-scale stochastic approximation algorithm. When applied\nto RL, we show how the proposed algorithm specializes to novel online\nsample-based methods that surpass or match the performance of the existing\nstate of the art. Finally, we support our theoretical results with numerical\nsimulations in RL.\n","authors":["Sihan Zeng","Thinh T. Doan"],"pdf_url":"https://arxiv.org/pdf/2405.09660v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.16103v4","updated":"2025-03-02T18:38:37Z","published":"2024-10-21T15:31:06Z","title":"LDAdam: Adaptive Optimization from Low-Dimensional Gradient Statistics","summary":"  We introduce LDAdam, a memory-efficient optimizer for training large models,\nthat performs adaptive optimization steps within lower dimensional subspaces,\nwhile consistently exploring the full parameter space during training. This\nstrategy keeps the optimizer's memory footprint to a fraction of the model\nsize. LDAdam relies on a new projection-aware update rule for the optimizer\nstates that allows for transitioning between subspaces, i.e., estimation of the\nstatistics of the projected gradients. To mitigate the errors due to low-rank\nprojection, LDAdam integrates a new generalized error feedback mechanism, which\nexplicitly accounts for both gradient and optimizer state compression. We prove\nthe convergence of LDAdam under standard assumptions, and show that LDAdam\nallows for accurate and efficient fine-tuning and pre-training of language\nmodels. Code is available at https://github.com/IST-DASLab/LDAdam\n","authors":["Thomas Robert","Mher Safaryan","Ionut-Vlad Modoranu","Dan Alistarh"],"pdf_url":"https://arxiv.org/pdf/2410.16103v4.pdf","comment":"39 pages, ICLR 2025"},{"id":"http://arxiv.org/abs/2312.15289v3","updated":"2025-03-02T18:36:56Z","published":"2023-12-23T16:10:53Z","title":"Fréchet Wavelet Distance: A Domain-Agnostic Metric for Image\n  Generation","summary":"  Modern metrics for generative learning like Fr\\'echet Inception Distance\n(FID) and DINOv2-Fr\\'echet Distance (FD-DINOv2) demonstrate impressive\nperformance. However, they suffer from various shortcomings, like a bias\ntowards specific generators and datasets. To address this problem, we propose\nthe Fr\\'echet Wavelet Distance (FWD) as a domain-agnostic metric based on the\nWavelet Packet Transform ($W_p$). FWD provides a sight across a broad spectrum\nof frequencies in images with a high resolution, preserving both spatial and\ntextural aspects. Specifically, we use $W_p$ to project generated and real\nimages to the packet coefficient space. We then compute the Fr\\'echet distance\nwith the resultant coefficients to evaluate the quality of a generator. This\nmetric is general-purpose and dataset-domain agnostic, as it does not rely on\nany pre-trained network, while being more interpretable due to its ability to\ncompute Fr\\'echet distance per packet, enhancing transparency. We conclude with\nan extensive evaluation of a wide variety of generators across various datasets\nthat the proposed FWD can generalize and improve robustness to domain shifts\nand various corruptions compared to other metrics.\n","authors":["Lokesh Veeramacheneni","Moritz Wolter","Hildegard Kuehne","Juergen Gall"],"pdf_url":"https://arxiv.org/pdf/2312.15289v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.02950v2","updated":"2025-03-02T18:31:59Z","published":"2024-08-06T04:28:16Z","title":"Kolmogorov-Arnold PointNet: Deep learning for prediction of fluid fields\n  on irregular geometries","summary":"  Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to\ntraditional Multilayer Perceptrons (MLPs) in deep learning. KANs have already\nbeen integrated into various architectures, such as convolutional neural\nnetworks, graph neural networks, and transformers, and their potential has been\nassessed for predicting physical quantities. However, the combination of KANs\nwith point-cloud-based neural networks (e.g., PointNet) for computational\nphysics has not yet been explored. To address this, we present\nKolmogorov-Arnold PointNet (KA-PointNet) as a novel supervised deep learning\nframework for the prediction of incompressible steady-state fluid flow fields\nin irregular domains, where the predicted fields are a function of the geometry\nof the domains. In KA-PointNet, we implement shared KANs in the segmentation\nbranch of the PointNet architecture. We utilize Jacobi polynomials to construct\nshared KANs. As a benchmark test case, we consider incompressible laminar\nsteady-state flow over a cylinder, where the geometry of its cross-section\nvaries over the data set. We investigate the performance of Jacobi polynomials\nwith different degrees as well as special cases of Jacobi polynomials such as\nLegendre polynomials, Chebyshev polynomials of the first and second kinds, and\nGegenbauer polynomials, in terms of the computational cost of training and\naccuracy of prediction of the test set. Additionally, we compare the\nperformance of PointNet with shared KANs (i.e., KA-PointNet) and PointNet with\nshared MLPs. It is observed that when the number of trainable parameters is\napproximately equal, PointNet with shared KANs (i.e., KA-PointNet) outperforms\nPointNet with shared MLPs. Moreover, KA-PointNet predicts the pressure and\nvelocity distributions along the surface of cylinders more accurately,\nresulting in more precise computations of lift and drag.\n","authors":["Ali Kashefi"],"pdf_url":"https://arxiv.org/pdf/2408.02950v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14105v4","updated":"2025-03-02T18:24:29Z","published":"2024-05-23T02:14:17Z","title":"Distributed Speculative Inference (DSI): Speculation Parallelism for\n  Provably Faster Lossless Language Model Inference","summary":"  This paper introduces distributed speculative inference (DSI), a novel\ninference algorithm that is provably faster than speculative inference (SI)\n[leviathan2023, chen2023, miao2024, sun2025, timor2025] and standard\nautoregressive inference (non-SI). Like other SI algorithms, DSI operates on\nfrozen language models (LMs), requiring no training or architectural\nmodifications, and it preserves the target distribution. Prior studies on SI\nhave demonstrated empirical speedups over non-SI--but rely on sufficiently fast\nand accurate drafters, which are often unavailable in practice. We identify a\ngap where SI can be slower than non-SI if drafters are too slow or inaccurate.\nWe close this gap by proving that DSI is faster than both SI and non-SI--given\nany drafters. DSI is therefore not only faster than SI, but also unlocks the\nacceleration of LMs for which SI fails. DSI leverages speculation parallelism\n(SP), a novel type of task parallelism, to orchestrate target and drafter\ninstances that overlap in time, establishing a new foundational tradeoff\nbetween computational resources and latency. Our simulations show that DSI is\n1.29-1.92x faster than SI in single-node setups for various off-the-shelf LMs\nand tasks. We open-source all our code.\n","authors":["Nadav Timor","Jonathan Mamou","Daniel Korat","Moshe Berchansky","Oren Pereg","Moshe Wasserblat","Tomer Galanti","Michal Gordon","David Harel"],"pdf_url":"https://arxiv.org/pdf/2405.14105v4.pdf","comment":"Published at ICLR 2025. (Link:\n  https://openreview.net/forum?id=cJd1BgZ9CS)"},{"id":"http://arxiv.org/abs/2411.10509v2","updated":"2025-03-02T18:17:14Z","published":"2024-11-15T15:39:04Z","title":"TESGNN: Temporal Equivariant Scene Graph Neural Networks for Efficient\n  and Robust Multi-View 3D Scene Understanding","summary":"  Scene graphs have proven to be highly effective for various scene\nunderstanding tasks due to their compact and explicit representation of\nrelational information. However, current methods often overlook the critical\nimportance of preserving symmetry when generating scene graphs from 3D point\nclouds, which can lead to reduced accuracy and robustness, particularly when\ndealing with noisy, multi-view data. Furthermore, a major limitation of prior\napproaches is the lack of temporal modeling to capture time-dependent\nrelationships among dynamically evolving entities in a scene. To address these\nchallenges, we propose Temporal Equivariant Scene Graph Neural Network\n(TESGNN), consisting of two key components: (1) an Equivariant Scene Graph\nNeural Network (ESGNN), which extracts information from 3D point clouds to\ngenerate scene graph while preserving crucial symmetry properties, and (2) a\nTemporal Graph Matching Network, which fuses scene graphs generated by ESGNN\nacross multiple time sequences into a unified global representation using an\napproximate graph-matching algorithm. Our combined architecture TESGNN\noutperforms current state-of-the-art methods in scene graph generation,\nachieving higher accuracy and faster training convergence. Moreover, we show\nthat leveraging the symmetry-preserving property produces a more stable and\naccurate global scene representation compared to existing approaches. Last but\nnot least, it is computationally efficient and easily implementable using\nexisting frameworks, making it well-suited for real-time applications in\nrobotics and computer vision. This approach paves the way for more robust and\nscalable solutions to complex multi-view scene understanding challenges. Our\nsource code is publicly available at: https://github.com/HySonLab/TESGraph\n","authors":["Quang P. M. Pham","Khoi T. N. Nguyen","Lan C. Ngo","Truong Do","Dezhen Song","Truong-Son Hy"],"pdf_url":"https://arxiv.org/pdf/2411.10509v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2407.00609"},{"id":"http://arxiv.org/abs/2407.13929v2","updated":"2025-03-02T18:17:11Z","published":"2024-07-18T22:33:52Z","title":"Unmasking Social Bots: How Confident Are We?","summary":"  Social bots remain a major vector for spreading disinformation on social\nmedia and a menace to the public. Despite the progress made in developing\nmultiple sophisticated social bot detection algorithms and tools, bot detection\nremains a challenging, unsolved problem that is fraught with uncertainty due to\nthe heterogeneity of bot behaviors, training data, and detection algorithms.\nDetection models often disagree on whether to label the same account as bot or\nhuman-controlled. However, they do not provide any measure of uncertainty to\nindicate how much we should trust their results. We propose to address both bot\ndetection and the quantification of uncertainty at the account level - a novel\nfeature of this research. This dual focus is crucial as it allows us to\nleverage additional information related to the quantified uncertainty of each\nprediction, thereby enhancing decision-making and improving the reliability of\nbot classifications. Specifically, our approach facilitates targeted\ninterventions for bots when predictions are made with high confidence and\nsuggests caution (e.g., gathering more data) when predictions are uncertain.\n","authors":["James Giroux","Ariyarathne Gangani","Alexander C. Nwala","Cristiano Fanelli"],"pdf_url":"https://arxiv.org/pdf/2407.13929v2.pdf","comment":"15 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2410.04642v3","updated":"2025-03-02T18:16:48Z","published":"2024-10-06T22:30:14Z","title":"The Optimization Landscape of SGD Across the Feature Learning Strength","summary":"  We consider neural networks (NNs) where the final layer is down-scaled by a\nfixed hyperparameter $\\gamma$. Recent work has identified $\\gamma$ as\ncontrolling the strength of feature learning. As $\\gamma$ increases, network\nevolution changes from \"lazy\" kernel dynamics to \"rich\" feature-learning\ndynamics, with a host of associated benefits including improved performance on\ncommon tasks. In this work, we conduct a thorough empirical investigation of\nthe effect of scaling $\\gamma$ across a variety of models and datasets in the\nonline training setting. We first examine the interaction of $\\gamma$ with the\nlearning rate $\\eta$, identifying several scaling regimes in the\n$\\gamma$-$\\eta$ plane which we explain theoretically using a simple model. We\nfind that the optimal learning rate $\\eta^*$ scales non-trivially with\n$\\gamma$. In particular, $\\eta^* \\propto \\gamma^2$ when $\\gamma \\ll 1$ and\n$\\eta^* \\propto \\gamma^{2/L}$ when $\\gamma \\gg 1$ for a feed-forward network of\ndepth $L$. Using this optimal learning rate scaling, we proceed with an\nempirical study of the under-explored \"ultra-rich\" $\\gamma \\gg 1$ regime. We\nfind that networks in this regime display characteristic loss curves, starting\nwith a long plateau followed by a drop-off, sometimes followed by one or more\nadditional staircase steps. We find networks of different large $\\gamma$ values\noptimize along similar trajectories up to a reparameterization of time. We\nfurther find that optimal online performance is often found at large $\\gamma$\nand could be missed if this hyperparameter is not tuned. Our findings indicate\nthat analytical study of the large-$\\gamma$ limit may yield useful insights\ninto the dynamics of representation learning in performant models.\n","authors":["Alexander Atanasov","Alexandru Meterez","James B. Simon","Cengiz Pehlevan"],"pdf_url":"https://arxiv.org/pdf/2410.04642v3.pdf","comment":"ICLR 2025 Final Copy, 40 Pages, 45 figures"},{"id":"http://arxiv.org/abs/2408.08531v2","updated":"2025-03-02T18:15:48Z","published":"2024-08-16T04:57:54Z","title":"Detecting Unsuccessful Students in Cybersecurity Exercises in Two\n  Different Learning Environments","summary":"  This full paper in the research track evaluates the usage of data logged from\ncybersecurity exercises in order to predict students who are potentially at\nrisk of performing poorly. Hands-on exercises are essential for learning since\nthey enable students to practice their skills. In cybersecurity, hands-on\nexercises are often complex and require knowledge of many topics. Therefore,\nstudents may miss solutions due to gaps in their knowledge and become\nfrustrated, which impedes their learning. Targeted aid by the instructor helps,\nbut since the instructor's time is limited, efficient ways to detect struggling\nstudents are needed. This paper develops automated tools to predict when a\nstudent is having difficulty. We formed a dataset with the actions of 313\nstudents from two countries and two learning environments: KYPO CRP and\nEDURange. These data are used in machine learning algorithms to predict the\nsuccess of students in exercises deployed in these environments. After\nextracting features from the data, we trained and cross-validated eight\nclassifiers for predicting the exercise outcome and evaluated their predictive\npower. The contribution of this paper is comparing two approaches to feature\nengineering, modeling, and classification performance on data from two learning\nenvironments. Using the features from either learning environment, we were able\nto detect and distinguish between successful and struggling students. A\ndecision tree classifier achieved the highest balanced accuracy and sensitivity\nwith data from both learning environments. The results show that activity data\nfrom cybersecurity exercises are suitable for predicting student success. In a\npotential application, such models can aid instructors in detecting struggling\nstudents and providing targeted help. We publish data and code for building\nthese models so that others can adopt or adapt them.\n","authors":["Valdemar Švábenský","Kristián Tkáčik","Aubrey Birdwell","Richard Weiss","Ryan S. Baker","Pavel Čeleda","Jan Vykopal","Jens Mache","Ankur Chattopadhyay"],"pdf_url":"https://arxiv.org/pdf/2408.08531v2.pdf","comment":"Published in the FIE 2024 conference proceedings, see\n  https://doi.org/10.1109/FIE61694.2024.10893135"},{"id":"http://arxiv.org/abs/2410.11112v5","updated":"2025-03-02T17:48:06Z","published":"2024-10-14T21:43:48Z","title":"Differentiable Weightless Neural Networks","summary":"  We introduce the Differentiable Weightless Neural Network (DWN), a model\nbased on interconnected lookup tables. Training of DWNs is enabled by a novel\nExtended Finite Difference technique for approximate differentiation of binary\nvalues. We propose Learnable Mapping, Learnable Reduction, and Spectral\nRegularization to further improve the accuracy and efficiency of these models.\nWe evaluate DWNs in three edge computing contexts: (1) an FPGA-based hardware\naccelerator, where they demonstrate superior latency, throughput, energy\nefficiency, and model area compared to state-of-the-art solutions, (2) a\nlow-power microcontroller, where they achieve preferable accuracy to XGBoost\nwhile subject to stringent memory constraints, and (3) ultra-low-cost chips,\nwhere they consistently outperform small models in both accuracy and projected\nhardware area. DWNs also compare favorably against leading approaches for\ntabular datasets, with higher average rank. Overall, our work positions DWNs as\na pioneering solution for edge-compatible high-throughput neural networks.\n","authors":["Alan T. L. Bacellar","Zachary Susskind","Mauricio Breternitz Jr.","Eugene John","Lizy K. John","Priscila M. V. Lima","Felipe M. G. França"],"pdf_url":"https://arxiv.org/pdf/2410.11112v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02372v2","updated":"2025-03-02T17:34:53Z","published":"2024-11-04T18:40:46Z","title":"Learning General-Purpose Biomedical Volume Representations using\n  Randomized Synthesis","summary":"  Current volumetric biomedical foundation models struggle to generalize as\npublic 3D datasets are small and do not cover the broad diversity of medical\nprocedures, conditions, anatomical regions, and imaging protocols. We address\nthis by creating a representation learning method that instead anticipates\nstrong domain shifts at training time itself. We first propose a data engine\nthat synthesizes highly variable training samples that would enable\ngeneralization to new biomedical contexts. To then train a single 3D network\nfor any voxel-level task, we develop a contrastive learning method that\npretrains the network to be stable against nuisance imaging variation simulated\nby the data engine, a key inductive bias for generalization. This network's\nfeatures can be used as robust representations of input images for downstream\ntasks and its weights provide a strong, dataset-agnostic initialization for\nfinetuning on new datasets. As a result, we set new standards across both\nmultimodality registration and few-shot segmentation, a first for any 3D\nbiomedical vision model, all without (pre-)training on any existing dataset of\nreal images.\n","authors":["Neel Dey","Benjamin Billot","Hallee E. Wong","Clinton J. Wang","Mengwei Ren","P. Ellen Grant","Adrian V. Dalca","Polina Golland"],"pdf_url":"https://arxiv.org/pdf/2411.02372v2.pdf","comment":"ICLR 2025: International Conference on Learning Representations. Code\n  and model weights available at https://github.com/neel-dey/anatomix.\n  Keywords: synthetic data, representation learning, medical image analysis,\n  image registration, image segmentation"},{"id":"http://arxiv.org/abs/2403.08743v2","updated":"2025-03-02T17:33:03Z","published":"2024-03-13T17:46:28Z","title":"Prompting Fairness: Integrating Causality to Debias Large Language\n  Models","summary":"  Large language models (LLMs), despite their remarkable capabilities, are\nsusceptible to generating biased and discriminatory responses. As LLMs\nincreasingly influence high-stakes decision-making (e.g., hiring and\nhealthcare), mitigating these biases becomes critical. In this work, we propose\na causality-guided debiasing framework to tackle social biases, aiming to\nreduce the objectionable dependence between LLMs' decisions and the social\ninformation in the input. Our framework introduces a novel perspective to\nidentify how social information can affect an LLM's decision through different\ncausal pathways. Leveraging these causal insights, we outline principled\nprompting strategies that regulate these pathways through selection mechanisms.\nThis framework not only unifies existing prompting-based debiasing techniques,\nbut also opens up new directions for reducing bias by encouraging the model to\nprioritize fact-based reasoning over reliance on biased social cues. We\nvalidate our framework through extensive experiments on real-world datasets\nacross multiple domains, demonstrating its effectiveness in debiasing LLM\ndecisions, even with only black-box access to the model.\n","authors":["Jingling Li","Zeyu Tang","Xiaoyu Liu","Peter Spirtes","Kun Zhang","Liu Leqi","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2403.08743v2.pdf","comment":"24 pages, 10 figures"},{"id":"http://arxiv.org/abs/2502.03391v3","updated":"2025-03-02T17:32:48Z","published":"2025-02-05T17:29:12Z","title":"Explain Yourself, Briefly! Self-Explaining Neural Networks with Concise\n  Sufficient Reasons","summary":"  *Minimal sufficient reasons* represent a prevalent form of explanation - the\nsmallest subset of input features which, when held constant at their\ncorresponding values, ensure that the prediction remains unchanged. Previous\n*post-hoc* methods attempt to obtain such explanations but face two main\nlimitations: (1) Obtaining these subsets poses a computational challenge,\nleading most scalable methods to converge towards suboptimal, less meaningful\nsubsets; (2) These methods heavily rely on sampling out-of-distribution input\nassignments, potentially resulting in counterintuitive behaviors. To tackle\nthese limitations, we propose in this work a self-supervised training approach,\nwhich we term *sufficient subset training* (SST). Using SST, we train models to\ngenerate concise sufficient reasons for their predictions as an integral part\nof their output. Our results indicate that our framework produces succinct and\nfaithful subsets substantially more efficiently than competing post-hoc\nmethods, while maintaining comparable predictive performance.\n","authors":["Shahaf Bassan","Ron Eliav","Shlomit Gur"],"pdf_url":"https://arxiv.org/pdf/2502.03391v3.pdf","comment":"To appear in ICLR 2025"},{"id":"http://arxiv.org/abs/2410.06262v2","updated":"2025-03-02T17:20:26Z","published":"2024-10-08T18:02:29Z","title":"SymDiff: Equivariant Diffusion via Stochastic Symmetrisation","summary":"  We propose SymDiff, a method for constructing equivariant diffusion models\nusing the framework of stochastic symmetrisation. SymDiff resembles a learned\ndata augmentation that is deployed at sampling time, and is lightweight,\ncomputationally efficient, and easy to implement on top of arbitrary\noff-the-shelf models. In contrast to previous work, SymDiff typically does not\nrequire any neural network components that are intrinsically equivariant,\navoiding the need for complex parameterisations or the use of higher-order\ngeometric features. Instead, our method can leverage highly scalable modern\narchitectures as drop-in replacements for these more constrained alternatives.\nWe show that this additional flexibility yields significant empirical benefit\nfor $\\mathrm{E}(3)$-equivariant molecular generation. To the best of our\nknowledge, this is the first application of symmetrisation to generative\nmodelling, suggesting its potential in this domain more generally.\n","authors":["Leo Zhang","Kianoosh Ashouritaklimi","Yee Whye Teh","Rob Cornish"],"pdf_url":"https://arxiv.org/pdf/2410.06262v2.pdf","comment":"Camera-ready version for ICLR 2025"},{"id":"http://arxiv.org/abs/2410.04810v2","updated":"2025-03-02T17:18:04Z","published":"2024-10-07T07:45:18Z","title":"FedBiP: Heterogeneous One-Shot Federated Learning with Personalized\n  Latent Diffusion Models","summary":"  One-Shot Federated Learning (OSFL), a special decentralized machine learning\nparadigm, has recently gained significant attention. OSFL requires only a\nsingle round of client data or model upload, which reduces communication costs\nand mitigates privacy threats compared to traditional FL. Despite these\npromising prospects, existing methods face challenges due to client data\nheterogeneity and limited data quantity when applied to real-world OSFL\nsystems. Recently, Latent Diffusion Models (LDM) have shown remarkable\nadvancements in synthesizing high-quality images through pretraining on\nlarge-scale datasets, thereby presenting a potential solution to overcome these\nissues. However, directly applying pretrained LDM to heterogeneous OSFL results\nin significant distribution shifts in synthetic data, leading to performance\ndegradation in classification models trained on such data. This issue is\nparticularly pronounced in rare domains, such as medical imaging, which are\nunderrepresented in LDM's pretraining data. To address this challenge, we\npropose Federated Bi-Level Personalization (FedBiP), which personalizes the\npretrained LDM at both instance-level and concept-level. Hereby, FedBiP\nsynthesizes images following the client's local data distribution without\ncompromising the privacy regulations. FedBiP is also the first approach to\nsimultaneously address feature space heterogeneity and client data scarcity in\nOSFL. Our method is validated through extensive experiments on three OSFL\nbenchmarks with feature space heterogeneity, as well as on challenging medical\nand satellite image datasets with label heterogeneity. The results demonstrate\nthe effectiveness of FedBiP, which substantially outperforms other OSFL\nmethods.\n","authors":["Haokun Chen","Hang Li","Yao Zhang","Jinhe Bi","Gengyuan Zhang","Yueqi Zhang","Philip Torr","Jindong Gu","Denis Krompass","Volker Tresp"],"pdf_url":"https://arxiv.org/pdf/2410.04810v2.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2410.03011v2","updated":"2025-03-02T17:17:22Z","published":"2024-10-03T21:42:21Z","title":"Towards Understanding the Universality of Transformers for Next-Token\n  Prediction","summary":"  Causal Transformers are trained to predict the next token for a given\ncontext. While it is widely accepted that self-attention is crucial for\nencoding the causal structure of sequences, the precise underlying mechanism\nbehind this in-context autoregressive learning ability remains unclear. In this\npaper, we take a step towards understanding this phenomenon by studying the\napproximation ability of Transformers for next-token prediction. Specifically,\nwe explore the capacity of causal Transformers to predict the next token\n$x_{t+1}$ given an autoregressive sequence $(x_1, \\dots, x_t)$ as a prompt,\nwhere $ x_{t+1} = f(x_t) $, and $ f $ is a context-dependent function that\nvaries with each sequence. On the theoretical side, we focus on specific\ninstances, namely when $ f $ is linear or when $ (x_t)_{t \\geq 1} $ is\nperiodic. We explicitly construct a Transformer (with linear, exponential, or\nsoftmax attention) that learns the mapping $f$ in-context through a causal\nkernel descent method. The causal kernel descent method we propose provably\nestimates $x_{t+1} $ based solely on past and current observations $ (x_1,\n\\dots, x_t) $, with connections to the Kaczmarz algorithm in Hilbert spaces. We\npresent experimental results that validate our theoretical findings and suggest\ntheir applicability to more general mappings $f$.\n","authors":["Michael E. Sander","Gabriel Peyré"],"pdf_url":"https://arxiv.org/pdf/2410.03011v2.pdf","comment":"ICLR 2025, 20 pages"},{"id":"http://arxiv.org/abs/2502.11882v3","updated":"2025-03-02T17:15:11Z","published":"2025-02-17T15:09:45Z","title":"Leveraging Dual Process Theory in Language Agent Framework for Real-time\n  Simultaneous Human-AI Collaboration","summary":"  Agents built on large language models (LLMs) have excelled in turn-by-turn\nhuman-AI collaboration but struggle with simultaneous tasks requiring real-time\ninteraction. Latency issues and the challenge of inferring variable human\nstrategies hinder their ability to make autonomous decisions without explicit\ninstructions. Through experiments with current independent System 1 and System\n2 methods, we validate the necessity of using Dual Process Theory (DPT) in\nreal-time tasks. We propose DPT-Agent, a novel language agent framework that\nintegrates System 1 and System 2 for efficient real-time simultaneous human-AI\ncollaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and\ncode-as-policy for fast, intuitive, and controllable decision-making.\nDPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous\nreflection to infer human intentions and perform reasoning-based autonomous\ndecisions. We demonstrate the effectiveness of DPT-Agent through further\nexperiments with rule-based agents and human collaborators, showing significant\nimprovements over mainstream LLM-based frameworks. DPT-Agent can effectively\nhelp LLMs convert correct slow thinking and reasoning into executable actions,\nthereby improving performance. To the best of our knowledge, DPT-Agent is the\nfirst language agent framework that achieves successful real-time simultaneous\nhuman-AI collaboration autonomously. Code of DPT-Agent can be found in\nhttps://github.com/sjtu-marl/DPT-Agent.\n","authors":["Shao Zhang","Xihuai Wang","Wenhao Zhang","Chaoran Li","Junru Song","Tingyu Li","Lin Qiu","Xuezhi Cao","Xunliang Cai","Wen Yao","Weinan Zhang","Xinbing Wang","Ying Wen"],"pdf_url":"https://arxiv.org/pdf/2502.11882v3.pdf","comment":"Preprint under review. Update the experimental results of the\n  DeepSeek-R1 series models, o3-mini-high and o3-mini-medium"},{"id":"http://arxiv.org/abs/2403.18035v4","updated":"2025-03-02T16:41:49Z","published":"2024-03-26T18:40:36Z","title":"Bidirectional Consistency Models","summary":"  Diffusion models (DMs) are capable of generating remarkably high-quality\nsamples by iteratively denoising a random vector, a process that corresponds to\nmoving along the probability flow ordinary differential equation (PF ODE).\nInterestingly, DMs can also invert an input image to noise by moving backward\nalong the PF ODE, a key operation for downstream tasks such as interpolation\nand image editing. However, the iterative nature of this process restricts its\nspeed, hindering its broader application. Recently, Consistency Models (CMs)\nhave emerged to address this challenge by approximating the integral of the PF\nODE, largely reducing the number of iterations. Yet, the absence of an explicit\nODE solver complicates the inversion process. To resolve this, we introduce\nBidirectional Consistency Model (BCM), which learns a single neural network\nthat enables both forward and backward traversal along the PF ODE, efficiently\nunifying generation and inversion tasks within one framework. We can train BCM\nfrom scratch or tune it using a pretrained consistency model, which reduces the\ntraining cost and increases scalability. We demonstrate that BCM enables\none-step generation and inversion while also allowing the use of additional\nsteps to enhance generation quality or reduce reconstruction error. We further\nshowcase BCM's capability in downstream tasks, such as interpolation and\ninpainting. Our code and weights are available at\nhttps://github.com/Mosasaur5526/BCM-iCT-torch.\n","authors":["Liangchen Li","Jiajun He"],"pdf_url":"https://arxiv.org/pdf/2403.18035v4.pdf","comment":"39 pages, 27 figures; a shorter version of this paper was acceppted\n  at the ICML 2024 Workshop on Structured Probabilistic Inference & Generative\n  Modeling"},{"id":"http://arxiv.org/abs/2412.07067v3","updated":"2025-03-02T16:40:03Z","published":"2024-12-10T00:19:28Z","title":"MoE-CAP: Benchmarking Cost, Accuracy and Performance of Sparse\n  Mixture-of-Experts Systems","summary":"  The Mixture-of-Experts (MoE) architecture is increasingly favored for scaling\nLarge Language Models (LLMs). Its key feature, sparse activation, selectively\nactivates only a subset of parameters (experts) per token, reducing memory\nbandwidth and compute FLOPs compared to dense models. To capitalize on this,\nMoE designers leverage heterogeneous compute and memory hardware to lower\nsystem costs. However, the interaction between model sparsity and hardware\nheterogeneity introduces trade-offs in Cost, Accuracy, and Performance (CAP).\nTo address this, we introduce MoE-CAP, a benchmarking method for evaluating\nsparse MoE systems across these three dimensions. Its key innovation is a\nsparsity-aware CAP analysis model, the first to integrate cost, performance,\nand accuracy metrics into a single diagram while estimating the impact of\nsparsity on system performance. MoE-CAP helps practitioners optimize hardware\nprovisioning for an MoE model-or vice versa. MoE-CAP supports various MoE\nmodels and provides more accurate metrics than existing methods.\n","authors":["Yao Fu","Yinsicheng Jiang","Yeqi Huang","Ping Nie","Zhan Lu","Leyang Xue","Congjie He","Man-Kit Sit","Jilong Xue","Li Dong","Ziming Miao","Kai Zou","Edoardo Ponti","Luo Mai"],"pdf_url":"https://arxiv.org/pdf/2412.07067v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2308.16471v4","updated":"2025-03-02T16:36:29Z","published":"2023-08-31T05:26:14Z","title":"Foundational Policy Acquisition via Multitask Learning for Motor Skill\n  Generation","summary":"  In this study, we propose a multitask reinforcement learning algorithm for\nfoundational policy acquisition to generate novel motor skills.\n\\textcolor{\\hcolor}{Learning the rich representation of the multitask policy is\na challenge in dynamic movement generation tasks because the policy needs to\ncope with changes in goals or environments with different reward functions or\nphysical parameters. Inspired by human sensorimotor adaptation mechanisms, we\ndeveloped the learning pipeline to construct the encoder-decoder networks and\nnetwork selection to facilitate foundational policy acquisition under multiple\nsituations. First, we compared the proposed method with previous multitask\nreinforcement learning methods in the standard multi-locomotion tasks. The\nresults showed that the proposed approach outperformed the baseline methods.\nThen, we applied the proposed method to the ball heading task using a monopod\nrobot model to evaluate skill generation performance. The results showed that\nthe proposed method was able to adapt to novel target positions or\ninexperienced ball restitution coefficients but to acquire a foundational\npolicy network, originally learned for heading motion, which can generate an\nentirely new overhead kicking skill.\n","authors":["Satoshi Yamamori","Jun Morimoto"],"pdf_url":"https://arxiv.org/pdf/2308.16471v4.pdf","comment":"11 pages, 9 figures"},{"id":"http://arxiv.org/abs/2406.03735v2","updated":"2025-03-02T16:21:37Z","published":"2024-06-06T04:19:55Z","title":"Phase-Amplitude Reduction-Based Imitation Learning","summary":"  In this study, we propose the use of the phase-amplitude reduction method to\nconstruct an imitation learning framework. Imitating human movement\ntrajectories is recognized as a promising strategy for generating a range of\nhuman-like robot movements. Unlike previous dynamical system-based imitation\nlearning approaches, our proposed method allows the robot not only to imitate a\nlimit cycle trajectory but also to replicate the transient movement from the\ninitial or disturbed state to the limit cycle. Consequently, our method offers\na safer imitation learning approach that avoids generating unpredictable\nmotions immediately after disturbances or from a specified initial state. We\nfirst validated our proposed method by reconstructing a simple limit-cycle\nattractor. We then compared the proposed approach with a conventional method on\na lemniscate trajectory tracking task with a simulated robot arm. Our findings\nconfirm that our proposed method can more accurately generate transient\nmovements to converge on a target periodic attractor compared to the previous\nstandard approach. Subsequently, we applied our method to a real robot arm to\nimitate periodic human movements.\n","authors":["Satoshi Yamamori","Jun Morimoto"],"pdf_url":"https://arxiv.org/pdf/2406.03735v2.pdf","comment":"21 pages, 8 figures"},{"id":"http://arxiv.org/abs/2502.18960v2","updated":"2025-03-02T16:14:51Z","published":"2025-02-26T09:17:04Z","title":"Nonparametric Heterogeneous Long-term Causal Effect Estimation via Data\n  Combination","summary":"  Long-term causal inference has drawn increasing attention in many scientific\ndomains. Existing methods mainly focus on estimating average long-term causal\neffects by combining long-term observational data and short-term experimental\ndata. However, it is still understudied how to robustly and effectively\nestimate heterogeneous long-term causal effects, significantly limiting\npractical applications. In this paper, we propose several two-stage style\nnonparametric estimators for heterogeneous long-term causal effect estimation,\nincluding propensity-based, regression-based, and multiple robust estimators.\nWe conduct a comprehensive theoretical analysis of their asymptotic properties\nunder mild assumptions, with the ultimate goal of building a better\nunderstanding of the conditions under which some estimators can be expected to\nperform better. Extensive experiments across several semi-synthetic and\nreal-world datasets validate the theoretical results and demonstrate the\neffectiveness of the proposed estimators.\n","authors":["Weilin Chen","Ruichu Cai","Junjie Wan","Zeqin Yang","José Miguel Hernández-Lobato"],"pdf_url":"https://arxiv.org/pdf/2502.18960v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20138v5","updated":"2025-03-02T15:57:39Z","published":"2024-12-28T12:54:06Z","title":"TradingAgents: Multi-Agents LLM Financial Trading Framework","summary":"  Significant progress has been made in automated problem-solving using\nsocieties of agents powered by large language models (LLMs). In finance,\nefforts have largely focused on single-agent systems handling specific tasks or\nmulti-agent frameworks independently gathering data. However, multi-agent\nsystems' potential to replicate real-world trading firms' collaborative\ndynamics remains underexplored. TradingAgents proposes a novel stock trading\nframework inspired by trading firms, featuring LLM-powered agents in\nspecialized roles such as fundamental analysts, sentiment analysts, technical\nanalysts, and traders with varied risk profiles. The framework includes Bull\nand Bear researcher agents assessing market conditions, a risk management team\nmonitoring exposure, and traders synthesizing insights from debates and\nhistorical data to make informed decisions. By simulating a dynamic,\ncollaborative trading environment, this framework aims to improve trading\nperformance. Detailed architecture and extensive experiments reveal its\nsuperiority over baseline models, with notable improvements in cumulative\nreturns, Sharpe ratio, and maximum drawdown, highlighting the potential of\nmulti-agent LLM frameworks in financial trading. TradingAgents is available at\nhttps://github.com/PioneerFintech.\n","authors":["Yijia Xiao","Edward Sun","Di Luo","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2412.20138v5.pdf","comment":"Multi-Agent AI in the Real World @ AAAI 2025"},{"id":"http://arxiv.org/abs/2408.11915v2","updated":"2025-03-02T15:55:14Z","published":"2024-08-21T18:06:15Z","title":"Video-Foley: Two-Stage Video-To-Sound Generation via Temporal Event\n  Condition For Foley Sound","summary":"  Foley sound synthesis is crucial for multimedia production, enhancing user\nexperience by synchronizing audio and video both temporally and semantically.\nRecent studies on automating this labor-intensive process through\nvideo-to-sound generation face significant challenges. Systems lacking explicit\ntemporal features suffer from poor alignment and controllability, while\ntimestamp-based models require costly and subjective human annotation. We\npropose Video-Foley, a video-to-sound system using Root Mean Square (RMS) as an\nintuitive condition with semantic timbre prompts (audio or text). RMS, a\nframe-level intensity envelope closely related to audio semantics, acts as a\ntemporal event feature to guide audio generation from video. The\nannotation-free self-supervised learning framework consists of two stages,\nVideo2RMS and RMS2Sound, incorporating novel ideas including RMS discretization\nand RMS-ControlNet with a pretrained text-to-audio model. Our extensive\nevaluation shows that Video-Foley achieves state-of-the-art performance in\naudio-visual alignment and controllability for sound timing, intensity, timbre,\nand nuance. Source code, model weights and demos are available on our companion\nwebsite. (https://jnwnlee.github.io/video-foley-demo)\n","authors":["Junwon Lee","Jaekwon Im","Dabin Kim","Juhan Nam"],"pdf_url":"https://arxiv.org/pdf/2408.11915v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18709v4","updated":"2025-03-02T15:37:39Z","published":"2023-10-28T13:37:52Z","title":"Audio-Visual Instance Segmentation","summary":"  In this paper, we propose a new multi-modal task, termed audio-visual\ninstance segmentation (AVIS), which aims to simultaneously identify, segment\nand track individual sounding object instances in audible videos. To facilitate\nthis research, we introduce a high-quality benchmark named AVISeg, containing\nover 90K instance masks from 26 semantic categories in 926 long videos.\nAdditionally, we propose a strong baseline model for this task. Our model first\nlocalizes sound source within each frame, and condenses object-specific\ncontexts into concise tokens. Then it builds long-range audio-visual\ndependencies between these tokens using window-based attention, and tracks\nsounding objects among the entire video sequences. Extensive experiments reveal\nthat our method performs best on AVISeg, surpassing the existing methods from\nrelated tasks. We further conduct the evaluation on several multi-modal large\nmodels. Unfortunately, they exhibits subpar performance on instance-level sound\nsource localization and temporal perception. We expect that AVIS will inspire\nthe community towards a more comprehensive multi-modal understanding. Dataset\nand code is available at https://github.com/ruohaoguo/avis.\n","authors":["Ruohao Guo","Xianghua Ying","Yaru Chen","Dantong Niu","Guangyao Li","Liao Qu","Yanyu Qi","Jinxing Zhou","Bowei Xing","Wenzhen Yue","Ji Shi","Qixun Wang","Peiliang Zhang","Buwen Liang"],"pdf_url":"https://arxiv.org/pdf/2310.18709v4.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2412.12164v2","updated":"2025-03-02T15:12:38Z","published":"2024-12-11T19:12:22Z","title":"GAMED: Knowledge Adaptive Multi-Experts Decoupling for Multimodal Fake\n  News Detection","summary":"  Multimodal fake news detection often involves modelling heterogeneous data\nsources, such as vision and language. Existing detection methods typically rely\non fusion effectiveness and cross-modal consistency to model the content,\ncomplicating understanding how each modality affects prediction accuracy.\nAdditionally, these methods are primarily based on static feature modelling,\nmaking it difficult to adapt to the dynamic changes and relationships between\ndifferent data modalities. This paper develops a significantly novel approach,\nGAMED, for multimodal modelling, which focuses on generating distinctive and\ndiscriminative features through modal decoupling to enhance cross-modal\nsynergies, thereby optimizing overall performance in the detection process.\nGAMED leverages multiple parallel expert networks to refine features and\npre-embed semantic knowledge to improve the experts' ability in information\nselection and viewpoint sharing. Subsequently, the feature distribution of each\nmodality is adaptively adjusted based on the respective experts' opinions.\nGAMED also introduces a novel classification technique to dynamically manage\ncontributions from different modalities, while improving the explainability of\ndecisions. Experimental results on the Fakeddit and Yang datasets demonstrate\nthat GAMED performs better than recently developed state-of-the-art models. The\nsource code can be accessed at https://github.com/slz0925/GAMED.\n","authors":["Lingzhi Shen","Yunfei Long","Xiaohao Cai","Imran Razzak","Guanming Chen","Kang Liu","Shoaib Jameel"],"pdf_url":"https://arxiv.org/pdf/2412.12164v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.07189v2","updated":"2025-03-02T15:05:58Z","published":"2024-02-11T12:54:07Z","title":"Improving LSH via Tensorized Random Projection","summary":"  Locality sensitive hashing (LSH) is a fundamental algorithmic toolkit used by\ndata scientists for approximate nearest neighbour search problems that have\nbeen used extensively in many large scale data processing applications such as\nnear duplicate detection, nearest neighbour search, clustering, etc. In this\nwork, we aim to propose faster and space efficient locality sensitive hash\nfunctions for Euclidean distance and cosine similarity for tensor data.\nTypically, the naive approach for obtaining LSH for tensor data involves first\nreshaping the tensor into vectors, followed by applying existing LSH methods\nfor vector data $E2LSH$ and $SRP$. However, this approach becomes impractical\nfor higher order tensors because the size of the reshaped vector becomes\nexponential in the order of the tensor. Consequently, the size of LSH\nparameters increases exponentially. To address this problem, we suggest two\nmethods for LSH for Euclidean distance and cosine similarity, namely\n$CP-E2LSH$, $TT-E2LSH$, and $CP-SRP$, $TT-SRP$, respectively, building on $CP$\nand tensor train $(TT)$ decompositions techniques. Our approaches are space\nefficient and can be efficiently applied to low rank $CP$ or $TT$ tensors. We\nprovide a rigorous theoretical analysis of our proposal on their correctness\nand efficacy.\n","authors":["Bhisham Dev Verma","Rameshwar Pratap"],"pdf_url":"https://arxiv.org/pdf/2402.07189v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.14154v2","updated":"2025-03-02T14:41:12Z","published":"2024-07-19T09:34:04Z","title":"Where is the Testbed for my Federated Learning Research?","summary":"  Progressing beyond centralized AI is of paramount importance, yet,\ndistributed AI solutions, in particular various federated learning (FL)\nalgorithms, are often not comprehensively assessed, which prevents the research\ncommunity from identifying the most promising approaches and practitioners from\nbeing convinced that a certain solution is deployment-ready. The largest hurdle\ntowards FL algorithm evaluation is the difficulty of conducting real-world\nexperiments over a variety of FL client devices and different platforms, with\ndifferent datasets and data distribution, all while assessing various\ndimensions of algorithm performance, such as inference accuracy, energy\nconsumption, and time to convergence, to name a few. In this paper, we present\nCoLExT, a real-world testbed for FL research. CoLExT is designed to streamline\nexperimentation with custom FL algorithms in a rich testbed configuration\nspace, with a large number of heterogeneous edge devices, ranging from\nsingle-board computers to smartphones, and provides real-time collection and\nvisualization of a variety of metrics through automatic instrumentation.\nAccording to our evaluation, porting FL algorithms to CoLExT requires minimal\ninvolvement from the developer, and the instrumentation introduces minimal\nresource usage overhead. Furthermore, through an initial investigation\ninvolving popular FL algorithms running on CoLExT, we reveal previously unknown\ntrade-offs, inefficiencies, and programming bugs.\n","authors":["Janez Božič","Amândio R. Faustino","Boris Radovič","Marco Canini","Veljko Pejović"],"pdf_url":"https://arxiv.org/pdf/2407.14154v2.pdf","comment":"SEC 2024"},{"id":"http://arxiv.org/abs/2410.10781v2","updated":"2025-03-02T14:37:53Z","published":"2024-10-14T17:50:28Z","title":"When Attention Sink Emerges in Language Models: An Empirical View","summary":"  Language Models (LMs) assign significant attention to the first token, even\nif it is not semantically important, which is known as attention sink. This\nphenomenon has been widely adopted in applications such as streaming/long\ncontext generation, KV cache optimization, inference acceleration, model\nquantization, and others. Despite its widespread use, a deep understanding of\nattention sink in LMs is still lacking. In this work, we first demonstrate that\nattention sinks exist universally in LMs with various inputs, even in small\nmodels. Furthermore, attention sink is observed to emerge during the LM\npre-training, motivating us to investigate how optimization, data distribution,\nloss function, and model architecture in LM pre-training influence its\nemergence. We highlight that attention sink emerges after effective\noptimization on sufficient training data. The sink position is highly\ncorrelated with the loss function and data distribution. Most importantly, we\nfind that attention sink acts more like key biases, storing extra attention\nscores, which could be non-informative and not contribute to the value\ncomputation. We also observe that this phenomenon (at least partially) stems\nfrom tokens' inner dependence on attention scores as a result of softmax\nnormalization. After relaxing such dependence by replacing softmax attention\nwith other attention operations, such as sigmoid attention without\nnormalization, attention sinks do not emerge in LMs up to 1B parameters. The\ncode is available at https://github.com/sail-sg/Attention-Sink.\n","authors":["Xiangming Gu","Tianyu Pang","Chao Du","Qian Liu","Fengzhuo Zhang","Cunxiao Du","Ye Wang","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2410.10781v2.pdf","comment":"ICLR 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2410.07137v2","updated":"2025-03-02T14:28:33Z","published":"2024-10-09T17:53:06Z","title":"Cheating Automatic LLM Benchmarks: Null Models Achieve High Win Rates","summary":"  Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and\nMT-Bench, have become popular for evaluating language models due to their\ncost-effectiveness and scalability compared to human evaluation. Achieving high\nwin rates on these benchmarks can significantly boost the promotional impact of\nnewly released language models. This promotional benefit may motivate tricks,\nsuch as manipulating model output length or style to game win rates, even\nthough several mechanisms have been developed to control length and disentangle\nstyle to reduce gameability. Nonetheless, we show that even a \"null model\" that\nalways outputs a constant response (irrelevant to input instructions) can cheat\nautomatic benchmarks and achieve top-ranked win rates: an 86.5% LC win rate on\nAlpacaEval 2.0; an 83.0 score on Arena-Hard-Auto; and a 9.55 score on MT-Bench.\nMoreover, the crafted cheating outputs are transferable because we assume that\nthe instructions of these benchmarks (e.g., 805 samples of AlpacaEval 2.0) are\nprivate and cannot be accessed. While our experiments are primarily\nproof-of-concept, an adversary could use LLMs to generate more imperceptible\ncheating responses, unethically benefiting from high win rates and promotional\nimpact. Our findings call for the development of anti-cheating mechanisms for\nreliable automatic benchmarks. The code is available at\nhttps://github.com/sail-sg/Cheating-LLM-Benchmarks.\n","authors":["Xiaosen Zheng","Tianyu Pang","Chao Du","Qian Liu","Jing Jiang","Min Lin"],"pdf_url":"https://arxiv.org/pdf/2410.07137v2.pdf","comment":"ICLR 2025 (Oral)"},{"id":"http://arxiv.org/abs/2410.16699v2","updated":"2025-03-02T14:18:13Z","published":"2024-10-22T05:11:45Z","title":"Graph Transformers Dream of Electric Flow","summary":"  We show theoretically and empirically that the linear Transformer, when\napplied to graph data, can implement algorithms that solve canonical problems\nsuch as electric flow and eigenvector decomposition. The Transformer has access\nto information on the input graph only via the graph's incidence matrix. We\npresent explicit weight configurations for implementing each algorithm, and we\nbound the constructed Transformers' errors by the errors of the underlying\nalgorithms. Our theoretical findings are corroborated by experiments on\nsynthetic data. Additionally, on a real-world molecular regression task, we\nobserve that the linear Transformer is capable of learning a more effective\npositional encoding than the default one based on Laplacian eigenvectors. Our\nwork is an initial step towards elucidating the inner-workings of the\nTransformer for graph data. Code is available at\nhttps://github.com/chengxiang/LinearGraphTransformer\n","authors":["Xiang Cheng","Lawrence Carin","Suvrit Sra"],"pdf_url":"https://arxiv.org/pdf/2410.16699v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15252v2","updated":"2025-03-02T14:10:09Z","published":"2024-05-24T06:22:01Z","title":"Accelerating 3D Molecule Generation via Jointly Geometric Optimal\n  Transport","summary":"  This paper proposes a new 3D molecule generation framework, called GOAT, for\nfast and effective 3D molecule generation based on the flow-matching optimal\ntransport objective. Specifically, we formulate a geometric transport formula\nfor measuring the cost of mapping multi-modal features (e.g., continuous atom\ncoordinates and categorical atom types) between a base distribution and a\ntarget data distribution. Our formula is solved within a joint, equivariant,\nand smooth representation space. This is achieved by transforming the\nmulti-modal features into a continuous latent space with equivariant networks.\nIn addition, we find that identifying optimal distributional coupling is\nnecessary for fast and effective transport between any two distributions. We\nfurther propose a mechanism for estimating and purifying optimal coupling to\ntrain the flow model with optimal transport. By doing so, GOAT can turn\narbitrary distribution couplings into new deterministic couplings, leading to\nan estimated optimal transport plan for fast 3D molecule generation. The\npurification filters out the subpar molecules to ensure the ultimate generation\nquality. We theoretically and empirically prove that the proposed optimal\ncoupling estimation and purification yield transport plan with non-increasing\ncost. Finally, extensive experiments show that GOAT enjoys the efficiency of\nsolving geometric optimal transport, leading to a double speedup compared to\nthe sub-optimal method while achieving the best generation quality regarding\nvalidity, uniqueness, and novelty. The code is available at\nhttps://github.com/WanyuGroup/ICLR2025-GOAT.\n","authors":["Haokai Hong","Wanyu Lin","Kay Chen Tan"],"pdf_url":"https://arxiv.org/pdf/2405.15252v2.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2302.01310v3","updated":"2025-03-02T13:29:19Z","published":"2023-02-02T18:33:34Z","title":"Knowledge Gradient for Multi-Objective Bayesian Optimization with\n  Decoupled Evaluations","summary":"  Multi-objective Bayesian optimization aims to find the Pareto front of\ntrade-offs between a set of expensive objectives while collecting as few\nsamples as possible. In some cases, it is possible to evaluate the objectives\nseparately, and a different latency or evaluation cost can be associated with\neach objective. This decoupling of the objectives presents an opportunity to\nlearn the Pareto front faster by avoiding unnecessary, expensive evaluations.\nWe propose a scalarization based knowledge gradient acquisition function which\naccounts for the different evaluation costs of the objectives. We prove\nasymptotic consistency of the estimator of the optimum for an arbitrary,\nD-dimensional, real compact search space and show empirically that the\nalgorithm performs comparably with the state of the art and significantly\noutperforms versions which always evaluate both objectives.\n","authors":["Jack M. Buckingham","Sebastian Rojas Gonzalez","Juergen Branke"],"pdf_url":"https://arxiv.org/pdf/2302.01310v3.pdf","comment":"36 pages. This preprint has not undergone peer review (when\n  applicable) or any post-submission improvements or corrections. The Version\n  of Record of this contribution is published in 'Evolutionary Multi-Criterion\n  Optimization', LNCS 15513, and is available online at\n  https://doi.org/10.1007/978-981-96-3538-2_9"},{"id":"http://arxiv.org/abs/2411.12556v2","updated":"2025-03-02T13:29:03Z","published":"2024-11-19T15:15:45Z","title":"UMGAD: Unsupervised Multiplex Graph Anomaly Detection","summary":"  Graph anomaly detection (GAD) is a critical task in graph machine learning,\nwith the primary objective of identifying anomalous nodes that deviate\nsignificantly from the majority. This task is widely applied in various\nreal-world scenarios, including fraud detection and social network analysis.\nHowever, existing GAD methods still face two major challenges: (1) They are\noften limited to detecting anomalies in single-type interaction graphs and\nstruggle with multiple interaction types in multiplex heterogeneous graphs. (2)\nIn unsupervised scenarios, selecting appropriate anomaly score thresholds\nremains a significant challenge for accurate anomaly detection. To address the\nabove challenges, we propose a novel Unsupervised Multiplex Graph Anomaly\nDetection method, named UMGAD. We first learn multi-relational correlations\namong nodes in multiplex heterogeneous graphs and capture anomaly information\nduring node attribute and structure reconstruction through graph-masked\nautoencoder (GMAE). Then, to further extract abnormal information, we generate\nattribute-level and subgraph-level augmented-view graphs respectively, and\nperform attribute and structure reconstruction through GMAE. Finally, we learn\nto optimize node attributes and structural features through contrastive\nlearning between original-view and augmented-view graphs to improve the model's\nability to capture anomalies. Meanwhile, we also propose a new anomaly score\nthreshold selection strategy, which allows the model to be independent of\nground truth information in real unsupervised scenarios. Extensive experiments\non four datasets show that our UMGAD significantly outperforms state-of-the-art\nmethods, achieving average improvements of 13.48% in AUC and 11.68% in Macro-F1\nacross all datasets.\n","authors":["Xiang Li","Jianpeng Qi","Zhongying Zhao","Guanjie Zheng","Lei Cao","Junyu Dong","Yanwei Yu"],"pdf_url":"https://arxiv.org/pdf/2411.12556v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.04810v2","updated":"2025-03-02T17:18:04Z","published":"2024-10-07T07:45:18Z","title":"FedBiP: Heterogeneous One-Shot Federated Learning with Personalized\n  Latent Diffusion Models","summary":"  One-Shot Federated Learning (OSFL), a special decentralized machine learning\nparadigm, has recently gained significant attention. OSFL requires only a\nsingle round of client data or model upload, which reduces communication costs\nand mitigates privacy threats compared to traditional FL. Despite these\npromising prospects, existing methods face challenges due to client data\nheterogeneity and limited data quantity when applied to real-world OSFL\nsystems. Recently, Latent Diffusion Models (LDM) have shown remarkable\nadvancements in synthesizing high-quality images through pretraining on\nlarge-scale datasets, thereby presenting a potential solution to overcome these\nissues. However, directly applying pretrained LDM to heterogeneous OSFL results\nin significant distribution shifts in synthetic data, leading to performance\ndegradation in classification models trained on such data. This issue is\nparticularly pronounced in rare domains, such as medical imaging, which are\nunderrepresented in LDM's pretraining data. To address this challenge, we\npropose Federated Bi-Level Personalization (FedBiP), which personalizes the\npretrained LDM at both instance-level and concept-level. Hereby, FedBiP\nsynthesizes images following the client's local data distribution without\ncompromising the privacy regulations. FedBiP is also the first approach to\nsimultaneously address feature space heterogeneity and client data scarcity in\nOSFL. Our method is validated through extensive experiments on three OSFL\nbenchmarks with feature space heterogeneity, as well as on challenging medical\nand satellite image datasets with label heterogeneity. The results demonstrate\nthe effectiveness of FedBiP, which substantially outperforms other OSFL\nmethods.\n","authors":["Haokun Chen","Hang Li","Yao Zhang","Jinhe Bi","Gengyuan Zhang","Yueqi Zhang","Philip Torr","Jindong Gu","Denis Krompass","Volker Tresp"],"pdf_url":"https://arxiv.org/pdf/2410.04810v2.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2408.11915v2","updated":"2025-03-02T15:55:14Z","published":"2024-08-21T18:06:15Z","title":"Video-Foley: Two-Stage Video-To-Sound Generation via Temporal Event\n  Condition For Foley Sound","summary":"  Foley sound synthesis is crucial for multimedia production, enhancing user\nexperience by synchronizing audio and video both temporally and semantically.\nRecent studies on automating this labor-intensive process through\nvideo-to-sound generation face significant challenges. Systems lacking explicit\ntemporal features suffer from poor alignment and controllability, while\ntimestamp-based models require costly and subjective human annotation. We\npropose Video-Foley, a video-to-sound system using Root Mean Square (RMS) as an\nintuitive condition with semantic timbre prompts (audio or text). RMS, a\nframe-level intensity envelope closely related to audio semantics, acts as a\ntemporal event feature to guide audio generation from video. The\nannotation-free self-supervised learning framework consists of two stages,\nVideo2RMS and RMS2Sound, incorporating novel ideas including RMS discretization\nand RMS-ControlNet with a pretrained text-to-audio model. Our extensive\nevaluation shows that Video-Foley achieves state-of-the-art performance in\naudio-visual alignment and controllability for sound timing, intensity, timbre,\nand nuance. Source code, model weights and demos are available on our companion\nwebsite. (https://jnwnlee.github.io/video-foley-demo)\n","authors":["Junwon Lee","Jaekwon Im","Dabin Kim","Juhan Nam"],"pdf_url":"https://arxiv.org/pdf/2408.11915v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18709v4","updated":"2025-03-02T15:37:39Z","published":"2023-10-28T13:37:52Z","title":"Audio-Visual Instance Segmentation","summary":"  In this paper, we propose a new multi-modal task, termed audio-visual\ninstance segmentation (AVIS), which aims to simultaneously identify, segment\nand track individual sounding object instances in audible videos. To facilitate\nthis research, we introduce a high-quality benchmark named AVISeg, containing\nover 90K instance masks from 26 semantic categories in 926 long videos.\nAdditionally, we propose a strong baseline model for this task. Our model first\nlocalizes sound source within each frame, and condenses object-specific\ncontexts into concise tokens. Then it builds long-range audio-visual\ndependencies between these tokens using window-based attention, and tracks\nsounding objects among the entire video sequences. Extensive experiments reveal\nthat our method performs best on AVISeg, surpassing the existing methods from\nrelated tasks. We further conduct the evaluation on several multi-modal large\nmodels. Unfortunately, they exhibits subpar performance on instance-level sound\nsource localization and temporal perception. We expect that AVIS will inspire\nthe community towards a more comprehensive multi-modal understanding. Dataset\nand code is available at https://github.com/ruohaoguo/avis.\n","authors":["Ruohao Guo","Xianghua Ying","Yaru Chen","Dantong Niu","Guangyao Li","Liao Qu","Yanyu Qi","Jinxing Zhou","Bowei Xing","Wenzhen Yue","Ji Shi","Qixun Wang","Peiliang Zhang","Buwen Liang"],"pdf_url":"https://arxiv.org/pdf/2310.18709v4.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2410.11817v2","updated":"2025-03-02T07:05:19Z","published":"2024-10-15T17:46:31Z","title":"Improving Long-Text Alignment for Text-to-Image Diffusion Models","summary":"  The rapid advancement of text-to-image (T2I) diffusion models has enabled\nthem to generate unprecedented results from given texts. However, as text\ninputs become longer, existing encoding methods like CLIP face limitations, and\naligning the generated images with long texts becomes challenging. To tackle\nthese issues, we propose LongAlign, which includes a segment-level encoding\nmethod for processing long texts and a decomposed preference optimization\nmethod for effective alignment training. For segment-level encoding, long texts\nare divided into multiple segments and processed separately. This method\novercomes the maximum input length limits of pretrained encoding models. For\npreference optimization, we provide decomposed CLIP-based preference models to\nfine-tune diffusion models. Specifically, to utilize CLIP-based preference\nmodels for T2I alignment, we delve into their scoring mechanisms and find that\nthe preference scores can be decomposed into two components: a text-relevant\npart that measures T2I alignment and a text-irrelevant part that assesses other\nvisual aspects of human preference. Additionally, we find that the\ntext-irrelevant part contributes to a common overfitting problem during\nfine-tuning. To address this, we propose a reweighting strategy that assigns\ndifferent weights to these two components, thereby reducing overfitting and\nenhancing alignment. After fine-tuning $512 \\times 512$ Stable Diffusion (SD)\nv1.5 for about 20 hours using our method, the fine-tuned SD outperforms\nstronger foundation models in T2I alignment, such as PixArt-$\\alpha$ and\nKandinsky v2.2. The code is available at\nhttps://github.com/luping-liu/LongAlign.\n","authors":["Luping Liu","Chao Du","Tianyu Pang","Zehan Wang","Chongxuan Li","Dong Xu"],"pdf_url":"https://arxiv.org/pdf/2410.11817v2.pdf","comment":null}]},"2025-03-01T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2409.01482v2","updated":"2025-03-01T23:34:06Z","published":"2024-09-02T22:17:18Z","title":"Masked Mixers for Language Generation and Retrieval","summary":"  Attention mechanisms that confer selective focus on a strict subset of input\nelements are nearly ubiquitous in language models today. We posit there to be\ndownside to the use of attention: most input information is lost. In support of\nthis idea we observe poor input representation accuracy in transformers and\nmore accurate representation in what we term masked mixers, which replace\nself-attention with masked convolutions. The masked mixer learns causal\nlanguage modeling more efficiently than early transformer implementations and\neven outperforms optimized, current transformers when training on small (<512)\nbut not larger context windows. Evidence is presented for the hypothesis that\ndifferences in transformer and masked mixer training efficiencies for various\ntasks are best predicted by input representation accuracy, or equivalently\nglobal invertibility. We hypothesize that the information loss exhibited by\ntransformers would be more detrimental to retrieval than generation, as the\nformer is more closely approximated by a bijective and thus invertible\nfunction. We find that masked mixers are more effective retrieval models both\nwhen the pretrained embedding model is unchanged as well as when the embedding\nmodel is modified via cosine similarity-based InfoNCE loss minimization. A\nsmall masked mixer is shown to outperform a large and near state-of-the-art\ntransformer-based retrieval model, despite the latter being trained with many\norders of magnitude more data and compute.\n","authors":["Benjamin L. Badger"],"pdf_url":"https://arxiv.org/pdf/2409.01482v2.pdf","comment":"31 pages, 8 figures, 3 tables, 9 supplementary figures, 13\n  supplementary tables"},{"id":"http://arxiv.org/abs/2410.14853v2","updated":"2025-03-01T23:22:15Z","published":"2024-10-18T20:35:28Z","title":"DFlow: Diverse Dialogue Flow Simulation with Large Language Models","summary":"  Developing language model-based dialogue agents requires effective data to\ntrain models that can follow specific task logic. However, most existing data\nsimulation methods focus on increasing diversity in language, topics, or\ndialogue acts at the utterance level, largely neglecting a critical aspect of\ntask logic diversity at the dialogue level. This paper proposes a novel data\nsimulation method designed to enhance the diversity of synthetic dialogues by\nfocusing on task execution logic. Our method uses LLMs to generate decision\ntree-structured task plans, which enables the derivation of diverse dialogue\ntrajectories for a given task. Each trajectory, referred to as a \"dialog flow\",\nguides the generation of a multi-turn dialogue that follows a unique\ntrajectory. We apply this method to generate a task-oriented dialogue dataset\ncomprising 3,886 dialogue flows across 15 different domains. We validate the\neffectiveness of this dataset using the next action prediction task, where\nmodels fine-tuned on our dataset outperform strong baselines, including GPT-4.\nUpon acceptance of this paper, we plan to release the code and data publicly.\n","authors":["Wanyu Du","Song Feng","James Gung","Lijia Sun","Yi Zhang","Saab Mansour","Yanjun Qi"],"pdf_url":"https://arxiv.org/pdf/2410.14853v2.pdf","comment":"16 pages"},{"id":"http://arxiv.org/abs/2408.09632v4","updated":"2025-03-01T23:19:07Z","published":"2024-08-19T01:30:14Z","title":"MoDeGPT: Modular Decomposition for Large Language Model Compression","summary":"  Large Language Models (LLMs) have reshaped the landscape of artificial\nintelligence by demonstrating exceptional performance across various tasks.\nHowever, substantial computational requirements make their deployment\nchallenging on devices with limited resources. Recently, compression methods\nusing low-rank matrix techniques have shown promise, yet these often lead to\ndegraded accuracy or introduce significant overhead in parameters and inference\nlatency. This paper introduces \\textbf{Mo}dular \\textbf{De}composition\n(MoDeGPT), a novel structured compression framework that does not need recovery\nfine-tuning while resolving the above drawbacks. MoDeGPT partitions the\nTransformer block into modules comprised of matrix pairs and reduces the hidden\ndimensions via reconstructing the module-level outputs. MoDeGPT is developed\nbased on a theoretical framework that utilizes three well-established matrix\ndecomposition algorithms -- Nystr\\\"om approximation, CR decomposition, and SVD\n-- and applies them to our redefined transformer modules. Our comprehensive\nexperiments show MoDeGPT, without backward propagation, matches or surpasses\nprevious structured compression methods that rely on gradient information, and\nsaves 98% of compute costs on compressing a 13B model. On \\textsc{Llama}-2/3\nand OPT models, MoDeGPT maintains 90-95% zero-shot performance with 25-30%\ncompression rates. Moreover, the compression can be done on a single GPU within\na few hours and increases the inference throughput by up to 46%.\n","authors":["Chi-Heng Lin","Shangqian Gao","James Seale Smith","Abhishek Patel","Shikhar Tuli","Yilin Shen","Hongxia Jin","Yen-Chang Hsu"],"pdf_url":"https://arxiv.org/pdf/2408.09632v4.pdf","comment":"31 pages, 9 figures"},{"id":"http://arxiv.org/abs/2410.12735v3","updated":"2025-03-01T23:03:21Z","published":"2024-10-16T16:51:01Z","title":"CREAM: Consistency Regularized Self-Rewarding Language Models","summary":"  Recent self-rewarding large language models (LLM) have successfully applied\nLLM-as-a-Judge to iteratively improve the alignment performance without the\nneed of human annotations for preference data. These methods commonly utilize\nthe same LLM to act as both the policy model (which generates responses) and\nthe reward model (which scores and ranks those responses). The ranked responses\nare then used as preference pairs to train the LLM via direct alignment\ntechnologies (e.g. DPO). However, it is noteworthy that throughout this\nprocess, there is no guarantee of accuracy in the rewarding and ranking, which\nis critical for ensuring accurate rewards and high-quality preference data.\nEmpirical results from relatively small LLMs (e.g., 7B parameters) also\nindicate that improvements from self-rewarding may diminish after several\niterations in certain situations, which we hypothesize is due to accumulated\nbias in the reward system. This bias can lead to unreliable preference data for\ntraining the LLM. To address this issue, we first formulate and analyze the\ngeneralized iterative preference fine-tuning framework for self-rewarding\nlanguage model. We then introduce the regularization to this generalized\nframework to mitigate the overconfident preference labeling in the\nself-rewarding process. Based on this theoretical insight, we propose a\nConsistency Regularized sElf-rewarding lAnguage Model (CREAM) that leverages\nthe consistency of rewards across different iterations to regularize the\nself-rewarding training, helping the model to learn from more reliable\npreference data. With this explicit regularization, our empirical results\ndemonstrate the superiority of CREAM in improving both reward consistency and\nalignment performance. The code is publicly available at\nhttps://github.com/Raibows/CREAM.\n","authors":["Zhaoyang Wang","Weilei He","Zhiyuan Liang","Xuchao Zhang","Chetan Bansal","Ying Wei","Weitong Zhang","Huaxiu Yao"],"pdf_url":"https://arxiv.org/pdf/2410.12735v3.pdf","comment":"To appear at ICLR 2025"},{"id":"http://arxiv.org/abs/2406.04604v4","updated":"2025-03-01T20:47:54Z","published":"2024-06-07T03:27:51Z","title":"Learning Task Decomposition to Assist Humans in Competitive Programming","summary":"  When using language models (LMs) to solve complex problems, humans might\nstruggle to understand the LM-generated solutions and repair the flawed ones.\nTo assist humans in repairing them, we propose to automatically decompose\ncomplex solutions into multiple simpler pieces that correspond to specific\nsubtasks. We introduce a novel objective for learning task decomposition,\ntermed assistive value (AssistV), which measures the feasibility and speed for\nhumans to repair the decomposed solution. We collect a dataset of human repair\nexperiences on different decomposed solutions. Utilizing the collected data as\nin-context examples, we then learn to critique, refine, and rank decomposed\nsolutions to improve AssistV. We validate our method under competitive\nprogramming problems: under 177 hours of human study, our method enables\nnon-experts to solve 33.3\\% more problems, speeds them up by 3.3x, and empowers\nthem to match unassisted experts.\n","authors":["Jiaxin Wen","Ruiqi Zhong","Pei Ke","Zhihong Shao","Hongning Wang","Minlie Huang"],"pdf_url":"https://arxiv.org/pdf/2406.04604v4.pdf","comment":"ACL 2024 Main Conference"},{"id":"http://arxiv.org/abs/2407.13193v3","updated":"2025-03-01T20:23:07Z","published":"2024-07-18T06:06:53Z","title":"Retrieval-Augmented Generation for Natural Language Processing: A Survey","summary":"  Large language models (LLMs) have demonstrated great success in various\nfields, benefiting from their huge amount of parameters that store knowledge.\nHowever, LLMs still suffer from several key issues, such as hallucination\nproblems, knowledge update issues, and lacking domain-specific expertise. The\nappearance of retrieval-augmented generation (RAG), which leverages an external\nknowledge database to augment LLMs, makes up those drawbacks of LLMs. This\npaper reviews all significant techniques of RAG, especially in the retriever\nand the retrieval fusions. Besides, tutorial codes are provided for\nimplementing the representative techniques in RAG. This paper further discusses\nthe RAG update, including RAG with/without knowledge update. Then, we introduce\nRAG evaluation and benchmarking, as well as the application of RAG in\nrepresentative NLP tasks and industrial scenarios. Finally, this paper\ndiscusses RAG's future directions and challenges for promoting this field's\ndevelopment.\n","authors":["Shangyu Wu","Ying Xiong","Yufei Cui","Haolun Wu","Can Chen","Ye Yuan","Lianming Huang","Xue Liu","Tei-Wei Kuo","Nan Guan","Chun Jason Xue"],"pdf_url":"https://arxiv.org/pdf/2407.13193v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19607v2","updated":"2025-03-01T19:33:15Z","published":"2025-02-26T22:45:08Z","title":"Revisiting Word Embeddings in the LLM Era","summary":"  Large Language Models (LLMs) have recently shown remarkable advancement in\nvarious NLP tasks. As such, a popular trend has emerged lately where NLP\nresearchers extract word/sentence/document embeddings from these large\ndecoder-only models and use them for various inference tasks with promising\nresults. However, it is still unclear whether the performance improvement of\nLLM-induced embeddings is merely because of scale or whether underlying\nembeddings they produce significantly differ from classical encoding models\nlike Word2Vec, GloVe, Sentence-BERT (SBERT) or Universal Sentence Encoder\n(USE). This is the central question we investigate in the paper by\nsystematically comparing classical decontextualized and contextualized word\nembeddings with the same for LLM-induced embeddings. Our results show that LLMs\ncluster semantically related words more tightly and perform better on analogy\ntasks in decontextualized settings. However, in contextualized settings,\nclassical models like SimCSE often outperform LLMs in sentence-level similarity\nassessment tasks, highlighting their continued relevance for fine-grained\nsemantics.\n","authors":["Yash Mahajan","Matthew Freestone","Sathyanarayanan Aakur","Santu Karmaker"],"pdf_url":"https://arxiv.org/pdf/2502.19607v2.pdf","comment":"This work was intended as a replacement of the older version,\n  arXiv:2402.11094, and any subsequent updates will appear there"},{"id":"http://arxiv.org/abs/2402.11094v3","updated":"2025-03-01T19:27:41Z","published":"2024-02-16T21:47:30Z","title":"Revisiting Word Embeddings in the LLM Era","summary":"  Large Language Models (LLMs) have recently shown remarkable advancement in\nvarious NLP tasks. As such, a popular trend has emerged lately where NLP\nresearchers extract word/sentence/document embeddings from these large\ndecoder-only models and use them for various inference tasks with promising\nresults. However, it is still unclear whether the performance improvement of\nLLM-induced embeddings is merely because of scale or whether underlying\nembeddings they produce significantly differ from classical encoding models\nlike Word2Vec, GloVe, Sentence-BERT (SBERT) or Universal Sentence Encoder\n(USE). This is the central question we investigate in the paper by\nsystematically comparing classical decontextualized and contextualized word\nembeddings with the same for LLM-induced embeddings. Our results show that LLMs\ncluster semantically related words more tightly and perform better on analogy\ntasks in decontextualized settings. However, in contextualized settings,\nclassical models like SimCSE often outperform LLMs in sentence-level similarity\nassessment tasks, highlighting their continued relevance for fine-grained\nsemantics.\n","authors":["Yash Mahajan","Matthew Freestone","Naman Bansal","Sathyanarayanan Aakur","Shubhra Kanti Karmaker Santu"],"pdf_url":"https://arxiv.org/pdf/2402.11094v3.pdf","comment":"This is an updated version of the older version: 2402.11094. We\n  accidentally submitted this article as a new submission (2502.19607), which\n  we have requested to withdraw. This version has 30 pages and 22 figures"},{"id":"http://arxiv.org/abs/2403.08694v4","updated":"2025-03-01T19:25:49Z","published":"2024-03-13T16:57:57Z","title":"TeaMs-RL: Teaching LLMs to Generate Better Instruction Datasets via\n  Reinforcement Learning","summary":"  The development of Large Language Models (LLMs) often confronts challenges\nstemming from the heavy reliance on human annotators in the reinforcement\nlearning with human feedback (RLHF) framework, or the frequent and costly\nexternal queries tied to the self-instruct paradigm. In this work, we pivot to\nReinforcement Learning (RL) -- but with a twist. Diverging from the typical\nRLHF, which refines LLMs following instruction data training, we use RL to\ndirectly generate the foundational instruction dataset that alone suffices for\nfine-tuning. Our method, TeaMs-RL, uses a suite of textual operations and\nrules, prioritizing the diversification of training datasets. It facilitates\nthe generation of high-quality data without excessive reliance on external\nadvanced models, paving the way for a single fine-tuning step and negating the\nneed for subsequent RLHF stages. Our findings highlight key advantages of our\napproach: reduced need for human involvement and fewer model queries (only\n5.73% of the strong baseline's total), along with enhanced capabilities of LLMs\nin crafting and comprehending complex instructions compared to strong\nbaselines, and substantially improved model privacy protection. Code is\navailable at the link: https://github.com/SafeRL-Lab/TeaMs-RL\n","authors":["Shangding Gu","Alois Knoll","Ming Jin"],"pdf_url":"https://arxiv.org/pdf/2403.08694v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09102v2","updated":"2025-03-01T19:06:35Z","published":"2024-10-09T12:52:41Z","title":"Instructional Segment Embedding: Improving LLM Safety with Instruction\n  Hierarchy","summary":"  Large Language Models (LLMs) are susceptible to security and safety threats,\nsuch as prompt injection, prompt extraction, and harmful requests. One major\ncause of these vulnerabilities is the lack of an instruction hierarchy. Modern\nLLM architectures treat all inputs equally, failing to distinguish between and\nprioritize various types of instructions, such as system messages, user\nprompts, and data. As a result, lower-priority user prompts may override more\ncritical system instructions, including safety protocols. Existing approaches\nto achieving instruction hierarchy, such as delimiters and instruction-based\ntraining, do not address this issue at the architectural level. We introduce\nthe Instructional Segment Embedding (ISE) technique, inspired by BERT, to\nmodern large language models, which embeds instruction priority information\ndirectly into the model. This approach enables models to explicitly\ndifferentiate and prioritize various instruction types, significantly improving\nsafety against malicious prompts that attempt to override priority rules. Our\nexperiments on the Structured Query and Instruction Hierarchy benchmarks\ndemonstrate an average robust accuracy increase of up to 15.75% and 18.68%,\nrespectively. Furthermore, we observe an improvement in instruction-following\ncapability of up to 4.1% evaluated on AlpacaEval. Overall, our approach offers\na promising direction for enhancing the safety and effectiveness of LLM\narchitectures.\n","authors":["Tong Wu","Shujian Zhang","Kaiqiang Song","Silei Xu","Sanqiang Zhao","Ravi Agrawal","Sathish Reddy Indurthi","Chong Xiang","Prateek Mittal","Wenxuan Zhou"],"pdf_url":"https://arxiv.org/pdf/2410.09102v2.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.20513v4","updated":"2025-03-01T18:58:42Z","published":"2024-10-27T16:52:21Z","title":"Self-correction is Not An Innate Capability in Large Language Models: A\n  Case Study of Moral Self-correction","summary":"  Though there has been intensive attention to the self-correction capability\nof Large Language Models (LLMs), conclusions regarding its effectiveness remain\nvaried. In this paper, we investigate a fundamental question: is moral\nself-correction an innate capability in LLMs? To explore this, we conduct (1) a\nmechanistic analysis of how key components of self-correction, such as\nChain-of-Thought (CoT) reasoning and external feedback, interact to enable\nmoral self-correction; and (2) a behavioral analysis of LLMs' ability to\ndistinguish between desired and undesired outputs, introducing a\nself-distinguish framework. Our mechanistic analysis reveals that LLMs struggle\nto effectively leverage helpful feedback, and conflicts can arise between\nfeedback and CoT reasoning. These limitations suggest that LLMs fail to\nidentify useful contextual information, instead prioritizing their own internal\nknowledge. Additionally, our behavioral analysis indicates that LLMs struggle\nto differentiate among their own outputs. Based on these empirical findings\nacross two analytical dimensions, mechanism and behavior, we argue that moral\nself-correction is not an innate capability of LLMs.\n","authors":["Zimo Qi","Guangliang Liu","Kristen Marie Johnson","Lu Cheng"],"pdf_url":"https://arxiv.org/pdf/2410.20513v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00053v3","updated":"2025-03-01T18:42:10Z","published":"2024-05-28T21:38:20Z","title":"Dual Process Learning: Controlling Use of In-Context vs. In-Weights\n  Strategies with Weight Forgetting","summary":"  Language models have the ability to perform in-context learning (ICL),\nallowing them to flexibly adapt their behavior based on context. This contrasts\nwith in-weights learning (IWL), where memorized information is encoded in model\nparameters after iterated observations of data. An ideal model should be able\nto flexibly deploy both of these abilities. Despite their apparent ability to\nlearn in-context, language models are known to struggle when faced with unseen\nor rarely seen tokens (Land & Bartolo, 2024). Hence, we study\n$\\textbf{structural in-context learning}$, which we define as the ability of a\nmodel to execute in-context learning on arbitrary novel tokens -- so called\nbecause the model must generalize on the basis of e.g. sentence structure or\ntask structure, rather than content encoded in token embeddings. We study\nstructural in-context algorithms on both synthetic and naturalistic tasks using\ntoy models, masked language models, and autoregressive language models. We find\nthat structural ICL appears before quickly disappearing early in LM\npretraining. While it has been shown that ICL can diminish during training\n(Singh et al., 2023), we find that prior work does not account for structural\nICL. Building on Chen et al. (2024) 's active forgetting method, we introduce\npretraining and finetuning methods that can modulate the preference for\nstructural ICL and IWL. Importantly, this allows us to induce a $\\textit{dual\nprocess strategy}$ where in-context and in-weights solutions coexist within a\nsingle model.\n","authors":["Suraj Anand","Michael A. Lepori","Jack Merullo","Ellie Pavlick"],"pdf_url":"https://arxiv.org/pdf/2406.00053v3.pdf","comment":"10 pages, 6 figures"},{"id":"http://arxiv.org/abs/2405.01649v4","updated":"2025-03-01T17:24:49Z","published":"2024-05-02T18:12:08Z","title":"Improving Complex Reasoning over Knowledge Graph with Logic-Aware\n  Curriculum Tuning","summary":"  Answering complex queries over incomplete knowledge graphs (KGs) is a\nchallenging job. Most previous works have focused on learning entity/relation\nembeddings and simulating first-order logic operators with various neural\nnetworks. However, they are bottlenecked by the inability to share world\nknowledge to improve logical reasoning, thus resulting in suboptimal\nperformance. In this paper, we propose a complex reasoning schema over KG upon\nlarge language models (LLMs), containing a curriculum-based logical-aware\ninstruction tuning framework, named LACT. Specifically, we augment the\narbitrary first-order logical queries via binary tree decomposition, to\nstimulate the reasoning capability of LLMs. To address the difficulty gap among\ndifferent types of complex queries, we design a simple and flexible logic-aware\ncurriculum learning framework. Experiments across widely used datasets\ndemonstrate that LACT has substantial improvements~(brings an average +5.5% MRR\nscore) over advanced methods, achieving the new state-of-the-art.\n","authors":["Tianle Xia","Liang Ding","Guojia Wan","Yibing Zhan","Bo Du","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2405.01649v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17173v2","updated":"2025-03-01T17:23:31Z","published":"2025-02-24T14:09:45Z","title":"Cheems: A Practical Guidance for Building and Evaluating Chinese Reward\n  Models from Scratch","summary":"  Reward models (RMs) are crucial for aligning large language models (LLMs)\nwith human preferences. However, most RM research is centered on English and\nrelies heavily on synthetic resources, which leads to limited and less reliable\ndatasets and benchmarks for Chinese. To address this gap, we introduce\nCheemsBench, a fully human-annotated RM evaluation benchmark within Chinese\ncontexts, and CheemsPreference, a large-scale and diverse preference dataset\nannotated through human-machine collaboration to support Chinese RM training.\nWe systematically evaluate open-source discriminative and generative RMs on\nCheemsBench and observe significant limitations in their ability to capture\nhuman preferences in Chinese scenarios. Additionally, based on\nCheemsPreference, we construct an RM that achieves state-of-the-art performance\non CheemsBench, demonstrating the necessity of human supervision in RM\ntraining. Our findings reveal that scaled AI-generated data struggles to fully\ncapture human preferences, emphasizing the importance of high-quality human\nsupervision in RM development.\n","authors":["Xueru Wen","Jie Lou","Zichao Li","Yaojie Lu","Xing Yu","Yuqiu Ji","Guohai Xu","Hongyu Lin","Ben He","Xianpei Han","Le Sun","Debing Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.17173v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.17631v2","updated":"2025-03-01T17:06:43Z","published":"2023-10-26T17:48:58Z","title":"JudgeLM: Fine-tuned Large Language Models are Scalable Judges","summary":"  Evaluating Large Language Models (LLMs) in open-ended scenarios is\nchallenging because existing benchmarks and metrics can not measure them\ncomprehensively. To address this problem, we propose to fine-tune LLMs as\nscalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in\nopen-ended benchmarks. We first propose a comprehensive, large-scale,\nhigh-quality dataset containing task seeds, LLMs-generated answers, and\nGPT-4-generated judgments for fine-tuning high-performance judges, as well as a\nnew benchmark for evaluating the judges. We train JudgeLM at different scales\nfrom 7B, 13B, to 33B parameters, and conduct a systematic analysis of its\ncapabilities and behaviors. We then analyze the key biases in fine-tuning LLM\nas a judge and consider them as position bias, knowledge bias, and format bias.\nTo address these issues, JudgeLM introduces a bag of techniques including swap\naugmentation, reference support, and reference drop, which clearly enhance the\njudge's performance. JudgeLM obtains the state-of-the-art judge performance on\nboth the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM\nis efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8\nA100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an\nagreement exceeding 90% that even surpasses human-to-human agreement. JudgeLM\nalso demonstrates extended capabilities in being judges of the single answer,\nmultimodal models, multiple answers, multi-turn chat, etc. Code is available at\nhttps://github.com/baaivision/JudgeLM.\n","authors":["Lianghui Zhu","Xinggang Wang","Xinlong Wang"],"pdf_url":"https://arxiv.org/pdf/2310.17631v2.pdf","comment":"JudgeLM is accepted by ICLR2025. Code is available at\n  https://github.com/baaivision/JudgeLM"},{"id":"http://arxiv.org/abs/2411.17637v2","updated":"2025-03-01T16:07:45Z","published":"2024-11-26T17:55:37Z","title":"On Limitations of LLM as Annotator for Low Resource Languages","summary":"  Low-resource languages face significant challenges due to the lack of\nsufficient linguistic data, resources, and tools for tasks such as supervised\nlearning, annotation, and classification. This shortage hinders the development\nof accurate models and datasets, making it difficult to perform critical NLP\ntasks like sentiment analysis or hate speech detection. To bridge this gap,\nLarge Language Models (LLMs) present an opportunity for potential annotators,\ncapable of generating datasets and resources for these underrepresented\nlanguages. In this paper, we focus on Marathi, a low-resource language, and\nevaluate the performance of both closed-source and open-source LLMs as\nannotators, while also comparing these results with fine-tuned BERT models. We\nassess models such as GPT-4o and Gemini 1.0 Pro, Gemma 2 (2B and 9B), and Llama\n3.1 (8B and 405B) on classification tasks including sentiment analysis, news\nclassification, and hate speech detection. Our findings reveal that while LLMs\nexcel in annotation tasks for high-resource languages like English, they still\nfall short when applied to Marathi. Even advanced models like GPT-4o and Llama\n3.1 405B underperform compared to fine-tuned BERT-based baselines, with GPT-4o\nand Llama 3.1 405B trailing fine-tuned BERT by accuracy margins of 10.2% and\n14.1%, respectively. This highlights the limitations of LLMs as annotators for\nlow-resource languages.\n","authors":["Suramya Jadhav","Abhay Shanbhag","Amogh Thakurdesai","Ridhima Sinare","Raviraj Joshi"],"pdf_url":"https://arxiv.org/pdf/2411.17637v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16806v3","updated":"2025-03-01T15:17:50Z","published":"2025-02-24T03:30:29Z","title":"CoT2Align: Cross-Chain of Thought Distillation via Optimal Transport\n  Alignment for Language Models with Different Tokenizers","summary":"  Large Language Models (LLMs) achieve state-of-the-art performance across\nvarious NLP tasks but face deployment challenges due to high computational\ncosts and memory constraints. Knowledge distillation (KD) is a promising\nsolution, transferring knowledge from large teacher models to smaller student\nmodels. However, existing KD methods often assume shared vocabularies and\ntokenizers, limiting their flexibility. While approaches like Universal Logit\nDistillation (ULD) and Dual-Space Knowledge Distillation (DSKD) address\nvocabulary mismatches, they overlook the critical \\textbf{reasoning-aware\ndistillation} aspect. To bridge this gap, we propose CoT2Align a universal KD\nframework that integrates Chain-of-Thought (CoT) augmentation and introduces\nCross-CoT Alignment to enhance reasoning transfer. Additionally, we extend\nOptimal Transport beyond token-wise alignment to a sequence-level and\nlayer-wise alignment approach that adapts to varying sequence lengths while\npreserving contextual integrity. Comprehensive experiments demonstrate that\nCoT2Align outperforms existing KD methods across different vocabulary settings,\nimproving reasoning capabilities and robustness in domain-specific tasks.\n","authors":["Anh Duc Le","Tu Vu","Nam Le Hai","Nguyen Thi Ngoc Diep","Linh Ngo Van","Trung Le","Thien Huu Nguyen"],"pdf_url":"https://arxiv.org/pdf/2502.16806v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.20207v2","updated":"2025-03-01T14:39:33Z","published":"2024-07-29T17:39:08Z","title":"QAEA-DR: A Unified Text Augmentation Framework for Dense Retrieval","summary":"  In dense retrieval, embedding long texts into dense vectors can result in\ninformation loss, leading to inaccurate query-text matching. Additionally,\nlow-quality texts with excessive noise or sparse key information are unlikely\nto align well with relevant queries. Recent studies mainly focus on improving\nthe sentence embedding model or retrieval process. In this work, we introduce a\nnovel text augmentation framework for dense retrieval. This framework\ntransforms raw documents into information-dense text formats, which supplement\nthe original texts to effectively address the aforementioned issues without\nmodifying embedding or retrieval methodologies. Two text representations are\ngenerated via large language models (LLMs) zero-shot prompting: question-answer\npairs and element-driven events. We term this approach QAEA-DR: unifying\nquestion-answer generation and event extraction in a text augmentation\nframework for dense retrieval. To further enhance the quality of generated\ntexts, a scoring-based evaluation and regeneration mechanism is introduced in\nLLM prompting. Our QAEA-DR model has a positive impact on dense retrieval,\nsupported by both theoretical analysis and empirical experiments.\n","authors":["Hongming Tan","Shaoxiong Zhan","Hai Lin","Hai-Tao Zheng","Wai Kin Chan"],"pdf_url":"https://arxiv.org/pdf/2407.20207v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16406v2","updated":"2025-03-01T14:22:20Z","published":"2024-02-26T08:59:05Z","title":"From RAGs to riches: Utilizing large language models to write documents\n  for clinical trials","summary":"  This manuscript has now been published: - Link to article on journal website:\nhttps://journals.sagepub.com/doi/10.1177/17407745251320806 - Pubmed link:\nhttps://pubmed.ncbi.nlm.nih.gov/40013826/\n","authors":["Nigel Markey","Ilyass El-Mansouri","Gaetan Rensonnet","Casper van Langen","Christoph Meier"],"pdf_url":"https://arxiv.org/pdf/2402.16406v2.pdf","comment":"6 pages, 2 figures"},{"id":"http://arxiv.org/abs/2409.06666v2","updated":"2025-03-01T12:59:49Z","published":"2024-09-10T17:34:34Z","title":"LLaMA-Omni: Seamless Speech Interaction with Large Language Models","summary":"  Models like GPT-4o enable real-time interaction with large language models\n(LLMs) through speech, significantly enhancing user experience compared to\ntraditional text-based interaction. However, there is still a lack of\nexploration on how to build speech interaction models based on open-source\nLLMs. To address this, we propose LLaMA-Omni, a novel model architecture\ndesigned for low-latency and high-quality speech interaction with LLMs.\nLLaMA-Omni integrates a pretrained speech encoder, a speech adaptor, an LLM,\nand a streaming speech decoder. It eliminates the need for speech\ntranscription, and can simultaneously generate text and speech responses\ndirectly from speech instructions with extremely low latency. We build our\nmodel based on the latest Llama-3.1-8B-Instruct model. To align the model with\nspeech interaction scenarios, we construct a dataset named InstructS2S-200K,\nwhich includes 200K speech instructions and corresponding speech responses.\nExperimental results show that compared to previous speech-language models,\nLLaMA-Omni provides better responses in both content and style, with a response\nlatency as low as 226ms. Additionally, training LLaMA-Omni takes less than 3\ndays on just 4 GPUs, paving the way for the efficient development of\nspeech-language models in the future.\n","authors":["Qingkai Fang","Shoutao Guo","Yan Zhou","Zhengrui Ma","Shaolei Zhang","Yang Feng"],"pdf_url":"https://arxiv.org/pdf/2409.06666v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2402.02611v3","updated":"2025-03-01T12:46:25Z","published":"2024-02-04T20:56:09Z","title":"FCoReBench: Can Large Language Models Solve Challenging First-Order\n  Combinatorial Reasoning Problems?","summary":"  Can the large language models (LLMs) solve challenging first-order\ncombinatorial reasoning problems such as graph coloring, knapsack, and\ncryptarithmetic? By first-order, we mean these problems can be instantiated\nwith potentially an infinite number of problem instances of varying sizes. They\nare also challenging being NP-hard and requiring several reasoning steps to\nreach a solution. While existing work has focused on coming up with datasets\nwith hard benchmarks, there is limited work which exploits the first-order\nnature of the problem structure. To address this challenge, we present\nFCoReBench, a dataset of 40 such challenging problems, along with scripts to\ngenerate problem instances of varying sizes and automatically verify and\ngenerate their solutions. We first observe that LLMs, even when aided by\nsymbolic solvers, perform rather poorly on our dataset, being unable to\nleverage the underlying structure of these problems. We specifically observe a\ndrop in performance with increasing problem size. In response, we propose a new\napproach, SymPro-LM, which combines LLMs with both symbolic solvers and program\ninterpreters, along with feedback from a few solved examples, to achieve huge\nperformance gains. Our proposed approach is robust to changes in the problem\nsize, and has the unique characteristic of not requiring any LLM call during\ninference time, unlike earlier approaches. As an additional experiment, we also\ndemonstrate SymPro-LM's effectiveness on other logical reasoning benchmarks.\n","authors":["Chinmay Mittal","Krishna Kartik"," Mausam","Parag Singla"],"pdf_url":"https://arxiv.org/pdf/2402.02611v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.04909v2","updated":"2025-03-01T12:40:09Z","published":"2024-08-09T07:31:06Z","title":"Surveying the Landscape of Image Captioning Evaluation: A Comprehensive\n  Taxonomy, Trends and Metrics Analysis","summary":"  The task of image captioning has recently been gaining popularity, and with\nit the complex task of evaluating the quality of image captioning models. In\nthis work, we present the first survey and taxonomy of over 70 different image\ncaptioning metrics and their usage in hundreds of papers, specifically designed\nto help users select the most suitable metric for their needs. We find that\ndespite the diversity of proposed metrics, the vast majority of studies rely on\nonly five popular metrics, which we show to be weakly correlated with human\nratings. We hypothesize that combining a diverse set of metrics can enhance\ncorrelation with human ratings. As an initial step, we demonstrate that a\nlinear regression-based ensemble method, which we call EnsembEval, trained on\none human ratings dataset, achieves improved correlation across five additional\ndatasets, showing there is a lot of room for improvement by leveraging a\ndiverse set of metrics.\n","authors":["Uri Berger","Gabriel Stanovsky","Omri Abend","Lea Frermann"],"pdf_url":"https://arxiv.org/pdf/2408.04909v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.04286v2","updated":"2025-03-01T11:19:12Z","published":"2024-05-07T12:57:01Z","title":"Who Wrote This? The Key to Zero-Shot LLM-Generated Text Detection Is\n  GECScore","summary":"  The efficacy of detectors for texts generated by large language models (LLMs)\nsubstantially depends on the availability of large-scale training data.\nHowever, white-box zero-shot detectors, which require no such data, are limited\nby the accessibility of the source model of the LLM-generated text. In this\npaper, we propose a simple yet effective black-box zero-shot detection approach\nbased on the observation that, from the perspective of LLMs, human-written\ntexts typically contain more grammatical errors than LLM-generated texts. This\napproach involves calculating the Grammar Error Correction Score (GECScore) for\nthe given text to differentiate between human-written and LLM-generated text.\nExperimental results show that our method outperforms current state-of-the-art\n(SOTA) zero-shot and supervised methods, achieving an average AUROC of 98.62%\nacross XSum and Writing Prompts dataset. Additionally, our approach\ndemonstrates strong reliability in the wild, exhibiting robust generalization\nand resistance to paraphrasing attacks. Data and code are available at:\nhttps://github.com/NLP2CT/GECScore.\n","authors":["Junchao Wu","Runzhe Zhan","Derek F. Wong","Shu Yang","Xuebo Liu","Lidia S. Chao","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.04286v2.pdf","comment":"COLING 2025"},{"id":"http://arxiv.org/abs/2502.17810v2","updated":"2025-03-01T11:14:44Z","published":"2025-02-25T03:31:48Z","title":"URO-Bench: A Comprehensive Benchmark for End-to-End Spoken Dialogue\n  Models","summary":"  In recent years, with advances in large language models (LLMs), end-to-end\nspoken dialogue models (SDMs) have made significant strides. Compared to\ntext-based LLMs, the evaluation of SDMs needs to take speech-related aspects\ninto account, such as paralinguistic information and speech quality. However,\nthere is still a lack of comprehensive evaluations for SDMs in speech-to-speech\n(S2S) scenarios. To address this gap, we propose URO-Bench, an extensive\nbenchmark for SDMs. Notably, URO-Bench is the first S2S benchmark that covers\nevaluations about multilingualism, multi-round dialogues, and paralinguistics.\nOur benchmark is divided into two difficulty levels: basic track and pro track,\nconsisting of 16 and 20 datasets respectively, evaluating the model's abilities\nin Understanding, Reasoning, and Oral conversation. Evaluations on our proposed\nbenchmark reveal that current open-source SDMs perform rather well in daily QA\ntasks, but lag behind their backbone LLMs in terms of instruction-following\nability and also suffer from catastrophic forgetting. Their performance in\nadvanced evaluations of paralinguistic information and audio understanding\nremains subpar, highlighting the need for further research in this direction.\nWe hope that URO-Bench can effectively facilitate the development of spoken\ndialogue models by providing a multifaceted evaluation of existing models and\nhelping to track progress in this area.\n","authors":["Ruiqi Yan","Xiquan Li","Wenxi Chen","Zhikang Niu","Chen Yang","Ziyang Ma","Kai Yu","Xie Chen"],"pdf_url":"https://arxiv.org/pdf/2502.17810v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.18448v3","updated":"2025-03-01T09:48:15Z","published":"2024-05-28T01:15:21Z","title":"Multi-objective Representation for Numbers in Clinical Narratives: A\n  CamemBERT-Bio-Based Alternative to Large-Scale LLMs","summary":"  The processing of numerical values is a rapidly developing area in the field\nof Language Models (LLMs). Despite numerous advancements achieved by previous\nresearch, significant challenges persist, particularly within the healthcare\ndomain. This paper investigates the limitations of Transformer models in\nunderstanding numerical values. \\textit{Objective:} this research aims to\ncategorize numerical values extracted from medical documents into eight\nspecific physiological categories using CamemBERT-bio. \\textit{Methods:} In a\ncontext where scalable methods and Large Language Models (LLMs) are emphasized,\nwe explore lifting the limitations of transformer-based models. We examine two\nstrategies: fine-tuning CamemBERT-bio on a small medical dataset, integrating\nLabel Embedding for Self-Attention (LESA), and combining LESA with additional\nenhancement techniques such as Xval. Given that CamemBERT-bio is already\npre-trained on a large medical dataset, the first approach aims to update its\nencoder with the newly added label embeddings technique. In contrast, the\nsecond approach seeks to develop multiple representations of numbers\n(contextual and magnitude-based) to achieve more robust number embeddings.\n\\textit{Results:} As anticipated, fine-tuning the standard CamemBERT-bio on our\nsmall medical dataset did not improve F1 scores. However, significant\nimprovements were observed with CamemBERT-bio + LESA, resulting in an over 13\\%\nincrease. Similar enhancements were noted when combining LESA with Xval,\noutperforming conventional methods and giving comparable results to GPT-4\n\\textit{Conclusions and Novelty:} This study introduces two innovative\ntechniques for handling numerical data, which are also applicable to other\nmodalities. We illustrate how these techniques can improve the performance of\nTransformer-based models, achieving more reliable classification results even\nwith small datasets.\n","authors":["Boammani Aser Lompo","Thanh-Dung Le"],"pdf_url":"https://arxiv.org/pdf/2405.18448v3.pdf","comment":"Under the revision. arXiv admin note: substantial text overlap with\n  arXiv:2404.10171"},{"id":"http://arxiv.org/abs/2410.06615v2","updated":"2025-03-01T08:32:14Z","published":"2024-10-09T07:12:24Z","title":"QA-Calibration of Language Model Confidence Scores","summary":"  To use generative question-and-answering (QA) systems for decision-making and\nin any critical application, these systems need to provide well-calibrated\nconfidence scores that reflect the correctness of their answers. Existing\ncalibration methods aim to ensure that the confidence score is, *on average*,\nindicative of the likelihood that the answer is correct. We argue, however,\nthat this standard (average-case) notion of calibration is difficult to\ninterpret for decision-making in generative QA. To address this, we generalize\nthe standard notion of average calibration and introduce QA-calibration, which\nensures calibration holds across different question-and-answer groups. We then\npropose discretized posthoc calibration schemes for achieving QA-calibration.\nWe establish distribution-free guarantees on the performance of this method and\nvalidate our method on confidence scores returned by elicitation prompts across\nmultiple QA benchmarks and large language models (LLMs).\n","authors":["Putra Manggala","Atalanti Mastakouri","Elke Kirschbaum","Shiva Prasad Kasiviswanathan","Aaditya Ramdas"],"pdf_url":"https://arxiv.org/pdf/2410.06615v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19870v2","updated":"2025-03-01T08:16:00Z","published":"2025-02-27T08:21:28Z","title":"MMKE-Bench: A Multimodal Editing Benchmark for Diverse Visual Knowledge","summary":"  Knowledge editing techniques have emerged as essential tools for updating the\nfactual knowledge of large language models (LLMs) and multimodal models (LMMs),\nallowing them to correct outdated or inaccurate information without retraining\nfrom scratch. However, existing benchmarks for multimodal knowledge editing\nprimarily focus on entity-level knowledge represented as simple triplets, which\nfail to capture the complexity of real-world multimodal information. To address\nthis issue, we introduce MMKE-Bench, a comprehensive MultiModal Knowledge\nEditing Benchmark, designed to evaluate the ability of LMMs to edit diverse\nvisual knowledge in real-world scenarios. MMKE-Bench addresses these\nlimitations by incorporating three types of editing tasks: visual entity\nediting, visual semantic editing, and user-specific editing. Besides,\nMMKE-Bench uses free-form natural language to represent and edit knowledge,\noffering a more flexible and effective format. The benchmark consists of 2,940\npieces of knowledge and 8,363 images across 33 broad categories, with\nevaluation questions automatically generated and human-verified. We assess five\nstate-of-the-art knowledge editing methods on three prominent LMMs, revealing\nthat no method excels across all criteria, and that visual and user-specific\nedits are particularly challenging. MMKE-Bench sets a new standard for\nevaluating the robustness of multimodal knowledge editing techniques, driving\nprogress in this rapidly evolving field.\n","authors":["Yuntao Du","Kailin Jiang","Zhi Gao","Chenrui Shi","Zilong Zheng","Siyuan Qi","Qing Li"],"pdf_url":"https://arxiv.org/pdf/2502.19870v2.pdf","comment":"Accept to ICLR2025. Project Page: https://mmke-bench-iclr.github.io/"},{"id":"http://arxiv.org/abs/2502.13922v3","updated":"2025-03-01T08:02:07Z","published":"2025-02-19T17:59:03Z","title":"LongPO: Long Context Self-Evolution of Large Language Models through\n  Short-to-Long Preference Optimization","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities\nthrough pretraining and alignment. However, superior short-context LLMs may\nunderperform in long-context scenarios due to insufficient long-context\nalignment. This alignment process remains challenging due to the impracticality\nof human annotation for extended contexts and the difficulty in balancing\nshort- and long-context performance. To address these challenges, we introduce\nLongPO, that enables short-context LLMs to self-evolve to excel on long-context\ntasks by internally transferring short-context capabilities. LongPO harnesses\nLLMs to learn from self-generated short-to-long preference data, comprising\npaired responses generated for identical instructions with long-context inputs\nand their compressed short-context counterparts, respectively. This preference\nreveals capabilities and potentials of LLMs cultivated during short-context\nalignment that may be diminished in under-aligned long-context scenarios.\nAdditionally, LongPO incorporates a short-to-long KL constraint to mitigate\nshort-context performance decline during long-context alignment. When applied\nto Mistral-7B-Instruct-v0.2 from 128K to 512K context lengths, LongPO fully\nretains short-context performance and largely outperforms naive SFT and DPO in\nboth long- and short-context tasks. Specifically, LongPO-trained models can\nachieve results on long-context benchmarks comparable to, or even surpassing,\nthose of superior LLMs (e.g., GPT-4-128K) that involve extensive long-context\nannotation and larger parameter scales. Our code is available at\nhttps://github.com/DAMO-NLP-SG/LongPO.\n","authors":["Guanzheng Chen","Xin Li","Michael Qizhe Shieh","Lidong Bing"],"pdf_url":"https://arxiv.org/pdf/2502.13922v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.10596v2","updated":"2025-03-01T06:33:01Z","published":"2025-02-14T23:00:49Z","title":"Post-training an LLM for RAG? Train on Self-Generated Demonstrations","summary":"  Large language models (LLMs) often struggle with knowledge intensive NLP\ntasks, such as answering \"Who won the latest World Cup?\" because the knowledge\nthey learn during training may be insufficient or outdated. Conditioning\ngeneration on retrieved documents -- a technique known as retrieval augmented\ngeneration (RAG) -- mitigates these shortcomings by allowing the model to\nleverage in-context information. Practitioners can improve LLM RAG performance\nby fine-tuning on retrieval-augmented instructions, but must beware that this\ncan cause undesirable model behaviors like hallucinations. We attribute this\ndegradation to the fact that the training data is likely to be\nout-of-distribution for the model and may suffer from quality issues, such as\nmisalignment between retrievals and target responses (since retrievals are\nfrequently added post-hoc). We propose a recipe for training RAG-enabled LLMs\nusing self-generated demonstrations, thereby avoiding training on\nout-of-distribution text and integrating retrievals into the LLM responses. We\nevaluate our method on knowledge intensive question answering (QA) tasks and\nshow that our method teaches LLMs to properly handle in-context retrievals and\nabstain from questions it will likely get wrong. Compared to conventional RA-IT\nmethods, our method prevents model degradation in non-RAG settings while\nexhibiting superior QA performance.\n","authors":["Matthew Finlayson","Ilia Kulikov","Daniel M. Bikel","Barlas Oguz","Xilun Chen","Aasish Pappu"],"pdf_url":"https://arxiv.org/pdf/2502.10596v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.21368v2","updated":"2025-03-01T06:14:00Z","published":"2024-07-31T06:34:38Z","title":"Prompting Medical Large Vision-Language Models to Diagnose Pathologies\n  by Visual Question Answering","summary":"  Large Vision-Language Models (LVLMs) have achieved significant success in\nrecent years, and they have been extended to the medical domain. Although\ndemonstrating satisfactory performance on medical Visual Question Answering\n(VQA) tasks, Medical LVLMs (MLVLMs) suffer from the hallucination problem,\nwhich makes them fail to diagnose complex pathologies. Moreover, they readily\nfail to learn minority pathologies due to imbalanced training data. We propose\ntwo prompting strategies for MLVLMs that reduce hallucination and improve VQA\nperformance. In the first strategy, we provide a detailed explanation of the\nqueried pathology. In the second strategy, we fine-tune a cheap, weak learner\nto achieve high performance on a specific metric, and textually provide its\njudgment to the MLVLM. Tested on the MIMIC-CXR-JPG and Chexpert datasets, our\nmethods significantly improve the diagnostic F1 score, with the highest\nincrease being 0.27. We also demonstrate that our prompting strategies can be\nextended to general LVLM domains. Based on POPE metrics, it effectively\nsuppresses the false negative predictions of existing LVLMs and improves Recall\nby approximately 0.07.\n","authors":["Danfeng Guo","Demetri Terzopoulos"],"pdf_url":"https://arxiv.org/pdf/2407.21368v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16880v2","updated":"2025-03-01T06:13:45Z","published":"2025-02-24T06:28:26Z","title":"CORAL: Learning Consistent Representations across Multi-step Training\n  with Lighter Speculative Drafter","summary":"  Speculative decoding is a powerful technique that accelerates Large Language\nModel (LLM) inference by leveraging a lightweight speculative draft model.\nHowever, existing designs suffers in performance due to misalignment between\ntraining and inference. Recent methods have tried to solve this issue by\nadopting a multi-step training strategy, but the complex inputs of different\ntraining steps make it harder for the draft model to converge. To address this,\nwe propose CORAL, a novel framework that improves both accuracy and efficiency\nin speculative drafting. CORAL introduces Cross-Step Representation Alignment,\na method that enhances consistency across multiple training steps,\nsignificantly improving speculative drafting performance. Additionally, we\nidentify the LM head as a major bottleneck in the inference speed of the draft\nmodel. We introduce a weight-grouping mechanism that selectively activates a\nsubset of LM head parameters during inference, substantially reducing the\nlatency of the draft model. We evaluate CORAL on three LLM families and three\nbenchmark datasets, achieving speedup ratios of 2.50x-4.07x, outperforming\nstate-of-the-art methods such as EAGLE-2 and HASS. Our results demonstrate that\nCORAL effectively mitigates training-inference misalignment and delivers\nsignificant speedup for modern LLMs with large vocabularies.\n","authors":["Yepeng Weng","Dianwen Mei","Huishi Qiu","Xujie Chen","Li Liu","Jiang Tian","Zhongchao Shi"],"pdf_url":"https://arxiv.org/pdf/2502.16880v2.pdf","comment":"Under Review"},{"id":"http://arxiv.org/abs/2501.19393v3","updated":"2025-03-01T06:07:39Z","published":"2025-01-31T18:48:08Z","title":"s1: Simple test-time scaling","summary":"  Test-time scaling is a promising new approach to language modeling that uses\nextra test-time compute to improve performance. Recently, OpenAI's o1 model\nshowed this capability but did not publicly share its methodology, leading to\nmany replication efforts. We seek the simplest approach to achieve test-time\nscaling and strong reasoning performance. First, we curate a small dataset s1K\nof 1,000 questions paired with reasoning traces relying on three criteria we\nvalidate through ablations: difficulty, diversity, and quality. Second, we\ndevelop budget forcing to control test-time compute by forcefully terminating\nthe model's thinking process or lengthening it by appending \"Wait\" multiple\ntimes to the model's generation when it tries to end. This can lead the model\nto double-check its answer, often fixing incorrect reasoning steps. After\nsupervised finetuning the Qwen2.5-32B-Instruct language model on s1K and\nequipping it with budget forcing, our model s1-32B exceeds o1-preview on\ncompetition math questions by up to 27% (MATH and AIME24). Further, scaling\ns1-32B with budget forcing allows extrapolating beyond its performance without\ntest-time intervention: from 50% to 57% on AIME24. Our model, data, and code\nare open-source at https://github.com/simplescaling/s1\n","authors":["Niklas Muennighoff","Zitong Yang","Weijia Shi","Xiang Lisa Li","Li Fei-Fei","Hannaneh Hajishirzi","Luke Zettlemoyer","Percy Liang","Emmanuel Candès","Tatsunori Hashimoto"],"pdf_url":"https://arxiv.org/pdf/2501.19393v3.pdf","comment":"46 pages (9 main), 10 figures, 15 tables"},{"id":"http://arxiv.org/abs/2406.17972v3","updated":"2025-03-01T04:10:03Z","published":"2024-06-25T23:07:18Z","title":"LABOR-LLM: Language-Based Occupational Representations with Large\n  Language Models","summary":"  Vafa et al. (2024) introduced a transformer-based econometric model, CAREER,\nthat predicts a worker's next job as a function of career history (an\n\"occupation model\"). CAREER was initially estimated (\"pre-trained\") using a\nlarge, unrepresentative resume dataset, which served as a \"foundation model,\"\nand parameter estimation was continued (\"fine-tuned\") using data from a\nrepresentative survey. CAREER had better predictive performance than\nbenchmarks. This paper considers an alternative where the resume-based\nfoundation model is replaced by a large language model (LLM). We convert\ntabular data from the survey into text files that resemble resumes and\nfine-tune the LLMs using these text files with the objective to predict the\nnext token (word). The resulting fine-tuned LLM is used as an input to an\noccupation model. Its predictive performance surpasses all prior models. We\ndemonstrate the value of fine-tuning and further show that by adding more\ncareer data from a different population, fine-tuning smaller LLMs surpasses the\nperformance of fine-tuning larger models.\n","authors":["Susan Athey","Herman Brunborg","Tianyu Du","Ayush Kanodia","Keyon Vafa"],"pdf_url":"https://arxiv.org/pdf/2406.17972v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.06615v2","updated":"2025-03-01T03:07:31Z","published":"2024-06-07T04:25:38Z","title":"Language Guided Skill Discovery","summary":"  Skill discovery methods enable agents to learn diverse emergent behaviors\nwithout explicit rewards. To make learned skills useful for unknown downstream\ntasks, obtaining a semantically diverse repertoire of skills is essential.\nWhile some approaches introduce a discriminator to distinguish skills and\nothers aim to increase state coverage, no existing work directly addresses the\n\"semantic diversity\" of skills. We hypothesize that leveraging the semantic\nknowledge of large language models (LLMs) can lead us to improve semantic\ndiversity of resulting behaviors. In this sense, we introduce Language Guided\nSkill Discovery (LGSD), a skill discovery framework that aims to directly\nmaximize the semantic diversity between skills. LGSD takes user prompts as\ninput and outputs a set of semantically distinctive skills. The prompts serve\nas a means to constrain the search space into a semantically desired subspace,\nand the generated LLM outputs guide the agent to visit semantically diverse\nstates within the subspace. We demonstrate that LGSD enables legged robots to\nvisit different user-intended areas on a plane by simply changing the prompt.\nFurthermore, we show that language guidance aids in discovering more diverse\nskills compared to five existing skill discovery methods in robot-arm\nmanipulation environments. Lastly, LGSD provides a simple way of utilizing\nlearned skills via natural language.\n","authors":["Seungeun Rho","Laura Smith","Tianyu Li","Sergey Levine","Xue Bin Peng","Sehoon Ha"],"pdf_url":"https://arxiv.org/pdf/2406.06615v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11418v3","updated":"2025-03-01T01:47:51Z","published":"2024-07-16T06:19:14Z","title":"Semantic Operators: A Declarative Model for Rich, AI-based Data\n  Processing","summary":"  The semantic capabilities of large language models (LLMs) have the potential\nto enable rich analytics and reasoning over vast knowledge corpora.\nUnfortunately, existing systems either empirically optimize expensive\nLLM-powered operations with no performance guarantees, or serve a limited set\nof row-wise LLM operations, providing limited robustness, expressiveness and\nusability. We introduce semantic operators, the first formalism for declarative\nand general-purpose AI-based transformations based on natural language\nspecifications (e.g., filtering, sorting, joining or aggregating records using\nnatural language criteria). Each operator opens a rich space for execution\nplans, similar to relational operators. Our model specifies the expected\nbehavior of each operator with a high-quality gold algorithm, and we develop an\noptimization framework that reduces cost, while providing accuracy guarantees\nwith respect to a gold algorithm. Using this approach, we propose several novel\noptimizations to accelerate semantic filtering, joining, group-by and top-k\noperations by up to $1,000\\times$. We implement semantic operators in the LOTUS\nsystem and demonstrate LOTUS' effectiveness on real, bulk-semantic processing\napplications, including fact-checking, biomedical multi-label classification,\nsearch, and topic analysis. We show that the semantic operator model is\nexpressive, capturing state-of-the-art AI pipelines in a few operator calls,\nand making it easy to express new pipelines that match or exceed quality of\nrecent LLM-based analytic systems by up to $170\\%$, while offering accuracy\nguarantees. Overall, LOTUS programs match or exceed the accuracy of\nstate-of-the-art AI pipelines for each task while running up to $3.6\\times$\nfaster than the highest-quality baselines. LOTUS is publicly available at\nhttps://github.com/lotus-data/lotus.\n","authors":["Liana Patel","Siddharth Jha","Melissa Pan","Harshit Gupta","Parth Asawa","Carlos Guestrin","Matei Zaharia"],"pdf_url":"https://arxiv.org/pdf/2407.11418v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.17514v3","updated":"2025-03-01T01:39:28Z","published":"2024-01-31T00:15:34Z","title":"How Useful is Continued Pre-Training for Generative Unsupervised Domain\n  Adaptation?","summary":"  Recent breakthroughs in scale have enabled the emergence of powerful\ngenerative language models, and the ability to fine-tune these models on\nvarious tasks by casting them into prompts or instructions. In this landscape,\nthe problem of Unsupervised Domain Adaptation (UDA), or the problem of\nleveraging knowledge from a labeled source domain to an unlabeled target\ndomain, has been left behind, with recent UDA methods still addressing\ndiscriminative classification. In particular, two popular UDA approaches,\ninvolving Continued Pre-Training (CPT) and learning domain invariant\nrepresentations, have been under-explored in the generative setting, signaling\na gap. In this work, we evaluate the utility of CPT for generative UDA. We\nfirst perform an empirical evaluation to measure the trade-offs between CPT and\nstrong methods promoting domain invariance. We further evaluate how well the\nbenefits of CPT extend to different architectures, tuning methods and data\nregimes. We then motivate the use of CPT by studying to what degree it benefits\nclassification performance on the target domain. Finally, we attempt to\nunderstand the mechanism behind which CPT improves classification performance\non the unlabeled target domain. Our findings suggest that a implicitly learns\nthe downstream task while predicting masked words informative to that task. Our\nwork connects the body of UDA research with that of instruction tuning,\nenabling an initial step towards a wider applicability of modern language\nmodels.\n","authors":["Rheeya Uppaal","Yixuan Li","Junjie Hu"],"pdf_url":"https://arxiv.org/pdf/2401.17514v3.pdf","comment":"Accepted to RepL4NLP at ACL 2024"},{"id":"http://arxiv.org/abs/2405.13967v5","updated":"2025-03-01T01:35:47Z","published":"2024-05-22T20:08:48Z","title":"Model Editing as a Robust and Denoised variant of DPO: A Case Study on\n  Toxicity","summary":"  Recent alignment algorithms such as direct preference optimization (DPO) have\nbeen developed to improve the safety of large language models (LLMs) by\ntraining these models to match human behaviors exemplified by preference data.\nHowever, these methods are both computationally intensive and lacking in\ncontrollability and transparency, inhibiting their widespread use. Furthermore,\nthese tuning-based methods require large-scale preference data for training and\nare susceptible to noisy preference data. In this paper, we introduce a\ntuning-free alignment alternative, ProFS (Projection Filter for Subspaces), and\ndemonstrate its effectiveness under the use case of toxicity reduction.\nGrounded on theory from factor analysis, ProFS is a sample-efficient model\nediting approach that identifies a toxic subspace in the model parameter space\nand reduces model toxicity by projecting away the detected subspace. The toxic\nsubspace is identified by extracting preference data embeddings from the\nlanguage model, and removing non-toxic information from these embeddings. We\nshow that ProFS is more sample-efficient than DPO, further showcasing greater\nrobustness to noisy data. Finally, we attempt to connect tuning based alignment\nwith editing, by establishing both theoretical and empirical connections\nbetween ProFS and DPO, showing that ProFS can be interpreted as a denoised\nversion of a single DPO step.\n","authors":["Rheeya Uppaal","Apratim Dey","Yiting He","Yiqiao Zhong","Junjie Hu"],"pdf_url":"https://arxiv.org/pdf/2405.13967v5.pdf","comment":"Accepted to ICLR 2025"},{"id":"http://arxiv.org/abs/2408.04650v2","updated":"2025-03-01T00:49:53Z","published":"2024-08-03T19:57:49Z","title":"Building Trust in Mental Health Chatbots: Safety Metrics and LLM-Based\n  Evaluation Tools","summary":"  Objective: This study aims to develop and validate an evaluation framework to\nensure the safety and reliability of mental health chatbots, which are\nincreasingly popular due to their accessibility, human-like interactions, and\ncontext-aware support. Materials and Methods: We created an evaluation\nframework with 100 benchmark questions and ideal responses, and five guideline\nquestions for chatbot responses. This framework, validated by mental health\nexperts, was tested on a GPT-3.5-turbo-based chatbot. Automated evaluation\nmethods explored included large language model (LLM)-based scoring, an agentic\napproach using real-time data, and embedding models to compare chatbot\nresponses against ground truth standards. Results: The results highlight the\nimportance of guidelines and ground truth for improving LLM evaluation\naccuracy. The agentic method, dynamically accessing reliable information,\ndemonstrated the best alignment with human assessments. Adherence to a\nstandardized, expert-validated framework significantly enhanced chatbot\nresponse safety and reliability. Discussion: Our findings emphasize the need\nfor comprehensive, expert-tailored safety evaluation metrics for mental health\nchatbots. While LLMs have significant potential, careful implementation is\nnecessary to mitigate risks. The superior performance of the agentic approach\nunderscores the importance of real-time data access in enhancing chatbot\nreliability. Conclusion: The study validated an evaluation framework for mental\nhealth chatbots, proving its effectiveness in improving safety and reliability.\nFuture work should extend evaluations to accuracy, bias, empathy, and privacy\nto ensure holistic assessment and responsible integration into healthcare.\nStandardized evaluations will build trust among users and professionals,\nfacilitating broader adoption and improved mental health support through\ntechnology.\n","authors":["Jung In Park","Mahyar Abbasian","Iman Azimi","Dawn T. Bounds","Angela Jun","Jaesu Han","Robert M. McCarron","Jessica Borelli","Parmida Safavi","Sanaz Mirbaha","Jia Li","Mona Mahmoudi","Carmen Wiedenhoeft","Amir M. Rahmani"],"pdf_url":"https://arxiv.org/pdf/2408.04650v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21637v2","updated":"2025-03-01T00:12:48Z","published":"2024-10-29T00:46:24Z","title":"Mitigating Paraphrase Attacks on Machine-Text Detectors via Paraphrase\n  Inversion","summary":"  High-quality paraphrases are easy to produce using instruction-tuned language\nmodels or specialized paraphrasing models. Although this capability has a\nvariety of benign applications, paraphrasing\nattacks$\\unicode{x2013}$paraphrases applied to machine-generated\ntexts$\\unicode{x2013}$are known to significantly degrade the performance of\nmachine-text detectors. This motivates us to consider the novel problem of\nparaphrase inversion, where, given paraphrased text, the objective is to\nrecover an approximation of the original text. The closer the approximation is\nto the original text, the better machine-text detectors will perform. We\npropose an approach which frames the problem as translation from paraphrased\ntext back to the original text, which requires examples of texts and\ncorresponding paraphrases to train the inversion model. Fortunately, such\ntraining data can easily be generated, given a corpus of original texts and one\nor more paraphrasing models. We find that language models such as GPT-4 and\nLlama-3 exhibit biases when paraphrasing which an inversion model can learn\nwith a modest amount of data. Perhaps surprisingly, we also find that such\nmodels generalize well, including to paraphrase models unseen at training time.\nFinally, we show that when combined with a paraphrased-text detector, our\ninversion models provide an effective defense against paraphrasing attacks, and\noverall our approach yields an average improvement of +22% AUROC across seven\nmachine-text detectors and three different domains.\n","authors":["Rafael Rivera Soto","Barry Chen","Nicholas Andrews"],"pdf_url":"https://arxiv.org/pdf/2410.21637v2.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2303.15263v5","updated":"2025-03-01T23:43:08Z","published":"2023-03-27T14:52:08Z","title":"Joint Person Identity, Gender and Age Estimation from Hand Images using\n  Deep Multi-Task Representation Learning","summary":"  In this paper, we propose a multi-task representation learning framework to\njointly estimate the identity, gender and age of individuals from their hand\nimages for the purpose of criminal investigations since the hand images are\noften the only available information in cases of serious crime such as sexual\nabuse. We investigate different up-to-date deep learning architectures and\ncompare their performance for joint estimation of identity, gender and age from\nhand images of perpetrators of serious crime. To simplify the age prediction,\nwe create age groups for the age estimation. We make extensive evaluations and\ncomparisons of both convolution-based and transformer-based deep learning\narchitectures on a publicly available 11k hands dataset. Our experimental\nanalysis shows that it is possible to efficiently estimate not only identity\nbut also other attributes such as gender and age of suspects jointly from hand\nimages for criminal investigations, which is crucial in assisting international\npolice forces in the court to identify and convict abusers.\n","authors":["Nathanael L. Baisa"],"pdf_url":"https://arxiv.org/pdf/2303.15263v5.pdf","comment":"arXiv admin note: text overlap with arXiv:2209.04821"},{"id":"http://arxiv.org/abs/2404.15709v3","updated":"2025-03-01T23:26:22Z","published":"2024-04-24T07:58:28Z","title":"ViViDex: Learning Vision-based Dexterous Manipulation from Human Videos","summary":"  In this work, we aim to learn a unified vision-based policy for\nmulti-fingered robot hands to manipulate a variety of objects in diverse poses.\nThough prior work has shown benefits of using human videos for policy learning,\nperformance gains have been limited by the noise in estimated trajectories.\nMoreover, reliance on privileged object information such as ground-truth object\nstates further limits the applicability in realistic scenarios. To address\nthese limitations, we propose a new framework ViViDex to improve vision-based\npolicy learning from human videos. It first uses reinforcement learning with\ntrajectory guided rewards to train state-based policies for each video,\nobtaining both visually natural and physically plausible trajectories from the\nvideo. We then rollout successful episodes from state-based policies and train\na unified visual policy without using any privileged information. We propose\ncoordinate transformation to further enhance the visual point cloud\nrepresentation, and compare behavior cloning and diffusion policy for the\nvisual policy training. Experiments both in simulation and on the real robot\ndemonstrate that ViViDex outperforms state-of-the-art approaches on three\ndexterous manipulation tasks.\n","authors":["Zerui Chen","Shizhe Chen","Etienne Arlaud","Ivan Laptev","Cordelia Schmid"],"pdf_url":"https://arxiv.org/pdf/2404.15709v3.pdf","comment":"Accepted by ICRA 2025. Project Page:\n  https://zerchen.github.io/projects/vividex.html"},{"id":"http://arxiv.org/abs/2502.20108v2","updated":"2025-03-01T23:17:26Z","published":"2025-02-27T14:02:14Z","title":"VDT-Auto: End-to-end Autonomous Driving with VLM-Guided Diffusion\n  Transformers","summary":"  In autonomous driving, dynamic environment and corner cases pose significant\nchallenges to the robustness of ego vehicle's decision-making. To address these\nchallenges, commencing with the representation of state-action mapping in the\nend-to-end autonomous driving paradigm, we introduce a novel pipeline,\nVDT-Auto. Leveraging the advancement of the state understanding of Visual\nLanguage Model (VLM), incorporating with diffusion Transformer-based action\ngeneration, our VDT-Auto parses the environment geometrically and contextually\nfor the conditioning of the diffusion process. Geometrically, we use a\nbird's-eye view (BEV) encoder to extract feature grids from the surrounding\nimages. Contextually, the structured output of our fine-tuned VLM is processed\ninto textual embeddings and noisy paths. During our diffusion process, the\nadded noise for the forward process is sampled from the noisy path output of\nthe fine-tuned VLM, while the extracted BEV feature grids and embedded texts\ncondition the reverse process of our diffusion Transformers. Our VDT-Auto\nachieved 0.52m on average L2 errors and 21% on average collision rate in the\nnuScenes open-loop planning evaluation. Moreover, the real-world demonstration\nexhibited prominent generalizability of our VDT-Auto. The code and dataset will\nbe released after acceptance.\n","authors":["Ziang Guo","Konstantin Gubernatorov","Selamawit Asfaw","Zakhar Yagudin","Dzmitry Tsetserukou"],"pdf_url":"https://arxiv.org/pdf/2502.20108v2.pdf","comment":"Submitted paper"},{"id":"http://arxiv.org/abs/2407.07516v2","updated":"2025-03-01T23:17:11Z","published":"2024-07-10T10:09:12Z","title":"HDKD: Hybrid Data-Efficient Knowledge Distillation Network for Medical\n  Image Classification","summary":"  Vision Transformers (ViTs) have achieved significant advancement in computer\nvision tasks due to their powerful modeling capacity. However, their\nperformance notably degrades when trained with insufficient data due to lack of\ninherent inductive biases. Distilling knowledge and inductive biases from a\nConvolutional Neural Network (CNN) teacher has emerged as an effective strategy\nfor enhancing the generalization of ViTs on limited datasets. Previous\napproaches to Knowledge Distillation (KD) have pursued two primary paths: some\nfocused solely on distilling the logit distribution from CNN teacher to ViT\nstudent, neglecting the rich semantic information present in intermediate\nfeatures due to the structural differences between them. Others integrated\nfeature distillation along with logit distillation, yet this introduced\nalignment operations that limits the amount of knowledge transferred due to\nmismatched architectures and increased the computational overhead. To this end,\nthis paper presents Hybrid Data-efficient Knowledge Distillation (HDKD)\nparadigm which employs a CNN teacher and a hybrid student. The choice of hybrid\nstudent serves two main aspects. First, it leverages the strengths of both\nconvolutions and transformers while sharing the convolutional structure with\nthe teacher model. Second, this shared structure enables the direct application\nof feature distillation without any information loss or additional\ncomputational overhead. Additionally, we propose an efficient light-weight\nconvolutional block named Mobile Channel-Spatial Attention (MBCSA), which\nserves as the primary convolutional block in both teacher and student models.\nExtensive experiments on two medical public datasets showcase the superiority\nof HDKD over other state-of-the-art models and its computational efficiency.\nSource code at: https://github.com/omarsherif200/HDKD\n","authors":["Omar S. EL-Assiouti","Ghada Hamed","Dina Khattab","Hala M. Ebied"],"pdf_url":"https://arxiv.org/pdf/2407.07516v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.16820v3","updated":"2025-03-01T22:41:18Z","published":"2024-04-25T17:58:43Z","title":"Revisiting Text-to-Image Evaluation with Gecko: On Metrics, Prompts, and\n  Human Ratings","summary":"  While text-to-image (T2I) generative models have become ubiquitous, they do\nnot necessarily generate images that align with a given prompt. While previous\nwork has evaluated T2I alignment by proposing metrics, benchmarks, and\ntemplates for collecting human judgements, the quality of these components is\nnot systematically measured. Human-rated prompt sets are generally small and\nthe reliability of the ratings -- and thereby the prompt set used to compare\nmodels -- is not evaluated. We address this gap by performing an extensive\nstudy evaluating auto-eval metrics and human templates. We provide three main\ncontributions: (1) We introduce a comprehensive skills-based benchmark that can\ndiscriminate models across different human templates. This skills-based\nbenchmark categorises prompts into sub-skills, allowing a practitioner to\npinpoint not only which skills are challenging, but at what level of complexity\na skill becomes challenging. (2) We gather human ratings across four templates\nand four T2I models for a total of >100K annotations. This allows us to\nunderstand where differences arise due to inherent ambiguity in the prompt and\nwhere they arise due to differences in metric and model quality. (3) Finally,\nwe introduce a new QA-based auto-eval metric that is better correlated with\nhuman ratings than existing metrics for our new dataset, across different human\ntemplates, and on TIFA160.\n","authors":["Olivia Wiles","Chuhan Zhang","Isabela Albuquerque","Ivana Kajić","Su Wang","Emanuele Bugliarello","Yasumasa Onoe","Chris Knutsen","Cyrus Rashtchian","Jordi Pont-Tuset","Aida Nematzadeh"],"pdf_url":"https://arxiv.org/pdf/2404.16820v3.pdf","comment":"Accepted to ICLR 2025 (Spotlight)"},{"id":"http://arxiv.org/abs/2312.04465v3","updated":"2025-03-01T22:24:56Z","published":"2023-12-07T17:35:49Z","title":"FitDiff: Robust monocular 3D facial shape and reflectance estimation\n  using Diffusion Models","summary":"  The remarkable progress in 3D face reconstruction has resulted in high-detail\nand photorealistic facial representations. Recently, Diffusion Models have\nrevolutionized the capabilities of generative methods by surpassing the\nperformance of GANs. In this work, we present FitDiff, a diffusion-based 3D\nfacial avatar generative model. Leveraging diffusion principles, our model\naccurately generates relightable facial avatars, utilizing an identity\nembedding extracted from an \"in-the-wild\" 2D facial image. The introduced\nmulti-modal diffusion model is the first to concurrently output facial\nreflectance maps (diffuse and specular albedo and normals) and shapes,\nshowcasing great generalization capabilities. It is solely trained on an\nannotated subset of a public facial dataset, paired with 3D reconstructions. We\nrevisit the typical 3D facial fitting approach by guiding a reverse diffusion\nprocess using perceptual and face recognition losses. Being the first 3D LDM\nconditioned on face recognition embeddings, FitDiff reconstructs relightable\nhuman avatars, that can be used as-is in common rendering engines, starting\nonly from an unconstrained facial image, and achieving state-of-the-art\nperformance.\n","authors":["Stathis Galanakis","Alexandros Lattas","Stylianos Moschoglou","Stefanos Zafeiriou"],"pdf_url":"https://arxiv.org/pdf/2312.04465v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01345v2","updated":"2025-03-01T22:11:10Z","published":"2024-10-02T09:02:34Z","title":"Towards Generalizable Vision-Language Robotic Manipulation: A Benchmark\n  and LLM-guided 3D Policy","summary":"  Generalizing language-conditioned robotic policies to new tasks remains a\nsignificant challenge, hampered by the lack of suitable simulation benchmarks.\nIn this paper, we address this gap by introducing GemBench, a novel benchmark\nto assess generalization capabilities of vision-language robotic manipulation\npolicies. GemBench incorporates seven general action primitives and four levels\nof generalization, spanning novel placements, rigid and articulated objects,\nand complex long-horizon tasks. We evaluate state-of-the-art approaches on\nGemBench and also introduce a new method. Our approach 3D-LOTUS leverages rich\n3D information for action prediction conditioned on language. While 3D-LOTUS\nexcels in both efficiency and performance on seen tasks, it struggles with\nnovel tasks. To address this, we present 3D-LOTUS++, a framework that\nintegrates 3D-LOTUS's motion planning capabilities with the task planning\ncapabilities of LLMs and the object grounding accuracy of VLMs. 3D-LOTUS++\nachieves state-of-the-art performance on novel tasks of GemBench, setting a new\nstandard for generalization in robotic manipulation. The benchmark, codes and\ntrained models are available at\nhttps://www.di.ens.fr/willow/research/gembench/.\n","authors":["Ricardo Garcia","Shizhe Chen","Cordelia Schmid"],"pdf_url":"https://arxiv.org/pdf/2410.01345v2.pdf","comment":"ICRA 2025"},{"id":"http://arxiv.org/abs/2407.19617v2","updated":"2025-03-01T19:16:08Z","published":"2024-07-29T00:39:51Z","title":"Leveraging Vision Language Models for Specialized Agricultural Tasks","summary":"  As Vision Language Models (VLMs) become increasingly accessible to farmers\nand agricultural experts, there is a growing need to evaluate their potential\nin specialized tasks. We present AgEval, a comprehensive benchmark for\nassessing VLMs' capabilities in plant stress phenotyping, offering a solution\nto the challenge of limited annotated data in agriculture. Our study explores\nhow general-purpose VLMs can be leveraged for domain-specific tasks with only a\nfew annotated examples, providing insights into their behavior and\nadaptability. AgEval encompasses 12 diverse plant stress phenotyping tasks,\nevaluating zero-shot and few-shot in-context learning performance of\nstate-of-the-art models including Claude, GPT, Gemini, and LLaVA. Our results\ndemonstrate VLMs' rapid adaptability to specialized tasks, with the\nbest-performing model showing an increase in F1 scores from 46.24% to 73.37% in\n8-shot identification. To quantify performance disparities across classes, we\nintroduce metrics such as the coefficient of variation (CV), revealing that\nVLMs' training impacts classes differently, with CV ranging from 26.02% to\n58.03%. We also find that strategic example selection enhances model\nreliability, with exact category examples improving F1 scores by 15.38% on\naverage. AgEval establishes a framework for assessing VLMs in agricultural\napplications, offering valuable benchmarks for future evaluations. Our findings\nsuggest that VLMs, with minimal few-shot examples, show promise as a viable\nalternative to traditional specialized models in plant stress phenotyping,\nwhile also highlighting areas for further refinement. Results and benchmark\ndetails are available at: https://github.com/arbab-ml/AgEval\n","authors":["Muhammad Arbab Arshad","Talukder Zaki Jubery","Tirtho Roy","Rim Nassiri","Asheesh K. Singh","Arti Singh","Chinmay Hegde","Baskar Ganapathysubramanian","Aditya Balu","Adarsh Krishnamurthy","Soumik Sarkar"],"pdf_url":"https://arxiv.org/pdf/2407.19617v2.pdf","comment":"Published at WACV 2025"},{"id":"http://arxiv.org/abs/2410.11019v2","updated":"2025-03-01T18:48:48Z","published":"2024-10-14T19:14:49Z","title":"ET-Former: Efficient Triplane Deformable Attention for 3D Semantic Scene\n  Completion From Monocular Camera","summary":"  We introduce ET-Former, a novel end-to-end algorithm for semantic scene\ncompletion using a single monocular camera. Our approach generates a semantic\noccupancy map from single RGB observation while simultaneously providing\nuncertainty estimates for semantic predictions. By designing a triplane-based\ndeformable attention mechanism, our approach improves geometric understanding\nof the scene than other SOTA approaches and reduces noise in semantic\npredictions. Additionally, through the use of a Conditional Variational\nAutoEncoder (CVAE), we estimate the uncertainties of these predictions. The\ngenerated semantic and uncertainty maps will help formulate navigation\nstrategies that facilitate safe and permissible decision making in the future.\nEvaluated on the Semantic-KITTI dataset, ET-Former achieves the highest\nIntersection over Union (IoU) and mean IoU (mIoU) scores while maintaining the\nlowest GPU memory usage, surpassing state-of-the-art (SOTA) methods. It\nimproves the SOTA scores of IoU from 44.71 to 51.49 and mIoU from 15.04 to\n16.30 on SeamnticKITTI test, with a notably low training memory consumption of\n10.9 GB. Project page: https://github.com/jingGM/ET-Former.git.\n","authors":["Jing Liang","He Yin","Xuewei Qi","Jong Jin Park","Min Sun","Rajasimman Madhivanan","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2410.11019v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17436v2","updated":"2025-03-01T18:48:22Z","published":"2025-02-24T18:59:55Z","title":"Towards Hierarchical Rectified Flow","summary":"  We formulate a hierarchical rectified flow to model data distributions. It\nhierarchically couples multiple ordinary differential equations (ODEs) and\ndefines a time-differentiable stochastic process that generates a data\ndistribution from a known source distribution. Each ODE resembles the ODE that\nis solved in a classic rectified flow, but differs in its domain, i.e.,\nlocation, velocity, acceleration, etc. Unlike the classic rectified flow\nformulation, which formulates a single ODE in the location domain and only\ncaptures the expected velocity field (sufficient to capture a multi-modal data\ndistribution), the hierarchical rectified flow formulation models the\nmulti-modal random velocity field, acceleration field, etc., in their entirety.\nThis more faithful modeling of the random velocity field enables integration\npaths to intersect when the underlying ODE is solved during data generation.\nIntersecting paths in turn lead to integration trajectories that are more\nstraight than those obtained in the classic rectified flow formulation, where\nintegration paths cannot intersect. This leads to modeling of data\ndistributions with fewer neural function evaluations. We empirically verify\nthis on synthetic 1D and 2D data as well as MNIST, CIFAR-10, and ImageNet-32\ndata. Our code is available at: https://riccizz.github.io/HRF/.\n","authors":["Yichi Zhang","Yici Yan","Alex Schwing","Zhizhen Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.17436v2.pdf","comment":"ICLR 2025; Project Page: https://riccizz.github.io/HRF/"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2502.20317v2","updated":"2025-03-01T23:21:58Z","published":"2025-02-27T17:42:52Z","title":"Mixture of Structural-and-Textual Retrieval over Text-rich Graph\n  Knowledge Bases","summary":"  Text-rich Graph Knowledge Bases (TG-KBs) have become increasingly crucial for\nanswering queries by providing textual and structural knowledge. However,\ncurrent retrieval methods often retrieve these two types of knowledge in\nisolation without considering their mutual reinforcement and some hybrid\nmethods even bypass structural retrieval entirely after neighboring\naggregation. To fill in this gap, we propose a Mixture of\nStructural-and-Textual Retrieval (MoR) to retrieve these two types of knowledge\nvia a Planning-Reasoning-Organizing framework. In the Planning stage, MoR\ngenerates textual planning graphs delineating the logic for answering queries.\nFollowing planning graphs, in the Reasoning stage, MoR interweaves structural\ntraversal and textual matching to obtain candidates from TG-KBs. In the\nOrganizing stage, MoR further reranks fetched candidates based on their\nstructural trajectory. Extensive experiments demonstrate the superiority of MoR\nin harmonizing structural and textual retrieval with insights, including uneven\nretrieving performance across different query logics and the benefits of\nintegrating structural trajectories for candidate reranking. Our code is\navailable at https://github.com/Yoega/MoR.\n","authors":["Yongjia Lei","Haoyu Han","Ryan A. Rossi","Franck Dernoncourt","Nedim Lipka","Mahantesh M Halappanavar","Jiliang Tang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2502.20317v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.12887v5","updated":"2025-03-01T22:46:43Z","published":"2021-09-27T09:17:53Z","title":"ICPE: An Item Cluster-Wise Pareto-Efficient Framework for Recommendation\n  Debiasing","summary":"  Recommender system based on historical user-item interactions is of vital\nimportance for web-based services. However, the observed data used to train the\nrecommender model suffers from severe bias issues. Practically, the item\nfrequency distribution of the dataset is a highly skewed power-law\ndistribution. Interactions of a small fraction of head items account for almost\nthe whole training data. The normal training paradigm from such biased data\ntends to repetitively generate recommendations from the head items, which\nfurther exacerbates the biases and affects the exploration of potentially\ninteresting items from the niche set. In this work, we innovatively explore the\ncentral theme of recommendation debiasing from an item cluster-wise\nmulti-objective optimization perspective. Aiming to balance the learning on\nvarious item clusters that differ in popularity during the training process, we\npropose a model-agnostic framework namely Item Cluster-Wise Pareto-Efficient\nRecommendation (ICPE). In detail, we define our item cluster-wise optimization\ntarget as the recommender model should balance all item clusters that differ in\npopularity, thus we set the model learning on each item cluster as a unique\noptimization objective. To achieve this goal, we first explore items'\npopularity levels from a novel causal reasoning perspective. Then, we devise\npopularity discrepancy-based bisecting clustering to separate the item\nclusters. Next, we adaptively find the overall harmonious gradient direction\nfor cluster-wise optimization objectives from a Pareto-efficient solver.\nFinally, in the prediction stage, we perform counterfactual inference to\nfurther eliminate the impact of global propensity. Extensive experimental\nresults verify the superiorities of ICPE on overall recommendation performance\nand biases elimination.\n","authors":["Yule Wang","Xin Xin","Yue Ding","Yunzhe Li","Dong Wang"],"pdf_url":"https://arxiv.org/pdf/2109.12887v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.20207v2","updated":"2025-03-01T14:39:33Z","published":"2024-07-29T17:39:08Z","title":"QAEA-DR: A Unified Text Augmentation Framework for Dense Retrieval","summary":"  In dense retrieval, embedding long texts into dense vectors can result in\ninformation loss, leading to inaccurate query-text matching. Additionally,\nlow-quality texts with excessive noise or sparse key information are unlikely\nto align well with relevant queries. Recent studies mainly focus on improving\nthe sentence embedding model or retrieval process. In this work, we introduce a\nnovel text augmentation framework for dense retrieval. This framework\ntransforms raw documents into information-dense text formats, which supplement\nthe original texts to effectively address the aforementioned issues without\nmodifying embedding or retrieval methodologies. Two text representations are\ngenerated via large language models (LLMs) zero-shot prompting: question-answer\npairs and element-driven events. We term this approach QAEA-DR: unifying\nquestion-answer generation and event extraction in a text augmentation\nframework for dense retrieval. To further enhance the quality of generated\ntexts, a scoring-based evaluation and regeneration mechanism is introduced in\nLLM prompting. Our QAEA-DR model has a positive impact on dense retrieval,\nsupported by both theoretical analysis and empirical experiments.\n","authors":["Hongming Tan","Shaoxiong Zhan","Hai Lin","Hai-Tao Zheng","Wai Kin Chan"],"pdf_url":"https://arxiv.org/pdf/2407.20207v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17901v3","updated":"2025-03-01T00:49:44Z","published":"2024-03-26T17:43:08Z","title":"Search and Society: Reimagining Information Access for Radical Futures","summary":"  Information retrieval (IR) research must understand and contend with the\nsocial implications of the technology it produces. Instead of adopting a\nreactionary strategy of trying to mitigate potential social harms from emerging\ntechnologies, the community should aim to proactively set the research agenda\nfor the kinds of systems we should build inspired by diverse explicitly stated\nsociotechnical imaginaries. The sociotechnical imaginaries that underpin the\ndesign and development of information access technologies needs to be\nexplicitly articulated, and we need to develop theories of change in context of\nthese diverse perspectives. Our guiding future imaginaries must be informed by\nother academic fields, such as human-computer interaction, information\nsciences, media studies, design, science and technology studies, social\nsciences, humanities, democratic theory, and critical theory, as well as legal\nand policy experts, civil rights and social justice activists, and artists,\namong others. In this perspective paper, we motivate why the community must\nconsider this radical shift in how we do research and what we work on, and\nsketch a path forward towards this transformation.\n","authors":["Bhaskar Mitra"],"pdf_url":"https://arxiv.org/pdf/2403.17901v3.pdf","comment":null}]},"2025-02-28T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2410.02642v2","updated":"2025-02-28T19:49:30Z","published":"2024-10-03T16:25:37Z","title":"Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers","summary":"  Information retrieval (IR) systems have played a vital role in modern digital\nlife and have cemented their continued usefulness in this new era of generative\nAI via retrieval-augmented generation. With strong language processing\ncapabilities and remarkable versatility, large language models (LLMs) have\nbecome popular choices for zero-shot re-ranking in IR systems. So far,\nLLM-based re-ranking methods rely on strong generative capabilities, which\nrestricts their use to either specialized or powerful proprietary models. Given\nthese restrictions, we ask: is autoregressive generation necessary and optimal\nfor LLMs to perform re-ranking? We hypothesize that there are abundant signals\nrelevant to re-ranking within LLMs that might not be used to their full\npotential via generation. To more directly leverage such signals, we propose\nin-context re-ranking (ICR), a novel method that leverages the change in\nattention pattern caused by the search query for accurate and efficient\nre-ranking. To mitigate the intrinsic biases in LLMs, we propose a calibration\nmethod using a content-free query. Due to the absence of generation, ICR only\nrequires two ($O(1)$) forward passes to re-rank $N$ documents, making it\nsubstantially more efficient than generative re-ranking methods that require at\nleast $O(N)$ forward passes. Our novel design also enables ICR to be applied to\nany LLM without specialized training while guaranteeing a well-formed ranking.\nExtensive experiments with two popular open-weight LLMs on standard single-hop\nand multi-hop information retrieval benchmarks show that ICR outperforms\nRankGPT while cutting the latency by more than 60% in practice. Through\ndetailed analyses, we show that ICR's performance is specially strong on tasks\nthat require more complex re-ranking signals. Our findings call for further\nexploration on novel ways of utilizing open-weight LLMs beyond text generation.\n","authors":["Shijie Chen","Bernal Jiménez Gutiérrez","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2410.02642v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.21195v1","updated":"2025-02-28T16:14:00Z","published":"2025-02-28T16:14:00Z","title":"Joint Modeling in Recommendations: A Survey","summary":"  In today's digital landscape, Deep Recommender Systems (DRS) play a crucial\nrole in navigating and customizing online content for individual preferences.\nHowever, conventional methods, which mainly depend on single recommendation\ntask, scenario, data modality and user behavior, are increasingly seen as\ninsufficient due to their inability to accurately reflect users' complex and\nchanging preferences. This gap underscores the need for joint modeling\napproaches, which are central to overcoming these limitations by integrating\ndiverse tasks, scenarios, modalities, and behaviors in the recommendation\nprocess, thus promising significant enhancements in recommendation precision,\nefficiency, and customization. In this paper, we comprehensively survey the\njoint modeling methods in recommendations. We begin by defining the scope of\njoint modeling through four distinct dimensions: multi-task, multi-scenario,\nmulti-modal, and multi-behavior modeling. Subsequently, we examine these\nmethods in depth, identifying and summarizing their underlying paradigms based\non the latest advancements and potential research trajectories. Ultimately, we\nhighlight several promising avenues for future exploration in joint modeling\nfor recommendations and provide a concise conclusion to our findings.\n","authors":["Xiangyu Zhao","Yichao Wang","Bo Chen","Jingtong Gao","Yuhao Wang","Xiaopeng Li","Pengyue Jia","Qidong Liu","Huifeng Guo","Ruiming Tang"],"pdf_url":"https://arxiv.org/pdf/2502.21195v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2302.03525"},{"id":"http://arxiv.org/abs/2502.21112v1","updated":"2025-02-28T14:52:25Z","published":"2025-02-28T14:52:25Z","title":"Optimizing Large Language Models for ESG Activity Detection in Financial\n  Texts","summary":"  The integration of Environmental, Social, and Governance (ESG) factors into\ncorporate decision-making is a fundamental aspect of sustainable finance.\nHowever, ensuring that business practices align with evolving regulatory\nframeworks remains a persistent challenge. AI-driven solutions for\nautomatically assessing the alignment of sustainability reports and\nnon-financial disclosures with specific ESG activities could greatly support\nthis process. Yet, this task remains complex due to the limitations of\ngeneral-purpose Large Language Models (LLMs) in domain-specific contexts and\nthe scarcity of structured, high-quality datasets. In this paper, we\ninvestigate the ability of current-generation LLMs to identify text related to\nenvironmental activities. Furthermore, we demonstrate that their performance\ncan be significantly enhanced through fine-tuning on a combination of original\nand synthetically generated data. To this end, we introduce ESG-Activities, a\nbenchmark dataset containing 1,325 labelled text segments classified according\nto the EU ESG taxonomy. Our experimental results show that fine-tuning on\nESG-Activities significantly enhances classification accuracy, with open models\nsuch as Llama 7B and Gemma 7B outperforming large proprietary solutions in\nspecific configurations. These findings have important implications for\nfinancial analysts, policymakers, and AI researchers seeking to enhance ESG\ntransparency and compliance through advanced natural language processing\ntechniques.\n","authors":["Mattia Birti","Francesco Osborne","Andrea Maurino"],"pdf_url":"https://arxiv.org/pdf/2502.21112v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.21067v1","updated":"2025-02-28T14:03:04Z","published":"2025-02-28T14:03:04Z","title":"Fast 3D point clouds retrieval for Large-scale 3D Place Recognition","summary":"  Retrieval in 3D point clouds is a challenging task that consists in\nretrieving the most similar point clouds to a given query within a reference of\n3D points. Current methods focus on comparing descriptors of point clouds in\norder to identify similar ones. Due to the complexity of this latter step, here\nwe focus on the acceleration of the retrieval by adapting the Differentiable\nSearch Index (DSI), a transformer-based approach initially designed for text\ninformation retrieval, for 3D point clouds retrieval. Our approach generates 1D\nidentifiers based on the point descriptors, enabling direct retrieval in\nconstant time. To adapt DSI to 3D data, we integrate Vision Transformers to map\ndescriptors to these identifiers while incorporating positional and semantic\nencoding. The approach is evaluated for place recognition on a public benchmark\ncomparing its retrieval capabilities against state-of-the-art methods, in terms\nof quality and speed of returned point clouds.\n","authors":["Chahine-Nicolas Zede","Laurent Carrafa","Valérie Gouet-Brunet"],"pdf_url":"https://arxiv.org/pdf/2502.21067v1.pdf","comment":"8 pages, 1 figures"},{"id":"http://arxiv.org/abs/2411.12064v2","updated":"2025-02-28T13:56:56Z","published":"2024-11-18T21:10:14Z","title":"TSPRank: Bridging Pairwise and Listwise Methods with a Bilinear\n  Travelling Salesman Model","summary":"  Traditional Learning-To-Rank (LETOR) approaches, including pairwise methods\nlike RankNet and LambdaMART, often fall short by solely focusing on pairwise\ncomparisons, leading to sub-optimal global rankings. Conversely, deep learning\nbased listwise methods, while aiming to optimise entire lists, require complex\ntuning and yield only marginal improvements over robust pairwise models. To\novercome these limitations, we introduce Travelling Salesman Problem Rank\n(TSPRank), a hybrid pairwise-listwise ranking method. TSPRank reframes the\nranking problem as a Travelling Salesman Problem (TSP), a well-known\ncombinatorial optimisation challenge that has been extensively studied for its\nnumerous solution algorithms and applications. This approach enables the\nmodelling of pairwise relationships and leverages combinatorial optimisation to\ndetermine the listwise ranking. This approach can be directly integrated as an\nadditional component into embeddings generated by existing backbone models to\nenhance ranking performance. Our extensive experiments across three backbone\nmodels on diverse tasks, including stock ranking, information retrieval, and\nhistorical events ordering, demonstrate that TSPRank significantly outperforms\nboth pure pairwise and listwise methods. Our qualitative analysis reveals that\nTSPRank's main advantage over existing methods is its ability to harness global\ninformation better while ranking. TSPRank's robustness and superior performance\nacross different domains highlight its potential as a versatile and effective\nLETOR solution.\n","authors":["Weixian Waylon Li","Yftah Ziser","Yifei Xie","Shay B. Cohen","Tiejun Ma"],"pdf_url":"https://arxiv.org/pdf/2411.12064v2.pdf","comment":"Accepted to ACM SIGKDD 2025 Research Track. The code and preprocessed\n  data are available at https://github.com/waylonli/TSPRank-KDD2025"},{"id":"http://arxiv.org/abs/2502.21024v1","updated":"2025-02-28T13:06:25Z","published":"2025-02-28T13:06:25Z","title":"Extending Dense Passage Retrieval with Temporal Information","summary":"  Temporal awareness is crucial in many information retrieval tasks,\nparticularly in scenarios where the relevance of documents depends on their\nalignment with the query's temporal context. Traditional retrieval methods such\nas BM25 and Dense Passage Retrieval (DPR) excel at capturing lexical and\nsemantic relevance but fall short in addressing time-sensitive queries. To\nbridge this gap, we introduce the temporal retrieval model that integrates\nexplicit temporal signals by incorporating query timestamps and document dates\ninto the representation space. Our approach ensures that retrieved passages are\nnot only topically relevant but also temporally aligned with user intent. We\nevaluate our approach on two large-scale benchmark datasets, ArchivalQA and\nChroniclingAmericaQA, achieving substantial performance gains over standard\nretrieval baselines. In particular, our model improves Top-1 retrieval accuracy\nby 6.63% and NDCG@10 by 3.79% on ArchivalQA, while yielding a 9.56% boost in\nTop-1 retrieval accuracy and 4.68% in NDCG@10 on ChroniclingAmericaQA.\nAdditionally, we introduce a time-sensitive negative sampling strategy, which\nrefines the model's ability to distinguish between temporally relevant and\nirrelevant documents during training. Our findings highlight the importance of\nexplicitly modeling time in retrieval systems and set a new standard for\nhandling temporally grounded queries.\n","authors":["Abdelrahman Abdallah","Bhawna Piryani","Jonas Wallat","Avishek Anand","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2502.21024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.17723v2","updated":"2025-02-28T12:56:52Z","published":"2024-01-31T10:35:53Z","title":"LoRec: Large Language Model for Robust Sequential Recommendation against\n  Poisoning Attacks","summary":"  Sequential recommender systems stand out for their ability to capture users'\ndynamic interests and the patterns of item-to-item transitions. However, the\ninherent openness of sequential recommender systems renders them vulnerable to\npoisoning attacks, where fraudulent users are injected into the training data\nto manipulate learned patterns. Traditional defense strategies predominantly\ndepend on predefined assumptions or rules extracted from specific known\nattacks, limiting their generalizability to unknown attack types. To solve the\nabove problems, considering the rich open-world knowledge encapsulated in Large\nLanguage Models (LLMs), our research initially focuses on the capabilities of\nLLMs in the detection of unknown fraudulent activities within recommender\nsystems, a strategy we denote as LLM4Dec. Empirical evaluations demonstrate the\nsubstantial capability of LLMs in identifying unknown fraudsters, leveraging\ntheir expansive, open-world knowledge.\n  Building upon this, we propose the integration of LLMs into defense\nstrategies to extend their effectiveness beyond the confines of known attacks.\nWe propose LoRec, an advanced framework that employs LLM-Enhanced Calibration\nto strengthen the robustness of sequential recommender systems against\npoisoning attacks. LoRec integrates an LLM-enhanced CalibraTor (LCT) that\nrefines the training process of sequential recommender systems with knowledge\nderived from LLMs, applying a user-wise reweighting to diminish the impact of\nfraudsters injected by attacks. By incorporating LLMs' open-world knowledge,\nthe LCT effectively converts the limited, specific priors or rules into a more\ngeneral pattern of fraudsters, offering improved defenses against poisoning\nattacks. Our comprehensive experiments validate that LoRec, as a general\nframework, significantly strengthens the robustness of sequential recommender\nsystems.\n","authors":["Kaike Zhang","Qi Cao","Yunfan Wu","Fei Sun","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2401.17723v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20995v1","updated":"2025-02-28T12:32:53Z","published":"2025-02-28T12:32:53Z","title":"The RAG Paradox: A Black-Box Attack Exploiting Unintentional\n  Vulnerabilities in Retrieval-Augmented Generation Systems","summary":"  With the growing adoption of retrieval-augmented generation (RAG) systems,\nrecent studies have introduced attack methods aimed at degrading their\nperformance. However, these methods rely on unrealistic white-box assumptions,\nsuch as attackers having access to RAG systems' internal processes. To address\nthis issue, we introduce a realistic black-box attack scenario based on the RAG\nparadox, where RAG systems inadvertently expose vulnerabilities while\nattempting to enhance trustworthiness. Because RAG systems reference external\ndocuments during response generation, our attack targets these sources without\nrequiring internal access. Our approach first identifies the external sources\ndisclosed by RAG systems and then automatically generates poisoned documents\nwith misinformation designed to match these sources. Finally, these poisoned\ndocuments are newly published on the disclosed sources, disrupting the RAG\nsystem's response generation process. Both offline and online experiments\nconfirm that this attack significantly reduces RAG performance without\nrequiring internal access. Furthermore, from an insider perspective within the\nRAG system, we propose a re-ranking method that acts as a fundamental\nsafeguard, offering minimal protection against unforeseen attacks.\n","authors":["Chanwoo Choi","Jinsoo Kim","Sukmin Cho","Soyeong Jeong","Buru Chang"],"pdf_url":"https://arxiv.org/pdf/2502.20995v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08248v2","updated":"2025-02-28T11:40:20Z","published":"2025-01-14T16:38:33Z","title":"Eliciting In-context Retrieval and Reasoning for Long-context Large\n  Language Models","summary":"  Recent advancements in long-context language models (LCLMs) promise to\ntransform Retrieval-Augmented Generation (RAG) by simplifying pipelines. With\ntheir expanded context windows, LCLMs can process entire knowledge bases and\nperform retrieval and reasoning directly -- a capability we define as\nIn-Context Retrieval and Reasoning (ICR^2). However, existing benchmarks like\nLOFT often overestimate LCLM performance by providing overly simplified\ncontexts. To address this, we introduce ICR^2, a benchmark that evaluates LCLMs\nin more realistic scenarios by including confounding passages retrieved with\nstrong retrievers. We then propose three methods to enhance LCLM performance:\n(1) retrieve-then-generate fine-tuning, (2) retrieval-attention-probing, which\nuses attention heads to filter and de-noise long contexts during decoding, and\n(3) joint retrieval head training alongside the generation head. Our evaluation\nof five well-known LCLMs on LOFT and ICR^2 demonstrates significant gains with\nour best approach applied to Mistral-7B: +17 and +15 points by Exact Match on\nLOFT, and +13 and +2 points on ICR^2, compared to vanilla RAG and supervised\nfine-tuning, respectively. It even outperforms GPT-4-Turbo on most tasks\ndespite being a much smaller model.\n","authors":["Yifu Qiu","Varun Embar","Yizhe Zhang","Navdeep Jaitly","Shay B. Cohen","Benjamin Han"],"pdf_url":"https://arxiv.org/pdf/2501.08248v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20937v1","updated":"2025-02-28T10:46:56Z","published":"2025-02-28T10:46:56Z","title":"Variations in Relevance Judgments and the Shelf Life of Test Collections","summary":"  The fundamental property of Cranfield-style evaluations, that system rankings\nare stable even when assessors disagree on individual relevance decisions, was\nvalidated on traditional test collections. However, the paradigm shift towards\nneural retrieval models affected the characteristics of modern test\ncollections, e.g., documents are short, judged with four grades of relevance,\nand information needs have no descriptions or narratives. Under these changes,\nit is unclear whether assessor disagreement remains negligible for system\ncomparisons. We investigate this aspect under the additional condition that the\nfew modern test collections are heavily re-used. Given more possible query\ninterpretations due to less formalized information needs, an ''expiration\ndate'' for test collections might be needed if top-effectiveness requires\noverfitting to a single interpretation of relevance. We run a reproducibility\nstudy and re-annotate the relevance judgments of the 2019 TREC Deep Learning\ntrack. We can reproduce prior work in the neural retrieval setting, showing\nthat assessor disagreement does not affect system rankings. However, we observe\nthat some models substantially degrade with our new relevance judgments, and\nsome have already reached the effectiveness of humans as rankers, providing\nevidence that test collections can expire.\n","authors":["Andrew Parry","Maik Fröbe","Harrisen Scells","Ferdinand Schlatt","Guglielmo Faggioli","Saber Zerhoudi","Sean MacAvaney","Eugene Yang"],"pdf_url":"https://arxiv.org/pdf/2502.20937v1.pdf","comment":"11 pages, 6 tables, 5 figures"},{"id":"http://arxiv.org/abs/2502.20936v1","updated":"2025-02-28T10:46:52Z","published":"2025-02-28T10:46:52Z","title":"WebFAQ: A Multilingual Collection of Natural Q&A Datasets for Dense\n  Retrieval","summary":"  We present WebFAQ, a large-scale collection of open-domain question answering\ndatasets derived from FAQ-style schema.org annotations. In total, the data\ncollection consists of 96 million natural question-answer (QA) pairs across 75\nlanguages, including 47 million (49%) non-English samples. WebFAQ further\nserves as the foundation for 20 monolingual retrieval benchmarks with a total\nsize of 11.2 million QA pairs (5.9 million non-English). These datasets are\ncarefully curated through refined filtering and near-duplicate detection,\nyielding high-quality resources for training and evaluating multilingual dense\nretrieval models. To empirically confirm WebFAQ's efficacy, we use the\ncollected QAs to fine-tune an in-domain pretrained XLM-RoBERTa model. Through\nthis process of dataset-specific fine-tuning, the model achieves significant\nretrieval performance gains, which generalize - beyond WebFAQ - to other\nmultilingual retrieval benchmarks evaluated in zero-shot setting. Last but not\nleast, we utilize WebFAQ to construct a set of QA-aligned bilingual corpora\nspanning over 1000 language pairs using state-of-the-art bitext mining and\nautomated LLM-assessed translation evaluation. Due to our advanced, automated\nmethod of bitext dataset generation, the resulting bilingual corpora\ndemonstrate higher translation quality compared to similar datasets. WebFAQ and\nall associated resources are publicly available on GitHub and HuggingFace.\n","authors":["Michael Dinzinger","Laura Caspari","Kanishka Ghosh Dastidar","Jelena Mitrović","Michael Granitzer"],"pdf_url":"https://arxiv.org/pdf/2502.20936v1.pdf","comment":"10 pages, 3 figures, 7 tables"},{"id":"http://arxiv.org/abs/2407.01449v6","updated":"2025-02-28T08:51:57Z","published":"2024-06-27T15:45:29Z","title":"ColPali: Efficient Document Retrieval with Vision Language Models","summary":"  Documents are visually rich structures that convey information through text,\nbut also figures, page layouts, tables, or even fonts. Since modern retrieval\nsystems mainly rely on the textual information they extract from document pages\nto index documents -often through lengthy and brittle processes-, they struggle\nto exploit key visual cues efficiently. This limits their capabilities in many\npractical document retrieval applications such as Retrieval Augmented\nGeneration (RAG). To benchmark current systems on visually rich document\nretrieval, we introduce the Visual Document Retrieval Benchmark ViDoRe,\ncomposed of various page-level retrieval tasks spanning multiple domains,\nlanguages, and practical settings. The inherent complexity and performance\nshortcomings of modern systems motivate a new concept; doing document retrieval\nby directly embedding the images of the document pages. We release ColPali, a\nVision Language Model trained to produce high-quality multi-vector embeddings\nfrom images of document pages. Combined with a late interaction matching\nmechanism, ColPali largely outperforms modern document retrieval pipelines\nwhile being drastically simpler, faster and end-to-end trainable. We release\nmodels, data, code and benchmarks under open licenses at https://hf.co/vidore.\n","authors":["Manuel Faysse","Hugues Sibille","Tony Wu","Bilel Omrani","Gautier Viaud","Céline Hudelot","Pierre Colombo"],"pdf_url":"https://arxiv.org/pdf/2407.01449v6.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.20826v1","updated":"2025-02-28T08:12:23Z","published":"2025-02-28T08:12:23Z","title":"CoTMR: Chain-of-Thought Multi-Scale Reasoning for Training-Free\n  Zero-Shot Composed Image Retrieval","summary":"  Zero-Shot Composed Image Retrieval (ZS-CIR) aims to retrieve target images by\nintegrating information from a composed query (reference image and modification\ntext) without training samples. Existing methods primarily combine caption\nmodels and large language models (LLMs) to generate target captions based on\ncomposed queries but face various issues such as incompatibility, visual\ninformation loss, and insufficient reasoning. In this work, we propose CoTMR, a\ntraining-free framework crafted for ZS-CIR with novel Chain-of-thought (CoT)\nand Multi-scale Reasoning. Instead of relying on caption models for modality\ntransformation, CoTMR employs the Large Vision-Language Model (LVLM) to achieve\nunified understanding and reasoning for composed queries. To enhance the\nreasoning reliability, we devise CIRCoT, which guides the LVLM through a\nstep-by-step inference process using predefined subtasks. Considering that\nexisting approaches focus solely on global-level reasoning, our CoTMR\nincorporates multi-scale reasoning to achieve more comprehensive inference via\nfine-grained predictions about the presence or absence of key elements at the\nobject scale. Further, we design a Multi-Grained Scoring (MGS) mechanism, which\nintegrates CLIP similarity scores of the above reasoning outputs with candidate\nimages to realize precise retrieval. Extensive experiments demonstrate that our\nCoTMR not only drastically outperforms previous methods across four prominent\nbenchmarks but also offers appealing interpretability.\n","authors":["Zelong Sun","Dong Jing","Zhiwu Lu"],"pdf_url":"https://arxiv.org/pdf/2502.20826v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20695v1","updated":"2025-02-28T04:03:23Z","published":"2025-02-28T04:03:23Z","title":"Scalable Overload-Aware Graph-Based Index Construction for\n  10-Billion-Scale Vector Similarity Search","summary":"  Approximate Nearest Neighbor Search (ANNS) is essential for modern\ndata-driven applications that require efficient retrieval of top-k results from\nmassive vector databases. Although existing graph-based ANNS algorithms achieve\na high recall rate on billion-scale datasets, their slow construction speed and\nlimited scalability hinder their applicability to large-scale industrial\nscenarios. In this paper, we introduce SOGAIC, the first Scalable\nOverload-Aware Graph-Based ANNS Index Construction system tailored for\nultra-large-scale vector databases: 1) We propose a dynamic data partitioning\nalgorithm with overload constraints that adaptively introduces overlaps among\nsubsets; 2) To enable efficient distributed subgraph construction, we employ a\nload-balancing task scheduling framework combined with an agglomerative merging\nstrategy; 3) Extensive experiments on various datasets demonstrate a reduction\nof 47.3% in average construction time compared to existing methods. The\nproposed method has also been successfully deployed in a real-world industrial\nsearch engine, managing over 10 billion daily updated vectors and serving\nhundreds of millions of users.\n","authors":["Yang Shi","Yiping Sun","Jiaolong Du","Xiaocheng Zhong","Zhiyong Wang","Yao Hu"],"pdf_url":"https://arxiv.org/pdf/2502.20695v1.pdf","comment":"Accepted by WWW'25"},{"id":"http://arxiv.org/abs/2502.20687v1","updated":"2025-02-28T03:40:37Z","published":"2025-02-28T03:40:37Z","title":"Unleashing the Potential of Two-Tower Models: Diffusion-Based\n  Cross-Interaction for Large-Scale Matching","summary":"  Two-tower models are widely adopted in the industrial-scale matching stage\nacross a broad range of application domains, such as content recommendations,\nadvertisement systems, and search engines. This model efficiently handles\nlarge-scale candidate item screening by separating user and item\nrepresentations. However, the decoupling network also leads to a neglect of\npotential information interaction between the user and item representations.\nCurrent state-of-the-art (SOTA) approaches include adding a shallow fully\nconnected layer(i.e., COLD), which is limited by performance and can only be\nused in the ranking stage. For performance considerations, another approach\nattempts to capture historical positive interaction information from the other\ntower by regarding them as the input features(i.e., DAT). Later research showed\nthat the gains achieved by this method are still limited because of lacking the\nguidance on the next user intent. To address the aforementioned challenges, we\npropose a \"cross-interaction decoupling architecture\" within our matching\nparadigm. This user-tower architecture leverages a diffusion module to\nreconstruct the next positive intention representation and employs a\nmixed-attention module to facilitate comprehensive cross-interaction. During\nthe next positive intention generation, we further enhance the accuracy of its\nreconstruction by explicitly extracting the temporal drift within user behavior\nsequences. Experiments on two real-world datasets and one industrial dataset\ndemonstrate that our method outperforms the SOTA two-tower models\nsignificantly, and our diffusion approach outperforms other generative models\nin reconstructing item representations.\n","authors":["Yihan Wang","Fei Xiong","Zhexin Han","Qi Song","Kaiqiao Zhan","Ben Wang"],"pdf_url":"https://arxiv.org/pdf/2502.20687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00944v3","updated":"2025-02-28T03:23:52Z","published":"2024-06-03T02:56:14Z","title":"A Theory for Token-Level Harmonization in Retrieval-Augmented Generation","summary":"  Retrieval-augmented generation (RAG) utilizes retrieved texts to enhance\nlarge language models (LLMs). Studies show that while RAG provides valuable\nexternal information (benefit), it may also mislead LLMs (detriment) with noisy\nor incorrect retrieved texts. Although many existing methods attempt to\npreserve benefit and avoid detriment, they lack a theoretical explanation for\nRAG. The benefit and detriment in the next token prediction of RAG remain a\nblack box that cannot be quantified or compared in an explainable manner, so\nexisting methods are data-driven, need additional utility evaluators or\npost-hoc. This paper takes the first step towards providing a theory to explain\nand trade off the benefit and detriment in RAG. First, we model RAG as the\nfusion between distribution of LLMs knowledge and distribution of retrieved\ntexts. Then, we formalize the trade-off between the value of external knowledge\n(benefit) and its potential risk of misleading LLMs (detriment) in next token\nprediction of RAG by distribution difference in this fusion. Finally, we prove\nthat the actual effect of RAG on the token, which is the comparison between\nbenefit and detriment, can be predicted without any training or accessing the\nutility of retrieval. Based on our theory, we propose a practical novel method,\nTok-RAG, which achieves collaborative generation between the pure LLM and RAG\nat token level to preserve benefit and avoid detriment. Experiments in\nreal-world tasks using LLMs such as OPT, LLaMA-2, and Mistral show the\neffectiveness of our method and support our theoretical findings.\n","authors":["Shicheng Xu","Liang Pang","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2406.00944v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.20640v1","updated":"2025-02-28T01:46:32Z","published":"2025-02-28T01:46:32Z","title":"LexRAG: Benchmarking Retrieval-Augmented Generation in Multi-Turn Legal\n  Consultation Conversation","summary":"  Retrieval-augmented generation (RAG) has proven highly effective in improving\nlarge language models (LLMs) across various domains. However, there is no\nbenchmark specifically designed to assess the effectiveness of RAG in the legal\ndomain, which restricts progress in this area. To fill this gap, we propose\nLexRAG, the first benchmark to evaluate RAG systems for multi-turn legal\nconsultations. LexRAG consists of 1,013 multi-turn dialogue samples and 17,228\ncandidate legal articles. Each sample is annotated by legal experts and\nconsists of five rounds of progressive questioning. LexRAG includes two key\ntasks: (1) Conversational knowledge retrieval, requiring accurate retrieval of\nrelevant legal articles based on multi-turn context. (2) Response generation,\nfocusing on producing legally sound answers. To ensure reliable\nreproducibility, we develop LexiT, a legal RAG toolkit that provides a\ncomprehensive implementation of RAG system components tailored for the legal\ndomain. Additionally, we introduce an LLM-as-a-judge evaluation pipeline to\nenable detailed and effective assessment. Through experimental analysis of\nvarious LLMs and retrieval methods, we reveal the key limitations of existing\nRAG systems in handling legal consultation conversations. LexRAG establishes a\nnew benchmark for the practical application of RAG systems in the legal domain,\nwith its code and data available at https://github.com/CSHaitao/LexRAG.\n","authors":["Haitao Li","Yifan Chen","Yiran Hu","Qingyao Ai","Junjie Chen","Xiaoyu Yang","Jianhui Yang","Yueyue Wu","Zeyang Liu","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2502.20640v1.pdf","comment":"10 pages"}],"Multimedia":[{"id":"http://arxiv.org/abs/2502.20904v1","updated":"2025-02-28T10:01:39Z","published":"2025-02-28T10:01:39Z","title":"DiffBrush:Just Painting the Art by Your Hands","summary":"  The rapid development of image generation and editing algorithms in recent\nyears has enabled ordinary user to produce realistic images. However, the\ncurrent AI painting ecosystem predominantly relies on text-driven diffusion\nmodels (T2I), which pose challenges in accurately capturing user requirements.\nFurthermore, achieving compatibility with other modalities incurs substantial\ntraining costs. To this end, we introduce DiffBrush, which is compatible with\nT2I models and allows users to draw and edit images. By manipulating and\nadapting the internal representation of the diffusion model, DiffBrush guides\nthe model-generated images to converge towards the user's hand-drawn sketches\nfor user's specific needs without additional training. DiffBrush achieves\ncontrol over the color, semantic, and instance of objects in images by\ncontinuously guiding the latent and instance-level attention map during the\ndenoising process of the diffusion model. Besides, we propose a latent\nregeneration, which refines the randomly sampled noise in the diffusion model,\nobtaining a better image generation layout. Finally, users only need to roughly\ndraw the mask of the instance (acceptable colors) on the canvas, DiffBrush can\nnaturally generate the corresponding instance at the corresponding location.\n","authors":["Jiaming Chu","Lei Jin","Tao Wang","Junliang Xing","Jian Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.20904v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20858v1","updated":"2025-02-28T09:01:30Z","published":"2025-02-28T09:01:30Z","title":"EyEar: Learning Audio Synchronized Human Gaze Trajectory Based on\n  Physics-Informed Dynamics","summary":"  Imitating how humans move their gaze in a visual scene is a vital research\nproblem for both visual understanding and psychology, kindling crucial\napplications such as building alive virtual characters. Previous studies aim to\npredict gaze trajectories when humans are free-viewing an image, searching for\nrequired targets, or looking for clues to answer questions in an image. While\nthese tasks focus on visual-centric scenarios, humans move their gaze also\nalong with audio signal inputs in more common scenarios. To fill this gap, we\nintroduce a new task that predicts human gaze trajectories in a visual scene\nwith synchronized audio inputs and provide a new dataset containing 20k gaze\npoints from 8 subjects. To effectively integrate audio information and simulate\nthe dynamic process of human gaze motion, we propose a novel learning framework\ncalled EyEar (Eye moving while Ear listening) based on physics-informed\ndynamics, which considers three key factors to predict gazes: eye inherent\nmotion tendency, vision salient attraction, and audio semantic attraction. We\nalso propose a probability density score to overcome the high individual\nvariability of gaze trajectories, thereby improving the stabilization of\noptimization and the reliability of the evaluation. Experimental results show\nthat EyEar outperforms all the baselines in the context of all evaluation\nmetrics, thanks to the proposed components in the learning model.\n","authors":["Xiaochuan Liu","Xin Cheng","Yuchong Sun","Xiaoxue Wu","Ruihua Song","Hao Sun","Denghao Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.20858v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20811v1","updated":"2025-02-28T07:53:40Z","published":"2025-02-28T07:53:40Z","title":"HAIC: Improving Human Action Understanding and Generation with Better\n  Captions for Multi-modal Large Language Models","summary":"  Recent Multi-modal Large Language Models (MLLMs) have made great progress in\nvideo understanding. However, their performance on videos involving human\nactions is still limited by the lack of high-quality data. To address this, we\nintroduce a two-stage data annotation pipeline. First, we design strategies to\naccumulate videos featuring clear human actions from the Internet. Second,\nvideos are annotated in a standardized caption format that uses human\nattributes to distinguish individuals and chronologically details their actions\nand interactions. Through this pipeline, we curate two datasets, namely\nHAICTrain and HAICBench. \\textbf{HAICTrain} comprises 126K video-caption pairs\ngenerated by Gemini-Pro and verified for training purposes. Meanwhile,\n\\textbf{HAICBench} includes 500 manually annotated video-caption pairs and\n1,400 QA pairs, for a comprehensive evaluation of human action understanding.\nExperimental results demonstrate that training with HAICTrain not only\nsignificantly enhances human understanding abilities across 4 benchmarks, but\ncan also improve text-to-video generation results. Both the HAICTrain and\nHAICBench are released at https://huggingface.co/datasets/KuaishouHAIC/HAIC.\n","authors":["Xiao Wang","Jingyun Hua","Weihong Lin","Yuanxing Zhang","Fuzheng Zhang","Jianlong Wu","Di Zhang","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2502.20811v1.pdf","comment":null}]},"2025-03-04T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2503.02881v1","updated":"2025-03-04T18:58:21Z","published":"2025-03-04T18:58:21Z","title":"Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for\n  Contact-Rich Manipulation","summary":"  Humans can accomplish complex contact-rich tasks using vision and touch, with\nhighly reactive capabilities such as quick adjustments to environmental changes\nand adaptive control of contact forces; however, this remains challenging for\nrobots. Existing visual imitation learning (IL) approaches rely on action\nchunking to model complex behaviors, which lacks the ability to respond\ninstantly to real-time tactile feedback during the chunk execution.\nFurthermore, most teleoperation systems struggle to provide fine-grained\ntactile / force feedback, which limits the range of tasks that can be\nperformed. To address these challenges, we introduce TactAR, a low-cost\nteleoperation system that provides real-time tactile feedback through Augmented\nReality (AR), along with Reactive Diffusion Policy (RDP), a novel slow-fast\nvisual-tactile imitation learning algorithm for learning contact-rich\nmanipulation skills. RDP employs a two-level hierarchy: (1) a slow latent\ndiffusion policy for predicting high-level action chunks in latent space at low\nfrequency, (2) a fast asymmetric tokenizer for closed-loop tactile feedback\ncontrol at high frequency. This design enables both complex trajectory modeling\nand quick reactive behavior within a unified framework. Through extensive\nevaluation across three challenging contact-rich tasks, RDP significantly\nimproves performance compared to state-of-the-art visual IL baselines through\nrapid response to tactile / force feedback. Furthermore, experiments show that\nRDP is applicable across different tactile / force sensors. Code and videos are\navailable on https://reactive-diffusion-policy.github.io/.\n","authors":["Han Xue","Jieji Ren","Wendi Chen","Gu Zhang","Yuan Fang","Guoying Gu","Huazhe Xu","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2503.02881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.06414v4","updated":"2025-03-04T18:34:50Z","published":"2024-11-10T10:31:21Z","title":"Psycho Gundam: Electroencephalography based real-time robotic control\n  system with deep learning","summary":"  The Psycho Frame, a sophisticated system primarily used in Universal Century\n(U.C.) series mobile suits for NEWTYPE pilots, has evolved as an integral\ncomponent in harnessing the latent potential of mental energy. Its ability to\namplify and resonate with the pilot's psyche enables real-time mental control,\ncreating unique applications such as psychomagnetic fields and sensory-based\nweaponry. This paper presents the development of a novel robotic control system\ninspired by the Psycho Frame, combining electroencephalography (EEG) and deep\nlearning for real-time control of robotic systems. By capturing and\ninterpreting brainwave data through EEG, the system extends human cognitive\ncommands to robotic actions, reflecting the seamless synchronization of thought\nand machine, much like the Psyco Frame's integration with a Newtype pilot's\nmental faculties. This research demonstrates how modern AI techniques can\nexpand the limits of human-machine interaction, potentially transcending\ntraditional input methods and enabling a deeper, more intuitive control of\ncomplex robotic systems.\n","authors":["Chi-Sheng Chen","Wei-Sheng Wang"],"pdf_url":"https://arxiv.org/pdf/2411.06414v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02834v1","updated":"2025-03-04T17:57:35Z","published":"2025-03-04T17:57:35Z","title":"MuBlE: MuJoCo and Blender simulation Environment and Benchmark for Task\n  Planning in Robot Manipulation","summary":"  Current embodied reasoning agents struggle to plan for long-horizon tasks\nthat require to physically interact with the world to obtain the necessary\ninformation (e.g. 'sort the objects from lightest to heaviest'). The\nimprovement of the capabilities of such an agent is highly dependent on the\navailability of relevant training environments. In order to facilitate the\ndevelopment of such systems, we introduce a novel simulation environment (built\non top of robosuite) that makes use of the MuJoCo physics engine and\nhigh-quality renderer Blender to provide realistic visual observations that are\nalso accurate to the physical state of the scene. It is the first simulator\nfocusing on long-horizon robot manipulation tasks preserving accurate physics\nmodeling. MuBlE can generate mutlimodal data for training and enable design of\nclosed-loop methods through environment interaction on two levels: visual -\naction loop, and control - physics loop. Together with the simulator, we\npropose SHOP-VRB2, a new benchmark composed of 10 classes of multi-step\nreasoning scenarios that require simultaneous visual and physical measurements.\n","authors":["Michal Nazarczuk","Karla Stepanova","Jan Kristof Behrens","Matej Hoffmann","Krystian Mikolajczyk"],"pdf_url":"https://arxiv.org/pdf/2503.02834v1.pdf","comment":"https://github.com/michaal94/MuBlE. arXiv admin note: substantial\n  text overlap with arXiv:2404.15194"},{"id":"http://arxiv.org/abs/2412.05313v6","updated":"2025-03-04T17:33:11Z","published":"2024-11-28T19:31:50Z","title":"λ: A Benchmark for Data-Efficiency in Long-Horizon Indoor Mobile\n  Manipulation Robotics","summary":"  Learning to execute long-horizon mobile manipulation tasks is crucial for\nadvancing robotics in household and workplace settings. However, current\napproaches are typically data-inefficient, underscoring the need for improved\nmodels that require realistically sized benchmarks to evaluate their\nefficiency. To address this, we introduce the LAMBDA ({\\lambda})\nbenchmark-Long-horizon Actions for Mobile-manipulation Benchmarking of Directed\nActivities-which evaluates the data efficiency of models on\nlanguage-conditioned, long-horizon, multi-room, multi-floor, pick-and-place\ntasks using a dataset of manageable size, more feasible for collection. Our\nbenchmark includes 571 human-collected demonstrations that provide realism and\ndiversity in simulated and real-world settings. Unlike planner-generated data,\nthese trajectories offer natural variability and replay-verifiability, ensuring\nrobust learning and evaluation. We leverage LAMBDA to benchmark current\nend-to-end learning methods and a modular neuro-symbolic approaches that\ncombines foundation models with task and motion planning. We find that\nend-to-end methods-even when pretrained-yield lower success rates, while\nneuro-symbolic methods perform significantly better and require less data.\n","authors":["Ahmed Jaafar","Shreyas Sundara Raman","Yichen Wei","Sudarshan Harithas","Sofia Juliani","Anneke Wernerfelt","Benedict Quartey","Ifrah Idrees","Jason Xinyu Liu","Stefanie Tellex"],"pdf_url":"https://arxiv.org/pdf/2412.05313v6.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2411.13079v2","updated":"2025-03-04T17:07:15Z","published":"2024-11-20T07:07:42Z","title":"Neural Internal Model Control: Learning a Robust Control Policy via\n  Predictive Error Feedback","summary":"  Accurate motion control in the face of disturbances within complex\nenvironments remains a major challenge in robotics. Classical model-based\napproaches often struggle with nonlinearities and unstructured disturbances,\nwhile RL-based methods can be fragile when encountering unseen scenarios. In\nthis paper, we propose a novel framework, Neural Internal Model Control, which\nintegrates model-based control with RL-based control to enhance robustness. Our\nframework streamlines the predictive model by applying Newton-Euler equations\nfor rigid-body dynamics, eliminating the need to capture complex\nhigh-dimensional nonlinearities. This internal model combines model-free RL\nalgorithms with predictive error feedback. Such a design enables a closed-loop\ncontrol structure to enhance the robustness and generalizability of the control\nsystem. We demonstrate the effectiveness of our framework on both quadrotors\nand quadrupedal robots, achieving superior performance compared to\nstate-of-the-art methods. Furthermore, real-world deployment on a quadrotor\nwith rope-suspended payloads highlights the framework's robustness in\nsim-to-real transfer. Our code is released at\nhttps://github.com/thu-uav/NeuralIMC.\n","authors":["Feng Gao","Chao Yu","Yu Wang","Yi Wu"],"pdf_url":"https://arxiv.org/pdf/2411.13079v2.pdf","comment":"Submitted to RAL"},{"id":"http://arxiv.org/abs/2503.02774v1","updated":"2025-03-04T16:44:53Z","published":"2025-03-04T16:44:53Z","title":"Digital Model-Driven Genetic Algorithm for Optimizing Layout and Task\n  Allocation in Human-Robot Collaborative Assemblies","summary":"  This paper addresses the optimization of human-robot collaborative work-cells\nbefore their physical deployment. Most of the times, such environments are\ndesigned based on the experience of the system integrators, often leading to\nsub-optimal solutions. Accurate simulators of the robotic cell, accounting for\nthe presence of the human as well, are available today and can be used in the\npre-deployment. We propose an iterative optimization scheme where a digital\nmodel of the work-cell is updated based on a genetic algorithm. The methodology\nfocuses on the layout optimization and task allocation, encoding both the\nproblems simultaneously in the design variables handled by the genetic\nalgorithm, while the task scheduling problem depends on the result of the\nupper-level one. The final solution balances conflicting objectives in the\nfitness function and is validated to show the impact of the objectives with\nrespect to a baseline, which represents possible initial choices selected based\non the human judgment.\n","authors":["Christian Cella","Matteo Bruce Robin","Marco Faroni","Andrea Maria Zanchettin","Paolo Rocco"],"pdf_url":"https://arxiv.org/pdf/2503.02774v1.pdf","comment":"Accepted at IEEE ICRA 2025 (Atlanta, USA)"},{"id":"http://arxiv.org/abs/2503.02752v1","updated":"2025-03-04T16:19:06Z","published":"2025-03-04T16:19:06Z","title":"Deep Learning-Enhanced Visual Monitoring in Hazardous Underwater\n  Environments with a Swarm of Micro-Robots","summary":"  Long-term monitoring and exploration of extreme environments, such as\nunderwater storage facilities, is costly, labor-intensive, and hazardous.\nAutomating this process with low-cost, collaborative robots can greatly improve\nefficiency. These robots capture images from different positions, which must be\nprocessed simultaneously to create a spatio-temporal model of the facility. In\nthis paper, we propose a novel approach that integrates data simulation, a\nmulti-modal deep learning network for coordinate prediction, and image\nreassembly to address the challenges posed by environmental disturbances\ncausing drift and rotation in the robots' positions and orientations. Our\napproach enhances the precision of alignment in noisy environments by\nintegrating visual information from snapshots, global positional context from\nmasks, and noisy coordinates. We validate our method through extensive\nexperiments using synthetic data that simulate real-world robotic operations in\nunderwater settings. The results demonstrate very high coordinate prediction\naccuracy and plausible image assembly, indicating the real-world applicability\nof our approach. The assembled images provide clear and coherent views of the\nunderwater environment for effective monitoring and inspection, showcasing the\npotential for broader use in extreme settings, further contributing to improved\nsafety, efficiency, and cost reduction in hazardous field monitoring. Code is\navailable on https://github.com/ChrisChen1023/Micro-Robot-Swarm.\n","authors":["Shuang Chen","Yifeng He","Barry Lennox","Farshad Arvin","Amir Atapour-Abarghouei"],"pdf_url":"https://arxiv.org/pdf/2503.02752v1.pdf","comment":"Accepted by ICRA 2025"},{"id":"http://arxiv.org/abs/2503.02748v1","updated":"2025-03-04T16:14:06Z","published":"2025-03-04T16:14:06Z","title":"Bridging VLM and KMP: Enabling Fine-grained robotic manipulation via\n  Semantic Keypoints Representation","summary":"  From early Movement Primitive (MP) techniques to modern Vision-Language\nModels (VLMs), autonomous manipulation has remained a pivotal topic in\nrobotics. As two extremes, VLM-based methods emphasize zero-shot and adaptive\nmanipulation but struggle with fine-grained planning. In contrast, MP-based\napproaches excel in precise trajectory generalization but lack decision-making\nability. To leverage the strengths of the two frameworks, we propose VL-MP,\nwhich integrates VLM with Kernelized Movement Primitives (KMP) via a\nlow-distortion decision information transfer bridge, enabling fine-grained\nrobotic manipulation under ambiguous situations. One key of VL-MP is the\naccurate representation of task decision parameters through semantic keypoints\nconstraints, leading to more precise task parameter generation. Additionally,\nwe introduce a local trajectory feature-enhanced KMP to support VL-MP, thereby\nachieving shape preservation for complex trajectories. Extensive experiments\nconducted in complex real-world environments validate the effectiveness of\nVL-MP for adaptive and fine-grained manipulation.\n","authors":["Junjie Zhu","Huayu Liu","Jin Wang","Bangrong Wen","Kaixiang Huang","Xiaofei Li","Haiyun Zhan","Guodong Lu"],"pdf_url":"https://arxiv.org/pdf/2503.02748v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02738v1","updated":"2025-03-04T15:57:32Z","published":"2025-03-04T15:57:32Z","title":"Variable-Friction In-Hand Manipulation for Arbitrary Objects via\n  Diffusion-Based Imitation Learning","summary":"  Dexterous in-hand manipulation (IHM) for arbitrary objects is challenging due\nto the rich and subtle contact process. Variable-friction manipulation is an\nalternative approach to dexterity, previously demonstrating robust and\nversatile 2D IHM capabilities with only two single-joint fingers. However, the\nhard-coded manipulation methods for variable friction hands are restricted to\nregular polygon objects and limited target poses, as well as requiring the\npolicy to be tailored for each object. This paper proposes an end-to-end\nlearning-based manipulation method to achieve arbitrary object manipulation for\nany target pose on real hardware, with minimal engineering efforts and data\ncollection. The method features a diffusion policy-based imitation learning\nmethod with co-training from simulation and a small amount of real-world data.\nWith the proposed framework, arbitrary objects including polygons and\nnon-polygons can be precisely manipulated to reach arbitrary goal poses within\n2 hours of training on an A100 GPU and only 1 hour of real-world data\ncollection. The precision is higher than previous customized object-specific\npolicies, achieving an average success rate of 71.3% with average pose error\nbeing 2.676 mm and 1.902 degrees.\n","authors":["Qiyang Yan","Zihan Ding","Xin Zhou","Adam J. Spiers"],"pdf_url":"https://arxiv.org/pdf/2503.02738v1.pdf","comment":"Accepted by ICRA 2025 Project website:\n  https://sites.google.com/view/vf-ihm-il/home"},{"id":"http://arxiv.org/abs/2503.02723v1","updated":"2025-03-04T15:41:40Z","published":"2025-03-04T15:41:40Z","title":"ImpedanceGPT: VLM-driven Impedance Control of Swarm of Mini-drones for\n  Intelligent Navigation in Dynamic Environment","summary":"  Swarm robotics plays a crucial role in enabling autonomous operations in\ndynamic and unpredictable environments. However, a major challenge remains\nensuring safe and efficient navigation in environments filled with both dynamic\nalive (e.g., humans) and dynamic inanimate (e.g., non-living objects)\nobstacles. In this paper, we propose ImpedanceGPT, a novel system that combines\na Vision-Language Model (VLM) with retrieval-augmented generation (RAG) to\nenable real-time reasoning for adaptive navigation of mini-drone swarms in\ncomplex environments.\n  The key innovation of ImpedanceGPT lies in the integration of VLM and RAG,\nwhich provides the drones with enhanced semantic understanding of their\nsurroundings. This enables the system to dynamically adjust impedance control\nparameters in response to obstacle types and environmental conditions. Our\napproach not only ensures safe and precise navigation but also improves\ncoordination between drones in the swarm.\n  Experimental evaluations demonstrate the effectiveness of the system. The\nVLM-RAG framework achieved an obstacle detection and retrieval accuracy of 80 %\nunder optimal lighting. In static environments, drones navigated dynamic\ninanimate obstacles at 1.4 m/s but slowed to 0.7 m/s with increased separation\naround humans. In dynamic environments, speed adjusted to 1.0 m/s near hard\nobstacles, while reducing to 0.6 m/s with higher deflection to safely avoid\nmoving humans.\n","authors":["Faryal Batool","Malaika Zafar","Yasheerah Yaqoot","Roohan Ahmed Khan","Muhammad Haris Khan","Aleksey Fedoseev","Dzmitry Tsetserukou"],"pdf_url":"https://arxiv.org/pdf/2503.02723v1.pdf","comment":"Submitted to IROS 2025"},{"id":"http://arxiv.org/abs/2503.02720v1","updated":"2025-03-04T15:36:19Z","published":"2025-03-04T15:36:19Z","title":"Vibration-Assisted Hysteresis Mitigation for Achieving High Compensation\n  Efficiency","summary":"  Tendon-sheath mechanisms (TSMs) are widely used in minimally invasive\nsurgical (MIS) applications, but their inherent hysteresis-caused by friction,\nbacklash, and tendon elongation-leads to significant tracking errors.\nConventional modeling and compensation methods struggle with these\nnonlinearities and require extensive parameter tuning. To address this, we\npropose a vibration-assisted hysteresis compensation approach, where controlled\nvibrational motion is applied along the tendon's movement direction to mitigate\nfriction and reduce dead zones. Experimental results demonstrate that the\nexerted vibration consistently reduces hysteresis across all tested\nfrequencies, decreasing RMSE by up to 23.41% (from 2.2345 mm to 1.7113 mm) and\nimproving correlation, leading to more accurate trajectory tracking. When\ncombined with a Temporal Convolutional Network (TCN)-based compensation model,\nvibration further enhances performance, achieving an 85.2% reduction in MAE\n(from 1.334 mm to 0.1969 mm). Without vibration, the TCN-based approach still\nreduces MAE by 72.3% (from 1.334 mm to 0.370 mm) under the same parameter\nsettings. These findings confirm that vibration effectively mitigates\nhysteresis, improving trajectory accuracy and enabling more efficient\ncompensation models with fewer trainable parameters. This approach provides a\nscalable and practical solution for TSM-based robotic applications,\nparticularly in MIS.\n","authors":["Myeongbo Park","Chunggil An","Junhyun Park","Jonghyun Kang","Minho Hwang"],"pdf_url":"https://arxiv.org/pdf/2503.02720v1.pdf","comment":"8 pages, 7 figures, and 2 tables"},{"id":"http://arxiv.org/abs/2503.02719v1","updated":"2025-03-04T15:33:27Z","published":"2025-03-04T15:33:27Z","title":"Scalable Multi-Robot Task Allocation and Coordination under Signal\n  Temporal Logic Specifications","summary":"  Motion planning with simple objectives, such as collision-avoidance and\ngoal-reaching, can be solved efficiently using modern planners. However, the\ncomplexity of the allowed tasks for these planners is limited. On the other\nhand, signal temporal logic (STL) can specify complex requirements, but\nSTL-based motion planning and control algorithms often face scalability issues,\nespecially in large multi-robot systems with complex dynamics. In this paper,\nwe propose an algorithm that leverages the best of the two worlds. We first use\na single-robot motion planner to efficiently generate a set of alternative\nreference paths for each robot. Then coordination requirements are specified\nusing STL, which is defined over the assignment of paths and robots' progress\nalong those paths. We use a Mixed Integer Linear Program (MILP) to compute task\nassignments and robot progress targets over time such that the STL\nspecification is satisfied. Finally, a local controller is used to track the\ntarget progress. Simulations demonstrate that our method can handle tasks with\ncomplex constraints and scales to large multi-robot teams and intricate task\nallocation scenarios.\n","authors":["Wenliang Liu","Nathalie Majcherczyk","Federico Pecora"],"pdf_url":"https://arxiv.org/pdf/2503.02719v1.pdf","comment":"Accepted by ICRA 2025"},{"id":"http://arxiv.org/abs/2412.02699v2","updated":"2025-03-04T15:26:33Z","published":"2024-12-03T18:59:54Z","title":"UniGraspTransformer: Simplified Policy Distillation for Scalable\n  Dexterous Robotic Grasping","summary":"  We introduce UniGraspTransformer, a universal Transformer-based network for\ndexterous robotic grasping that simplifies training while enhancing scalability\nand performance. Unlike prior methods such as UniDexGrasp++, which require\ncomplex, multi-step training pipelines, UniGraspTransformer follows a\nstreamlined process: first, dedicated policy networks are trained for\nindividual objects using reinforcement learning to generate successful grasp\ntrajectories; then, these trajectories are distilled into a single, universal\nnetwork. Our approach enables UniGraspTransformer to scale effectively,\nincorporating up to 12 self-attention blocks for handling thousands of objects\nwith diverse poses. Additionally, it generalizes well to both idealized and\nreal-world inputs, evaluated in state-based and vision-based settings. Notably,\nUniGraspTransformer generates a broader range of grasping poses for objects in\nvarious shapes and orientations, resulting in more diverse grasp strategies.\nExperimental results demonstrate significant improvements over\nstate-of-the-art, UniDexGrasp++, across various object categories, achieving\nsuccess rate gains of 3.5%, 7.7%, and 10.1% on seen objects, unseen objects\nwithin seen categories, and completely unseen objects, respectively, in the\nvision-based setting. Project page:\nhttps://dexhand.github.io/UniGraspTransformer.\n","authors":["Wenbo Wang","Fangyun Wei","Lei Zhou","Xi Chen","Lin Luo","Xiaohan Yi","Yizhong Zhang","Yaobo Liang","Chang Xu","Yan Lu","Jiaolong Yang","Baining Guo"],"pdf_url":"https://arxiv.org/pdf/2412.02699v2.pdf","comment":"Accepted to CVPR 2025. Project page:\n  https://dexhand.github.io/UniGraspTransformer"},{"id":"http://arxiv.org/abs/2503.02700v1","updated":"2025-03-04T15:17:40Z","published":"2025-03-04T15:17:40Z","title":"Multi-Strategy Enhanced COA for Path Planning in Autonomous Navigation","summary":"  Autonomous navigation is reshaping various domains in people's life by\nenabling efficient and safe movement in complex environments. Reliable\nnavigation requires algorithmic approaches that compute optimal or near-optimal\ntrajectories while satisfying task-specific constraints and ensuring obstacle\navoidance. However, existing methods struggle with slow convergence and\nsuboptimal solutions, particularly in complex environments, limiting their\nreal-world applicability. To address these limitations, this paper presents the\nMulti-Strategy Enhanced Crayfish Optimization Algorithm (MCOA), a novel\napproach integrating three key strategies: 1) Refractive Opposition Learning,\nenhancing population diversity and global exploration, 2) Stochastic\nCentroid-Guided Exploration, balancing global and local search to prevent\npremature convergence, and 3) Adaptive Competition-Based Selection, dynamically\nadjusting selection pressure for faster convergence and improved solution\nquality. Empirical evaluations underscore the remarkable planning speed and the\namazing solution quality of MCOA in both 3D Unmanned Aerial Vehicle (UAV) and\n2D mobile robot path planning. Against 11 baseline algorithms, MCOA achieved a\n69.2% reduction in computational time and a 16.7% improvement in minimizing\noverall path cost in 3D UAV scenarios. Furthermore, in 2D path planning, MCOA\noutperformed baseline approaches by 44% on average, with an impressive 75.6%\nadvantage in the largest 60*60 grid setting. These findings validate MCOA as a\npowerful tool for optimizing autonomous navigation in complex environments. The\nsource code is available at: https://github.com/coedv-hub/MCOA.\n","authors":["Yifei Wang","Jacky Keung","Haohan Xu","Yuchen Cao","Zhenyu Mao"],"pdf_url":"https://arxiv.org/pdf/2503.02700v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02698v1","updated":"2025-03-04T15:14:41Z","published":"2025-03-04T15:14:41Z","title":"FlowPlan: Zero-Shot Task Planning with LLM Flow Engineering for Robotic\n  Instruction Following","summary":"  Robotic instruction following tasks require seamless integration of visual\nperception, task planning, target localization, and motion execution. However,\nexisting task planning methods for instruction following are either data-driven\nor underperform in zero-shot scenarios due to difficulties in grounding lengthy\ninstructions into actionable plans under operational constraints. To address\nthis, we propose FlowPlan, a structured multi-stage LLM workflow that elevates\nzero-shot pipeline and bridges the performance gap between zero-shot and\ndata-driven in-context learning methods. By decomposing the planning process\ninto modular stages--task information retrieval, language-level reasoning,\nsymbolic-level planning, and logical evaluation--FlowPlan generates logically\ncoherent action sequences while adhering to operational constraints and further\nextracts contextual guidance for precise instance-level target localization.\nBenchmarked on the ALFRED and validated in real-world applications, our method\nachieves competitive performance relative to data-driven in-context learning\nmethods and demonstrates adaptability across diverse environments. This work\nadvances zero-shot task planning in robotic systems without reliance on labeled\ndata. Project website: https://instruction-following-project.github.io/.\n","authors":["Zijun Lin","Chao Tang","Hanjing Ye","Hong Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.02698v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.15390v2","updated":"2025-03-04T15:11:06Z","published":"2025-02-21T11:27:49Z","title":"Self-Mixing Laser Interferometry for Robotic Tactile Sensing","summary":"  Self-mixing interferometry (SMI) has been lauded for its sensitivity in\ndetecting microvibrations, while requiring no physical contact with its target.\nIn robotics, microvibrations have traditionally been interpreted as a marker\nfor object slip, and recently as a salient indicator of extrinsic contact. We\npresent the first-ever robotic fingertip making use of SMI for slip and\nextrinsic contact sensing. The design is validated through measurement of\ncontrolled vibration sources, both before and after encasing the readout\ncircuit in its fingertip package. Then, the SMI fingertip is compared to\nacoustic sensing through four experiments. The results are distilled into a\ntechnology decision map. SMI was found to be more sensitive to subtle slip\nevents and significantly more resilient against ambient noise. We conclude that\nthe integration of SMI in robotic fingertips offers a new, promising branch of\ntactile sensing in robotics. Design and data files are available at\nhttps://github.com/RemkoPr/icra2025-SMI-tactile-sensing.\n","authors":["Remko Proesmans","Ward Goossens","Lowiek Van den Stockt","Lowie Christiaen","Francis wyffels"],"pdf_url":"https://arxiv.org/pdf/2502.15390v2.pdf","comment":"Final version for IEEE ICRA2025 conference"},{"id":"http://arxiv.org/abs/2503.02687v1","updated":"2025-03-04T15:02:07Z","published":"2025-03-04T15:02:07Z","title":"Class-Aware PillarMix: Can Mixed Sample Data Augmentation Enhance 3D\n  Object Detection with Radar Point Clouds?","summary":"  Due to the significant effort required for data collection and annotation in\n3D perception tasks, mixed sample data augmentation (MSDA) has been widely\nstudied to generate diverse training samples by mixing existing data. Recently,\nmany MSDA techniques have been developed for point clouds, but they mainly\ntarget LiDAR data, leaving their application to radar point clouds largely\nunexplored. In this paper, we examine the feasibility of applying existing MSDA\nmethods to radar point clouds and identify several challenges in adapting these\ntechniques. These obstacles stem from the radar's irregular angular\ndistribution, deviations from a single-sensor polar layout in multi-radar\nsetups, and point sparsity. To address these issues, we propose Class-Aware\nPillarMix (CAPMix), a novel MSDA approach that applies MixUp at the pillar\nlevel in 3D point clouds, guided by class labels. Unlike methods that rely a\nsingle mix ratio to the entire sample, CAPMix assigns an independent ratio to\neach pillar, boosting sample diversity. To account for the density of different\nclasses, we use class-specific distributions: for dense objects (e.g., large\nvehicles), we skew ratios to favor points from another sample, while for sparse\nobjects (e.g., pedestrians), we sample more points from the original. This\nclass-aware mixing retains critical details and enriches each sample with new\ninformation, ultimately generating more diverse training data. Experimental\nresults demonstrate that our method not only significantly boosts performance\nbut also outperforms existing MSDA approaches across two datasets (Bosch Street\nand K-Radar). We believe that this straightforward yet effective approach will\nspark further investigation into MSDA techniques for radar data.\n","authors":["Miao Zhang","Sherif Abdulatif","Benedikt Loesch","Marco Altmann","Bin Yang"],"pdf_url":"https://arxiv.org/pdf/2503.02687v1.pdf","comment":"8 pages, 6 figures, 4 tables, submitted to 2025 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2025)"},{"id":"http://arxiv.org/abs/2409.00866v2","updated":"2025-03-04T14:44:45Z","published":"2024-09-01T23:23:01Z","title":"A Roadside Unit for Infrastructure Assisted Intersection Control of\n  Autonomous Vehicles","summary":"  Recent advances in autonomous vehicle technologies and cellular network\nspeeds motivate developments in vehicle-to-everything (V2X) communications.\nEnhanced road safety features and improved fuel efficiency are some of the\nmotivations behind V2X for future transportation systems. Adaptive intersection\ncontrol systems have considerable potential to achieve these goals by\nminimizing idle times and predicting short-term future traffic conditions.\nIntegrating V2X into traffic management systems introduces the infrastructure\nnecessary to make roads safer for all users and initiates the shift towards\nmore intelligent and connected cities. To demonstrate our control algorithm, we\nimplement both a simulated and real-world representation of a 4-way\nintersection and crosswalk scenario with 2 self-driving electric vehicles, a\nroadside unit (RSU), and a traffic light. Our architecture reduces acceleration\nand braking through intersections by up to 75.35%, which has been shown to\nminimize fuel consumption in gas vehicles. We propose a cost-effective solution\nto intelligent and connected intersection control to serve as a\nproof-of-concept model suitable as the basis for continued research and\ndevelopment. Code for this project is available at\nhttps://github.com/MMachado05/REU-2024.\n","authors":["Michael Evans","Marcial Machado","Rickey Johnson","Anna Vadella","Luis Escamilla","Beñat Froemming-Aldanondo","Tatiana Rastoskueva","Milan Jostes","Devson Butani","Ryan Kaddis","Chan-Jin Chung","Joshua Siegel"],"pdf_url":"https://arxiv.org/pdf/2409.00866v2.pdf","comment":"Supported by the National Science Foundation under Grants No. 2150292\n  and 2150096"},{"id":"http://arxiv.org/abs/2409.10824v2","updated":"2025-03-04T14:38:14Z","published":"2024-09-17T01:31:31Z","title":"Robustness of LiDAR-Based Pose Estimation: Evaluating and Improving\n  Odometry and Localization Under Common Point Cloud Corruptions","summary":"  Accurate and reliable pose estimation, i.e., determining the precise position\nand orientation of autonomous robots and vehicles, is critical for tasks like\nnavigation and mapping. LiDAR is a widely used sensor for pose estimation, with\nodometry and localization being two primary tasks. LiDAR odometry estimates the\nrelative motion between consecutive scans, while LiDAR localization aligns\nreal-time scans with a pre-recorded map to obtain a global pose. Although they\nhave different objectives and application scenarios, both rely on point cloud\nregistration as the underlying technique and face shared challenges of data\ncorruption caused by adverse conditions (e.g., rain). While state-of-the-art\n(SOTA) pose estimation systems achieved high accuracy on clean data, their\nrobustness to corrupted data remains unclear. In this work, we propose a\nframework to systematically evaluate five SOTA LiDAR pose estimation systems\nacross 18 synthetic real-world point cloud corruptions. Our experiments reveal\nthat odometry systems degrade significantly under specific corruptions, with\nrelative position errors increasing from 0.5% to more than 80%, while\nlocalization systems remain highly robust. We further demonstrate that\ndenoising techniques can effectively mitigate the adverse effects of\nnoise-induced corruptions, and re-training learning-based systems with\ncorrupted data significantly enhances the robustness against various corruption\ntypes.\n","authors":["Bo Yang","Tri Minh Triet Pham","Jinqiu Yang"],"pdf_url":"https://arxiv.org/pdf/2409.10824v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20125v2","updated":"2025-03-04T14:15:40Z","published":"2025-02-27T14:16:22Z","title":"Discovering Antagonists in Networks of Systems: Robot Deployment","summary":"  A contextual anomaly detection method is proposed and applied to the physical\nmotions of a robot swarm executing a coverage task. Using simulations of a\nswarm's normal behavior, a normalizing flow is trained to predict the\nlikelihood of a robot motion within the current context of its environment.\nDuring application, the predicted likelihood of the observed motions is used by\na detection criterion that categorizes a robot agent as normal or antagonistic.\nThe proposed method is evaluated on five different strategies of antagonistic\nbehavior. Importantly, only readily available simulated data of normal robot\nbehavior is used for training such that the nature of the anomalies need not be\nknown beforehand. The best detection criterion correctly categorizes at least\n80% of each antagonistic type while maintaining a false positive rate of less\nthan 5% for normal robot agents. Additionally, the method is validated in\nhardware experiments, yielding results similar to the simulated scenarios.\nCompared to the state-of-the-art approach, both the predictive performance of\nthe normalizing flow and the robustness of the detection criterion are\nincreased.\n","authors":["Ingeborg Wenger","Peter Eberhard","Henrik Ebel"],"pdf_url":"https://arxiv.org/pdf/2502.20125v2.pdf","comment":"reduced file size"},{"id":"http://arxiv.org/abs/2503.02649v1","updated":"2025-03-04T14:13:14Z","published":"2025-03-04T14:13:14Z","title":"Learning-Based Passive Fault-Tolerant Control of a Quadrotor with Rotor\n  Failure","summary":"  This paper proposes a learning-based passive fault-tolerant control (PFTC)\nmethod for quadrotor capable of handling arbitrary single-rotor failures,\nincluding conditions ranging from fault-free to complete rotor failure, without\nrequiring any rotor fault information or controller switching. Unlike existing\nmethods that treat rotor faults as disturbances and rely on a single controller\nfor multiple fault scenarios, our approach introduces a novel\nSelector-Controller network structure. This architecture integrates fault\ndetection module and the controller into a unified policy network, effectively\ncombining the adaptability to multiple fault scenarios of PFTC with the\nsuperior control performance of active fault-tolerant control (AFTC). To\noptimize performance, the policy network is trained using a hybrid framework\nthat synergizes reinforcement learning (RL), behavior cloning (BC), and\nsupervised learning with fault information. Extensive simulations and\nreal-world experiments validate the proposed method, demonstrating significant\nimprovements in fault response speed and position tracking performance compared\nto state-of-the-art PFTC and AFTC approaches.\n","authors":["Jiehao Chen","Kaidong Zhao","Zihan Liu","YanJie Li","Yunjiang Lou"],"pdf_url":"https://arxiv.org/pdf/2503.02649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02634v1","updated":"2025-03-04T13:59:51Z","published":"2025-03-04T13:59:51Z","title":"Velocity-free task-space regulator for robot manipulators with external\n  disturbances","summary":"  This paper addresses the problem of task-space robust regulation of robot\nmanipulators subject to external disturbances. A velocity-free control law is\nproposed by combining the internal model principle and the passivity-based\noutput-feedback control approach. The developed output-feedback controller\nensures not only asymptotic convergence of the regulation error but also\nsuppression of unwanted external step/sinusoidal disturbances. The potential of\nthe proposed method lies in its simplicity, intuitively appealing, and simple\ngain selection criteria for synthesis of multi-joint robot manipulator control\nsystems.\n","authors":["Haiwen Wu","Bayu Jayawardhana","Dabo Xu"],"pdf_url":"https://arxiv.org/pdf/2503.02634v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02624v1","updated":"2025-03-04T13:49:12Z","published":"2025-03-04T13:49:12Z","title":"Human-aligned Safe Reinforcement Learning for Highway On-Ramp Merging in\n  Dense Traffic","summary":"  Most reinforcement learning (RL) approaches for the decision-making of\nautonomous driving consider safety as a reward instead of a cost, which makes\nit hard to balance the tradeoff between safety and other objectives. Human risk\npreference has also rarely been incorporated, and the trained policy might be\neither conservative or aggressive for users. To this end, this study proposes a\nhuman-aligned safe RL approach for autonomous merging, in which the high-level\ndecision problem is formulated as a constrained Markov decision process (CMDP)\nthat incorporates users' risk preference into the safety constraints, followed\nby a model predictive control (MPC)-based low-level control. The safety level\nof RL policy can be adjusted by computing cost limits of CMDP's constraints\nbased on risk preferences and traffic density using a fuzzy control method. To\nfilter out unsafe or invalid actions, we design an action shielding mechanism\nthat pre-executes RL actions using an MPC method and performs collision checks\nwith surrounding agents. We also provide theoretical proof to validate the\neffectiveness of the shielding mechanism in enhancing RL's safety and sample\nefficiency. Simulation experiments in multiple levels of traffic densities show\nthat our method can significantly reduce safety violations without sacrificing\ntraffic efficiency. Furthermore, due to the use of risk preference-aware\nconstraints in CMDP and action shielding, we can not only adjust the safety\nlevel of the final policy but also reduce safety violations during the training\nstage, proving a promising solution for online learning in real-world\nenvironments.\n","authors":["Yang Li","Shijie Yuan","Yuan Chang","Xiaolong Chen","Qisong Yang","Zhiyuan Yang","Hongmao Qin"],"pdf_url":"https://arxiv.org/pdf/2503.02624v1.pdf","comment":"20 pages, 16 figures"},{"id":"http://arxiv.org/abs/2503.02600v1","updated":"2025-03-04T13:20:42Z","published":"2025-03-04T13:20:42Z","title":"Resource-Efficient Affordance Grounding with Complementary Depth and\n  Semantic Prompts","summary":"  Affordance refers to the functional properties that an agent perceives and\nutilizes from its environment, and is key perceptual information required for\nrobots to perform actions. This information is rich and multimodal in nature.\nExisting multimodal affordance methods face limitations in extracting useful\ninformation, mainly due to simple structural designs, basic fusion methods, and\nlarge model parameters, making it difficult to meet the performance\nrequirements for practical deployment. To address these issues, this paper\nproposes the BiT-Align image-depth-text affordance mapping framework. The\nframework includes a Bypass Prompt Module (BPM) and a Text Feature Guidance\n(TFG) attention selection mechanism. BPM integrates the auxiliary modality\ndepth image directly as a prompt to the primary modality RGB image, embedding\nit into the primary modality encoder without introducing additional encoders.\nThis reduces the model's parameter count and effectively improves functional\nregion localization accuracy. The TFG mechanism guides the selection and\nenhancement of attention heads in the image encoder using textual features,\nimproving the understanding of affordance characteristics. Experimental results\ndemonstrate that the proposed method achieves significant performance\nimprovements on public AGD20K and HICO-IIF datasets. On the AGD20K dataset,\ncompared with the current state-of-the-art method, we achieve a 6.0%\nimprovement in the KLD metric, while reducing model parameters by 88.8%,\ndemonstrating practical application values. The source code will be made\npublicly available at https://github.com/DAWDSE/BiT-Align.\n","authors":["Yizhou Huang","Fan Yang","Guoliang Zhu","Gen Li","Hao Shi","Yukun Zuo","Wenrui Chen","Zhiyong Li","Kailun Yang"],"pdf_url":"https://arxiv.org/pdf/2503.02600v1.pdf","comment":"The source code will be made publicly available at\n  https://github.com/DAWDSE/BiT-Align"},{"id":"http://arxiv.org/abs/2503.02587v1","updated":"2025-03-04T13:10:48Z","published":"2025-03-04T13:10:48Z","title":"Learning Dexterous In-Hand Manipulation with Multifingered Hands via\n  Visuomotor Diffusion","summary":"  We present a framework for learning dexterous in-hand manipulation with\nmultifingered hands using visuomotor diffusion policies. Our system enables\ncomplex in-hand manipulation tasks, such as unscrewing a bottle lid with one\nhand, by leveraging a fast and responsive teleoperation setup for the\nfour-fingered Allegro Hand. We collect high-quality expert demonstrations using\nan augmented reality (AR) interface that tracks hand movements and applies\ninverse kinematics and motion retargeting for precise control. The AR headset\nprovides real-time visualization, while gesture controls streamline\nteleoperation. To enhance policy learning, we introduce a novel demonstration\noutlier removal approach based on HDBSCAN clustering and the Global-Local\nOutlier Score from Hierarchies (GLOSH) algorithm, effectively filtering out\nlow-quality demonstrations that could degrade performance. We evaluate our\napproach extensively in real-world settings and provide all experimental videos\non the project website: https://dex-manip.github.io/\n","authors":["Piotr Koczy","Michael C. Welle","Danica Kragic"],"pdf_url":"https://arxiv.org/pdf/2503.02587v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02584v1","updated":"2025-03-04T13:07:54Z","published":"2025-03-04T13:07:54Z","title":"Research on visual simultaneous localization and mapping technology\n  based on near infrared light","summary":"  In view of the problems that visual simultaneous localization and mapping\n(VSLAM) are susceptible to environmental light interference and luminosity\ninconsistency, the visual simultaneous localization and mapping technology\nbased on near infrared perception (NIR-VSLAM) is proposed. In order to avoid\nambient light interference, the near infrared light is innovatively selected as\nthe light source. The luminosity parameter estimation of error energy function,\nhalo factor and exposure time and the light source irradiance correction method\nare proposed in this paper, which greatly improves the positioning accuracy of\nDirect Sparse Odometry (DSO). The feasibility of the proposed method in four\nlarge scenes is verified, which provides the reference for visual positioning\nin automatic driving and mobile robot.\n","authors":["Rui Ma","Mengfang Liu","Boliang Li","Xinghui Li"],"pdf_url":"https://arxiv.org/pdf/2503.02584v1.pdf","comment":"12 pages, 9 figures, 2 tables"},{"id":"http://arxiv.org/abs/2503.02581v1","updated":"2025-03-04T13:04:46Z","published":"2025-03-04T13:04:46Z","title":"Unveiling the Potential of Segment Anything Model 2 for RGB-Thermal\n  Semantic Segmentation with Language Guidance","summary":"  The perception capability of robotic systems relies on the richness of the\ndataset. Although Segment Anything Model 2 (SAM2), trained on large datasets,\ndemonstrates strong perception potential in perception tasks, its inherent\ntraining paradigm prevents it from being suitable for RGB-T tasks. To address\nthese challenges, we propose SHIFNet, a novel SAM2-driven Hybrid Interaction\nParadigm that unlocks the potential of SAM2 with linguistic guidance for\nefficient RGB-Thermal perception. Our framework consists of two key components:\n(1) Semantic-Aware Cross-modal Fusion (SACF) module that dynamically balances\nmodality contributions through text-guided affinity learning, overcoming SAM2's\ninherent RGB bias; (2) Heterogeneous Prompting Decoder (HPD) that enhances\nglobal semantic information through a semantic enhancement module and then\ncombined with category embeddings to amplify cross-modal semantic consistency.\nWith 32.27M trainable parameters, SHIFNet achieves state-of-the-art\nsegmentation performance on public benchmarks, reaching 89.8% on PST900 and\n67.8% on FMB, respectively. The framework facilitates the adaptation of\npre-trained large models to RGB-T segmentation tasks, effectively mitigating\nthe high costs associated with data collection while endowing robotic systems\nwith comprehensive perception capabilities. The source code will be made\npublicly available at https://github.com/iAsakiT3T/SHIFNet.\n","authors":["Jiayi Zhao","Fei Teng","Kai Luo","Guoqiang Zhao","Zhiyong Li","Xu Zheng","Kailun Yang"],"pdf_url":"https://arxiv.org/pdf/2503.02581v1.pdf","comment":"The source code will be made publicly available at\n  https://github.com/iAsakiT3T/SHIFNet"},{"id":"http://arxiv.org/abs/2503.02578v1","updated":"2025-03-04T13:00:30Z","published":"2025-03-04T13:00:30Z","title":"TS-CGNet: Temporal-Spatial Fusion Meets Centerline-Guided Diffusion for\n  BEV Mapping","summary":"  Bird's Eye View (BEV) perception technology is crucial for autonomous\ndriving, as it generates top-down 2D maps for environment perception,\nnavigation, and decision-making. Nevertheless, the majority of current BEV map\ngeneration studies focusing on visual map generation lack depth-aware reasoning\ncapabilities. They exhibit limited efficacy in managing occlusions and handling\ncomplex environments, with a notable decline in perceptual performance under\nadverse weather conditions or low-light scenarios. Therefore, this paper\nproposes TS-CGNet, which leverages Temporal-Spatial fusion with\nCenterline-Guided diffusion. This visual framework, grounded in prior\nknowledge, is designed for integration into any existing network for building\nBEV maps. Specifically, this framework is decoupled into three parts: Local\nmapping system involves the initial generation of semantic maps using purely\nvisual information; The Temporal-Spatial Aligner Module (TSAM) integrates\nhistorical information into mapping generation by applying transformation\nmatrices; The Centerline-Guided Diffusion Model (CGDM) is a prediction module\nbased on the diffusion model. CGDM incorporates centerline information through\nspatial-attention mechanisms to enhance semantic segmentation reconstruction.\nWe construct BEV semantic segmentation maps by our methods on the public\nnuScenes and the robustness benchmarks under various corruptions. Our method\nimproves 1.90%, 1.73%, and 2.87% for perceived ranges of 60x30m, 120x60m, and\n240x60m in the task of BEV HD mapping. TS-CGNet attains an improvement of 1.92%\nfor perceived ranges of 100x100m in the task of BEV semantic mapping. Moreover,\nTS-CGNet achieves an average improvement of 2.92% in detection accuracy under\nvarying weather conditions and sensor interferences in the perception range of\n240x60m. The source code will be publicly available at\nhttps://github.com/krabs-H/TS-CGNet.\n","authors":["Xinying Hong","Siyu Li","Kang Zeng","Hao Shi","Bomin Peng","Kailun Yang","Zhiyong Li"],"pdf_url":"https://arxiv.org/pdf/2503.02578v1.pdf","comment":"The source code will be publicly available at\n  https://github.com/krabs-H/TS-CGNet"},{"id":"http://arxiv.org/abs/2503.02572v1","updated":"2025-03-04T12:54:05Z","published":"2025-03-04T12:54:05Z","title":"RaceVLA: VLA-based Racing Drone Navigation with Human-like Behaviour","summary":"  RaceVLA presents an innovative approach for autonomous racing drone\nnavigation by leveraging Visual-Language-Action (VLA) to emulate human-like\nbehavior. This research explores the integration of advanced algorithms that\nenable drones to adapt their navigation strategies based on real-time\nenvironmental feedback, mimicking the decision-making processes of human\npilots. The model, fine-tuned on a collected racing drone dataset, demonstrates\nstrong generalization despite the complexity of drone racing environments.\nRaceVLA outperforms OpenVLA in motion (75.0 vs 60.0) and semantic\ngeneralization (45.5 vs 36.3), benefiting from the dynamic camera and\nsimplified motion tasks. However, visual (79.6 vs 87.0) and physical (50.0 vs\n76.7) generalization were slightly reduced due to the challenges of maneuvering\nin dynamic environments with varying object sizes. RaceVLA also outperforms\nRT-2 across all axes - visual (79.6 vs 52.0), motion (75.0 vs 55.0), physical\n(50.0 vs 26.7), and semantic (45.5 vs 38.8), demonstrating its robustness for\nreal-time adjustments in complex environments. Experiments revealed an average\nvelocity of 1.04 m/s, with a maximum speed of 2.02 m/s, and consistent\nmaneuverability, demonstrating RaceVLA's ability to handle high-speed scenarios\neffectively. These findings highlight the potential of RaceVLA for\nhigh-performance navigation in competitive racing contexts. The RaceVLA\ncodebase, pretrained weights, and dataset are available at this http URL:\nhttps://racevla.github.io/\n","authors":["Valerii Serpiva","Artem Lykov","Artyom Myshlyaev","Muhammad Haris Khan","Ali Alridha Abdulkarim","Oleg Sautenkov","Dzmitry Tsetserukou"],"pdf_url":"https://arxiv.org/pdf/2503.02572v1.pdf","comment":"6 pages, 6 figures. Submitted to IROS 2025"},{"id":"http://arxiv.org/abs/2503.02552v1","updated":"2025-03-04T12:25:01Z","published":"2025-03-04T12:25:01Z","title":"World Models for Anomaly Detection during Model-Based Reinforcement\n  Learning Inference","summary":"  Learning-based controllers are often purposefully kept out of real-world\napplications due to concerns about their safety and reliability. We explore how\nstate-of-the-art world models in Model-Based Reinforcement Learning can be\nutilized beyond the training phase to ensure a deployed policy only operates\nwithin regions of the state-space it is sufficiently familiar with. This is\nachieved by continuously monitoring discrepancies between a world model's\npredictions and observed system behavior during inference. It allows for\ntriggering appropriate measures, such as an emergency stop, once an error\nthreshold is surpassed. This does not require any task-specific knowledge and\nis thus universally applicable. Simulated experiments on established robot\ncontrol tasks show the effectiveness of this method, recognizing changes in\nlocal robot geometry and global gravitational magnitude. Real-world experiments\nusing an agile quadcopter further demonstrate the benefits of this approach by\ndetecting unexpected forces acting on the vehicle. These results indicate how\neven in new and adverse conditions, safe and reliable operation of otherwise\nunpredictable learning-based controllers can be achieved.\n","authors":["Fabian Domberg","Georg Schildbach"],"pdf_url":"https://arxiv.org/pdf/2503.02552v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.10117v2","updated":"2025-03-04T12:17:16Z","published":"2024-03-15T09:06:50Z","title":"Do Visual-Language Grid Maps Capture Latent Semantics?","summary":"  Visual-language models (VLMs) have recently been introduced in robotic\nmapping using the latent representations, i.e., embeddings, of the VLMs to\nrepresent semantics in the map. They allow moving from a limited set of\nhuman-created labels toward open-vocabulary scene understanding, which is very\nuseful for robots when operating in complex real-world environments and\ninteracting with humans. While there is anecdotal evidence that maps built this\nway support downstream tasks, such as navigation, rigorous analysis of the\nquality of the maps using these embeddings is missing. In this paper, we\npropose a way to analyze the quality of maps created using VLMs. We investigate\ntwo critical properties of map quality: queryability and distinctness. The\nevaluation of queryability addresses the ability to retrieve information from\nthe embeddings. We investigate intra-map distinctness to study the ability of\nthe embeddings to represent abstract semantic classes and inter-map\ndistinctness to evaluate the generalization properties of the representation.\nWe propose metrics to evaluate these properties and evaluate two\nstate-of-the-art mapping methods, VLMaps and OpenScene, using two encoders,\nLSeg and OpenSeg, using real-world data from the Matterport3D data set. Our\nfindings show that while 3D features improve queryability, they are not scale\ninvariant, whereas image-based embeddings generalize to multiple map\nresolutions. This allows the image-based methods to maintain smaller map sizes,\nwhich can be crucial for using these methods in real-world deployments.\nFurthermore, we show that the choice of the encoder has an effect on the\nresults. The results imply that properly thresholding open-vocabulary queries\nis an open problem.\n","authors":["Matti Pekkanen","Tsvetomila Mihaylova","Francesco Verdoja","Ville Kyrki"],"pdf_url":"https://arxiv.org/pdf/2403.10117v2.pdf","comment":"Submitted to IEEE-IROS-2025"},{"id":"http://arxiv.org/abs/2503.02525v1","updated":"2025-03-04T11:36:45Z","published":"2025-03-04T11:36:45Z","title":"Magic in Human-Robot Interaction (HRI)","summary":"  \"Magic\" is referred to here and there in the robotics literature, from\n\"magical moments\" afforded by a mobile bubble machine, to \"spells\" intended to\nentertain and motivate children--but what exactly could this concept mean for\ndesigners? Here, we present (1) some theoretical discussion on how magic could\ninform interaction designs based on reviewing the literature, followed by (2) a\npractical description of using such ideas to develop a simplified prototype,\nwhich received an award in an international robot magic competition. Although\nthis topic can be considered unusual and some negative connotations exist\n(e.g., unrealistic thinking can be referred to as magical), our results seem to\nsuggest that magic, in the experiential, supernatural, and illusory senses of\nthe term, could be useful to consider in various robot design contexts, also\nfor artifacts like home assistants and autonomous vehicles--thus, inviting\nfurther discussion and exploration.\n","authors":["Martin Cooney","Alexey Vinel"],"pdf_url":"https://arxiv.org/pdf/2503.02525v1.pdf","comment":"Accepted Version of a Paper Published in IEEE, 10 pages, in the 34th\n  annual workshop of the Swedish Artificial Intelligence Society (SAIS 2022),\n  2022"},{"id":"http://arxiv.org/abs/2503.02509v1","updated":"2025-03-04T11:19:03Z","published":"2025-03-04T11:19:03Z","title":"Impact of Temporal Delay on Radar-Inertial Odometry","summary":"  Accurate ego-motion estimation is a critical component of any autonomous\nsystem. Conventional ego-motion sensors, such as cameras and LiDARs, may be\ncompromised in adverse environmental conditions, such as fog, heavy rain, or\ndust. Automotive radars, known for their robustness to such conditions, present\nthemselves as complementary sensors or a promising alternative within the\nego-motion estimation frameworks. In this paper we propose a novel\nRadar-Inertial Odometry (RIO) system that integrates an automotive radar and an\ninertial measurement unit. The key contribution is the integration of online\ntemporal delay calibration within the factor graph optimization framework that\ncompensates for potential time offsets between radar and IMU measurements. To\nvalidate the proposed approach we have conducted thorough experimental analysis\non real-world radar and IMU data. The results show that, even without scan\nmatching or target tracking, integration of online temporal calibration\nsignificantly reduces localization error compared to systems that disregard\ntime synchronization, thus highlighting the important role of, often neglected,\naccurate temporal alignment in radar-based sensor fusion systems for autonomous\nnavigation.\n","authors":["Vlaho-Josip Štironja","Luka Petrović","Juraj Peršić","Ivan Marković","Ivan Petrović"],"pdf_url":"https://arxiv.org/pdf/2503.02509v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02505v1","updated":"2025-03-04T11:16:46Z","published":"2025-03-04T11:16:46Z","title":"ROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment","summary":"  We aim to develop a goal specification method that is semantically clear,\nspatially sensitive, and intuitive for human users to guide agent interactions\nin embodied environments. Specifically, we propose a novel cross-view goal\nalignment framework that allows users to specify target objects using\nsegmentation masks from their own camera views rather than the agent's\nobservations. We highlight that behavior cloning alone fails to align the\nagent's behavior with human intent when the human and agent camera views differ\nsignificantly. To address this, we introduce two auxiliary objectives:\ncross-view consistency loss and target visibility loss, which explicitly\nenhance the agent's spatial reasoning ability. According to this, we develop\nROCKET-2, a state-of-the-art agent trained in Minecraft, achieving an\nimprovement in the efficiency of inference 3x to 6x. We show ROCKET-2 can\ndirectly interpret goals from human camera views for the first time, paving the\nway for better human-agent interaction.\n","authors":["Shaofei Cai","Zhancun Mu","Anji Liu","Yitao Liang"],"pdf_url":"https://arxiv.org/pdf/2503.02505v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20054v2","updated":"2025-03-04T10:53:25Z","published":"2025-02-27T12:53:58Z","title":"Night-Voyager: Consistent and Efficient Nocturnal Vision-Aided State\n  Estimation in Object Maps","summary":"  Accurate and robust state estimation at nighttime is essential for autonomous\nrobotic navigation to achieve nocturnal or round-the-clock tasks. An intuitive\nquestion arises: Can low-cost standard cameras be exploited for nocturnal state\nestimation? Regrettably, most existing visual methods may fail under adverse\nillumination conditions, even with active lighting or image enhancement. A\npivotal insight, however, is that streetlights in most urban scenarios act as\nstable and salient prior visual cues at night, reminiscent of stars in deep\nspace aiding spacecraft voyage in interstellar navigation. Inspired by this, we\npropose Night-Voyager, an object-level nocturnal vision-aided state estimation\nframework that leverages prior object maps and keypoints for versatile\nlocalization. We also find that the primary limitation of conventional visual\nmethods under poor lighting conditions stems from the reliance on pixel-level\nmetrics. In contrast, metric-agnostic, non-pixel-level object detection serves\nas a bridge between pixel-level and object-level spaces, enabling effective\npropagation and utilization of object map information within the system.\nNight-Voyager begins with a fast initialization to solve the global\nlocalization problem. By employing an effective two-stage cross-modal data\nassociation, the system delivers globally consistent state updates using\nmap-based observations. To address the challenge of significant uncertainties\nin visual observations at night, a novel matrix Lie group formulation and a\nfeature-decoupled multi-state invariant filter are introduced, ensuring\nconsistent and efficient estimation. Through comprehensive experiments in both\nsimulation and diverse real-world scenarios (spanning approximately 12.3 km),\nNight-Voyager showcases its efficacy, robustness, and efficiency, filling a\ncritical gap in nocturnal vision-aided state estimation.\n","authors":["Tianxiao Gao","Mingle Zhao","Chengzhong Xu","Hui Kong"],"pdf_url":"https://arxiv.org/pdf/2502.20054v2.pdf","comment":"IEEE Transactions on Robotics (T-RO), 2025"},{"id":"http://arxiv.org/abs/2409.06948v2","updated":"2025-03-04T10:38:01Z","published":"2024-09-11T02:00:54Z","title":"Equivariant Filter for Tightly Coupled LiDAR-Inertial Odometry","summary":"  Pose estimation is a crucial problem in simultaneous localization and mapping\n(SLAM). However, developing a robust and consistent state estimator remains a\nsignificant challenge, as the traditional extended Kalman filter (EKF)\nstruggles to handle the model nonlinearity, especially for inertial measurement\nunit (IMU) and light detection and ranging (LiDAR). To provide a consistent and\nefficient solution of pose estimation, we propose Eq-LIO, a robust state\nestimator for tightly coupled LIO systems based on an equivariant filter (EqF).\nCompared with the invariant Kalman filter based on the $\\SE_2(3)$ group\nstructure, the EqF uses the symmetry of the semi-direct product group to couple\nthe system state including IMU bias, navigation state and LiDAR extrinsic\ncalibration state, thereby suppressing linearization error and improving the\nbehavior of the estimator in the event of unexpected state changes. The\nproposed Eq-LIO owns natural consistency and higher robustness, which is\ntheoretically proven with mathematical derivation and experimentally verified\nthrough a series of tests on both public and private datasets.\n","authors":["Anbo Tao","Yarong Luo","Chunxi Xia","Chi Guo","Xingxing Li"],"pdf_url":"https://arxiv.org/pdf/2409.06948v2.pdf","comment":"Accepted by ICRA 2025"},{"id":"http://arxiv.org/abs/2503.02465v1","updated":"2025-03-04T10:21:58Z","published":"2025-03-04T10:21:58Z","title":"UAV-VLRR: Vision-Language Informed NMPC for Rapid Response in UAV Search\n  and Rescue","summary":"  Emergency search and rescue (SAR) operations often require rapid and precise\ntarget identification in complex environments where traditional manual drone\ncontrol is inefficient. In order to address these scenarios, a rapid SAR\nsystem, UAV-VLRR (Vision-Language-Rapid-Response), is developed in this\nresearch. This system consists of two aspects: 1) A multimodal system which\nharnesses the power of Visual Language Model (VLM) and the natural language\nprocessing capabilities of ChatGPT-4o (LLM) for scene interpretation. 2) A\nnon-linearmodel predictive control (NMPC) with built-in obstacle avoidance for\nrapid response by a drone to fly according to the output of the multimodal\nsystem. This work aims at improving response times in emergency SAR operations\nby providing a more intuitive and natural approach to the operator to plan the\nSAR mission while allowing the drone to carry out that mission in a rapid and\nsafe manner. When tested, our approach was faster on an average by 33.75% when\ncompared with an off-the-shelf autopilot and 54.6% when compared with a human\npilot. Video of UAV-VLRR: https://youtu.be/KJqQGKKt1xY\n","authors":["Yasheerah Yaqoot","Muhammad Ahsan Mustafa","Oleg Sautenkov","Dzmitry Tsetserukou"],"pdf_url":"https://arxiv.org/pdf/2503.02465v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09382v3","updated":"2025-03-04T10:19:22Z","published":"2024-02-14T18:26:58Z","title":"Safe Distributed Control of Multi-Robot Systems with Communication\n  Delays","summary":"  Safe operation of multi-robot systems is critical, especially in\ncommunication-degraded environments such as underwater for seabed mapping,\nunderground caves for navigation, and in extraterrestrial missions for assembly\nand construction. We address safety of networked autonomous systems where the\ninformation exchanged between robots incurs communication delays. We formalize\na notion of distributed control barrier function for multi-robot systems, a\nsafety certificate amenable to a distributed implementation, which provides\nformal ground to using graph neural networks to learn safe distributed\ncontrollers. Further, we observe that learning a distributed controller\nignoring delays can severely degrade safety. We finally propose a\npredictor-based framework to train a safe distributed controller under\ncommunication delays, where the current state of nearby robots is predicted\nfrom received data and age-of-information. Numerical experiments on multi-robot\ncollision avoidance show that our predictor-based approach can significantly\nimprove the safety of a learned distributed controller under communication\ndelays. A video abstract is available at https://youtu.be/Hcu1Ri32Spk.\n","authors":["Luca Ballotta","Rajat Talak"],"pdf_url":"https://arxiv.org/pdf/2402.09382v3.pdf","comment":"Copyright (c) 2025 IEEE. Personal use of this material is permitted.\n  However, permission to use this material for any other purposes must be\n  obtained from the IEEE by sending a request to pubs-permissions@ieee.org"},{"id":"http://arxiv.org/abs/2503.02454v1","updated":"2025-03-04T10:02:53Z","published":"2025-03-04T10:02:53Z","title":"UAV-VLPA*: A Vision-Language-Path-Action System for Optimal Route\n  Generation on a Large Scales","summary":"  The UAV-VLPA* (Visual-Language-Planning-and-Action) system represents a\ncutting-edge advancement in aerial robotics, designed to enhance communication\nand operational efficiency for unmanned aerial vehicles (UAVs). By integrating\nadvanced planning capabilities, the system addresses the Traveling Salesman\nProblem (TSP) to optimize flight paths, reducing the total trajectory length by\n18.5\\% compared to traditional methods. Additionally, the incorporation of the\nA* algorithm enables robust obstacle avoidance, ensuring safe and efficient\nnavigation in complex environments. The system leverages satellite imagery\nprocessing combined with the Visual Language Model (VLM) and GPT's natural\nlanguage processing capabilities, allowing users to generate detailed flight\nplans through simple text commands. This seamless fusion of visual and\nlinguistic analysis empowers precise decision-making and mission planning,\nmaking UAV-VLPA* a transformative tool for modern aerial operations. With its\nunmatched operational efficiency, navigational safety, and user-friendly\nfunctionality, UAV-VLPA* sets a new standard in autonomous aerial robotics,\npaving the way for future innovations in the field.\n","authors":["Oleg Sautenkov","Aibek Akhmetkazy","Yasheerah Yaqoot","Muhammad Ahsan Mustafa","Grik Tadevosyan","Artem Lykov","Dzmitry Tsetserukou"],"pdf_url":"https://arxiv.org/pdf/2503.02454v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2501.05014"},{"id":"http://arxiv.org/abs/2411.00476v2","updated":"2025-03-04T09:44:08Z","published":"2024-11-01T09:43:49Z","title":"PlanScope: Learning to Plan Within Decision Scope Does Matter","summary":"  In the context of autonomous driving, learning-based methods have been\npromising for the development of planning modules. During the training process\nof planning modules, directly minimizing the discrepancy between expert-driving\nlogs and planning output is widely deployed. In general, driving logs consist\nof suddenly appearing obstacles or swiftly changing traffic signals, which\ntypically necessitate swift and nuanced adjustments in driving maneuvers.\nConcurrently, future trajectories of the vehicles exhibit their long-term\ndecisions, such as adhering to a reference lane or circumventing stationary\nobstacles. Due to the unpredictable influence of future events in driving logs,\nreasoning bias could be naturally introduced to learning based planning\nmodules, which leads to a possible degradation of driving performance. To\naddress this issue, we identify the decisions and their corresponding time\nhorizons, and characterize a so-called decision scope by retaining decisions\nwithin derivable horizons only, to mitigate the effect of irrational behaviors\ncaused by unpredictable events. Several viable implementations have been\nproposed, among which batch normalization along the temporal dimension is\nparticularly effective and achieves superior performance. It consistently\noutperforms baseline methods in terms of driving scores, as demonstrated\nthrough closed-loop evaluations on the nuPlan dataset. Essentially, this\napproach accommodates an appealing plug-and-play feature to enhance the\nclosed-loop performance of other learning-based planning models.\n","authors":["Ren Xin","Jie Cheng","Hongji Liu","Jun Ma"],"pdf_url":"https://arxiv.org/pdf/2411.00476v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14616v2","updated":"2025-03-04T09:30:03Z","published":"2024-09-22T22:50:06Z","title":"Learning to Refine Input Constrained Control Barrier Functions via\n  Uncertainty-Aware Online Parameter Adaptation","summary":"  Control Barrier Functions (CBFs) have become powerful tools for ensuring\nsafety in nonlinear systems. However, finding valid CBFs that guarantee\npersistent safety and feasibility remains an open challenge, especially in\nsystems with input constraints. Traditional approaches often rely on manually\ntuning the parameters of the class K functions of the CBF conditions a priori.\nThe performance of CBF-based controllers is highly sensitive to these fixed\nparameters, potentially leading to overly conservative behavior or safety\nviolations. To overcome these issues, this paper introduces a learning-based\noptimal control framework for online adaptation of Input Constrained CBF\n(ICCBF) parameters in discrete-time nonlinear systems. Our method employs a\nprobabilistic ensemble neural network to predict the performance and risk\nmetrics, as defined in this work, for candidate parameters, accounting for both\nepistemic and aleatoric uncertainties. We propose a two-step verification\nprocess using Jensen-Renyi Divergence and distributionally-robust Conditional\nValue at Risk to identify valid parameters. This enables dynamic refinement of\nICCBF parameters based on current state and nearby environments, optimizing\nperformance while ensuring safety within the verified parameter set.\nExperimental results demonstrate that our method outperforms both\nfixed-parameter and existing adaptive methods in robot navigation scenarios\nacross safety and performance metrics.\n","authors":["Taekyung Kim","Robin Inho Kee","Dimitra Panagou"],"pdf_url":"https://arxiv.org/pdf/2409.14616v2.pdf","comment":"2025 IEEE International Conference on Robotics and Automation (ICRA).\n  Project page: https://www.taekyung.me/online-adaptive-cbf"},{"id":"http://arxiv.org/abs/2502.07005v4","updated":"2025-03-04T08:58:53Z","published":"2025-02-10T20:10:25Z","title":"Geometry-aware RL for Manipulation of Varying Shapes and Deformable\n  Objects","summary":"  Manipulating objects with varying geometries and deformable objects is a\nmajor challenge in robotics. Tasks such as insertion with different objects or\ncloth hanging require precise control and effective modelling of complex\ndynamics. In this work, we frame this problem through the lens of a\nheterogeneous graph that comprises smaller sub-graphs, such as actuators and\nobjects, accompanied by different edge types describing their interactions.\nThis graph representation serves as a unified structure for both rigid and\ndeformable objects tasks, and can be extended further to tasks comprising\nmultiple actuators. To evaluate this setup, we present a novel and challenging\nreinforcement learning benchmark, including rigid insertion of diverse objects,\nas well as rope and cloth manipulation with multiple end-effectors. These tasks\npresent a large search space, as both the initial and target configurations are\nuniformly sampled in 3D space. To address this issue, we propose a novel\ngraph-based policy model, dubbed Heterogeneous Equivariant Policy (HEPi),\nutilizing $SE(3)$ equivariant message passing networks as the main backbone to\nexploit the geometric symmetry. In addition, by modeling explicit\nheterogeneity, HEPi can outperform Transformer-based and non-heterogeneous\nequivariant policies in terms of average returns, sample efficiency, and\ngeneralization to unseen objects. Our project page is available at\nhttps://thobotics.github.io/hepi.\n","authors":["Tai Hoang","Huy Le","Philipp Becker","Vien Anh Ngo","Gerhard Neumann"],"pdf_url":"https://arxiv.org/pdf/2502.07005v4.pdf","comment":"Accepted at ICLR 2025 (Oral)"},{"id":"http://arxiv.org/abs/2503.02408v1","updated":"2025-03-04T08:50:19Z","published":"2025-03-04T08:50:19Z","title":"Predictive Kinematic Coordinate Control for Aerial Manipulators based on\n  Modified Kinematics Learning","summary":"  High-precision manipulation has always been a developmental goal for aerial\nmanipulators. This paper investigates the kinematic coordinate control issue in\naerial manipulators. We propose a predictive kinematic coordinate control\nmethod, which includes a learning-based modified kinematic model and a model\npredictive control (MPC) scheme based on weight allocation. Compared to\nexisting methods, our proposed approach offers several attractive features.\nFirst, the kinematic model incorporates closed-loop dynamics characteristics\nand online residual learning. Compared to methods that do not consider\nclosed-loop dynamics and residuals, our proposed method has improved accuracy\nby 59.6$\\%$. Second, a MPC scheme that considers weight allocation has been\nproposed, which can coordinate the motion strategies of quadcopters and\nmanipulators. Compared to methods that do not consider weight allocation, the\nproposed method can meet the requirements of more tasks. The proposed approach\nis verified through complex trajectory tracking and moving target tracking\nexperiments. The results validate the effectiveness of the proposed method.\n","authors":["Zhengzhen Li","Jiahao Shen","Mengyu Ji","Huazi Cao","Shiyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2503.02408v1.pdf","comment":"accepted by ICRA 2025"},{"id":"http://arxiv.org/abs/2503.02405v1","updated":"2025-03-04T08:47:18Z","published":"2025-03-04T08:47:18Z","title":"A comparison of visual representations for real-world reinforcement\n  learning in the context of vacuum gripping","summary":"  When manipulating objects in the real world, we need reactive feedback\npolicies that take into account sensor information to inform decisions. This\nstudy aims to determine how different encoders can be used in a reinforcement\nlearning (RL) framework to interpret the spatial environment in the local\nsurroundings of a robot arm. Our investigation focuses on comparing real-world\nvision with 3D scene inputs, exploring new architectures in the process. We\nbuilt on the SERL framework, providing us with a sample efficient and stable RL\nfoundation we could build upon, while keeping training times minimal. The\nresults of this study indicate that spatial information helps to significantly\noutperform the visual counterpart, tested on a box picking task with a vacuum\ngripper. The code and videos of the evaluations are available at\nhttps://github.com/nisutte/voxel-serl.\n","authors":["Nico Sutter","Valentin N. Hartmann","Stelian Coros"],"pdf_url":"https://arxiv.org/pdf/2503.02405v1.pdf","comment":"8 pager, 5 Figures, 5 Tables"},{"id":"http://arxiv.org/abs/2410.14383v3","updated":"2025-03-04T08:39:43Z","published":"2024-10-18T11:20:00Z","title":"MARLIN: Multi-Agent Reinforcement Learning Guided by Language-Based\n  Inter-Robot Negotiation","summary":"  Multi-agent reinforcement learning is a key method for training multi-robot\nsystems over a series of episodes in which robots are rewarded or punished\naccording to their performance; only once the system is trained to a suitable\nstandard is it deployed in the real world. If the system is not trained enough,\nthe task will likely not be completed and could pose a risk to the surrounding\nenvironment. We introduce Multi-Agent Reinforcement Learning guided by\nLanguage-based Inter-Robot Negotiation (MARLIN), in which the training process\nrequires fewer training episodes to reach peak performance. Robots are equipped\nwith large language models that negotiate and debate a task, producing plans\nused to guide the policy during training. The approach dynamically switches\nbetween using reinforcement learning and large language model-based action\nnegotiation throughout training. This reduces the number of training episodes\nrequired, compared to standard multi-agent reinforcement learning, and hence\nallows the system to be deployed to physical hardware earlier. The performance\nof this approach is evaluated against multi-agent reinforcement learning,\nshowing that our hybrid method achieves comparable results with significantly\nreduced training time.\n","authors":["Toby Godfrey","William Hunt","Mohammad D. Soorati"],"pdf_url":"https://arxiv.org/pdf/2410.14383v3.pdf","comment":"6 pages, 6 figures, 1 table"},{"id":"http://arxiv.org/abs/2502.18041v3","updated":"2025-03-04T08:38:58Z","published":"2025-02-25T09:57:18Z","title":"OpenFly: A Versatile Toolchain and Large-scale Benchmark for Aerial\n  Vision-Language Navigation","summary":"  Vision-Language Navigation (VLN) aims to guide agents through an environment\nby leveraging both language instructions and visual cues, playing a pivotal\nrole in embodied AI. Indoor VLN has been extensively studied, whereas outdoor\naerial VLN remains underexplored. The potential reason is that outdoor aerial\nview encompasses vast areas, making data collection more challenging, which\nresults in a lack of benchmarks. To address this problem, we propose OpenFly, a\nplatform comprising a versatile toolchain and large-scale benchmark for aerial\nVLN. Firstly, we develop a highly automated toolchain for data collection,\nenabling automatic point cloud acquisition, scene semantic segmentation, flight\ntrajectory creation, and instruction generation. Secondly, based on the\ntoolchain, we construct a large-scale aerial VLN dataset with 100k\ntrajectories, covering diverse heights and lengths across 18 scenes. The\ncorresponding visual data are generated using various rendering engines and\nadvanced techniques, including Unreal Engine, GTA V, Google Earth, and 3D\nGaussian Splatting (3D GS). All data exhibit high visual quality. Particularly,\n3D GS supports real-to-sim rendering, further enhancing the realism of the\ndataset. Thirdly, we propose OpenFly-Agent, a keyframe-aware VLN model, which\ntakes language instructions, current observations, and historical keyframes as\ninput, and outputs flight actions directly. Extensive analyses and experiments\nare conducted, showcasing the superiority of our OpenFly platform and\nOpenFly-Agent. The toolchain, dataset, and codes will be open-sourced.\n","authors":["Yunpeng Gao","Chenhui Li","Zhongrui You","Junli Liu","Zhen Li","Pengan Chen","Qizhi Chen","Zhonghan Tang","Liansheng Wang","Penghui Yang","Yiwen Tang","Yuhang Tang","Shuai Liang","Songyi Zhu","Ziqin Xiong","Yifei Su","Xinyi Ye","Jianan Li","Yan Ding","Dong Wang","Zhigang Wang","Bin Zhao","Xuelong Li"],"pdf_url":"https://arxiv.org/pdf/2502.18041v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.00200v2","updated":"2025-03-04T08:26:29Z","published":"2025-02-28T21:38:17Z","title":"Unified Video Action Model","summary":"  A unified video and action model holds significant promise for robotics,\nwhere videos provide rich scene information for action prediction, and actions\nprovide dynamics information for video prediction. However, effectively\ncombining video generation and action prediction remains challenging, and\ncurrent video generation-based methods struggle to match the performance of\ndirect policy learning in action accuracy and inference speed. To bridge this\ngap, we introduce the Unified Video Action model (UVA), which jointly optimizes\nvideo and action predictions to achieve both high accuracy and efficient action\ninference. The key lies in learning a joint video-action latent representation\nand decoupling video-action decoding. The joint latent representation bridges\nthe visual and action domains, effectively modeling the relationship between\nvideo and action sequences. Meanwhile, the decoupled decoding, powered by two\nlightweight diffusion heads, enables high-speed action inference by bypassing\nvideo generation during inference. Such a unified framework further enables\nversatile functionality through masked input training. By selectively masking\nactions or videos, a single model can tackle diverse tasks beyond policy\nlearning, such as forward and inverse dynamics modeling and video generation.\nVia an extensive set of experiments, we demonstrate that UVA can serve as a\ngeneral-purpose solution for a wide range of robotics tasks, such as policy\nlearning, forward/inverse dynamics and video observation prediction, without\ncompromising performance compared to methods tailored for specific\napplications. Results are best viewed on\nhttps://unified-video-action-model.github.io/.\n","authors":["Shuang Li","Yihuai Gao","Dorsa Sadigh","Shuran Song"],"pdf_url":"https://arxiv.org/pdf/2503.00200v2.pdf","comment":"Project website: https://unified-video-action-model.github.io/"},{"id":"http://arxiv.org/abs/2405.14093v4","updated":"2025-03-04T08:24:20Z","published":"2024-05-23T01:43:54Z","title":"A Survey on Vision-Language-Action Models for Embodied AI","summary":"  Embodied AI is widely recognized as a key element of artificial general\nintelligence because it involves controlling embodied agents to perform tasks\nin the physical world. Building on the success of large language models and\nvision-language models, a new category of multimodal models -- referred to as\nvision-language-action models (VLAs) -- has emerged to address\nlanguage-conditioned robotic tasks in embodied AI by leveraging their distinct\nability to generate actions. In recent years, a myriad of VLAs have been\ndeveloped, making it imperative to capture the rapidly evolving landscape\nthrough a comprehensive survey. To this end, we present the first survey on\nVLAs for embodied AI. This work provides a detailed taxonomy of VLAs, organized\ninto three major lines of research. The first line focuses on individual\ncomponents of VLAs. The second line is dedicated to developing control policies\nadept at predicting low-level actions. The third line comprises high-level task\nplanners capable of decomposing long-horizon tasks into a sequence of subtasks,\nthereby guiding VLAs to follow more general user instructions. Furthermore, we\nprovide an extensive summary of relevant resources, including datasets,\nsimulators, and benchmarks. Finally, we discuss the challenges faced by VLAs\nand outline promising future directions in embodied AI. We have created a\nproject associated with this survey, which is available at\nhttps://github.com/yueen-ma/Awesome-VLA.\n","authors":["Yueen Ma","Zixing Song","Yuzheng Zhuang","Jianye Hao","Irwin King"],"pdf_url":"https://arxiv.org/pdf/2405.14093v4.pdf","comment":"Project page: https://github.com/yueen-ma/Awesome-VLA"},{"id":"http://arxiv.org/abs/2503.02387v1","updated":"2025-03-04T08:23:01Z","published":"2025-03-04T08:23:01Z","title":"RGBSQGrasp: Inferring Local Superquadric Primitives from Single RGB\n  Image for Graspability-Aware Bin Picking","summary":"  Bin picking is a challenging robotic task due to occlusions and physical\nconstraints that limit visual information for object recognition and grasping.\nExisting approaches often rely on known CAD models or prior object geometries,\nrestricting generalization to novel or unknown objects. Other methods directly\nregress grasp poses from RGB-D data without object priors, but the inherent\nnoise in depth sensing and the lack of object understanding make grasp\nsynthesis and evaluation more difficult. Superquadrics (SQ) offer a compact,\ninterpretable shape representation that captures the physical and graspability\nunderstanding of objects. However, recovering them from limited viewpoints is\nchallenging, as existing methods rely on multiple perspectives for\nnear-complete point cloud reconstruction, limiting their effectiveness in\nbin-picking. To address these challenges, we propose \\textbf{RGBSQGrasp}, a\ngrasping framework that leverages superquadric shape primitives and foundation\nmetric depth estimation models to infer grasp poses from a monocular RGB camera\n-- eliminating the need for depth sensors. Our framework integrates a\nuniversal, cross-platform dataset generation pipeline, a foundation model-based\nobject point cloud estimation module, a global-local superquadric fitting\nnetwork, and an SQ-guided grasp pose sampling module. By integrating these\ncomponents, RGBSQGrasp reliably infers grasp poses through geometric reasoning,\nenhancing grasp stability and adaptability to unseen objects. Real-world\nrobotic experiments demonstrate a 92\\% grasp success rate, highlighting the\neffectiveness of RGBSQGrasp in packed bin-picking environments.\n","authors":["Yifeng Xu","Fan Zhu","Ye Li","Sebastian Ren","Xiaonan Huang","Yuhao Chen"],"pdf_url":"https://arxiv.org/pdf/2503.02387v1.pdf","comment":"8 pages, 7 figures, In submission to IROS2025"},{"id":"http://arxiv.org/abs/2503.02383v1","updated":"2025-03-04T08:19:32Z","published":"2025-03-04T08:19:32Z","title":"Introspective Loop Closure for SLAM with 4D Imaging Radar","summary":"  Simultaneous Localization and Mapping (SLAM) allows mobile robots to navigate\nwithout external positioning systems or pre-existing maps. Radar is emerging as\na valuable sensing tool, especially in vision-obstructed environments, as it is\nless affected by particles than lidars or cameras. Modern 4D imaging radars\nprovide three-dimensional geometric information and relative velocity\nmeasurements, but they bring challenges, such as a small field of view and\nsparse, noisy point clouds. Detecting loop closures in SLAM is critical for\nreducing trajectory drift and maintaining map accuracy. However, the\ndirectional nature of 4D radar data makes identifying loop closures, especially\nfrom reverse viewpoints, difficult due to limited scan overlap. This article\nexplores using 4D radar for loop closure in SLAM, focusing on similar and\nopposing viewpoints. We generate submaps for a denser environment\nrepresentation and use introspective measures to reject false detections in\nfeature-degenerate environments. Our experiments show accurate loop closure\ndetection in geometrically diverse settings for both similar and opposing\nviewpoints, improving trajectory estimation with up to 82 % improvement in ATE\nand rejecting false positives in self-similar environments.\n","authors":["Maximilian Hilger","Vladimír Kubelka","Daniel Adolfsson","Ralf Becker","Henrik Andreasson","Achim J. Lilienthal"],"pdf_url":"https://arxiv.org/pdf/2503.02383v1.pdf","comment":"This paper has been accepted for publication in the IEEE\n  International Conference on Robotics and Automation(ICRA), 2025"},{"id":"http://arxiv.org/abs/2503.02372v1","updated":"2025-03-04T07:58:15Z","published":"2025-03-04T07:58:15Z","title":"Label-Efficient LiDAR Panoptic Segmentation","summary":"  A main bottleneck of learning-based robotic scene understanding methods is\nthe heavy reliance on extensive annotated training data, which often limits\ntheir generalization ability. In LiDAR panoptic segmentation, this challenge\nbecomes even more pronounced due to the need to simultaneously address both\nsemantic and instance segmentation from complex, high-dimensional point cloud\ndata. In this work, we address the challenge of LiDAR panoptic segmentation\nwith very few labeled samples by leveraging recent advances in label-efficient\nvision panoptic segmentation. To this end, we propose a novel method,\nLimited-Label LiDAR Panoptic Segmentation (L3PS), which requires only a minimal\namount of labeled data. Our approach first utilizes a label-efficient 2D\nnetwork to generate panoptic pseudo-labels from a small set of annotated\nimages, which are subsequently projected onto point clouds. We then introduce a\nnovel 3D refinement module that capitalizes on the geometric properties of\npoint clouds. By incorporating clustering techniques, sequential scan\naccumulation, and ground point separation, this module significantly enhances\nthe accuracy of the pseudo-labels, improving segmentation quality by up to\n+10.6 PQ and +7.9 mIoU. We demonstrate that these refined pseudo-labels can be\nused to effectively train off-the-shelf LiDAR segmentation networks. Through\nextensive experiments, we show that L3PS not only outperforms existing methods\nbut also substantially reduces the annotation burden. We release the code of\nour work at https://l3ps.cs.uni-freiburg.de.\n","authors":["Ahmet Selim Çanakçı","Niclas Vödisch","Kürsat Petek","Wolfram Burgard","Abhinav Valada"],"pdf_url":"https://arxiv.org/pdf/2503.02372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01363v2","updated":"2025-03-04T07:51:38Z","published":"2025-03-03T09:58:04Z","title":"FABG : End-to-end Imitation Learning for Embodied Affective Human-Robot\n  Interaction","summary":"  This paper proposes FABG (Facial Affective Behavior Generation), an\nend-to-end imitation learning system for human-robot interaction, designed to\ngenerate natural and fluid facial affective behaviors. In interaction,\neffectively obtaining high-quality demonstrations remains a challenge. In this\nwork, we develop an immersive virtual reality (VR) demonstration system that\nallows operators to perceive stereoscopic environments. This system ensures\n\"the operator's visual perception matches the robot's sensory input\" and \"the\noperator's actions directly determine the robot's behaviors\" - as if the\noperator replaces the robot in human interaction engagements. We propose a\nprediction-driven latency compensation strategy to reduce robotic reaction\ndelays and enhance interaction fluency. FABG naturally acquires human\ninteractive behaviors and subconscious motions driven by intuition, eliminating\nmanual behavior scripting. We deploy FABG on a real-world 25-degree-of-freedom\n(DoF) humanoid robot, validating its effectiveness through four fundamental\ninteraction tasks: expression response, dynamic gaze, foveated attention, and\ngesture recognition, supported by data collection and policy training. Project\nwebsite: https://cybergenies.github.io\n","authors":["Yanghai Zhang","Changyi Liu","Keting Fu","Wenbin Zhou","Qingdu Li","Jianwei Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.01363v2.pdf","comment":"Project website: https://cybergenies.github.io"},{"id":"http://arxiv.org/abs/2503.02369v1","updated":"2025-03-04T07:50:32Z","published":"2025-03-04T07:50:32Z","title":"JPDS-NN: Reinforcement Learning-Based Dynamic Task Allocation for\n  Agricultural Vehicle Routing Optimization","summary":"  The Entrance Dependent Vehicle Routing Problem (EDVRP) is a variant of the\nVehicle Routing Problem (VRP) where the scale of cities influences routing\noutcomes, necessitating consideration of their entrances. This paper addresses\nEDVRP in agriculture, focusing on multi-parameter vehicle planning for\nirregularly shaped fields. To address the limitations of traditional methods,\nsuch as heuristic approaches, which often overlook field geometry and entrance\nconstraints, we propose a Joint Probability Distribution Sampling Neural\nNetwork (JPDS-NN) to effectively solve the EDVRP. The network uses an\nencoder-decoder architecture with graph transformers and attention mechanisms\nto model routing as a Markov Decision Process, and is trained via reinforcement\nlearning for efficient and rapid end-to-end planning. Experimental results\nindicate that JPDS-NN reduces travel distances by 48.4-65.4%, lowers fuel\nconsumption by 14.0-17.6%, and computes two orders of magnitude faster than\nbaseline methods, while demonstrating 15-25% superior performance in dynamic\narrangement scenarios. Ablation studies validate the necessity of\ncross-attention and pre-training. The framework enables scalable, intelligent\nrouting for large-scale farming under dynamic constraints.\n","authors":["Yixuan Fan","Haotian Xu","Mengqiao Liu","Qing Zhuo","Tao Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.02369v1.pdf","comment":"8 pages, 7 figures, submitted to IROS 2025"},{"id":"http://arxiv.org/abs/2503.01274v2","updated":"2025-03-04T07:45:21Z","published":"2025-03-03T07:56:47Z","title":"DnD Filter: Differentiable State Estimation for Dynamic Systems using\n  Diffusion Models","summary":"  This paper proposes the DnD Filter, a differentiable filter that utilizes\ndiffusion models for state estimation of dynamic systems. Unlike conventional\ndifferentiable filters, which often impose restrictive assumptions on process\nnoise (e.g., Gaussianity), DnD Filter enables a nonlinear state update without\nsuch constraints by conditioning a diffusion model on both the predicted state\nand observational data, capitalizing on its ability to approximate complex\ndistributions. We validate its effectiveness on both a simulated task and a\nreal-world visual odometry task, where DnD Filter consistently outperforms\nexisting baselines. Specifically, it achieves a 25\\% improvement in estimation\naccuracy on the visual odometry task compared to state-of-the-art\ndifferentiable filters, and even surpasses differentiable smoothers that\nutilize future measurements. To the best of our knowledge, DnD Filter\nrepresents the first successful attempt to leverage diffusion models for state\nestimation, offering a flexible and powerful framework for nonlinear estimation\nunder noisy measurements.\n","authors":["Ziyu Wan","Lin Zhao"],"pdf_url":"https://arxiv.org/pdf/2503.01274v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.09022v2","updated":"2025-03-04T07:42:22Z","published":"2024-11-13T20:59:30Z","title":"DART-LLM: Dependency-Aware Multi-Robot Task Decomposition and Execution\n  using Large Language Models","summary":"  Large Language Models (LLMs) have demonstrated promising reasoning\ncapabilities in robotics; however, their application in multi-robot systems\nremains limited, particularly in handling task dependencies. This paper\nintroduces DART-LLM, a novel framework that employs Directed Acyclic Graphs\n(DAGs) to model task dependencies, enabling the decomposition of natural\nlanguage instructions into well-coordinated subtasks for multi-robot execution.\nDART-LLM comprises four key components: a Question-Answering (QA) LLM module\nfor dependency-aware task decomposition, a Breakdown Function module for robot\nassignment, an Actuation module for execution, and a Vision-Language Model\n(VLM)-based object detector for environmental perception, achieving end-to-end\ntask execution. Experimental results across three task complexity levels\ndemonstrate that DART-LLM achieves state-of-the-art performance, significantly\noutperforming the baseline across all evaluation metrics. Among the tested\nmodels, DeepSeek-r1-671B achieves the highest success rate, whereas\nLlama-3.1-8B exhibits superior response time reliability. Ablation studies\nfurther confirm that explicit dependency modeling notably enhances the\nperformance of smaller models, facilitating efficient deployment on\nresource-constrained platforms. Please refer to the project website\nhttps://wyd0817.github.io/project-dart-llm/ for videos and code.\n","authors":["Yongdong Wang","Runze Xiao","Jun Younes Louhi Kasahara","Ryosuke Yajima","Keiji Nagatani","Atsushi Yamashita","Hajime Asama"],"pdf_url":"https://arxiv.org/pdf/2411.09022v2.pdf","comment":"The work was first submitted to an IEEE conference on September 15,\n  2024"},{"id":"http://arxiv.org/abs/2502.20041v3","updated":"2025-03-04T07:37:57Z","published":"2025-02-27T12:29:44Z","title":"3D-AffordanceLLM: Harnessing Large Language Models for Open-Vocabulary\n  Affordance Detection in 3D Worlds","summary":"  3D Affordance detection is a challenging problem with broad applications on\nvarious robotic tasks. Existing methods typically formulate the detection\nparadigm as a label-based semantic segmentation task. This paradigm relies on\npredefined labels and lacks the ability to comprehend complex natural language,\nresulting in limited generalization in open-world scene. To address these\nlimitations, we reformulate the traditional affordance detection paradigm into\n\\textit{Instruction Reasoning Affordance Segmentation} (IRAS) task. This task\nis designed to output a affordance mask region given a query reasoning text,\nwhich avoids fixed categories of input labels. We accordingly propose the\n\\textit{3D-AffordanceLLM} (3D-ADLLM), a framework designed for reasoning\naffordance detection in 3D open-scene. Specifically, 3D-ADLLM introduces large\nlanguage models (LLMs) to 3D affordance perception with a custom-designed\ndecoder for generating affordance masks, thus achieving open-world reasoning\naffordance detection. In addition, given the scarcity of 3D affordance datasets\nfor training large models, we seek to extract knowledge from general\nsegmentation data and transfer it to affordance detection. Thus, we propose a\nmulti-stage training strategy that begins with a novel pre-training task, i.e.,\n\\textit{Referring Object Part Segmentation}~(ROPS). This stage is designed to\nequip the model with general recognition and segmentation capabilities at the\nobject-part level. Then followed by fine-tuning with the IRAS task, 3D-ADLLM\nobtains the reasoning ability for affordance detection. In summary, 3D-ADLLM\nleverages the rich world knowledge and human-object interaction reasoning\nability of LLMs, achieving approximately an 8\\% improvement in mIoU on\nopen-vocabulary affordance detection tasks.\n","authors":["Hengshuo Chu","Xiang Deng","Qi Lv","Xiaoyang Chen","Yinchuan Li","Jianye Hao","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2502.20041v3.pdf","comment":"ICLR"},{"id":"http://arxiv.org/abs/2503.02353v1","updated":"2025-03-04T07:22:34Z","published":"2025-03-04T07:22:34Z","title":"Controllable Motion Generation via Diffusion Modal Coupling","summary":"  Diffusion models have recently gained significant attention in robotics due\nto their ability to generate multi-modal distributions of system states and\nbehaviors. However, a key challenge remains: ensuring precise control over the\ngenerated outcomes without compromising realism. This is crucial for\napplications such as motion planning or trajectory forecasting, where adherence\nto physical constraints and task-specific objectives is essential. We propose a\nnovel framework that enhances controllability in diffusion models by leveraging\nmulti-modal prior distributions and enforcing strong modal coupling. This\nallows us to initiate the denoising process directly from distinct prior modes\nthat correspond to different possible system behaviors, ensuring sampling to\nalign with the training distribution. We evaluate our approach on motion\nprediction using the Waymo dataset and multi-task control in Maze2D\nenvironments. Experimental results show that our framework outperforms both\nguidance-based techniques and conditioned models with unimodal priors,\nachieving superior fidelity, diversity, and controllability, even in the\nabsence of explicit conditioning. Overall, our approach provides a more\nreliable and scalable solution for controllable motion generation in robotics.\n","authors":["Luobin Wang","Hongzhan Yu","Chenning Yu","Sicun Gao","Henrik Christensen"],"pdf_url":"https://arxiv.org/pdf/2503.02353v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.16281v2","updated":"2025-03-04T07:06:42Z","published":"2024-02-26T03:54:32Z","title":"RobKiNet: Robotic Kinematics Informed Neural Network for Optimal Robot\n  Configuration Prediction","summary":"  Task and Motion Planning (TAMP) is essential for robots to interact with the\nworld and accomplish complex tasks. The TAMP problem involves a critical gap:\nexploring the robot's configuration parameters (such as chassis position and\nrobotic arm joint angles) within continuous space to ensure that task-level\nglobal constraints are met while also enhancing the efficiency of subsequent\nmotion planning. Existing methods still have significant room for improvement\nin terms of efficiency. Recognizing that robot kinematics is a key factor in\nmotion planning, we propose a framework called the Robotic Kinematics Informed\nNeural Network (RobKiNet) as a bridge between task and motion layers. RobKiNet\nintegrates kinematic knowledge into neural networks to train models capable of\nefficient configuration prediction. We designed a Chassis Motion Predictor(CMP)\nand a Full Motion Predictor(FMP) using RobKiNet, which employed two entirely\ndifferent sets of forward and inverse kinematics constraints to achieve loosely\ncoupled control and whole-body control, respectively. Experiments demonstrate\nthat CMP and FMP can predict configuration parameters with 96.67% and 98%\naccuracy, respectively. That means that the corresponding motion planning can\nachieve a speedup of 24.24x and 153x compared to random sampling. Furthermore,\nRobKiNet demonstrates remarkable data efficiency. CMP only requires 1/71 and\nFMP only requires 1/15052 of the training data for the same prediction accuracy\ncompared to other deep learning methods. These results demonstrate the great\npotential of RoboKiNet in robot applications.\n","authors":["Yanlong Peng","Zhigang Wang","Yisheng Zhang","Pengxu Chang","Ziwen He","Kai Gu","Hongshen Zhang","Ming Chen"],"pdf_url":"https://arxiv.org/pdf/2402.16281v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02311v1","updated":"2025-03-04T06:13:53Z","published":"2025-03-04T06:13:53Z","title":"Target Return Optimizer for Multi-Game Decision Transformer","summary":"  Achieving autonomous agents with robust generalization capabilities across\ndiverse games and tasks remains one of the ultimate goals in AI research.\nRecent advancements in transformer-based offline reinforcement learning,\nexemplified by the MultiGame Decision Transformer [Lee et al., 2022], have\nshown remarkable performance across various games or tasks. However, these\napproaches depend heavily on human expertise, presenting substantial challenges\nfor practical deployment, particularly in scenarios with limited prior\ngame-specific knowledge. In this paper, we propose an algorithm called\nMulti-Game Target Return Optimizer (MTRO) to autonomously determine\ngame-specific target returns within the Multi-Game Decision Transformer\nframework using solely offline datasets. MTRO addresses the existing\nlimitations by automating the target return configuration process, leveraging\nenvironmental reward information extracted from offline datasets. Notably, MTRO\ndoes not require additional training, enabling seamless integration into\nexisting Multi-Game Decision Transformer architectures. Our experimental\nevaluations on Atari games demonstrate that MTRO enhances the performance of RL\npolicies across a wide array of games, underscoring its potential to advance\nthe field of autonomous agent development.\n","authors":["Kensuke Tatematsu","Akifumi Wachi"],"pdf_url":"https://arxiv.org/pdf/2503.02311v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2503.02310v1","updated":"2025-03-04T06:12:08Z","published":"2025-03-04T06:12:08Z","title":"Accelerating Vision-Language-Action Model Integrated with Action\n  Chunking via Parallel Decoding","summary":"  Vision-Language-Action (VLA) models demonstrate remarkable potential for\ngeneralizable robotic manipulation. The performance of VLA models can be\nimproved by integrating with action chunking, a critical technique for\neffective control. However, action chunking linearly scales up action\ndimensions in VLA models with increased chunking sizes. This reduces the\ninference efficiency. To tackle this problem, we propose PD-VLA, the first\nparallel decoding framework for VLA models integrated with action chunking. Our\nframework reformulates autoregressive decoding as a nonlinear system solved by\nparallel fixed-point iterations. This approach preserves model performance with\nmathematical guarantees while significantly improving decoding speed. In\naddition, it enables training-free acceleration without architectural changes,\nas well as seamless synergy with existing acceleration techniques. Extensive\nsimulations validate that our PD-VLA maintains competitive success rates while\nachieving 2.52 times execution frequency on manipulators (with 7 degrees of\nfreedom) compared with the fundamental VLA model. Furthermore, we\nexperimentally identify the most effective settings for acceleration. Finally,\nreal-world experiments validate its high applicability across different tasks.\n","authors":["Wenxuan Song","Jiayi Chen","Pengxiang Ding","Han Zhao","Wei Zhao","Zhide Zhong","Zongyuan Ge","Jun Ma","Haoang Li"],"pdf_url":"https://arxiv.org/pdf/2503.02310v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02300v1","updated":"2025-03-04T06:00:15Z","published":"2025-03-04T06:00:15Z","title":"Diffusion-Based mmWave Radar Point Cloud Enhancement Driven by Range\n  Images","summary":"  Millimeter-wave (mmWave) radar has attracted significant attention in\nrobotics and autonomous driving. However, despite the perception stability in\nharsh environments, the point cloud generated by mmWave radar is relatively\nsparse while containing significant noise, which limits its further\ndevelopment. Traditional mmWave radar enhancement approaches often struggle to\nleverage the effectiveness of diffusion models in super-resolution, largely due\nto the unnatural range-azimuth heatmap (RAH) or bird's eye view (BEV)\nrepresentation. To overcome this limitation, we propose a novel method that\npioneers the application of fusing range images with image diffusion models,\nachieving accurate and dense mmWave radar point clouds that are similar to\nLiDAR. Benefitting from the projection that aligns with human observation, the\nrange image representation of mmWave radar is close to natural images, allowing\nthe knowledge from pre-trained image diffusion models to be effectively\ntransferred, significantly improving the overall performance. Extensive\nevaluations on both public datasets and self-constructed datasets demonstrate\nthat our approach provides substantial improvements, establishing a new\nstate-of-the-art performance in generating truly three-dimensional LiDAR-like\npoint clouds via mmWave radar.\n","authors":["Ruixin Wu","Zihan Li","Jin Wang","Xiangyu Xu","Huan Yu","Zhi Zheng","Kaixiang Huang","Guodong Lu"],"pdf_url":"https://arxiv.org/pdf/2503.02300v1.pdf","comment":"8 pages, 7 figures, submitted to 2025 IROS. This work has been\n  submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2503.01450v2","updated":"2025-03-04T05:23:52Z","published":"2025-03-03T11:59:03Z","title":"POPGym Arcade: Parallel Pixelated POMDPs","summary":"  We introduce POPGym Arcade, a benchmark consisting of 7 pixel-based\nenvironments each with three difficulties, utilizing a single observation and\naction space. Each environment offers both fully observable and partially\nobservable variants, enabling counterfactual studies on partial observability.\nPOPGym Arcade utilizes JIT compilation on hardware accelerators to achieve\nsubstantial speedups over CPU-bound environments. Moreover, this enables\nPodracer-style architectures to further increase hardware utilization and\ntraining speed. We evaluate memory models on our environments using a Podracer\nvariant of Q learning, and examine the results. Finally, we generate memory\nsaliency maps, uncovering how memories propagate through policies. Our library\nis available at https://github.com/bolt-research/popgym_arcade.\n","authors":["Zekang Wang","Zhe He","Edan Toledo","Steven Morad"],"pdf_url":"https://arxiv.org/pdf/2503.01450v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02280v1","updated":"2025-03-04T05:04:35Z","published":"2025-03-04T05:04:35Z","title":"Model-Based Capacitive Touch Sensing in Soft Robotics: Achieving Robust\n  Tactile Interactions for Artistic Applications","summary":"  In this paper, we present a touch technology to achieve tactile interactivity\nfor human-robot interaction (HRI) in soft robotics. By combining a capacitive\ntouch sensor with an online solid mechanics simulation provided by the SOFA\nframework, contact detection is achieved for arbitrary shapes. Furthermore, the\nimplementation of the capacitive touch technology presented here is selectively\nsensitive to human touch (conductive objects), while it is largely unaffected\nby the deformations created by the pneumatic actuation of our soft robot.\nMulti-touch interactions are also possible. We evaluated our approach with an\norganic soft robotics sculpture that was created by a visual artist. In\nparticular, we evaluate that the touch localization capabilities are robust\nunder the deformation of the device. We discuss the potential this approach has\nfor the arts and entertainment as well as other domains.\n","authors":["Carolina Silva-Plata","Carlos Rosel","Barnabas Gavin Cangan","Hosam Alagi","Björn Hein","Robert K. Katzschmann","Rubén Fernández","Yosra Mojtahedi","Stefan Escaida Navarro"],"pdf_url":"https://arxiv.org/pdf/2503.02280v1.pdf","comment":"8 pages, 17 figures. Accepted at IEEE Robotics and Automation Letters\n  (RA-L) 2025"},{"id":"http://arxiv.org/abs/2503.02277v1","updated":"2025-03-04T04:57:01Z","published":"2025-03-04T04:57:01Z","title":"Active Robot Curriculum Learning from Online Human Demonstrations","summary":"  Learning from Demonstrations (LfD) allows robots to learn skills from human\nusers, but its effectiveness can suffer due to sub-optimal teaching, especially\nfrom untrained demonstrators. Active LfD aims to improve this by letting robots\nactively request demonstrations to enhance learning. However, this may lead to\nfrequent context switches between various task situations, increasing the human\ncognitive load and introducing errors to demonstrations. Moreover, few prior\nstudies in active LfD have examined how these active query strategies may\nimpact human teaching in aspects beyond user experience, which can be crucial\nfor developing algorithms that benefit both robot learning and human teaching.\nTo tackle these challenges, we propose an active LfD method that optimizes the\nquery sequence of online human demonstrations via Curriculum Learning (CL),\nwhere demonstrators are guided to provide demonstrations in situations of\ngradually increasing difficulty. We evaluate our method across four simulated\nrobotic tasks with sparse rewards and conduct a user study (N=26) to\ninvestigate the influence of active LfD methods on human teaching regarding\nteaching performance, post-guidance teaching adaptivity, and teaching\ntransferability. Our results show that our method significantly improves\nlearning performance compared to three other LfD baselines in terms of the\nfinal success rate of the converged policy and sample efficiency. Additionally,\nresults from our user study indicate that our method significantly reduces the\ntime required from human demonstrators and decreases failed demonstration\nattempts. It also enhances post-guidance human teaching in both seen and unseen\nscenarios compared to another active LfD baseline, indicating enhanced teaching\nperformance, greater post-guidance teaching adaptivity, and better teaching\ntransferability achieved by our method.\n","authors":["Muhan Hou","Koen Hindriks","A. E. Eiben","Kim Baraka"],"pdf_url":"https://arxiv.org/pdf/2503.02277v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02275v1","updated":"2025-03-04T04:47:40Z","published":"2025-03-04T04:47:40Z","title":"ForaNav: Insect-inspired Online Target-oriented Navigation for MAVs in\n  Tree Plantations","summary":"  Autonomous Micro Air Vehicles (MAVs) are becoming essential in precision\nagriculture to enhance efficiency and reduce labor costs through targeted,\nreal-time operations. However, existing unmanned systems often rely on\nGPS-based navigation, which is prone to inaccuracies in rural areas and limits\nflight paths to predefined routes, resulting in operational inefficiencies. To\naddress these challenges, this paper presents ForaNav, an insect-inspired\nnavigation strategy for autonomous navigation in plantations. The proposed\nmethod employs an enhanced Histogram of Oriented Gradient (HOG)-based tree\ndetection approach, integrating hue-saturation histograms and global HOG\nfeature variance with hierarchical HOG extraction to distinguish oil palm trees\nfrom visually similar objects. Inspired by insect foraging behavior, the MAV\ndynamically adjusts its path based on detected trees and employs a recovery\nmechanism to stay on course if a target is temporarily lost. We demonstrate\nthat our detection method generalizes well to different tree types while\nmaintaining lower CPU usage, lower temperature, and higher FPS than lightweight\ndeep learning models, making it well-suited for real-time applications. Flight\ntest results across diverse real-world scenarios show that the MAV successfully\ndetects and approaches all trees without prior tree location, validating its\neffectiveness for agricultural automation.\n","authors":["Weijie Kuang","Hann Woei Ho","Ye Zhou","Shahrel Azmin Suandi"],"pdf_url":"https://arxiv.org/pdf/2503.02275v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02265v1","updated":"2025-03-04T04:25:02Z","published":"2025-03-04T04:25:02Z","title":"Towards Fluorescence-Guided Autonomous Robotic Partial Nephrectomy on\n  Novel Tissue-Mimicking Hydrogel Phantoms","summary":"  Autonomous robotic systems hold potential for improving renal tumor resection\naccuracy and patient outcomes. We present a fluorescence-guided robotic system\ncapable of planning and executing incision paths around exophytic renal tumors\nwith a clinically relevant resection margin. Leveraging point cloud\nobservations, the system handles irregular tumor shapes and distinguishes\nhealthy from tumorous tissue based on near-infrared imaging, akin to\nindocyanine green staining in partial nephrectomy. Tissue-mimicking phantoms\nare crucial for the development of autonomous robotic surgical systems for\ninterventions where acquiring ex-vivo animal tissue is infeasible, such as\ncancer of the kidney and renal pelvis. To this end, we propose novel\nhydrogel-based kidney phantoms with exophytic tumors that mimic the physical\nand visual behavior of tissue, and are compatible with electrosurgical\ninstruments, a common limitation of silicone-based phantoms. In contrast to\nprevious hydrogel phantoms, we mix the material with near-infrared dye to\nenable fluorescence-guided tumor segmentation. Autonomous real-world robotic\nexperiments validate our system and phantoms, achieving an average margin\naccuracy of 1.44 mm in a completion time of 69 sec.\n","authors":["Ethan Kilmer","Joseph Chen","Jiawei Ge","Preksha Sarda","Richard Cha","Kevin Cleary","Lauren Shepard","Ahmed Ezzat Ghazi","Paul Maria Scheikl","Axel Krieger"],"pdf_url":"https://arxiv.org/pdf/2503.02265v1.pdf","comment":"8 pages. 7 figures. Preprint of an article accepted for publication\n  in the Journal of Medical Robotics Research, 2025. Copyright World Scientific\n  Publishing Company [https://worldscientific.com/worldscinet/jmrr]"},{"id":"http://arxiv.org/abs/2503.02256v1","updated":"2025-03-04T04:13:23Z","published":"2025-03-04T04:13:23Z","title":"Continual Multi-Robot Learning from Black-Box Visual Place Recognition\n  Models","summary":"  In the context of visual place recognition (VPR), continual learning (CL)\ntechniques offer significant potential for avoiding catastrophic forgetting\nwhen learning new places. However, existing CL methods often focus on knowledge\ntransfer from a known model to a new one, overlooking the existence of unknown\nblack-box models. We explore a novel multi-robot CL approach that enables\nknowledge transfer from black-box VPR models (teachers), such as those of local\nrobots encountered by traveler robots (students) in unknown environments.\nSpecifically, we introduce Membership Inference Attack, or MIA, the only major\nprivacy attack applicable to black-box models, and leverage it to reconstruct\npseudo training sets, which serve as the key knowledge to be exchanged between\nrobots, from black-box VPR models. Furthermore, we aim to overcome the\ninherently low sampling efficiency of MIA by leveraging insights on place class\nprediction distribution and un-learned class detection imported from the VPR\nliterature as a prior distribution. We also analyze both the individual effects\nof these methods and their combined impact. Experimental results demonstrate\nthat our black-box MIA (BB-MIA) approach is remarkably powerful despite its\nsimplicity, significantly enhancing the VPR capability of lower-performing\nrobots through brief communication with other robots. This study contributes to\noptimizing knowledge sharing between robots in VPR and enhancing autonomy in\nopen-world environments with multi-robot systems that are fault-tolerant and\nscalable.\n","authors":["Kenta Tsukahara","Kanji Tanaka","Daiki Iwata","Jonathan Tay Yu Liang"],"pdf_url":"https://arxiv.org/pdf/2503.02256v1.pdf","comment":"6 pages, 4 figures, technical report"},{"id":"http://arxiv.org/abs/2503.02249v1","updated":"2025-03-04T03:55:10Z","published":"2025-03-04T03:55:10Z","title":"Large Language Models as Natural Selector for Embodied Soft Robot Design","summary":"  Designing soft robots is a complex and iterative process that demands\ncross-disciplinary expertise in materials science, mechanics, and control,\noften relying on intuition and extensive experimentation. While Large Language\nModels (LLMs) have demonstrated impressive reasoning abilities, their capacity\nto learn and apply embodied design principles--crucial for creating functional\nrobotic systems--remains largely unexplored. This paper introduces\nRoboCrafter-QA, a novel benchmark to evaluate whether LLMs can learn\nrepresentations of soft robot designs that effectively bridge the gap between\nhigh-level task descriptions and low-level morphological and material choices.\nRoboCrafter-QA leverages the EvoGym simulator to generate a diverse set of soft\nrobot design challenges, spanning robotic locomotion, manipulation, and\nbalancing tasks. Our experiments with state-of-the-art multi-modal LLMs reveal\nthat while these models exhibit promising capabilities in learning design\nrepresentations, they struggle with fine-grained distinctions between designs\nwith subtle performance differences. We further demonstrate the practical\nutility of LLMs for robot design initialization. Our code and benchmark will be\navailable to encourage the community to foster this exciting research\ndirection.\n","authors":["Changhe Chen","Xiaohao Xu","Xiangdong Wang","Xiaonan Huang"],"pdf_url":"https://arxiv.org/pdf/2503.02249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02247v1","updated":"2025-03-04T03:51:36Z","published":"2025-03-04T03:51:36Z","title":"WMNav: Integrating Vision-Language Models into World Models for Object\n  Goal Navigation","summary":"  Object Goal Navigation-requiring an agent to locate a specific object in an\nunseen environment-remains a core challenge in embodied AI. Although recent\nprogress in Vision-Language Model (VLM)-based agents has demonstrated promising\nperception and decision-making abilities through prompting, none has yet\nestablished a fully modular world model design that reduces risky and costly\ninteractions with the environment by predicting the future state of the world.\nWe introduce WMNav, a novel World Model-based Navigation framework powered by\nVision-Language Models (VLMs). It predicts possible outcomes of decisions and\nbuilds memories to provide feedback to the policy module. To retain the\npredicted state of the environment, WMNav proposes the online maintained\nCuriosity Value Map as part of the world model memory to provide dynamic\nconfiguration for navigation policy. By decomposing according to a human-like\nthinking process, WMNav effectively alleviates the impact of model\nhallucination by making decisions based on the feedback difference between the\nworld model plan and observation. To further boost efficiency, we implement a\ntwo-stage action proposer strategy: broad exploration followed by precise\nlocalization. Extensive evaluation on HM3D and MP3D validates WMNav surpasses\nexisting zero-shot benchmarks in both success rate and exploration efficiency\n(absolute improvement: +3.2% SR and +3.2% SPL on HM3D, +13.5% SR and +1.1% SPL\non MP3D). Project page: https://b0b8k1ng.github.io/WMNav/.\n","authors":["Dujun Nie","Xianda Guo","Yiqun Duan","Ruijun Zhang","Long Chen"],"pdf_url":"https://arxiv.org/pdf/2503.02247v1.pdf","comment":"8 pages, 5 figures"},{"id":"http://arxiv.org/abs/2502.12191v2","updated":"2025-03-04T02:57:23Z","published":"2025-02-15T08:33:25Z","title":"AnyTouch: Learning Unified Static-Dynamic Representation across Multiple\n  Visuo-tactile Sensors","summary":"  Visuo-tactile sensors aim to emulate human tactile perception, enabling\nrobots to precisely understand and manipulate objects. Over time, numerous\nmeticulously designed visuo-tactile sensors have been integrated into robotic\nsystems, aiding in completing various tasks. However, the distinct data\ncharacteristics of these low-standardized visuo-tactile sensors hinder the\nestablishment of a powerful tactile perception system. We consider that the key\nto addressing this issue lies in learning unified multi-sensor representations,\nthereby integrating the sensors and promoting tactile knowledge transfer\nbetween them. To achieve unified representation of this nature, we introduce\nTacQuad, an aligned multi-modal multi-sensor tactile dataset from four\ndifferent visuo-tactile sensors, which enables the explicit integration of\nvarious sensors. Recognizing that humans perceive the physical environment by\nacquiring diverse tactile information such as texture and pressure changes, we\nfurther propose to learn unified multi-sensor representations from both static\nand dynamic perspectives. By integrating tactile images and videos, we present\nAnyTouch, a unified static-dynamic multi-sensor representation learning\nframework with a multi-level structure, aimed at both enhancing comprehensive\nperceptual abilities and enabling effective cross-sensor transfer. This\nmulti-level architecture captures pixel-level details from tactile data via\nmasked modeling and enhances perception and transferability by learning\nsemantic-level sensor-agnostic features through multi-modal alignment and\ncross-sensor matching. We provide a comprehensive analysis of multi-sensor\ntransferability, and validate our method on various datasets and in the\nreal-world pouring task. Experimental results show that our method outperforms\nexisting methods, exhibits outstanding static and dynamic perception\ncapabilities across various sensors.\n","authors":["Ruoxuan Feng","Jiangyu Hu","Wenke Xia","Tianci Gao","Ao Shen","Yuhao Sun","Bin Fang","Di Hu"],"pdf_url":"https://arxiv.org/pdf/2502.12191v2.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2408.14769v2","updated":"2025-03-04T02:53:51Z","published":"2024-08-27T04:10:22Z","title":"Points2Plans: From Point Clouds to Long-Horizon Plans with Composable\n  Relational Dynamics","summary":"  We present Points2Plans, a framework for composable planning with a\nrelational dynamics model that enables robots to solve long-horizon\nmanipulation tasks from partial-view point clouds. Given a language instruction\nand a point cloud of the scene, our framework initiates a hierarchical planning\nprocedure, whereby a language model generates a high-level plan and a\nsampling-based planner produces constraint-satisfying continuous parameters for\nmanipulation primitives sequenced according to the high-level plan. Key to our\napproach is the use of a relational dynamics model as a unifying interface\nbetween the continuous and symbolic representations of states and actions, thus\nfacilitating language-driven planning from high-dimensional perceptual input\nsuch as point clouds. Whereas previous relational dynamics models require\ntraining on datasets of multi-step manipulation scenarios that align with the\nintended test scenarios, Points2Plans uses only single-step simulated training\ndata while generalizing zero-shot to a variable number of steps during\nreal-world evaluations. We evaluate our approach on tasks involving geometric\nreasoning, multi-object interactions, and occluded object reasoning in both\nsimulated and real-world settings. Results demonstrate that Points2Plans offers\nstrong generalization to unseen long-horizon tasks in the real world, where it\nsolves over 85% of evaluated tasks while the next best baseline solves only\n50%.\n","authors":["Yixuan Huang","Christopher Agia","Jimmy Wu","Tucker Hermans","Jeannette Bohg"],"pdf_url":"https://arxiv.org/pdf/2408.14769v2.pdf","comment":"Project page: https://sites.google.com/stanford.edu/points2plans. 23\n  pages, 11 figures. Accepted to the IEEE International Conference on Robotics\n  and Automation (ICRA) 2025"},{"id":"http://arxiv.org/abs/2503.02208v1","updated":"2025-03-04T02:40:17Z","published":"2025-03-04T02:40:17Z","title":"ADMM-MCBF-LCA: A Layered Control Architecture for Safe Real-Time\n  Navigation","summary":"  We consider the problem of safe real-time navigation of a robot in a dynamic\nenvironment with moving obstacles of arbitrary smooth geometries and input\nsaturation constraints. We assume that the robot detects and models nearby\nobstacle boundaries with a short-range sensor and that this detection is\nerror-free. This problem presents three main challenges: i) input constraints,\nii) safety, and iii) real-time computation. To tackle all three challenges, we\npresent a layered control architecture (LCA) consisting of an offline path\nlibrary generation layer, and an online path selection and safety layer. To\novercome the limitations of reactive methods, our offline path library consists\nof feasible controllers, feedback gains, and reference trajectories. To handle\ncomputational burden and safety, we solve online path selection and generate\nsafe inputs that run at 100 Hz. Through simulations on Gazebo and Fetch\nhardware in an indoor environment, we evaluate our approach against baselines\nthat are layered, end-to-end, or reactive. Our experiments demonstrate that\namong all algorithms, only our proposed LCA is able to complete tasks such as\nreaching a goal, safely. When comparing metrics such as safety, input error,\nand success rate, we show that our approach generates safe and feasible inputs\nthroughout the robot execution.\n","authors":["Anusha Srikanthan","Yifan Xue","Vijay Kumar","Nikolai Matni","Nadia Figueroa"],"pdf_url":"https://arxiv.org/pdf/2503.02208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02198v1","updated":"2025-03-04T02:18:40Z","published":"2025-03-04T02:18:40Z","title":"Zero-Shot Sim-to-Real Visual Quadrotor Control with Hard Constraints","summary":"  We present the first framework demonstrating zero-shot sim-to-real transfer\nof visual control policies learned in a Neural Radiance Field (NeRF)\nenvironment for quadrotors to fly through racing gates. Robust transfer from\nsimulation to real flight poses a major challenge, as standard simulators often\nlack sufficient visual fidelity. To address this, we construct a photorealistic\nsimulation environment of quadrotor racing tracks, called FalconGym, which\nprovides effectively unlimited synthetic images for training. Within FalconGym,\nwe develop a pipelined approach for crossing gates that combines (i) a Neural\nPose Estimator (NPE) coupled with a Kalman filter to reliably infer quadrotor\nposes from single-frame RGB images and IMU data, and (ii) a\nself-attention-based multi-modal controller that adaptively integrates visual\nfeatures and pose estimation. This multi-modal design compensates for\nperception noise and intermittent gate visibility. We train this controller\npurely in FalconGym with imitation learning and deploy the resulting policy to\nreal hardware with no additional fine-tuning. Simulation experiments on three\ndistinct tracks (circle, U-turn and figure-8) demonstrate that our controller\noutperforms a vision-only state-of-the-art baseline in both success rate and\ngate-crossing accuracy. In 30 live hardware flights spanning three tracks and\n120 gates, our controller achieves a 95.8% success rate and an average error of\njust 10 cm when flying through 38 cm-radius gates.\n","authors":["Yan Miao","Will Shen","Sayan Mitra"],"pdf_url":"https://arxiv.org/pdf/2503.02198v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02188v1","updated":"2025-03-04T01:50:50Z","published":"2025-03-04T01:50:50Z","title":"RPF-Search: Field-based Search for Robot Person Following in Unknown\n  Dynamic Environments","summary":"  Autonomous robot person-following (RPF) systems are crucial for personal\nassistance and security but suffer from target loss due to occlusions in\ndynamic, unknown environments. Current methods rely on pre-built maps and\nassume static environments, limiting their effectiveness in real-world\nsettings. There is a critical gap in re-finding targets under topographic\n(e.g., walls, corners) and dynamic (e.g., moving pedestrians) occlusions. In\nthis paper, we propose a novel heuristic-guided search framework that\ndynamically builds environmental maps while following the target and resolves\nvarious occlusions by prioritizing high-probability areas for locating the\ntarget. For topographic occlusions, a belief-guided search field is constructed\nand used to evaluate the likelihood of the target's presence, while for dynamic\nocclusions, a fluid-field approach allows the robot to adaptively follow or\novertake moving occluders. Past motion cues and environmental observations\nrefine the search decision over time. Our results demonstrate that the proposed\nmethod outperforms existing approaches in terms of search efficiency and\nsuccess rates, both in simulations and real-world tests. Our target search\nmethod enhances the adaptability and reliability of RPF systems in unknown and\ndynamic environments to support their use in real-world applications. Our code,\nvideo, experimental results and appendix are available at\nhttps://medlartea.github.io/rpf-search/.\n","authors":["Hanjing Ye","Kuanqi Cai","Yu Zhan","Bingyi Xia","Arash Ajoudani","Hong Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.02188v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2503.02158v1","updated":"2025-03-04T00:41:10Z","published":"2025-03-04T00:41:10Z","title":"Design and Control of A Tilt-Rotor Tailsitter Aircraft with Pivoting\n  VTOL Capability","summary":"  Tailsitter aircraft attract considerable interest due to their capabilities\nof both agile hover and high speed forward flight. However, traditional\ntailsitters that use aerodynamic control surfaces face the challenge of limited\ncontrol effectiveness and associated actuator saturation during vertical flight\nand transitions. Conversely, tailsitters relying solely on tilting rotors have\nthe drawback of insufficient roll control authority in forward flight. This\npaper proposes a tilt-rotor tailsitter aircraft with both elevons and tilting\nrotors as a promising solution. By implementing a cascaded weighted least\nsquares (WLS) based incremental nonlinear dynamic inversion (INDI) controller,\nthe drone successfully achieved autonomous waypoint tracking in outdoor\nexperiments at a cruise airspeed of 16 m/s, including transitions between\nforward flight and hover without actuator saturation. Wind tunnel experiments\nconfirm improved roll control compared to tilt-rotor-only configurations, while\ncomparative outdoor flight tests highlight the vehicle's superior control over\nelevon-only designs during critical phases such as vertical descent and\ntransitions. Finally, we also show that the tilt-rotors allow for an autonomous\ntakeoff and landing with a unique pivoting capability that demonstrates\nstability and robustness under wind disturbances.\n","authors":["Ziqing Ma","Ewoud J. J. Smeur","Guido C. H. E. de Croon"],"pdf_url":"https://arxiv.org/pdf/2503.02158v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02143v1","updated":"2025-03-04T00:19:32Z","published":"2025-03-04T00:19:32Z","title":"Four Principles for Physically Interpretable World Models","summary":"  As autonomous systems are increasingly deployed in open and uncertain\nsettings, there is a growing need for trustworthy world models that can\nreliably predict future high-dimensional observations. The learned latent\nrepresentations in world models lack direct mapping to meaningful physical\nquantities and dynamics, limiting their utility and interpretability in\ndownstream planning, control, and safety verification. In this paper, we argue\nfor a fundamental shift from physically informed to physically interpretable\nworld models - and crystallize four principles that leverage symbolic knowledge\nto achieve these ends: (1) structuring latent spaces according to the physical\nintent of variables, (2) learning aligned invariant and equivariant\nrepresentations of the physical world, (3) adapting training to the varied\ngranularity of supervision signals, and (4) partitioning generative outputs to\nsupport scalability and verifiability. We experimentally demonstrate the value\nof each principle on two benchmarks. This paper opens several intriguing\nresearch directions to achieve and capitalize on full physical interpretability\nin world models.\n","authors":["Jordan Peper","Zhenjiang Mao","Yuang Geng","Siyuan Pan","Ivan Ruchkin"],"pdf_url":"https://arxiv.org/pdf/2503.02143v1.pdf","comment":"Equal contribution by the first two authors"},{"id":"http://arxiv.org/abs/2410.06437v2","updated":"2025-03-04T23:49:01Z","published":"2024-10-09T00:45:02Z","title":"LocoVR: Multiuser Indoor Locomotion Dataset in Virtual Reality","summary":"  Understanding human locomotion is crucial for AI agents such as robots,\nparticularly in complex indoor home environments. Modeling human trajectories\nin these spaces requires insight into how individuals maneuver around physical\nobstacles and manage social navigation dynamics. These dynamics include subtle\nbehaviors influenced by proxemics - the social use of space, such as stepping\naside to allow others to pass or choosing longer routes to avoid collisions.\nPrevious research has developed datasets of human motion in indoor scenes, but\nthese are often limited in scale and lack the nuanced social navigation\ndynamics common in home environments. To address this, we present LocoVR, a\ndataset of 7000+ two-person trajectories captured in virtual reality from over\n130 different indoor home environments. LocoVR provides accurate trajectory\ndata and precise spatial information, along with rich examples of\nsocially-motivated movement behaviors. For example, the dataset captures\ninstances of individuals navigating around each other in narrow spaces,\nadjusting paths to respect personal boundaries in living areas, and\ncoordinating movements in high-traffic zones like entryways and kitchens. Our\nevaluation shows that LocoVR significantly enhances model performance in three\npractical indoor tasks utilizing human trajectories, and demonstrates\npredicting socially-aware navigation patterns in home environments.\n","authors":["Kojiro Takeyama","Yimeng Liu","Misha Sra"],"pdf_url":"https://arxiv.org/pdf/2410.06437v2.pdf","comment":"This paper has been accepted to ICLR2025"},{"id":"http://arxiv.org/abs/2404.02318v3","updated":"2025-03-04T23:24:28Z","published":"2024-04-02T21:33:57Z","title":"ZeroCAP: Zero-Shot Multi-Robot Context Aware Pattern Formation via Large\n  Language Models","summary":"  Incorporating language comprehension into robotic operations unlocks\nsignificant advancements in robotics, but also presents distinct challenges,\nparticularly in executing spatially oriented tasks like pattern formation. This\npaper introduces ZeroCAP, a novel system that integrates large language models\nwith multi-robot systems for zero-shot context aware pattern formation.\nGrounded in the principles of language-conditioned robotics, ZeroCAP leverages\nthe interpretative power of language models to translate natural language\ninstructions into actionable robotic configurations. This approach combines the\nsynergy of vision-language models, cutting-edge segmentation techniques and\nshape descriptors, enabling the realization of complex, context-driven pattern\nformations in the realm of multi robot coordination. Through extensive\nexperiments, we demonstrate the systems proficiency in executing complex\ncontext aware pattern formations across a spectrum of tasks, from surrounding\nand caging objects to infilling regions. This not only validates the system's\ncapability to interpret and implement intricate context-driven tasks but also\nunderscores its adaptability and effectiveness across varied environments and\nscenarios. The experimental videos and additional information about this work\ncan be found at https://sites.google.com/view/zerocap/home.\n","authors":["Vishnunandan L. N. Venkatesh","Byung-Cheol Min"],"pdf_url":"https://arxiv.org/pdf/2404.02318v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03045v1","updated":"2025-03-04T22:51:50Z","published":"2025-03-04T22:51:50Z","title":"ArticuBot: Learning Universal Articulated Object Manipulation Policy via\n  Large Scale Simulation","summary":"  This paper presents ArticuBot, in which a single learned policy enables a\nrobotics system to open diverse categories of unseen articulated objects in the\nreal world. This task has long been challenging for robotics due to the large\nvariations in the geometry, size, and articulation types of such objects. Our\nsystem, Articubot, consists of three parts: generating a large number of\ndemonstrations in physics-based simulation, distilling all generated\ndemonstrations into a point cloud-based neural policy via imitation learning,\nand performing zero-shot sim2real transfer to real robotics systems. Utilizing\nsampling-based grasping and motion planning, our demonstration generalization\npipeline is fast and effective, generating a total of 42.3k demonstrations over\n322 training articulated objects. For policy learning, we propose a novel\nhierarchical policy representation, in which the high-level policy learns the\nsub-goal for the end-effector, and the low-level policy learns how to move the\nend-effector conditioned on the predicted goal. We demonstrate that this\nhierarchical approach achieves much better object-level generalization compared\nto the non-hierarchical version. We further propose a novel weighted\ndisplacement model for the high-level policy that grounds the prediction into\nthe existing 3D structure of the scene, outperforming alternative policy\nrepresentations. We show that our learned policy can zero-shot transfer to\nthree different real robot settings: a fixed table-top Franka arm across two\ndifferent labs, and an X-Arm on a mobile base, opening multiple unseen\narticulated objects across two labs, real lounges, and kitchens. Videos and\ncode can be found on our project website: https://articubot.github.io/.\n","authors":["Yufei Wang","Ziyu Wang","Mino Nakura","Pratik Bhowal","Chia-Liang Kuo","Yi-Ting Chen","Zackory Erickson","David Held"],"pdf_url":"https://arxiv.org/pdf/2503.03045v1.pdf","comment":null},{"id":"http://arxiv.org/abs/1710.05512v2","updated":"2025-03-04T22:29:30Z","published":"2017-10-16T05:32:38Z","title":"The Feeling of Success: Does Touch Sensing Help Predict Grasp Outcomes?","summary":"  A successful grasp requires careful balancing of the contact forces. Deducing\nwhether a particular grasp will be successful from indirect measurements, such\nas vision, is therefore quite challenging, and direct sensing of contacts\nthrough touch sensing provides an appealing avenue toward more successful and\nconsistent robotic grasping. However, in order to fully evaluate the value of\ntouch sensing for grasp outcome prediction, we must understand how touch\nsensing can influence outcome prediction accuracy when combined with other\nmodalities. Doing so using conventional model-based techniques is exceptionally\ndifficult. In this work, we investigate the question of whether touch sensing\naids in predicting grasp outcomes within a multimodal sensing framework that\ncombines vision and touch. To that end, we collected more than 9,000 grasping\ntrials using a two-finger gripper equipped with GelSight high-resolution\ntactile sensors on each finger, and evaluated visuo-tactile deep neural network\nmodels to directly predict grasp outcomes from either modality individually,\nand from both modalities together. Our experimental results indicate that\nincorporating tactile readings substantially improve grasping performance.\n","authors":["Roberto Calandra","Andrew Owens","Manu Upadhyaya","Wenzhen Yuan","Justin Lin","Edward H. Adelson","Sergey Levine"],"pdf_url":"https://arxiv.org/pdf/1710.05512v2.pdf","comment":"10 pages, published at the 1st Annual Conference on Robot Learning\n  (CoRL), Code and dataset available at:\n  https://lasr.org/research/feeling-of-success"},{"id":"http://arxiv.org/abs/2402.10711v2","updated":"2025-03-04T21:46:10Z","published":"2024-02-16T14:14:23Z","title":"StableLego: Stability Analysis of Block Stacking Assembly","summary":"  Structural stability is a necessary condition for successful construction of\nan assembly. However, designing a stable assembly requires a non-trivial effort\nsince a slight variation in the design could significantly affect the\nstructural stability. To address the challenge, this paper studies the\nstability of assembly structures, in particular, block stacking assembly. The\npaper proposes a new optimization formulation, which optimizes over force\nbalancing equations, for inferring the structural stability of 3D block\nstacking structures. The proposed stability analysis is verified on\nhand-crafted Lego examples. The experiment results demonstrate that the\nproposed method can correctly predict whether the structure is stable. In\naddition, it outperforms the existing methods since it can accurately locate\nthe weakest parts in the design, and more importantly, solve any given assembly\nstructures. To further validate the proposed method, we provide\n\\textit{StableLego}: a comprehensive dataset including 50k+ 3D objects with\ntheir Lego layouts. We test the proposed stability analysis and include the\nstability inference for each corresponding object in StableLego. Our code and\nthe dataset are available at\nhttps://github.com/intelligent-control-lab/StableLego.\n","authors":["Ruixuan Liu","Kangle Deng","Ziwei Wang","Changliu Liu"],"pdf_url":"https://arxiv.org/pdf/2402.10711v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.20267v2","updated":"2025-03-04T21:33:17Z","published":"2024-10-26T20:37:57Z","title":"Learning Maximal Safe Sets Using Hypernetworks for MPC-based Local\n  Trajectory Planning in Unknown Environments","summary":"  This paper presents a novel learning-based approach for online estimation of\nmaximal safe sets for local trajectory planning in unknown static environments.\nThe neural representation of a set is used as the terminal set constraint for a\nmodel predictive control (MPC) local planner, resulting in improved recursive\nfeasibility and safety. To achieve real-time performance and desired\ngeneralization properties, we employ the idea of hypernetworks. We use the\nHamilton-Jacobi (HJ) reachability analysis as the source of supervision during\nthe training process, allowing us to consider general nonlinear dynamics and\narbitrary constraints. The proposed method is extensively evaluated against\nrelevant baselines in simulations for different environments and robot\ndynamics. The results show a success rate increase of up to 52 \\% compared to\nthe best baseline while maintaining comparable execution speed. Additionally,\nwe deploy our proposed method, NTC-MPC, on a physical robot and demonstrate its\nability to safely avoid obstacles in scenarios where the baselines fail.\n","authors":["Bojan Derajić","Mohamed-Khalil Bouzidi","Sebastian Bernhard","Wolfgang Hönig"],"pdf_url":"https://arxiv.org/pdf/2410.20267v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03002v1","updated":"2025-03-04T20:57:38Z","published":"2025-03-04T20:57:38Z","title":"Multi-Step Deep Koopman Network (MDK-Net) for Vehicle Control in Frenet\n  Frame","summary":"  The highly nonlinear dynamics of vehicles present a major challenge for the\npractical implementation of optimal and Model Predictive Control (MPC)\napproaches in path planning and following. Koopman operator theory offers a\nglobal linear representation of nonlinear dynamical systems, making it a\npromising framework for optimization-based vehicle control. This paper\nintroduces a novel deep learning-based Koopman modeling approach that employs\ndeep neural networks to capture the full vehicle dynamics-from pedal and\nsteering inputs to chassis states-within a curvilinear Frenet frame. The\nsuperior accuracy of the Koopman model compared to identified linear models is\nshown for a double lane change maneuver. Furthermore, it is shown that an MPC\ncontroller deploying the Koopman model provides significantly improved\nperformance while maintaining computational efficiency comparable to a linear\nMPC.\n","authors":["Mohammad Abtahi","Mahdis Rabbani","Armin Abdolmohammadi","Shima Nazari"],"pdf_url":"https://arxiv.org/pdf/2503.03002v1.pdf","comment":"This work has been submitted for IROS 2025 conference"},{"id":"http://arxiv.org/abs/2311.12151v2","updated":"2025-03-04T20:45:46Z","published":"2023-11-20T20:03:34Z","title":"Teaching Robots to Build Simulations of Themselves","summary":"  The emergence of vision catalysed a pivotal evolutionary advancement,\nenabling organisms not only to perceive but also to interact intelligently with\ntheir environment. This transformation is mirrored by the evolution of robotic\nsystems, where the ability to leverage vision to simulate and predict their own\ndynamics marks a leap towards autonomy and self-awareness. Humans utilize\nvision to record experiences and internally simulate potential actions. For\nexample, we can imagine that, if we stand up and raise our arms, the body will\nform a T shape without physical movement. Similarly, simulation allows robots\nto plan and predict the outcomes of potential actions without execution. Here\nwe introduce a self-supervised learning framework to enable robots to model and\npredict their morphology, kinematics and motor control using only brief raw\nvideo data, eliminating the need for extensive real-world data collection and\nkinematic priors. By observing their own movements, akin to humans watching\ntheir reflection in a mirror, robots learn an ability to simulate themselves\nand predict their spatial motion for various tasks. Our results demonstrate\nthat this self-learned simulation not only enables accurate motion planning but\nalso allows the robot to detect abnormalities and recover from damage.\n","authors":["Yuhang Hu","Jiong Lin","Hod Lipson"],"pdf_url":"https://arxiv.org/pdf/2311.12151v2.pdf","comment":"This paper was published on Nature Machine Intelligence"},{"id":"http://arxiv.org/abs/2503.02992v1","updated":"2025-03-04T20:35:20Z","published":"2025-03-04T20:35:20Z","title":"RAILGUN: A Unified Convolutional Policy for Multi-Agent Path Finding\n  Across Different Environments and Tasks","summary":"  Multi-Agent Path Finding (MAPF), which focuses on finding collision-free\npaths for multiple robots, is crucial for applications ranging from aerial\nswarms to warehouse automation. Solving MAPF is NP-hard so learning-based\napproaches for MAPF have gained attention, particularly those leveraging deep\nneural networks. Nonetheless, despite the community's continued efforts, all\nlearning-based MAPF planners still rely on decentralized planning due to\nvariability in the number of agents and map sizes. We have developed the first\ncentralized learning-based policy for MAPF problem called RAILGUN. RAILGUN is\nnot an agent-based policy but a map-based policy. By leveraging a CNN-based\narchitecture, RAILGUN can generalize across different maps and handle any\nnumber of agents. We collect trajectories from rule-based methods to train our\nmodel in a supervised way. In experiments, RAILGUN outperforms most baseline\nmethods and demonstrates great zero-shot generalization capabilities on various\ntasks, maps and agent numbers that were not seen in the training dataset.\n","authors":["Yimin Tang","Xiao Xiong","Jingyi Xi","Jiaoyang Li","Erdem Bıyık","Sven Koenig"],"pdf_url":"https://arxiv.org/pdf/2503.02992v1.pdf","comment":"7 pages"},{"id":"http://arxiv.org/abs/2503.01096v2","updated":"2025-03-04T20:25:26Z","published":"2025-03-03T01:50:50Z","title":"Optimal Trajectory Planning for Cooperative Manipulation with Multiple\n  Quadrotors Using Control Barrier Functions","summary":"  In this paper, we present a novel trajectory planning algorithm for\ncooperative manipulation with multiple quadrotors using control barrier\nfunctions (CBFs). Our approach addresses the complex dynamics of a system in\nwhich a team of quadrotors transports and manipulates a cable-suspended\nrigid-body payload in environments cluttered with obstacles. The proposed\nalgorithm ensures obstacle avoidance for the entire system, including the\nquadrotors, cables, and the payload in all six degrees of freedom (DoF). We\nintroduce the use of CBFs to enable safe and smooth maneuvers, effectively\nnavigating through cluttered environments while accommodating the system's\nnonlinear dynamics. To simplify complex constraints, the system components are\nmodeled as convex polytopes, and the Duality theorem is employed to reduce the\ncomputational complexity of the optimization problem. We validate the\nperformance of our planning approach both in simulation and real-world\nenvironments using multiple quadrotors. The results demonstrate the\neffectiveness of the proposed approach in achieving obstacle avoidance and safe\ntrajectory generation for cooperative transportation tasks.\n","authors":["Arpan Pallar","Guanrui Li","Mrunal Sarvaiya","Giuseppe Loianno"],"pdf_url":"https://arxiv.org/pdf/2503.01096v2.pdf","comment":"This paper has been accepted for publication in the IEEE\n  International Conference on Robotics and Automation(ICRA), 2025. Please cite\n  the paper using appropriate formats"},{"id":"http://arxiv.org/abs/2502.17834v2","updated":"2025-03-04T19:55:28Z","published":"2025-02-25T04:29:11Z","title":"Impact of Object Weight in Handovers: Inspiring Robotic Grip Release and\n  Motion from Human Handovers","summary":"  This work explores the effect of object weight on human motion and grip\nrelease during handovers to enhance the naturalness, safety, and efficiency of\nrobot-human interactions. We introduce adaptive robotic strategies based on the\nanalysis of human handover behavior with varying object weights. The key\ncontributions of this work includes the development of an adaptive grip-release\nstrategy for robots, a detailed analysis of how object weight influences human\nmotion to guide robotic motion adaptations, and the creation of\nhandover-datasets incorporating various object weights, including the YCB\nhandover dataset. By aligning robotic grip release and motion with human\nbehavior, this work aims to improve robot-human handovers for different\nweighted objects. We also evaluate these human-inspired adaptive robotic\nstrategies in robot-to-human handovers to assess their effectiveness and\nperformance and demonstrate that they outperform the baseline approaches in\nterms of naturalness, efficiency, and user perception.\n","authors":["Parag Khanna","Mårten Björkman","Christian Smith"],"pdf_url":"https://arxiv.org/pdf/2502.17834v2.pdf","comment":"In Submission at IEEE-IEEE Transactions on Robotics. Changes:\n  Corrected typos; Added 2 references for object weight impact on handovers;\n  added Figures 20, 21, and 22 in Results in Section VI for further comparative\n  analysis"},{"id":"http://arxiv.org/abs/2409.07003v3","updated":"2025-03-04T19:36:45Z","published":"2024-09-11T04:31:09Z","title":"ODYSSEE: Oyster Detection Yielded by Sensor Systems on Edge Electronics","summary":"  Oysters are a vital keystone species in coastal ecosystems, providing\nsignificant economic, environmental, and cultural benefits. As the importance\nof oysters grows, so does the relevance of autonomous systems for their\ndetection and monitoring. However, current monitoring strategies often rely on\ndestructive methods. While manual identification of oysters from video footage\nis non-destructive, it is time-consuming, requires expert input, and is further\ncomplicated by the challenges of the underwater environment.\n  To address these challenges, we propose a novel pipeline using stable\ndiffusion to augment a collected real dataset with realistic synthetic data.\nThis method enhances the dataset used to train a YOLOv10-based vision model.\nThe model is then deployed and tested on an edge platform in underwater\nrobotics, achieving a state-of-the-art 0.657 mAP@50 for oyster detection on the\nAqua2 platform.\n","authors":["Xiaomin Lin","Vivek Mange","Arjun Suresh","Bernhard Neuberger","Aadi Palnitkar","Brendan Campbell","Alan Williams","Kleio Baxevani","Jeremy Mallette","Alhim Vera","Markus Vincze","Ioannis Rekleitis","Herbert G. Tanner","Yiannis Aloimonos"],"pdf_url":"https://arxiv.org/pdf/2409.07003v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02955v1","updated":"2025-03-04T19:20:17Z","published":"2025-03-04T19:20:17Z","title":"Monocular visual simultaneous localization and mapping: (r)evolution\n  from geometry to deep learning-based pipelines","summary":"  With the rise of deep learning, there is a fundamental change in visual SLAM\nalgorithms toward developing different modules trained as end-to-end pipelines.\nHowever, regardless of the implementation domain, visual SLAM's performance is\nsubject to diverse environmental challenges, such as dynamic elements in\noutdoor environments, harsh imaging conditions in underwater environments, or\nblurriness in high-speed setups. These environmental challenges need to be\nidentified to study the real-world viability of SLAM implementations. Motivated\nby the aforementioned challenges, this paper surveys the current state of\nvisual SLAM algorithms according to the two main frameworks: geometry-based and\nlearning-based SLAM. First, we introduce a general formulation of the SLAM\npipeline that includes most of the implementations in the literature. Second,\nthose implementations are classified and surveyed for geometry and\nlearning-based SLAM. After that, environment-specific challenges are formulated\nto enable experimental evaluation of the resilience of different visual SLAM\nclasses to varying imaging conditions. We address two significant issues in\nsurveying visual SLAM, providing (1) a consistent classification of visual SLAM\npipelines and (2) a robust evaluation of their performance under different\ndeployment conditions. Finally, we give our take on future opportunities for\nvisual SLAM implementations.\n","authors":["Olaya Alvarez-Tunon","Yury Brodskiy","Erdal Kayacan"],"pdf_url":"https://arxiv.org/pdf/2503.02955v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02954v1","updated":"2025-03-04T19:20:11Z","published":"2025-03-04T19:20:11Z","title":"Reliable and Efficient Multi-Agent Coordination via Graph Neural Network\n  Variational Autoencoders","summary":"  Multi-agent coordination is crucial for reliable multi-robot navigation in\nshared spaces such as automated warehouses. In regions of dense robot traffic,\nlocal coordination methods may fail to find a deadlock-free solution. In these\nscenarios, it is appropriate to let a central unit generate a global schedule\nthat decides the passing order of robots. However, the runtime of such\ncentralized coordination methods increases significantly with the problem\nscale. In this paper, we propose to leverage Graph Neural Network Variational\nAutoencoders (GNN-VAE) to solve the multi-agent coordination problem at scale\nfaster than through centralized optimization. We formulate the coordination\nproblem as a graph problem and collect ground truth data using a Mixed-Integer\nLinear Program (MILP) solver. During training, our learning framework encodes\ngood quality solutions of the graph problem into a latent space. At inference\ntime, solution samples are decoded from the sampled latent variables, and the\nlowest-cost sample is selected for coordination. Finally, the feasible proposal\nwith the highest performance index is selected for the deployment. By\nconstruction, our GNN-VAE framework returns solutions that always respect the\nconstraints of the considered coordination problem. Numerical results show that\nour approach trained on small-scale problems can achieve high-quality solutions\neven for large-scale problems with 250 robots, being much faster than other\nbaselines. Project page: https://mengyuest.github.io/gnn-vae-coord\n","authors":["Yue Meng","Nathalie Majcherczyk","Wenliang Liu","Scott Kiesel","Chuchu Fan","Federico Pecora"],"pdf_url":"https://arxiv.org/pdf/2503.02954v1.pdf","comment":"Accepted by 2025 International Conference on Robotics and Automation\n  (ICRA 2025)"},{"id":"http://arxiv.org/abs/2503.02924v1","updated":"2025-03-04T18:59:00Z","published":"2025-03-04T18:59:00Z","title":"Diverse Controllable Diffusion Policy with Signal Temporal Logic","summary":"  Generating realistic simulations is critical for autonomous system\napplications such as self-driving and human-robot interactions. However,\ndriving simulators nowadays still have difficulty in generating controllable,\ndiverse, and rule-compliant behaviors for road participants: Rule-based models\ncannot produce diverse behaviors and require careful tuning, whereas\nlearning-based methods imitate the policy from data but are not designed to\nfollow the rules explicitly. Besides, the real-world datasets are by nature\n\"single-outcome\", making the learning method hard to generate diverse\nbehaviors. In this paper, we leverage Signal Temporal Logic (STL) and Diffusion\nModels to learn controllable, diverse, and rule-aware policy. We first\ncalibrate the STL on the real-world data, then generate diverse synthetic data\nusing trajectory optimization, and finally learn the rectified diffusion policy\non the augmented dataset. We test on the NuScenes dataset and our approach can\nachieve the most diverse rule-compliant trajectories compared to other\nbaselines, with a runtime 1/17X to the second-best approach. In the closed-loop\ntesting, our approach reaches the highest diversity, rule satisfaction rate,\nand the least collision rate. Our method can generate varied characteristics\nconditional on different STL parameters in testing. A case study on human-robot\nencounter scenarios shows our approach can generate diverse and\nclosed-to-oracle trajectories. The annotation tool, augmented dataset, and code\nare available at https://github.com/mengyuest/pSTL-diffusion-policy.\n","authors":["Yue Meng","Chuchu fan"],"pdf_url":"https://arxiv.org/pdf/2503.02924v1.pdf","comment":"Accepted by IEEE Robotics and Automation Letters (RA-L), October 2024"},{"id":"http://arxiv.org/abs/2503.02916v1","updated":"2025-03-04T11:07:27Z","published":"2025-03-04T11:07:27Z","title":"Monocular Person Localization under Camera Ego-motion","summary":"  Localizing a person from a moving monocular camera is critical for\nHuman-Robot Interaction (HRI). To estimate the 3D human position from a 2D\nimage, existing methods either depend on the geometric assumption of a fixed\ncamera or use a position regression model trained on datasets containing little\ncamera ego-motion. These methods are vulnerable to fierce camera ego-motion,\nresulting in inaccurate person localization. We consider person localization as\na part of a pose estimation problem. By representing a human with a four-point\nmodel, our method jointly estimates the 2D camera attitude and the person's 3D\nlocation through optimization. Evaluations on both public datasets and real\nrobot experiments demonstrate our method outperforms baselines in person\nlocalization accuracy. Our method is further implemented into a\nperson-following system and deployed on an agile quadruped robot.\n","authors":["Yu Zhan","Hanjing Ye","Hong Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.02916v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2503.02913v1","updated":"2025-03-04T08:05:14Z","published":"2025-03-04T08:05:14Z","title":"Towards Robust Multi-UAV Collaboration: MARL with Noise-Resilient\n  Communication and Attention Mechanisms","summary":"  Efficient path planning for unmanned aerial vehicles (UAVs) is crucial in\nremote sensing and information collection. As task scales expand, the\ncooperative deployment of multiple UAVs significantly improves information\ncollection efficiency. However, collaborative communication and decision-making\nfor multiple UAVs remain major challenges in path planning, especially in noisy\nenvironments. To efficiently accomplish complex information collection tasks in\n3D space and address robust communication issues, we propose a multi-agent\nreinforcement learning (MARL) framework for UAV path planning based on the\nCounterfactual Multi-Agent Policy Gradients (COMA) algorithm. The framework\nincorporates attention mechanism-based UAV communication protocol and\ntraining-deployment system, significantly improving communication robustness\nand individual decision-making capabilities in noisy conditions. Experiments\nconducted on both synthetic and real-world datasets demonstrate that our method\noutperforms existing algorithms in terms of path planning efficiency and\nrobustness, especially in noisy environments, achieving a 78\\% improvement in\nentropy reduction.\n","authors":["Zilin Zhao","Chishui Chen","Haotian Shi","Jiale Chen","Xuanlin Yue","Zhejian Yang","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2503.02913v1.pdf","comment":null}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2503.02882v1","updated":"2025-03-04T18:58:42Z","published":"2025-03-04T18:58:42Z","title":"Bringing Comparative Cognition To Computers","summary":"  Researchers are increasingly subjecting artificial intelligence systems to\npsychological testing. But to rigorously compare their cognitive capacities\nwith humans and other animals, we must avoid both over- and under-stating our\nsimilarities and differences. By embracing a comparative approach, we can\nintegrate AI cognition research into the broader cognitive sciences.\n","authors":["Konstantinos Voudouris","Lucy G. Cheke","Eric Schulz"],"pdf_url":"https://arxiv.org/pdf/2503.02882v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02881v1","updated":"2025-03-04T18:58:21Z","published":"2025-03-04T18:58:21Z","title":"Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for\n  Contact-Rich Manipulation","summary":"  Humans can accomplish complex contact-rich tasks using vision and touch, with\nhighly reactive capabilities such as quick adjustments to environmental changes\nand adaptive control of contact forces; however, this remains challenging for\nrobots. Existing visual imitation learning (IL) approaches rely on action\nchunking to model complex behaviors, which lacks the ability to respond\ninstantly to real-time tactile feedback during the chunk execution.\nFurthermore, most teleoperation systems struggle to provide fine-grained\ntactile / force feedback, which limits the range of tasks that can be\nperformed. To address these challenges, we introduce TactAR, a low-cost\nteleoperation system that provides real-time tactile feedback through Augmented\nReality (AR), along with Reactive Diffusion Policy (RDP), a novel slow-fast\nvisual-tactile imitation learning algorithm for learning contact-rich\nmanipulation skills. RDP employs a two-level hierarchy: (1) a slow latent\ndiffusion policy for predicting high-level action chunks in latent space at low\nfrequency, (2) a fast asymmetric tokenizer for closed-loop tactile feedback\ncontrol at high frequency. This design enables both complex trajectory modeling\nand quick reactive behavior within a unified framework. Through extensive\nevaluation across three challenging contact-rich tasks, RDP significantly\nimproves performance compared to state-of-the-art visual IL baselines through\nrapid response to tactile / force feedback. Furthermore, experiments show that\nRDP is applicable across different tactile / force sensors. Code and videos are\navailable on https://reactive-diffusion-policy.github.io/.\n","authors":["Han Xue","Jieji Ren","Wendi Chen","Gu Zhang","Yuan Fang","Guoying Gu","Huazhe Xu","Cewu Lu"],"pdf_url":"https://arxiv.org/pdf/2503.02881v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02879v1","updated":"2025-03-04T18:58:13Z","published":"2025-03-04T18:58:13Z","title":"Wikipedia in the Era of LLMs: Evolution and Risks","summary":"  In this paper, we present a thorough analysis of the impact of Large Language\nModels (LLMs) on Wikipedia, examining the evolution of Wikipedia through\nexisting data and using simulations to explore potential risks. We begin by\nanalyzing page views and article content to study Wikipedia's recent changes\nand assess the impact of LLMs. Subsequently, we evaluate how LLMs affect\nvarious Natural Language Processing (NLP) tasks related to Wikipedia, including\nmachine translation and retrieval-augmented generation (RAG). Our findings and\nsimulation results reveal that Wikipedia articles have been influenced by LLMs,\nwith an impact of approximately 1%-2% in certain categories. If the machine\ntranslation benchmark based on Wikipedia is influenced by LLMs, the scores of\nthe models may become inflated, and the comparative results among models might\nshift as well. Moreover, the effectiveness of RAG might decrease if the\nknowledge base becomes polluted by LLM-generated content. While LLMs have not\nyet fully changed Wikipedia's language and knowledge structures, we believe\nthat our empirical findings signal the need for careful consideration of\npotential future risks.\n","authors":["Siming Huang","Yuliang Xu","Mingmeng Geng","Yao Wan","Dongping Chen"],"pdf_url":"https://arxiv.org/pdf/2503.02879v1.pdf","comment":"We release all the experimental dataset and source code at:\n  https://github.com/HSM316/LLM_Wikipedia"},{"id":"http://arxiv.org/abs/2503.02878v1","updated":"2025-03-04T18:58:11Z","published":"2025-03-04T18:58:11Z","title":"Language Models can Self-Improve at State-Value Estimation for Better\n  Search","summary":"  Collecting ground truth task completion rewards or human demonstrations for\nmulti-step reasoning tasks is often cost-prohibitive and time-consuming,\nespecially in interactive domains like web tasks. To address this bottleneck,\nwe present self-taught lookahead, a self-supervised method that leverages\nstate-transition dynamics to train a value model capable of effectively guiding\nlanguage model-controlled search. We find that moderately sized (8 billion\nparameters) open-weight value models improved with self-taught lookahead can\nmatch the performance of using a frontier LLM such as gpt-4o as the value\nmodel. Furthermore, we find that self-taught lookahead improves performance by\n20% while reducing costs 37x compared to previous LLM-based tree search,\nwithout relying on ground truth rewards.\n","authors":["Ethan Mendes","Alan Ritter"],"pdf_url":"https://arxiv.org/pdf/2503.02878v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.00043v2","updated":"2025-03-04T18:47:38Z","published":"2025-02-25T23:36:19Z","title":"VOILA: Evaluation of MLLMs For Perceptual Understanding and Analogical\n  Reasoning","summary":"  Multimodal Large Language Models (MLLMs) have become a powerful tool for\nintegrating visual and textual information. Despite their exceptional\nperformance on visual understanding benchmarks, measuring their ability to\nreason abstractly across multiple images remains a significant challenge. To\naddress this, we introduce VOILA, a large-scale, open-ended, dynamic benchmark\ndesigned to evaluate MLLMs' perceptual understanding and abstract relational\nreasoning. VOILA employs an analogical mapping approach in the visual domain,\nrequiring models to generate an image that completes an analogy between two\ngiven image pairs, reference and application, without relying on predefined\nchoices. Our experiments demonstrate that the analogical reasoning tasks in\nVOILA present a challenge to MLLMs. Through multi-step analysis, we reveal that\ncurrent MLLMs struggle to comprehend inter-image relationships and exhibit\nlimited capabilities in high-level relational reasoning. Notably, we observe\nthat performance improves when following a multi-step strategy of least-to-most\nprompting. Comprehensive evaluations on open-source models and GPT-4o show that\non text-based answers, the best accuracy for challenging scenarios is 13%\n(LLaMa 3.2) and even for simpler tasks is only 29% (GPT-4o), while human\nperformance is significantly higher at 70% across both difficulty levels.\n","authors":["Nilay Yilmaz","Maitreya Patel","Yiran Lawrence Luo","Tejas Gokhale","Chitta Baral","Suren Jayasuriya","Yezhou Yang"],"pdf_url":"https://arxiv.org/pdf/2503.00043v2.pdf","comment":"Accepted at ICLR 2025. Code and data: https://github.com/nlylmz/Voila"},{"id":"http://arxiv.org/abs/2410.23637v2","updated":"2025-03-04T18:40:33Z","published":"2024-10-31T05:07:01Z","title":"Anytime-Constrained Equilibria in Polynomial Time","summary":"  We extend anytime constraints to the Markov game setting and the\ncorresponding solution concept of an anytime-constrained equilibrium (ACE).\nThen, we present a comprehensive theory of anytime-constrained equilibria that\nincludes (1) a computational characterization of feasible policies, (2) a\nfixed-parameter tractable algorithm for computing ACE, and (3) a\npolynomial-time algorithm for approximately computing ACE. Since computing a\nfeasible policy is NP-hard even for two-player zero-sum games, our\napproximation guarantees are optimal so long as $P \\neq NP$. We also develop\nthe first theory of efficient computation for action-constrained Markov games,\nwhich may be of independent interest.\n","authors":["Jeremy McMahan"],"pdf_url":"https://arxiv.org/pdf/2410.23637v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02861v1","updated":"2025-03-04T18:39:28Z","published":"2025-03-04T18:39:28Z","title":"Evaluation of Architectural Synthesis Using Generative AI","summary":"  Recent advancements in multimodal Generative AI have the potential to\ndemocratize specialized architectural tasks, such as interpreting technical\ndrawings and creating 3D CAD models, which traditionally require expert\nknowledge. This paper presents a comparative evaluation of two systems: GPT-4o\nand Claude 3.5, in the task of architectural 3D synthesis. We conduct a case\nstudy on two buildings from Palladio's Four Books of Architecture (1965): Villa\nRotonda and Palazzo Porto. High-level architectural models and drawings of\nthese buildings were prepared, inspired by Palladio's original texts and\ndrawings. Through sequential text and image prompting, we assess the systems'\nabilities in (1) interpreting 2D and 3D representations of buildings from\ndrawings, (2) encoding the buildings into a CAD software script, and (3)\nself-improving based on outputs. While both systems successfully generate\nindividual parts, they struggle to accurately assemble these parts into the\ndesired spatial relationships, with Claude 3.5 demonstrating better\nperformance, particularly in self-correcting its output. This study contributes\nto ongoing research on benchmarking the strengths and weaknesses of\noff-the-shelf AI systems in performing intelligent human tasks that require\ndiscipline-specific knowledge. The findings highlight the potential of\nlanguage-enabled AI systems to act as collaborative technical assistants in the\narchitectural design process.\n","authors":["Jingfei Huang","Alexandros Haridis"],"pdf_url":"https://arxiv.org/pdf/2503.02861v1.pdf","comment":"10 pages, 7 figures"},{"id":"http://arxiv.org/abs/2503.02857v1","updated":"2025-03-04T18:33:22Z","published":"2025-03-04T18:33:22Z","title":"Deepfake-Eval-2024: A Multi-Modal In-the-Wild Benchmark of Deepfakes\n  Circulated in 2024","summary":"  In the age of increasingly realistic generative AI, robust deepfake detection\nis essential for mitigating fraud and disinformation. While many deepfake\ndetectors report high accuracy on academic datasets, we show that these\nacademic benchmarks are out of date and not representative of recent deepfakes.\nWe introduce Deepfake-Eval-2024, a new deepfake detection benchmark consisting\nof in-the-wild deepfakes collected from social media and deepfake detection\nplatform users in 2024. Deepfake-Eval-2024 consists of 44 hours of videos, 56.5\nhours of audio, and 1,975 images, encompassing the latest manipulation\ntechnologies. The benchmark contains diverse media content from 88 different\nwebsites in 52 different languages. We find that the performance of open-source\nstate-of-the-art deepfake detection models drops precipitously when evaluated\non Deepfake-Eval-2024, with AUC decreasing by 50% for video, 48% for audio, and\n45% for image models compared to previous benchmarks. We also evaluate\ncommercial deepfake detection models and models finetuned on\nDeepfake-Eval-2024, and find that they have superior performance to\noff-the-shelf open-source models, but they do not yet reach the accuracy of\nhuman deepfake forensic analysts. The dataset is available at\nhttps://github.com/nuriachandra/Deepfake-Eval-2024.\n","authors":["Nuria Alina Chandra","Ryan Murtfeldt","Lin Qiu","Arnab Karmakar","Hannah Lee","Emmanuel Tanumihardja","Kevin Farhat","Ben Caffee","Sejin Paik","Changyeon Lee","Jongwook Choi","Aerin Kim","Oren Etzioni"],"pdf_url":"https://arxiv.org/pdf/2503.02857v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02854v1","updated":"2025-03-04T18:31:02Z","published":"2025-03-04T18:31:02Z","title":"(How) Do Language Models Track State?","summary":"  Transformer language models (LMs) exhibit behaviors -- from storytelling to\ncode generation -- that appear to require tracking the unobserved state of an\nevolving world. How do they do so? We study state tracking in LMs trained or\nfine-tuned to compose permutations (i.e., to compute the order of a set of\nobjects after a sequence of swaps). Despite the simple algebraic structure of\nthis problem, many other tasks (e.g., simulation of finite automata and\nevaluation of boolean expressions) can be reduced to permutation composition,\nmaking it a natural model for state tracking in general. We show that LMs\nconsistently learn one of two state tracking mechanisms for this task. The\nfirst closely resembles the \"associative scan\" construction used in recent\ntheoretical work by Liu et al. (2023) and Merrill et al. (2024). The second\nuses an easy-to-compute feature (permutation parity) to partially prune the\nspace of outputs, then refines this with an associative scan. The two\nmechanisms exhibit markedly different robustness properties, and we show how to\nsteer LMs toward one or the other with intermediate training tasks that\nencourage or suppress the heuristics. Our results demonstrate that transformer\nLMs, whether pretrained or fine-tuned, can learn to implement efficient and\ninterpretable state tracking mechanisms, and the emergence of these mechanisms\ncan be predicted and controlled.\n","authors":["Belinda Z. Li","Zifan Carl Guo","Jacob Andreas"],"pdf_url":"https://arxiv.org/pdf/2503.02854v1.pdf","comment":"21 pages, 17 figures, 1 table"},{"id":"http://arxiv.org/abs/2503.02849v1","updated":"2025-03-04T18:24:33Z","published":"2025-03-04T18:24:33Z","title":"Multimodal Deep Learning for Subtype Classification in Breast Cancer\n  Using Histopathological Images and Gene Expression Data","summary":"  Molecular subtyping of breast cancer is crucial for personalized treatment\nand prognosis. Traditional classification approaches rely on either\nhistopathological images or gene expression profiling, limiting their\npredictive power. In this study, we propose a deep multimodal learning\nframework that integrates histopathological images and gene expression data to\nclassify breast cancer into BRCA.Luminal and BRCA.Basal / Her2 subtypes. Our\napproach employs a ResNet-50 model for image feature extraction and fully\nconnected layers for gene expression processing, with a cross-attention fusion\nmechanism to enhance modality interaction. We conduct extensive experiments\nusing five-fold cross-validation, demonstrating that our multimodal integration\noutperforms unimodal approaches in terms of classification accuracy,\nprecision-recall AUC, and F1-score. Our findings highlight the potential of\ndeep learning for robust and interpretable breast cancer subtype\nclassification, paving the way for improved clinical decision-making.\n","authors":["Amin Honarmandi Shandiz"],"pdf_url":"https://arxiv.org/pdf/2503.02849v1.pdf","comment":"9 pages, 9 figures"},{"id":"http://arxiv.org/abs/2412.16172v2","updated":"2025-03-04T18:15:36Z","published":"2024-12-07T00:15:24Z","title":"LABIIUM: AI-Enhanced Zero-configuration Measurement Automation System","summary":"  The complexity of laboratory environments requires solutions that simplify\ninstrument interaction and enhance measurement automation. Traditional tools\noften require configuration, software, and programming skills, creating\nbarriers to productivity. Previous approaches, including dedicated software\nsuites and custom scripts, frequently fall short in providing user-friendly\nsolutions that align with programming practices. We present LABIIUM, an\nAI-enhanced, zero-configuration measurement automation system designed to\nstreamline experimental workflows and improve user productivity. LABIIUM\nintegrates an AI assistant powered by Large Language Models (LLMs) to generate\ncode. LABIIUM's Lab-Automation-Measurement Bridges (LAMBs) enable seamless\ninstrument connectivity using standard tools such as VSCode and Python,\neliminating setup overhead. To demonstrate its capabilities, we conducted\nexperiments involving the measurement of the parametric transfer curve of a\nsimple two-transistor inverting amplifier with a current source load. The AI\nassistant was evaluated using different prompt scenarios and compared with\nmultiple models, including Claude Sonnet 3.5, Gemini Pro 1.5, and GPT-4o. An\nexpert solution implementing the Gradient-Weighted Adaptive Stochastic Sampling\n(GWASS) method was used as a baseline. The solutions generated by the AI\nassistant were compared with the expert solution and a uniform linear sweep\nbaseline with 10,000 points. The graph results show that the LLMs were able to\nsuccessfully complete the most basic uniform sweep, but LLMs were unable to\ndevelop adaptive sweeping algorithms to compete with GWASS. The evaluation\nunderscores LABIIUM's ability to enhance laboratory productivity and support\ndigital transformation in research and industry, and emphasizes the future work\nrequired to improve LLM performance in Electronic Measurement Science Tasks.\n","authors":["Emmanuel A. Olowe","Danial Chitnis"],"pdf_url":"https://arxiv.org/pdf/2412.16172v2.pdf","comment":"accepted for IEEE I2MTC 2025"},{"id":"http://arxiv.org/abs/2410.01335v2","updated":"2025-03-04T18:15:16Z","published":"2024-10-02T08:53:07Z","title":"Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language\n  Models","summary":"  Model merging, such as model souping, is the practice of combining different\nmodels with the same architecture together without further training. In this\nwork, we present a model merging methodology that addresses the difficulty of\nfine-tuning Large Language Models (LLMs) for target tasks in non-English\nlanguages, where task-specific data is often unavailable. We focus on\nmathematical reasoning and without in-language math data, facilitate\ncross-lingual transfer by composing language and math capabilities. Starting\nfrom the same pretrained model, we fine-tune separate \"experts\" on math\ninstruction data in English and on generic instruction data in the target\nlanguage. We then replace the top and bottom transformer layers of the math\nexpert directly with layers from the language expert, which consequently\nenhances math performance in the target language. The resulting merged models\noutperform the individual experts and other merging methods on the math\nbenchmark, MGSM, by 10% across four major languages where math instruction data\nis scarce. In addition, this layer swapping is simple, inexpensive, and\nintuitive, as it is based on an interpretative analysis of the most important\nparameter changes during the fine-tuning of each expert. The ability to\nsuccessfully re-compose LLMs for cross-lingual transfer in this manner opens up\nfuture possibilities to combine model expertise, create modular solutions, and\ntransfer reasoning capabilities across languages all post hoc.\n","authors":["Lucas Bandarkar","Benjamin Muller","Pritish Yuvraj","Rui Hou","Nayan Singhal","Hongjiang Lv","Bing Liu"],"pdf_url":"https://arxiv.org/pdf/2410.01335v2.pdf","comment":"ICLR 2025, Spotlight Paper, In The Thirteenth International\n  Conference on Learning Representations, 2025"},{"id":"http://arxiv.org/abs/2406.05516v3","updated":"2025-03-04T18:07:34Z","published":"2024-06-08T16:35:31Z","title":"Verbalized Probabilistic Graphical Modeling","summary":"  Human cognition excels at transcending sensory input and forming latent\nrepresentations that structure our understanding of the world. Although Large\nLanguage Models (LLMs) can produce chain-of-thought reasoning, they lack a\nprincipled framework to capture latent structures and model uncertainty,\nespecially in compositional reasoning tasks. We propose Verbalized\nProbabilistic Graphical Modeling (vPGM), a Bayesian prompting framework that\nguides LLMs to simulate key principles of Probabilistic Graphical Models (PGMs)\nin natural language. Unlike many traditional probabilistic methods requiring\nsubstantial domain expertise or specialized training, vPGM bypasses\nexpert-driven model design, making it well-suited for scenarios with limited\nassumptions or scarce data. We evaluated our model on several compositional\nreasoning tasks, both close-ended and open-ended. Our results indicate that the\nmodel effectively enhances confidence calibration and text generation quality.\n","authors":["Hengguan Huang","Xing Shen","Songtao Wang","Lingfa Meng","Dianbo Liu","Hao Wang","Samir Bhatt"],"pdf_url":"https://arxiv.org/pdf/2406.05516v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02836v1","updated":"2025-03-04T17:59:17Z","published":"2025-03-04T17:59:17Z","title":"SeqFusion: Sequential Fusion of Pre-Trained Models for Zero-Shot\n  Time-Series Forecasting","summary":"  Unlike traditional time-series forecasting methods that require extensive\nin-task data for training, zero-shot forecasting can directly predict future\nvalues given a target time series without additional training data. Current\nzero-shot approaches primarily rely on pre-trained generalized models, with\ntheir performance often depending on the variety and relevance of the\npre-training data, which can raise privacy concerns. Instead of collecting\ndiverse pre-training data, we introduce SeqFusion in this work, a novel\nframework that collects and fuses diverse pre-trained models (PTMs)\nsequentially for zero-shot forecasting. Based on the specific temporal\ncharacteristics of the target time series, SeqFusion selects the most suitable\nPTMs from a batch of pre-collected PTMs, performs sequential predictions, and\nfuses all the predictions while using minimal data to protect privacy. Each of\nthese PTMs specializes in different temporal patterns and forecasting tasks,\nallowing SeqFusion to select by measuring distances in a shared representation\nspace of the target time series with each PTM. Experiments demonstrate that\nSeqFusion achieves competitive accuracy in zero-shot forecasting compared to\nstate-of-the-art methods.\n","authors":["Ting-Ji Huang","Xu-Yang Chen","Han-Jia Ye"],"pdf_url":"https://arxiv.org/pdf/2503.02836v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02832v1","updated":"2025-03-04T17:57:09Z","published":"2025-03-04T17:57:09Z","title":"AlignDistil: Token-Level Language Model Alignment as Adaptive Policy\n  Distillation","summary":"  In modern large language models (LLMs), LLM alignment is of crucial\nimportance and is typically achieved through methods such as reinforcement\nlearning from human feedback (RLHF) and direct preference optimization (DPO).\nHowever, in most existing methods for LLM alignment, all tokens in the response\nare optimized using a sparse, response-level reward or preference annotation.\nThe ignorance of token-level rewards may erroneously punish high-quality tokens\nor encourage low-quality tokens, resulting in suboptimal performance and slow\nconvergence speed. To address this issue, we propose AlignDistil, an\nRLHF-equivalent distillation method for token-level reward optimization.\nSpecifically, we introduce the reward learned by DPO into the RLHF objective\nand theoretically prove the equivalence between this objective and a\ntoken-level distillation process, where the teacher distribution linearly\ncombines the logits from the DPO model and a reference model. On this basis, we\nfurther bridge the accuracy gap between the reward from the DPO model and the\npure reward model, by building a contrastive DPO reward with a normal and a\nreverse DPO model. Moreover, to avoid under- and over-optimization on different\ntokens, we design a token adaptive logit extrapolation mechanism to construct\nan appropriate teacher distribution for each token. Experimental results\ndemonstrate the superiority of our AlignDistil over existing methods and\nshowcase fast convergence due to its token-level distributional reward\noptimization.\n","authors":["Songming Zhang","Xue Zhang","Tong Zhang","Bojie Hu","Yufeng Chen","Jinan Xu"],"pdf_url":"https://arxiv.org/pdf/2503.02832v1.pdf","comment":"15 pages, 2 figures"},{"id":"http://arxiv.org/abs/2303.13326v2","updated":"2025-03-04T17:56:58Z","published":"2023-03-23T15:05:16Z","title":"Decentralized Adversarial Training over Graphs","summary":"  The vulnerability of machine learning models to adversarial attacks has been\nattracting considerable attention in recent years. Most existing studies focus\non the behavior of stand-alone single-agent learners. In comparison, this work\nstudies adversarial training over graphs, where individual agents are subjected\nto perturbations of varied strength levels across space. It is expected that\ninteractions by linked agents, and the heterogeneity of the attack models that\nare possible over the graph, can help enhance robustness in view of the\ncoordination power of the group. Using a min-max formulation of distributed\nlearning, we develop a decentralized adversarial training framework for\nmulti-agent systems. Specifically, we devise two decentralized adversarial\ntraining algorithms by relying on two popular decentralized learning\nstrategies--diffusion and consensus. We analyze the convergence properties of\nthe proposed framework for strongly-convex, convex, and non-convex\nenvironments, and illustrate the enhanced robustness to adversarial attacks.\n","authors":["Ying Cao","Elsa Rizk","Stefan Vlaski","Ali H. Sayed"],"pdf_url":"https://arxiv.org/pdf/2303.13326v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2303.01936"},{"id":"http://arxiv.org/abs/2503.02824v1","updated":"2025-03-04T17:49:07Z","published":"2025-03-04T17:49:07Z","title":"Developing a PET/CT Foundation Model for Cross-Modal Anatomical and\n  Functional Imaging","summary":"  In oncology, Positron Emission Tomography-Computed Tomography (PET/CT) is\nwidely used in cancer diagnosis, staging, and treatment monitoring, as it\ncombines anatomical details from CT with functional metabolic activity and\nmolecular marker expression information from PET. However, existing artificial\nintelligence-driven PET/CT analyses rely predominantly on task-specific models\ntrained from scratch or on limited datasets, limiting their generalizability\nand robustness. To address this, we propose a foundation model approach\nspecifically designed for multimodal PET/CT imaging. We introduce the\nCross-Fraternal Twin Masked Autoencoder (FratMAE), a novel framework that\neffectively integrates whole-body anatomical and functional or molecular\ninformation. FratMAE employs separate Vision Transformer (ViT) encoders for PET\nand CT scans, along with cross-attention decoders that enable synergistic\ninteractions between modalities during masked autoencoder training.\nAdditionally, it incorporates textual metadata to enhance PET representation\nlearning. By pre-training on PET/CT datasets, FratMAE captures intricate\ncross-modal relationships and global uptake patterns, achieving superior\nperformance on downstream tasks and demonstrating its potential as a\ngeneralizable foundation model.\n","authors":["Yujin Oh","Robert Seifert","Yihan Cao","Christoph Clement","Justin Ferdinandus","Constantin Lapa","Alessandro Liebich","Michelle Amon","Johanna Enke","Sifan Song","Runqi Meng","Fang Zeng","Ning Guo","Xiang Li","Pedram Heidari","Axel Rominger","Kuangyu Shi","Quanzheng Li"],"pdf_url":"https://arxiv.org/pdf/2503.02824v1.pdf","comment":"11 pages, 2 figures, 3 tables"},{"id":"http://arxiv.org/abs/2503.02823v1","updated":"2025-03-04T17:48:48Z","published":"2025-03-04T17:48:48Z","title":"A Multimodal Symphony: Integrating Taste and Sound through Generative AI","summary":"  In recent decades, neuroscientific and psychological research has traced\ndirect relationships between taste and auditory perceptions. This article\nexplores multimodal generative models capable of converting taste information\ninto music, building on this foundational research. We provide a brief review\nof the state of the art in this field, highlighting key findings and\nmethodologies. We present an experiment in which a fine-tuned version of a\ngenerative music model (MusicGEN) is used to generate music based on detailed\ntaste descriptions provided for each musical piece. The results are promising:\naccording the participants' ($n=111$) evaluation, the fine-tuned model produces\nmusic that more coherently reflects the input taste descriptions compared to\nthe non-fine-tuned model. This study represents a significant step towards\nunderstanding and developing embodied interactions between AI, sound, and\ntaste, opening new possibilities in the field of generative AI. We release our\ndataset, code and pre-trained model at: https://osf.io/xs5jy/.\n","authors":["Matteo Spanio","Massimiliano Zampini","Antonio Rodà","Franco Pierucci"],"pdf_url":"https://arxiv.org/pdf/2503.02823v1.pdf","comment":"17 pages, 6 figures (2 + 2 figures with 2 subfigures each)"},{"id":"http://arxiv.org/abs/2503.02812v1","updated":"2025-03-04T17:37:49Z","published":"2025-03-04T17:37:49Z","title":"Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression","summary":"  Autoregressive language models rely on a Key-Value (KV) Cache, which avoids\nre-computing past hidden states during generation, making it faster. As model\nsizes and context lengths grow, the KV Cache becomes a significant memory\nbottleneck, which calls for compression methods that limit its size during\ngeneration. In this paper, we discover surprising properties of Query (Q) and\nKey (K) vectors that allow us to efficiently approximate attention scores\nwithout computing the attention maps. We propose Q-Filters, a training-free KV\nCache compression method that filters out less crucial Key-Value pairs based on\na single context-agnostic projection. Contrarily to many alternatives,\nQ-Filters is compatible with FlashAttention, as it does not require direct\naccess to attention weights. Experimental results in long-context settings\ndemonstrate that Q-Filters is competitive with attention-based compression\nmethods such as SnapKV in retrieval tasks while consistently outperforming\nefficient compression schemes such as Streaming-LLM in generation setups.\nNotably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task\nwith a x32 compression level while reducing the generation perplexity drop by\nup to 65% in text generation compared to Streaming-LLM.\n","authors":["Nathan Godey","Alessio Devoto","Yu Zhao","Simone Scardapane","Pasquale Minervini","Éric de la Clergerie","Benoît Sagot"],"pdf_url":"https://arxiv.org/pdf/2503.02812v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.05313v6","updated":"2025-03-04T17:33:11Z","published":"2024-11-28T19:31:50Z","title":"λ: A Benchmark for Data-Efficiency in Long-Horizon Indoor Mobile\n  Manipulation Robotics","summary":"  Learning to execute long-horizon mobile manipulation tasks is crucial for\nadvancing robotics in household and workplace settings. However, current\napproaches are typically data-inefficient, underscoring the need for improved\nmodels that require realistically sized benchmarks to evaluate their\nefficiency. To address this, we introduce the LAMBDA ({\\lambda})\nbenchmark-Long-horizon Actions for Mobile-manipulation Benchmarking of Directed\nActivities-which evaluates the data efficiency of models on\nlanguage-conditioned, long-horizon, multi-room, multi-floor, pick-and-place\ntasks using a dataset of manageable size, more feasible for collection. Our\nbenchmark includes 571 human-collected demonstrations that provide realism and\ndiversity in simulated and real-world settings. Unlike planner-generated data,\nthese trajectories offer natural variability and replay-verifiability, ensuring\nrobust learning and evaluation. We leverage LAMBDA to benchmark current\nend-to-end learning methods and a modular neuro-symbolic approaches that\ncombines foundation models with task and motion planning. We find that\nend-to-end methods-even when pretrained-yield lower success rates, while\nneuro-symbolic methods perform significantly better and require less data.\n","authors":["Ahmed Jaafar","Shreyas Sundara Raman","Yichen Wei","Sudarshan Harithas","Sofia Juliani","Anneke Wernerfelt","Benedict Quartey","Ifrah Idrees","Jason Xinyu Liu","Stefanie Tellex"],"pdf_url":"https://arxiv.org/pdf/2412.05313v6.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2311.12836v2","updated":"2025-03-04T17:24:53Z","published":"2023-10-03T16:09:07Z","title":"AI-based association analysis for medical imaging using latent-space\n  geometric confounder correction","summary":"  This study addresses the challenges of confounding effects and\ninterpretability in artificial-intelligence-based medical image analysis.\nWhereas existing literature often resolves confounding by removing\nconfounder-related information from latent representations, this strategy risks\naffecting image reconstruction quality in generative models, thus limiting\ntheir applicability in feature visualization. To tackle this, we propose a\ndifferent strategy that retains confounder-related information in latent\nrepresentations while finding an alternative confounder-free representation of\nthe image data.\n  Our approach views the latent space of an autoencoder as a vector space,\nwhere imaging-related variables, such as the learning target (t) and confounder\n(c), have a vector capturing their variability. The confounding problem is\naddressed by searching a confounder-free vector which is orthogonal to the\nconfounder-related vector but maximally collinear to the target-related vector.\nTo achieve this, we introduce a novel correlation-based loss that not only\nperforms vector searching in the latent space, but also encourages the encoder\nto generate latent representations linearly correlated with the variables.\nSubsequently, we interpret the confounder-free representation by sampling and\nreconstructing images along the confounder-free vector.\n  The efficacy and flexibility of our proposed method are demonstrated across\nthree applications, accommodating multiple confounders and utilizing diverse\nimage modalities. Results affirm the method's effectiveness in reducing\nconfounder influences, preventing wrong or misleading associations, and\noffering a unique visual interpretation for in-depth investigations by clinical\nand epidemiological researchers. The code is released in the following GitLab\nrepository:\nhttps://gitlab.com/radiology/compopbio/ai_based_association_analysis}\n","authors":["Xianjing Liu","Bo Li","Meike W. Vernooij","Eppo B. Wolvius","Gennady V. Roshchupkin","Esther E. Bron"],"pdf_url":"https://arxiv.org/pdf/2311.12836v2.pdf","comment":"Accepted by Medical Image Analysis"},{"id":"http://arxiv.org/abs/2503.02797v1","updated":"2025-03-04T17:15:31Z","published":"2025-03-04T17:15:31Z","title":"A Causal Framework for Aligning Image Quality Metrics and Deep Neural\n  Network Robustness","summary":"  Image quality plays an important role in the performance of deep neural\nnetworks (DNNs) and DNNs have been widely shown to exhibit sensitivity to\nchanges in imaging conditions. Large-scale datasets often contain images under\na wide range of conditions prompting a need to quantify and understand their\nunderlying quality distribution in order to better characterize DNN performance\nand robustness. Aligning the sensitivities of image quality metrics and DNNs\nensures that estimates of quality can act as proxies for image/dataset\ndifficulty independent of the task models trained/evaluated on the data.\nConventional image quality assessment (IQA) seeks to measure and align quality\nrelative to human perceptual judgments, but here we seek a quality measure that\nis not only sensitive to imaging conditions but also well-aligned with DNN\nsensitivities. We first ask whether conventional IQA metrics are also\ninformative of DNN performance. In order to answer this question, we reframe\nIQA from a causal perspective and examine conditions under which quality\nmetrics are predictive of DNN performance. We show theoretically and\nempirically that current IQA metrics are weak predictors of DNN performance in\nthe context of classification. We then use our causal framework to provide an\nalternative formulation and a new image quality metric that is more strongly\ncorrelated with DNN performance and can act as a prior on performance without\ntraining new task models. Our approach provides a means to directly estimate\nthe quality distribution of large-scale image datasets towards characterizing\nthe relationship between dataset composition and DNN performance.\n","authors":["Nathan Drenkow","Mathias Unberath"],"pdf_url":"https://arxiv.org/pdf/2503.02797v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13079v2","updated":"2025-03-04T17:07:15Z","published":"2024-11-20T07:07:42Z","title":"Neural Internal Model Control: Learning a Robust Control Policy via\n  Predictive Error Feedback","summary":"  Accurate motion control in the face of disturbances within complex\nenvironments remains a major challenge in robotics. Classical model-based\napproaches often struggle with nonlinearities and unstructured disturbances,\nwhile RL-based methods can be fragile when encountering unseen scenarios. In\nthis paper, we propose a novel framework, Neural Internal Model Control, which\nintegrates model-based control with RL-based control to enhance robustness. Our\nframework streamlines the predictive model by applying Newton-Euler equations\nfor rigid-body dynamics, eliminating the need to capture complex\nhigh-dimensional nonlinearities. This internal model combines model-free RL\nalgorithms with predictive error feedback. Such a design enables a closed-loop\ncontrol structure to enhance the robustness and generalizability of the control\nsystem. We demonstrate the effectiveness of our framework on both quadrotors\nand quadrupedal robots, achieving superior performance compared to\nstate-of-the-art methods. Furthermore, real-world deployment on a quadrotor\nwith rope-suspended payloads highlights the framework's robustness in\nsim-to-real transfer. Our code is released at\nhttps://github.com/thu-uav/NeuralIMC.\n","authors":["Feng Gao","Chao Yu","Yu Wang","Yi Wu"],"pdf_url":"https://arxiv.org/pdf/2411.13079v2.pdf","comment":"Submitted to RAL"},{"id":"http://arxiv.org/abs/2503.00079v2","updated":"2025-03-04T17:01:11Z","published":"2025-02-27T23:32:03Z","title":"AI Literacy in K-12 and Higher Education in the Wake of Generative AI:\n  An Integrative Review","summary":"  Even though AI literacy has emerged as a prominent education topic in the\nwake of generative AI, its definition remains vague. There is little consensus\namong researchers and practitioners on how to discuss and design AI literacy\ninterventions. The term has been used to describe both learning activities that\ntrain undergraduate students to use ChatGPT effectively and having kindergarten\nchildren interact with social robots. This paper applies an integrative review\nmethod to examine empirical and theoretical AI literacy studies published since\n2020. In synthesizing the 124 reviewed studies, three ways to conceptualize\nliteracy-functional, critical, and indirectly beneficial-and three perspectives\non AI-technical detail, tool, and sociocultural-were identified, forming a\nframework that reflects the spectrum of how AI literacy is approached in\npractice. The framework highlights the need for more specialized terms within\nAI literacy discourse and indicates research gaps in certain AI literacy\nobjectives.\n","authors":["Xingjian Gu","Barbara J. Ericson"],"pdf_url":"https://arxiv.org/pdf/2503.00079v2.pdf","comment":"29 pages, 7 figures; previous version replaced due to incorrect\n  parsing of author name"},{"id":"http://arxiv.org/abs/2503.02784v1","updated":"2025-03-04T16:57:53Z","published":"2025-03-04T16:57:53Z","title":"Do Not Trust Licenses You See -- Dataset Compliance Requires\n  Massive-Scale AI-Powered Lifecycle Tracing","summary":"  This paper argues that a dataset's legal risk cannot be accurately assessed\nby its license terms alone; instead, tracking dataset redistribution and its\nfull lifecycle is essential. However, this process is too complex for legal\nexperts to handle manually at scale. Tracking dataset provenance, verifying\nredistribution rights, and assessing evolving legal risks across multiple\nstages require a level of precision and efficiency that exceeds human\ncapabilities. Addressing this challenge effectively demands AI agents that can\nsystematically trace dataset redistribution, analyze compliance, and identify\nlegal risks. We develop an automated data compliance system called NEXUS and\nshow that AI can perform these tasks with higher accuracy, efficiency, and\ncost-effectiveness than human experts. Our massive legal analysis of 17,429\nunique entities and 8,072 license terms using this approach reveals the\ndiscrepancies in legal rights between the original datasets before\nredistribution and their redistributed subsets, underscoring the necessity of\nthe data lifecycle-aware compliance. For instance, we find that out of 2,852\ndatasets with commercially viable individual license terms, only 605 (21%) are\nlegally permissible for commercialization. This work sets a new standard for AI\ndata governance, advocating for a framework that systematically examines the\nentire lifecycle of dataset redistribution to ensure transparent, legal, and\nresponsible dataset management.\n","authors":["Jaekyeom Kim","Sungryull Sohn","Gerrard Jeongwon Jo","Jihoon Choi","Kyunghoon Bae","Hwayoung Lee","Yongmin Park","Honglak Lee"],"pdf_url":"https://arxiv.org/pdf/2503.02784v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02783v1","updated":"2025-03-04T16:56:34Z","published":"2025-03-04T16:56:34Z","title":"IterPref: Focal Preference Learning for Code Generation via Iterative\n  Debugging","summary":"  Preference learning enhances Code LLMs beyond supervised fine-tuning by\nleveraging relative quality comparisons. Existing methods construct preference\npairs from\n  candidates based on test case success, treating the higher pass rate sample\nas positive and the lower as negative. However, this approach does not pinpoint\nspecific errors in the code, which prevents the model from learning more\ninformative error correction patterns, as aligning failing code as a whole\nlacks the granularity needed to capture meaningful error-resolution\nrelationships. To address these issues, we propose IterPref, a new preference\nalignment framework that mimics human iterative debugging to refine Code LLMs.\nIterPref explicitly locates error regions and aligns the corresponding tokens\nvia a tailored DPO algorithm. To generate informative pairs, we introduce the\nCodeFlow dataset, where samples are iteratively refined until passing tests,\nwith modifications capturing error corrections. Extensive experiments show that\na diverse suite of Code LLMs equipped with IterPref achieves significant\nperformance gains in code generation and improves on challenging tasks like\nBigCodeBench. In-depth analysis reveals that IterPref yields fewer errors. Our\ncode and data will be made publicaly available.\n","authors":["Jie Wu","Haoling Li","Xin Zhang","Jianwen Luo","Yangyu Huang","Ruihang Chu","Yujiu Yang","Scarlett Li"],"pdf_url":"https://arxiv.org/pdf/2503.02783v1.pdf","comment":"The code and data will be released soon"},{"id":"http://arxiv.org/abs/2503.02776v1","updated":"2025-03-04T16:49:37Z","published":"2025-03-04T16:49:37Z","title":"Implicit Bias in LLMs: A Survey","summary":"  Due to the implement of guardrails by developers, Large language models\n(LLMs) have demonstrated exceptional performance in explicit bias tests.\nHowever, bias in LLMs may occur not only explicitly, but also implicitly, much\nlike humans who consciously strive for impartiality yet still harbor implicit\nbias. The unconscious and automatic nature of implicit bias makes it\nparticularly challenging to study. This paper provides a comprehensive review\nof the existing literature on implicit bias in LLMs. We begin by introducing\nkey concepts, theories and methods related to implicit bias in psychology,\nextending them from humans to LLMs. Drawing on the Implicit Association Test\n(IAT) and other psychological frameworks, we categorize detection methods into\nthree primary approaches: word association, task-oriented text generation and\ndecision-making. We divide our taxonomy of evaluation metrics for implicit bias\ninto two categories: single-value-based metrics and comparison-value-based\nmetrics. We classify datasets into two types: sentences with masked tokens and\ncomplete sentences, incorporating datasets from various domains to reflect the\nbroad application of LLMs. Although research on mitigating implicit bias in\nLLMs is still limited, we summarize existing efforts and offer insights on\nfuture challenges. We aim for this work to serve as a clear guide for\nresearchers and inspire innovative ideas to advance exploration in this task.\n","authors":["Xinru Lin","Luyang Li"],"pdf_url":"https://arxiv.org/pdf/2503.02776v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14827v2","updated":"2025-03-04T16:43:01Z","published":"2025-02-20T18:45:00Z","title":"Exploring Advanced Techniques for Visual Question Answering: A\n  Comprehensive Comparison","summary":"  Visual Question Answering (VQA) has emerged as a pivotal task in the\nintersection of computer vision and natural language processing, requiring\nmodels to understand and reason about visual content in response to natural\nlanguage questions. Analyzing VQA datasets is essential for developing robust\nmodels that can handle the complexities of multimodal reasoning. Several\napproaches have been developed to examine these datasets, each offering\ndistinct perspectives on question diversity, answer distribution, and\nvisual-textual correlations. Despite significant progress, existing VQA models\nface challenges related to dataset bias, limited model complexity, commonsense\nreasoning gaps, rigid evaluation methods, and generalization to real world\nscenarios. This paper offers a detailed study of the original VQA dataset,\nbaseline models and methods along with a comparative study of five advanced VQA\nmodels, ABC-CNN, KICNLE, Masked Vision and Language Modeling, BLIP-2, and OFA,\neach employing distinct methods to address these ongoing challenges.\n","authors":["Aiswarya Baby","Tintu Thankom Koshy"],"pdf_url":"https://arxiv.org/pdf/2502.14827v2.pdf","comment":"8 pages, No figures"},{"id":"http://arxiv.org/abs/2503.02773v1","updated":"2025-03-04T16:42:46Z","published":"2025-03-04T16:42:46Z","title":"Prime Convolutional Model: Breaking the Ground for Theoretical\n  Explainability","summary":"  In this paper, we propose a new theoretical approach to Explainable AI.\nFollowing the Scientific Method, this approach consists in formulating on the\nbasis of empirical evidence, a mathematical model to explain and predict the\nbehaviors of Neural Networks. We apply the method to a case study created in a\ncontrolled environment, which we call Prime Convolutional Model (p-Conv for\nshort). p-Conv operates on a dataset consisting of the first one million\nnatural numbers and is trained to identify the congruence classes modulo a\ngiven integer $m$. Its architecture uses a convolutional-type neural network\nthat contextually processes a sequence of $B$ consecutive numbers to each\ninput. We take an empirical approach and exploit p-Conv to identify the\ncongruence classes of numbers in a validation set using different values for\n$m$ and $B$. The results show that the different behaviors of p-Conv (i.e.,\nwhether it can perform the task or not) can be modeled mathematically in terms\nof $m$ and $B$. The inferred mathematical model reveals interesting patterns\nable to explain when and why p-Conv succeeds in performing task and, if not,\nwhich error pattern it follows.\n","authors":["Francesco Panelli","Doaa Almhaithawi","Tania Cerquitelli","Alessandro Bellini"],"pdf_url":"https://arxiv.org/pdf/2503.02773v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00081v3","updated":"2025-03-04T16:42:06Z","published":"2024-09-30T17:24:21Z","title":"From homeostasis to resource sharing: Biologically and economically\n  aligned multi-objective multi-agent AI safety benchmarks","summary":"  Developing safe, aligned agentic AI systems requires comprehensive empirical\ntesting, yet many existing benchmarks neglect crucial themes aligned with\nbiology and economics, both time-tested fundamental sciences describing our\nneeds and preferences. To address this gap, the present work focuses on\nintroducing biologically and economically motivated themes that have been\nneglected in current mainstream discussions on AI safety - namely a set of\nmulti-objective, multi-agent alignment benchmarks that emphasize homeostasis\nfor bounded and biological objectives, diminishing returns for unbounded,\ninstrumental, and business objectives, sustainability principle, and resource\nsharing. We implemented eight main benchmark environments on the above themes,\nto illustrate key pitfalls and challenges in agentic AI-s, such as unboundedly\nmaximizing a homeostatic objective, over-optimizing one objective at the\nexpense of others, neglecting safety constraints, or depleting shared\nresources.\n","authors":["Roland Pihlakas","Joel Pyykkö"],"pdf_url":"https://arxiv.org/pdf/2410.00081v3.pdf","comment":"20 pages, 13 figures, 1 tables"},{"id":"http://arxiv.org/abs/2503.00320v2","updated":"2025-03-04T16:36:54Z","published":"2025-03-01T03:15:13Z","title":"Shifting Power: Leveraging LLMs to Simulate Human Aversion in ABMs of\n  Bilateral Financial Exchanges, A bond market study","summary":"  Bilateral markets, such as those for government bonds, involve decentralized\nand opaque transactions between market makers (MMs) and clients, posing\nsignificant challenges for traditional modeling approaches. To address these\ncomplexities, we introduce TRIBE an agent-based model augmented with a large\nlanguage model (LLM) to simulate human-like decision-making in trading\nenvironments. TRIBE leverages publicly available data and stylized facts to\ncapture realistic trading dynamics, integrating human biases like risk aversion\nand ambiguity sensitivity into the decision-making processes of agents. Our\nresearch yields three key contributions: first, we demonstrate that integrating\nLLMs into agent-based models to enhance client agency is feasible and enriches\nthe simulation of agent behaviors in complex markets; second, we find that even\nslight trade aversion encoded within the LLM leads to a complete cessation of\ntrading activity, highlighting the sensitivity of market dynamics to agents'\nrisk profiles; third, we show that incorporating human-like variability shifts\npower dynamics towards clients and can disproportionately affect the entire\nsystem, often resulting in systemic agent collapse across simulations. These\nfindings underscore the emergent properties that arise when introducing\nstochastic, human-like decision processes, revealing new system behaviors that\nenhance the realism and complexity of artificial societies.\n","authors":["Alicia Vidler","Toby Walsh"],"pdf_url":"https://arxiv.org/pdf/2503.00320v2.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2502.17403v2","updated":"2025-03-04T16:36:52Z","published":"2025-02-24T18:30:36Z","title":"Large Language Models are Powerful EHR Encoders","summary":"  Electronic Health Records (EHRs) offer rich potential for clinical\nprediction, yet their inherent complexity and heterogeneity pose significant\nchallenges for traditional machine learning approaches. Domain-specific EHR\nfoundation models trained on large collections of unlabeled EHR data have\ndemonstrated promising improvements in predictive accuracy and generalization;\nhowever, their training is constrained by limited access to diverse,\nhigh-quality datasets and inconsistencies in coding standards and healthcare\npractices. In this study, we explore the possibility of using general-purpose\nLarge Language Models (LLMs) based embedding methods as EHR encoders. By\nserializing patient records into structured Markdown text, transforming codes\ninto human-readable descriptors, we leverage the extensive generalization\ncapabilities of LLMs pretrained on vast public corpora, thereby bypassing the\nneed for proprietary medical datasets. We systematically evaluate two\nstate-of-the-art LLM-embedding models, GTE-Qwen2-7B-Instruct and\nLLM2Vec-Llama3.1-8B-Instruct, across 15 diverse clinical prediction tasks from\nthe EHRSHOT benchmark, comparing their performance to an EHRspecific foundation\nmodel, CLIMBR-T-Base, and traditional machine learning baselines. Our results\ndemonstrate that LLM-based embeddings frequently match or exceed the\nperformance of specialized models, even in few-shot settings, and that their\neffectiveness scales with the size of the underlying LLM and the available\ncontext window. Overall, our findings demonstrate that repurposing LLMs for EHR\nencoding offers a scalable and effective approach for clinical prediction,\ncapable of overcoming the limitations of traditional EHR modeling and\nfacilitating more interoperable and generalizable healthcare applications.\n","authors":["Stefan Hegselmann","Georg von Arnim","Tillmann Rheude","Noel Kronenberg","David Sontag","Gerhard Hindricks","Roland Eils","Benjamin Wild"],"pdf_url":"https://arxiv.org/pdf/2502.17403v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.06435v2","updated":"2025-03-04T16:22:34Z","published":"2024-12-09T12:21:20Z","title":"Simulating Human-like Daily Activities with Desire-driven Autonomy","summary":"  Desires motivate humans to interact autonomously with the complex world. In\ncontrast, current AI agents require explicit task specifications, such as\ninstructions or reward functions, which constrain their autonomy and behavioral\ndiversity. In this paper, we introduce a Desire-driven Autonomous Agent (D2A)\nthat can enable a large language model (LLM) to autonomously propose and select\ntasks, motivated by satisfying its multi-dimensional desires. Specifically, the\nmotivational framework of D2A is mainly constructed by a dynamic Value System,\ninspired by the Theory of Needs. It incorporates an understanding of human-like\ndesires, such as the need for social interaction, personal fulfillment, and\nself-care. At each step, the agent evaluates the value of its current state,\nproposes a set of candidate activities, and selects the one that best aligns\nwith its intrinsic motivations. We conduct experiments on Concordia, a\ntext-based simulator, to demonstrate that our agent generates coherent,\ncontextually relevant daily activities while exhibiting variability and\nadaptability similar to human behavior. A comparative analysis with other\nLLM-based agents demonstrates that our approach significantly enhances the\nrationality of the simulated activities.\n","authors":["Yiding Wang","Yuxuan Chen","Fangwei Zhong","Long Ma","Yizhou Wang"],"pdf_url":"https://arxiv.org/pdf/2412.06435v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02749v1","updated":"2025-03-04T16:14:16Z","published":"2025-03-04T16:14:16Z","title":"Improving Oil Slick Trajectory Simulations with Bayesian Optimization","summary":"  Accurate simulations of oil spill trajectories are essential for supporting\npractitioners' response and mitigating environmental and socioeconomic impacts.\nNumerical models, such as MEDSLIK-II, simulate advection, dispersion, and\ntransformation processes of oil particles. However, simulations heavily rely on\naccurate parameter tuning, still based on expert knowledge and manual\ncalibration. To overcome these limitations, we integrate the MEDSLIK-II\nnumerical oil spill model with a Bayesian optimization framework to iteratively\nestimate the best physical parameter configuration that yields simulation\ncloser to satellite observations of the slick. We focus on key parameters, such\nas horizontal diffusivity and drift factor, maximizing the Fraction Skill Score\n(FSS) as a measure of spatio-temporal overlap between simulated and observed\noil distributions. We validate the framework for the Baniyas oil incident that\noccurred in Syria between August 23 and September 4, 2021, which released over\n12,000 $m^3$ of oil. We show that, on average, the proposed approach\nsystematically improves the FSS from 5.82% to 11.07% compared to control\nsimulations initialized with default parameters. The optimization results in\nconsistent improvement across multiple time steps, particularly during periods\nof increased drift variability, demonstrating the robustness of our method in\ndynamic environmental conditions.\n","authors":["Gabriele Accarino","Marco M. De Carlo","Igor Atake","Donatello Elia","Anusha L. Dissanayake","Antonio Augusto Sepp Neves","Juan Peña Ibañez","Italo Epicoco","Paola Nassisi","Sandro Fiore","Giovanni Coppini"],"pdf_url":"https://arxiv.org/pdf/2503.02749v1.pdf","comment":"29 pages, 10 figures, 3 tables, research paper"},{"id":"http://arxiv.org/abs/2503.02733v1","updated":"2025-03-04T15:54:57Z","published":"2025-03-04T15:54:57Z","title":"UAR-NVC: A Unified AutoRegressive Framework for Memory-Efficient Neural\n  Video Compression","summary":"  Implicit Neural Representations (INRs) have demonstrated significant\npotential in video compression by representing videos as neural networks.\nHowever, as the number of frames increases, the memory consumption for training\nand inference increases substantially, posing challenges in\nresource-constrained scenarios. Inspired by the success of traditional video\ncompression frameworks, which process video frame by frame and can efficiently\ncompress long videos, we adopt this modeling strategy for INRs to decrease\nmemory consumption, while aiming to unify the frameworks from the perspective\nof timeline-based autoregressive modeling. In this work, we present a novel\nunderstanding of INR models from an autoregressive (AR) perspective and\nintroduce a Unified AutoRegressive Framework for memory-efficient Neural Video\nCompression (UAR-NVC). UAR-NVC integrates timeline-based and INR-based neural\nvideo compression under a unified autoregressive paradigm. It partitions videos\ninto several clips and processes each clip using a different INR model\ninstance, leveraging the advantages of both compression frameworks while\nallowing seamless adaptation to either in form. To further reduce temporal\nredundancy between clips, we design two modules to optimize the initialization,\ntraining, and compression of these model parameters. UAR-NVC supports\nadjustable latencies by varying the clip length. Extensive experimental results\ndemonstrate that UAR-NVC, with its flexible video clip setting, can adapt to\nresource-constrained environments and significantly improve performance\ncompared to different baseline models.\n","authors":["Jia Wang","Xinfeng Zhang","Gai Zhang","Jun Zhu","Lv Tang","Li Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.02733v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02720v1","updated":"2025-03-04T15:36:19Z","published":"2025-03-04T15:36:19Z","title":"Vibration-Assisted Hysteresis Mitigation for Achieving High Compensation\n  Efficiency","summary":"  Tendon-sheath mechanisms (TSMs) are widely used in minimally invasive\nsurgical (MIS) applications, but their inherent hysteresis-caused by friction,\nbacklash, and tendon elongation-leads to significant tracking errors.\nConventional modeling and compensation methods struggle with these\nnonlinearities and require extensive parameter tuning. To address this, we\npropose a vibration-assisted hysteresis compensation approach, where controlled\nvibrational motion is applied along the tendon's movement direction to mitigate\nfriction and reduce dead zones. Experimental results demonstrate that the\nexerted vibration consistently reduces hysteresis across all tested\nfrequencies, decreasing RMSE by up to 23.41% (from 2.2345 mm to 1.7113 mm) and\nimproving correlation, leading to more accurate trajectory tracking. When\ncombined with a Temporal Convolutional Network (TCN)-based compensation model,\nvibration further enhances performance, achieving an 85.2% reduction in MAE\n(from 1.334 mm to 0.1969 mm). Without vibration, the TCN-based approach still\nreduces MAE by 72.3% (from 1.334 mm to 0.370 mm) under the same parameter\nsettings. These findings confirm that vibration effectively mitigates\nhysteresis, improving trajectory accuracy and enabling more efficient\ncompensation models with fewer trainable parameters. This approach provides a\nscalable and practical solution for TSM-based robotic applications,\nparticularly in MIS.\n","authors":["Myeongbo Park","Chunggil An","Junhyun Park","Jonghyun Kang","Minho Hwang"],"pdf_url":"https://arxiv.org/pdf/2503.02720v1.pdf","comment":"8 pages, 7 figures, and 2 tables"},{"id":"http://arxiv.org/abs/2502.20704v3","updated":"2025-03-04T15:30:35Z","published":"2025-02-28T04:25:42Z","title":"Fuzzy Speculative Decoding for a Tunable Accuracy-Runtime Tradeoff","summary":"  Speculative Decoding (SD) enforces strict distributional equivalence to the\ntarget model, limiting potential speed ups as distributions of near-equivalence\nachieve comparable outcomes in many cases. Furthermore, enforcing\ndistributional equivalence means that users are unable to trade deviations from\nthe target model distribution for further inference speed gains. To address\nthese limitations, we introduce Fuzzy Speculative Decoding (FSD) - a decoding\nalgorithm that generalizes SD by accepting candidate tokens purely based on the\ndivergences between the target and draft model distributions. By allowing for\ncontrolled divergence from the target model, FSD enables users to flexibly\ntrade generation quality for inference speed. Across several benchmarks, our\nmethod is able to achieve significant runtime improvements of over 5 tokens per\nsecond faster than SD at only an approximate 2% absolute reduction in benchmark\naccuracy. In many cases, FSD is even able to match SD benchmark accuracy at\nover 2 tokens per second faster, demonstrating that distributional equivalence\nis not necessary to maintain target model performance.\n","authors":["Maximilian Holsman","Yukun Huang","Bhuwan Dhingra"],"pdf_url":"https://arxiv.org/pdf/2502.20704v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09230v3","updated":"2025-03-04T15:26:30Z","published":"2024-10-11T20:06:21Z","title":"Improving Semantic Understanding in Speech Language Models via\n  Brain-tuning","summary":"  Speech language models align with human brain responses to natural language\nto an impressive degree. However, current models rely heavily on low-level\nspeech features, indicating they lack brain-relevant semantics which limits\ntheir utility as model organisms of semantic processing in the brain. In this\nwork, we address this limitation by inducing brain-relevant bias directly into\nthe models via fine-tuning with fMRI recordings of people listening to natural\nstories, a process we name brain-tuning. After testing it on 3 different\npretrained model families, we show that brain-tuning not only improves overall\nalignment with new brain recordings in semantic language regions, but also\nreduces the reliance on low-level speech features for this alignment.\nExcitingly, we further show that brain-tuning leads to 1) consistent\nimprovements in performance on a range of downstream tasks and 2) a\nrepresentational space with increased semantic preference. Our results provide\nconverging evidence, for the first time, that incorporating brain signals into\nthe training of language models improves the models' semantic understanding.\n","authors":["Omer Moussa","Dietrich Klakow","Mariya Toneva"],"pdf_url":"https://arxiv.org/pdf/2410.09230v3.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.18658v2","updated":"2025-03-04T15:26:19Z","published":"2025-02-25T21:37:25Z","title":"Assistance or Disruption? Exploring and Evaluating the Design and\n  Trade-offs of Proactive AI Programming Support","summary":"  AI programming tools enable powerful code generation, and recent prototypes\nattempt to reduce user effort with proactive AI agents, but their impact on\nprogramming workflows remains unexplored. We introduce and evaluate\nCodellaborator, a design probe LLM agent that initiates programming assistance\nbased on editor activities and task context. We explored three interface\nvariants to assess trade-offs between increasingly salient AI support:\nprompt-only, proactive agent, and proactive agent with presence and context\n(Codellaborator). In a within-subject study (N=18), we find that proactive\nagents increase efficiency compared to prompt-only paradigm, but also incur\nworkflow disruptions. However, presence indicators and interaction context\nsupport alleviated disruptions and improved users' awareness of AI processes.\nWe underscore trade-offs of Codellaborator on user control, ownership, and code\nunderstanding, emphasizing the need to adapt proactivity to programming\nprocesses. Our research contributes to the design exploration and evaluation of\nproactive AI systems, presenting design implications on AI-integrated\nprogramming workflow.\n","authors":["Kevin Pu","Daniel Lazaro","Ian Arawjo","Haijun Xia","Ziang Xiao","Tovi Grossman","Yan Chen"],"pdf_url":"https://arxiv.org/pdf/2502.18658v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2103.03223v5","updated":"2025-03-04T15:20:55Z","published":"2021-03-04T18:51:06Z","title":"A Comparative Evaluation of Quantification Methods","summary":"  Quantification represents the problem of estimating the distribution of class\nlabels on unseen data. It also represents a growing research field in\nsupervised machine learning, for which a large variety of different algorithms\nhas been proposed in recent years. However, a comprehensive empirical\ncomparison of quantification methods that supports algorithm selection is not\navailable yet. In this work, we close this research gap by conducting a\nthorough empirical performance comparison of 24 different quantification\nmethods on overall more than 40 data sets, considering binary as well as\nmulticlass quantification settings. We observe that no single algorithm\ngenerally outperforms all competitors, but identify a group of methods\nincluding the threshold selection-based Median Sweep and TSMax methods, the DyS\nframework including the HDy method, Forman's mixture model, and Friedman's\nmethod that performs best in the binary setting. For the multiclass setting, we\nobserve that a different, broad group of algorithms yields good performance,\nincluding the HDx method, the Generalized Probabilistic Adjusted Count, the\nreadme method, the energy distance minimization method, the EM algorithm for\nquantification, and Friedman's method. We also find that tuning the underlying\nclassifiers has in most cases only a limited impact on the quantification\nperformance. More generally, we find that the performance on multiclass\nquantification is inferior to the results obtained in the binary setting. Our\nresults can guide practitioners who intend to apply quantification algorithms\nand help researchers to identify opportunities for future research.\n","authors":["Tobias Schumacher","Markus Strohmaier","Florian Lemmerich"],"pdf_url":"https://arxiv.org/pdf/2103.03223v5.pdf","comment":"41 pages, 18 figures, 9 tables"},{"id":"http://arxiv.org/abs/2503.02703v1","updated":"2025-03-04T15:18:50Z","published":"2025-03-04T15:18:50Z","title":"Generative Tools for Graphical Assets: Empirical Guidelines based on\n  Game Designers' and Developers' Preferences","summary":"  Graphical assets play an important role in the design and development of\ngames. There is potential in the use of generative tools, to aid in creating\ngraphical assets, thus improving game design and development pipelines.\nHowever, there is little research to address how the generative methods can fit\ninto the wider pipeline. We conducted a user study with 16 game designers and\ndevelopers to examine their preferences regarding generative tools for\ngraphical assets. The findings highlight that early design stage is preferred\nby all participants (mean values above 0.67 and p < .001 for early stages).\nDesigners and developers prefer to use such tools for creating large amounts of\nvariations at the cost of quality as they can improve the quality of the\nartefacts once they generate a suitable asset (mean value 0.17 where 1 is high\nquality, p < .001). They also strongly (mean value .78, p < .001) raised the\nneed for better integration of such tools in existing design and development\nenvironments and the need for the outputs to be in common data formats, to be\nmanipulatable and integrate smoothly into existing environments (mean 3.5 out\nof 5, p = .004). The study also highlights the requirement for further emphasis\non the needs of the users to incorporate these tools effectively in existing\npipelines. Informed by these results, we provide a set of guidelines for\ncreating tools that meet the expectations and needs of game designers and\ndevelopers.\n","authors":["Kaisei Fukaya","Damon Daylamani-Zad","Harry Agius"],"pdf_url":"https://arxiv.org/pdf/2503.02703v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02701v1","updated":"2025-03-04T15:17:57Z","published":"2025-03-04T15:17:57Z","title":"MindBridge: Scalable and Cross-Model Knowledge Editing via\n  Memory-Augmented Modality","summary":"  Knowledge editing is a technique for efficiently and accurately updating the\nknowledge of large language models (LLMs) to alleviate obsolescence and correct\nerrors. However, most existing methods overfit to specific models, causing\nedited knowledge to be discarded during each LLM update and requiring frequent\nre-editing, which is particularly burdensome in today's rapidly evolving\nopen-source community. To address this issue, we propose the problem of\ncross-model knowledge editing and introduce MindBridge, a scalable solution\ninspired by the low coupling between modality processing and LLMs in\nmulti-modal models. MindBridge introduces the novel concept of memory modality,\nwhich encodes edited knowledge as an independent modality. It first performs\nLLM-agnostic pre-training of the memory modality and then integrates it with\nvarious LLMs. Extensive experiments on multiple LLMs and popular knowledge\nediting datasets demonstrate that MindBridge achieves superior performance even\nin editing tens of thousands of knowledge entries and can flexibly adapt to\ndifferent LLMs. Our code is available at\nhttps://github.com/CrashBugger/MindBridge.\n","authors":["Shuaike Li","Kai Zhang","Qi Liu","Enhong Chen"],"pdf_url":"https://arxiv.org/pdf/2503.02701v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.15753v2","updated":"2025-03-04T15:17:17Z","published":"2024-06-22T06:43:51Z","title":"The Perils of Optimizing Learned Reward Functions: Low Training Error\n  Does Not Guarantee Low Regret","summary":"  In reinforcement learning, specifying reward functions that capture the\nintended task can be very challenging. Reward learning aims to address this\nissue by learning the reward function. However, a learned reward model may have\na low error on the data distribution, and yet subsequently produce a policy\nwith large regret. We say that such a reward model has an error-regret\nmismatch. The main source of an error-regret mismatch is the distributional\nshift that commonly occurs during policy optimization. In this paper, we\nmathematically show that a sufficiently low expected test error of the reward\nmodel guarantees low worst-case regret, but that for any fixed expected test\nerror, there exist realistic data distributions that allow for error-regret\nmismatch to occur. We then show that similar problems persist even when using\npolicy regularization techniques, commonly employed in methods such as RLHF. We\nhope our results stimulate the theoretical and empirical study of improved\nmethods to learn reward models, and better ways to measure their quality\nreliably.\n","authors":["Lukas Fluri","Leon Lang","Alessandro Abate","Patrick Forré","David Krueger","Joar Skalse"],"pdf_url":"https://arxiv.org/pdf/2406.15753v2.pdf","comment":"70 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.18495v2","updated":"2025-03-04T15:16:52Z","published":"2025-02-19T01:37:24Z","title":"A Comprehensive Survey on Composed Image Retrieval","summary":"  Composed Image Retrieval (CIR) is an emerging yet challenging task that\nallows users to search for target images using a multimodal query, comprising a\nreference image and a modification text specifying the user's desired changes\nto the reference image. Given its significant academic and practical value, CIR\nhas become a rapidly growing area of interest in the computer vision and\nmachine learning communities, particularly with the advances in deep learning.\nTo the best of our knowledge, there is currently no comprehensive review of CIR\nto provide a timely overview of this field. Therefore, we synthesize insights\nfrom over 120 publications in top conferences and journals, including ACM TOIS,\nSIGIR, and CVPR In particular, we systematically categorize existing supervised\nCIR and zero-shot CIR models using a fine-grained taxonomy. For a comprehensive\nreview, we also briefly discuss approaches for tasks closely related to CIR,\nsuch as attribute-based CIR and dialog-based CIR. Additionally, we summarize\nbenchmark datasets for evaluation and analyze existing supervised and zero-shot\nCIR methods by comparing experimental results across multiple datasets.\nFurthermore, we present promising future directions in this field, offering\npractical insights for researchers interested in further exploration. The\ncurated collection of related works is maintained and continuously updated in\nhttps://github.com/haokunwen/Awesome-Composed-Image-Retrieval.\n","authors":["Xuemeng Song","Haoqiang Lin","Haokun Wen","Bohan Hou","Mingzhu Xu","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2502.18495v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.20903v4","updated":"2025-03-04T15:05:02Z","published":"2024-12-30T12:29:02Z","title":"WalkVLM:Aid Visually Impaired People Walking by Vision Language Model","summary":"  Approximately 200 million individuals around the world suffer from varying\ndegrees of visual impairment, making it crucial to leverage AI technology to\noffer walking assistance for these people. With the recent progress of\nvision-language models (VLMs), applying VLMs to offer walking guidance has\nbecome popular. However, the existing methods of walking guidance are mainly\nbased on self-curated question-answering datasets that are not publicly\naccessible, without a standardized benchmark for training or evaluation.\nMoreover, walking assistance often requires real-time streaming video analysis\nand the generation of concise yet informative reminders, making VLMs struggle\ndue to excessive responses and low efficiency in inferences. In this paper, we\nintroduce the first large-scale dataset dedicated to walking assistance,\ncomprising 12,000 video-annotation pairs, to provide a unified benchmark for\ntraining and evaluating systems to help visually-impaired individuals walk.\nFurthermore, a WalkVLM model is proposed, which employs chain of thought for\nhierarchical planning to generate concise but informative reminders and\nutilizes temporal-aware adaptive prediction to reduce the temporal redundancy\nof reminders. Finally, we have established a solid benchmark for blind walking\ntask and verified the advantages of WalkVLM in stream video processing for this\ntask compared to other VLMs. Our dataset and code are available at\nhttps://walkvlm2024.github.io.\n","authors":["Zhiqiang Yuan","Ting Zhang","Ying Deng","Jiapei Zhang","Yeshuang Zhu","Zexi Jia","Jie Zhou","Jinchao Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.20903v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02691v1","updated":"2025-03-04T15:03:47Z","published":"2025-03-04T15:03:47Z","title":"Memory Efficient Continual Learning for Edge-Based Visual Anomaly\n  Detection","summary":"  Visual Anomaly Detection (VAD) is a critical task in computer vision with\nnumerous real-world applications. However, deploying these models on edge\ndevices presents significant challenges, such as constrained computational and\nmemory resources. Additionally, dynamic data distributions in real-world\nsettings necessitate continuous model adaptation, further complicating\ndeployment under limited resources. To address these challenges, we present a\nnovel investigation into the problem of Continual Learning for Visual Anomaly\nDetection (CLAD) on edge devices. We evaluate the STFPM approach, given its low\nmemory footprint on edge devices, which demonstrates good performance when\ncombined with the Replay approach. Furthermore, we propose to study the\nbehavior of a recently proposed approach, PaSTe, specifically designed for the\nedge but not yet explored in the Continual Learning context. Our results show\nthat PaSTe is not only a lighter version of STPFM, but it also achieves\nsuperior anomaly detection performance, improving the f1 pixel performance by\n10% with the Replay technique. In particular, the structure of PaSTe allows us\nto test it using a series of Compressed Replay techniques, reducing memory\noverhead by a maximum of 91.5% compared to the traditional Replay for STFPM.\nOur study proves the feasibility of deploying VAD models that adapt and learn\nincrementally on CLAD scenarios on resource-constrained edge devices.\n","authors":["Manuel Barusco","Lorenzo D'Antoni","Davide Dalle Pezze","Francesco Borsatti","Gian Antonio Susto"],"pdf_url":"https://arxiv.org/pdf/2503.02691v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2303.11858v3","updated":"2025-03-04T15:03:02Z","published":"2023-03-21T13:59:15Z","title":"Modeling Relational Patterns for Logical Query Answering over Knowledge\n  Graphs","summary":"  Answering first-order logical (FOL) queries over knowledge graphs (KG)\nremains a challenging task mainly due to KG incompleteness. Query embedding\napproaches this problem by computing the low-dimensional vector representations\nof entities, relations, and logical queries. KGs exhibit relational patterns\nsuch as symmetry and composition and modeling the patterns can further enhance\nthe performance of query embedding models. However, the role of such patterns\nin answering FOL queries by query embedding models has not been yet studied in\nthe literature. In this paper, we fill in this research gap and empower FOL\nqueries reasoning with pattern inference by introducing an inductive bias that\nallows for learning relation patterns. To this end, we develop a novel query\nembedding method, RoConE, that defines query regions as geometric cones and\nalgebraic query operators by rotations in complex space. RoConE combines the\nadvantages of Cone as a well-specified geometric representation for query\nembedding, and also the rotation operator as a powerful algebraic operation for\npattern inference. Our experimental results on several benchmark datasets\nconfirm the advantage of relational patterns for enhancing logical query\nanswering task.\n","authors":["Yunjie He","Mojtaba Nayyeri","Bo Xiong","Yuqicheng Zhu","Evgeny Kharlamov","Steffen Staab"],"pdf_url":"https://arxiv.org/pdf/2303.11858v3.pdf","comment":"The results reported in this paper are included in our accepted paper\n  arXiv:2407.09212 at ECAI 2024"},{"id":"http://arxiv.org/abs/2503.02687v1","updated":"2025-03-04T15:02:07Z","published":"2025-03-04T15:02:07Z","title":"Class-Aware PillarMix: Can Mixed Sample Data Augmentation Enhance 3D\n  Object Detection with Radar Point Clouds?","summary":"  Due to the significant effort required for data collection and annotation in\n3D perception tasks, mixed sample data augmentation (MSDA) has been widely\nstudied to generate diverse training samples by mixing existing data. Recently,\nmany MSDA techniques have been developed for point clouds, but they mainly\ntarget LiDAR data, leaving their application to radar point clouds largely\nunexplored. In this paper, we examine the feasibility of applying existing MSDA\nmethods to radar point clouds and identify several challenges in adapting these\ntechniques. These obstacles stem from the radar's irregular angular\ndistribution, deviations from a single-sensor polar layout in multi-radar\nsetups, and point sparsity. To address these issues, we propose Class-Aware\nPillarMix (CAPMix), a novel MSDA approach that applies MixUp at the pillar\nlevel in 3D point clouds, guided by class labels. Unlike methods that rely a\nsingle mix ratio to the entire sample, CAPMix assigns an independent ratio to\neach pillar, boosting sample diversity. To account for the density of different\nclasses, we use class-specific distributions: for dense objects (e.g., large\nvehicles), we skew ratios to favor points from another sample, while for sparse\nobjects (e.g., pedestrians), we sample more points from the original. This\nclass-aware mixing retains critical details and enriches each sample with new\ninformation, ultimately generating more diverse training data. Experimental\nresults demonstrate that our method not only significantly boosts performance\nbut also outperforms existing MSDA approaches across two datasets (Bosch Street\nand K-Radar). We believe that this straightforward yet effective approach will\nspark further investigation into MSDA techniques for radar data.\n","authors":["Miao Zhang","Sherif Abdulatif","Benedikt Loesch","Marco Altmann","Bin Yang"],"pdf_url":"https://arxiv.org/pdf/2503.02687v1.pdf","comment":"8 pages, 6 figures, 4 tables, submitted to 2025 IEEE/RSJ\n  International Conference on Intelligent Robots and Systems (IROS 2025)"},{"id":"http://arxiv.org/abs/2503.02686v1","updated":"2025-03-04T14:58:59Z","published":"2025-03-04T14:58:59Z","title":"Seeding for Success: Skill and Stochasticity in Tabletop Games","summary":"  Games often incorporate random elements in the form of dice or shuffled card\ndecks. This randomness is a key contributor to the player experience and the\nvariety of game situations encountered. There is a tension between a level of\nrandomness that makes the game interesting and contributes to the player\nenjoyment of a game, and a level at which the outcome itself is effectively\nrandom and the game becomes dull. The optimal level for a game will depend on\nthe design goals and target audience. We introduce a new technique to quantify\nthe level of randomness in game outcome and use it to compare 15 tabletop games\nand disentangle the different contributions to the overall randomness from\nspecific parts of some games. We further explore the interaction between game\nrandomness and player skill, and how this innate randomness can affect error\nanalysis in common game experiments.\n","authors":["James Goodman","Diego Perez-Liebana","Simon Lucas"],"pdf_url":"https://arxiv.org/pdf/2503.02686v1.pdf","comment":"Published in IEEE Transactions on Games, 2025"},{"id":"http://arxiv.org/abs/2503.02682v1","updated":"2025-03-04T14:54:45Z","published":"2025-03-04T14:54:45Z","title":"MPO: Boosting LLM Agents with Meta Plan Optimization","summary":"  Recent advancements in large language models (LLMs) have enabled LLM-based\nagents to successfully tackle interactive planning tasks. However, despite\ntheir successes, existing approaches often suffer from planning hallucinations\nand require retraining for each new agent. To address these challenges, we\npropose the Meta Plan Optimization (MPO) framework, which enhances agent\nplanning capabilities by directly incorporating explicit guidance. Unlike\nprevious methods that rely on complex knowledge, which either require\nsignificant human effort or lack quality assurance, MPO leverages high-level\ngeneral guidance through meta plans to assist agent planning and enables\ncontinuous optimization of the meta plans based on feedback from the agent's\ntask execution. Our experiments conducted on two representative tasks\ndemonstrate that MPO significantly outperforms existing baselines. Moreover,\nour analysis indicates that MPO provides a plug-and-play solution that enhances\nboth task completion efficiency and generalization capabilities in previous\nunseen scenarios.\n","authors":["Weimin Xiong","Yifan Song","Qingxiu Dong","Bingchan Zhao","Feifan Song","Xun Wang","Sujian Li"],"pdf_url":"https://arxiv.org/pdf/2503.02682v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02675v1","updated":"2025-03-04T14:46:34Z","published":"2025-03-04T14:46:34Z","title":"State of play and future directions in industrial computer vision AI\n  standards","summary":"  The recent tremendous advancements in the areas of Artificial Intelligence\n(AI) and Deep Learning (DL) have also resulted into corresponding remarkable\nprogress in the field of Computer Vision (CV), showcasing robust technological\nsolutions in a wide range of application sectors of high industrial interest\n(e.g., healthcare, autonomous driving, automation, etc.). Despite the\noutstanding performance of CV systems in specific domains, their development\nand exploitation at industrial-scale necessitates, among other, the addressing\nof requirements related to the reliability, transparency, trustworthiness,\nsecurity, safety, and robustness of the developed AI models. The latter raises\nthe imperative need for the development of efficient, comprehensive and\nwidely-adopted industrial standards. In this context, this study investigates\nthe current state of play regarding the development of industrial computer\nvision AI standards, emphasizing on critical aspects, like model\ninterpretability, data quality, and regulatory compliance. In particular, a\nsystematic analysis of launched and currently developing CV standards, proposed\nby the main international standardization bodies (e.g. ISO/IEC, IEEE, DIN,\netc.) is performed. The latter is complemented by a comprehensive discussion on\nthe current challenges and future directions observed in this regularization\nendeavor.\n","authors":["Artemis Stefanidou","Panagiotis Radoglou-Grammatikis","Vasileios Argyriou","Panagiotis Sarigiannidis","Iraklis Varlamis","Georgios Th. Papadopoulos"],"pdf_url":"https://arxiv.org/pdf/2503.02675v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.06057v3","updated":"2025-03-04T14:33:50Z","published":"2024-07-08T15:59:44Z","title":"Variational Best-of-N Alignment","summary":"  Best-of-N (BoN) is a popular and effective algorithm for aligning language\nmodels to human preferences. The algorithm works as follows: at inference time,\nN samples are drawn from the language model, and the sample with the highest\nreward, as judged by a reward model, is returned as the output. Despite its\neffectiveness, BoN is computationally expensive; it reduces sampling throughput\nby a factor of N. To make BoN more efficient at inference time, one strategy is\nto fine-tune the language model to mimic what BoN does during inference. To\nachieve this, we derive the distribution induced by the BoN algorithm. We then\npropose to fine-tune the language model to minimize backward KL divergence to\nthe BoN distribution. Our approach is analogous to mean-field variational\ninference and, thus, we term it variational BoN (vBoN). To the extent this\nfine-tuning is successful and we end up with a good approximation, we have\nreduced the inference cost by a factor of N. Our experiments on controlled\ngeneration and summarization tasks show that BoN is the most effective\nalignment method, and our variational approximation to BoN achieves the closest\nperformance to BoN and surpasses models fine-tuned using the standard\nKL-constrained RL objective. In the controlled generation task, vBoN appears\nmore frequently on the Pareto frontier of reward and KL divergence compared to\nother alignment methods. In the summarization task, vBoN achieves high reward\nvalues across various sampling temperatures.\n","authors":["Afra Amini","Tim Vieira","Elliott Ash","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2407.06057v3.pdf","comment":"Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2503.02660v1","updated":"2025-03-04T14:22:08Z","published":"2025-03-04T14:22:08Z","title":"A dataset-free approach for self-supervised learning of 3D reflectional\n  symmetries","summary":"  In this paper, we explore a self-supervised model that learns to detect the\nsymmetry of a single object without requiring a dataset-relying solely on the\ninput object itself. We hypothesize that the symmetry of an object can be\ndetermined by its intrinsic features, eliminating the need for large datasets\nduring training. Additionally, we design a self-supervised learning strategy\nthat removes the necessity of ground truth labels. These two key elements make\nour approach both effective and efficient, addressing the prohibitive costs\nassociated with constructing large, labeled datasets for this task. The novelty\nof our method lies in computing features for each point on the object based on\nthe idea that symmetric points should exhibit similar visual appearances. To\nachieve this, we leverage features extracted from a foundational image model to\ncompute a visual descriptor for the points. This approach equips the point\ncloud with visual features that facilitate the optimization of our\nself-supervised model. Experimental results demonstrate that our method\nsurpasses the state-of-the-art models trained on large datasets. Furthermore,\nour model is more efficient, effective, and operates with minimal computational\nand data resources.\n","authors":["Issac Aguirre","Ivan Sipiran","Gabriel Montañana"],"pdf_url":"https://arxiv.org/pdf/2503.02660v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02650v1","updated":"2025-03-04T14:14:28Z","published":"2025-03-04T14:14:28Z","title":"The Effectiveness of Large Language Models in Transforming Unstructured\n  Text to Standardized Formats","summary":"  The exponential growth of unstructured text data presents a fundamental\nchallenge in modern data management and information retrieval. While Large\nLanguage Models (LLMs) have shown remarkable capabilities in natural language\nprocessing, their potential to transform unstructured text into standardized,\nstructured formats remains largely unexplored - a capability that could\nrevolutionize data processing workflows across industries. This study breaks\nnew ground by systematically evaluating LLMs' ability to convert unstructured\nrecipe text into the structured Cooklang format. Through comprehensive testing\nof four models (GPT-4o, GPT-4o-mini, Llama3.1:70b, and Llama3.1:8b), an\ninnovative evaluation approach is introduced that combines traditional metrics\n(WER, ROUGE-L, TER) with specialized metrics for semantic element\nidentification. Our experiments reveal that GPT-4o with few-shot prompting\nachieves breakthrough performance (ROUGE-L: 0.9722, WER: 0.0730), demonstrating\nfor the first time that LLMs can reliably transform domain-specific\nunstructured text into structured formats without extensive training. Although\nmodel performance generally scales with size, we uncover surprising potential\nin smaller models like Llama3.1:8b for optimization through targeted\nfine-tuning. These findings open new possibilities for automated structured\ndata generation across various domains, from medical records to technical\ndocumentation, potentially transforming the way organizations process and\nutilize unstructured information.\n","authors":["William Brach","Kristián Košťál","Michal Ries"],"pdf_url":"https://arxiv.org/pdf/2503.02650v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.11760v3","updated":"2025-03-04T14:04:07Z","published":"2024-08-21T16:32:03Z","title":"R2Det: Exploring Relaxed Rotation Equivariance in 2D object detection","summary":"  Group Equivariant Convolution (GConv) empowers models to explore underlying\nsymmetry in data, improving performance. However, real-world scenarios often\ndeviate from ideal symmetric systems caused by physical permutation,\ncharacterized by non-trivial actions of a symmetry group, resulting in\nasymmetries that affect the outputs, a phenomenon known as Symmetry Breaking.\nTraditional GConv-based methods are constrained by rigid operational rules\nwithin group space, assuming data remains strictly symmetry after limited group\ntransformations. This limitation makes it difficult to adapt to\nSymmetry-Breaking and non-rigid transformations. Motivated by this, we mainly\nfocus on a common scenario: Rotational Symmetry-Breaking. By relaxing strict\ngroup transformations within Strict Rotation-Equivariant group $\\mathbf{C}_n$,\nwe redefine a Relaxed Rotation-Equivariant group $\\mathbf{R}_n$ and introduce a\nnovel Relaxed Rotation-Equivariant GConv (R2GConv) with only a minimal increase\nof $4n$ parameters compared to GConv. Based on R2GConv, we propose a Relaxed\nRotation-Equivariant Network (R2Net) as the backbone and develop a Relaxed\nRotation-Equivariant Object Detector (R2Det) for 2D object detection.\nExperimental results demonstrate the effectiveness of the proposed R2GConv in\nnatural image classification, and R2Det achieves excellent performance in 2D\nobject detection with improved generalization capabilities and robustness. The\ncode is available in \\texttt{https://github.com/wuer5/r2det}.\n","authors":["Zhiqiang Wu","Yingjie Liu","Hanlin Dong","Xuan Tang","Jian Yang","Bo Jin","Mingsong Chen","Xian Wei"],"pdf_url":"https://arxiv.org/pdf/2408.11760v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02636v1","updated":"2025-03-04T14:01:10Z","published":"2025-03-04T14:01:10Z","title":"YARE-GAN: Yet Another Resting State EEG-GAN","summary":"  Generative Adversarial Networks (GANs) have shown promise in synthesising\nrealistic neural data, yet their potential for unsupervised representation\nlearning in resting-state EEG remains under explored. In this study, we\nimplement a Wasserstein GAN with Gradient Penalty (WGAN-GP) to generate\nmulti-channel resting-state EEG data and assess the quality of the synthesised\nsignals through both visual and feature-based evaluations. Our results indicate\nthat the model effectively captures the statistical and spectral\ncharacteristics of real EEG data, although challenges remain in replicating\nhigh-frequency oscillations in the frontal region. Additionally, we demonstrate\nthat the Critic's learned representations can be fine-tuned for age group\nclassification, achieving an out-of-sample accuracy, significantly better than\na shuffled-label baseline. These findings suggest that generative models can\nserve not only as EEG data generators but also as unsupervised feature\nextractors, reducing the need for manual feature engineering. This study\nhighlights the potential of GAN-based unsupervised learning for EEG analysis,\nsuggesting avenues for more data-efficient deep learning applications in\nneuroscience.\n","authors":["Yeganeh Farahzadi","Morteza Ansarinia","Zoltan Kekecs"],"pdf_url":"https://arxiv.org/pdf/2503.02636v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20092v3","updated":"2025-03-04T14:00:03Z","published":"2025-02-27T13:51:56Z","title":"WalnutData: A UAV Remote Sensing Dataset of Green Walnuts and Model\n  Evaluation","summary":"  The UAV technology is gradually maturing and can provide extremely powerful\nsupport for smart agriculture and precise monitoring. Currently, there is no\ndataset related to green walnuts in the field of agricultural computer vision.\nThus, in order to promote the algorithm design in the field of agricultural\ncomputer vision, we used UAV to collect remote-sensing data from 8 walnut\nsample plots. Considering that green walnuts are subject to various lighting\nconditions and occlusion, we constructed a large-scale dataset with a\nhigher-granularity of target features - WalnutData. This dataset contains a\ntotal of 30,240 images and 706,208 instances, and there are 4 target\ncategories: being illuminated by frontal light and unoccluded (A1), being\nbacklit and unoccluded (A2), being illuminated by frontal light and occluded\n(B1), and being backlit and occluded (B2). Subsequently, we evaluated many\nmainstream algorithms on WalnutData and used these evaluation results as the\nbaseline standard. The dataset and all evaluation results can be obtained at\nhttps://github.com/1wuming/WalnutData.\n","authors":["Mingjie Wu","Chenggui Yang","Huihua Wang","Chen Xue","Yibo Wang","Haoyu Wang","Yansong Wang","Can Peng","Yuqi Han","Ruoyu Li","Lijun Yun","Zaiqing Chen","Yuelong Xia"],"pdf_url":"https://arxiv.org/pdf/2502.20092v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02631v1","updated":"2025-03-04T13:56:18Z","published":"2025-03-04T13:56:18Z","title":"Reflection on Data Storytelling Tools in the Generative AI Era from the\n  Human-AI Collaboration Perspective","summary":"  Human-AI collaborative tools attract attentions from the data storytelling\ncommunity to lower the barrier of expertise and streamline the workflow. The\nrecent advance in large-scale generative AI techniques, e.g., large language\nmodels (LLMs) and text-to-image models, has the potential to enhance data\nstorytelling with their power in visual and narration generation. After two\nyears since these techniques were publicly available, it is important to\nreflect our progress of applying them and have an outlook for future\nopportunities. To achieve the goal, we compare the collaboration patterns of\nthe latest tools with those of earlier ones using a dedicated framework for\nunderstanding human-AI collaboration in data storytelling. Through comparison,\nwe identify persistent collaboration patterns, e.g., human-creator +\nAI-assistant, and emerging ones, e.g., AI-creator + human-reviewer. The\nbenefits of these AI techniques and other implications to human-AI\ncollaboration are also revealed. We further propose future directions to\nhopefully ignite innovations.\n","authors":["Haotian Li","Yun Wang","Huamin Qu"],"pdf_url":"https://arxiv.org/pdf/2503.02631v1.pdf","comment":"This paper is a sequel to the CHI 24 paper \"Where Are We So Far?\n  Understanding Data Storytelling Tools from the Perspective of Human-AI\n  Collaboration (https://doi.org/10.1145/3613904.3642726), aiming to refresh\n  our understanding with the latest advancements"},{"id":"http://arxiv.org/abs/2503.02628v1","updated":"2025-03-04T13:53:43Z","published":"2025-03-04T13:53:43Z","title":"Towards Event Extraction with Massive Types: LLM-based Collaborative\n  Annotation and Partitioning Extraction","summary":"  Developing a general-purpose extraction system that can extract events with\nmassive types is a long-standing target in Event Extraction (EE). In doing so,\nthe challenge comes from two aspects: 1) The absence of an efficient and\neffective annotation method. 2) The absence of a powerful extraction method can\nhandle massive types. For the first challenge, we propose a collaborative\nannotation method based on Large Language Models (LLMs). Through collaboration\namong multiple LLMs, it first refines annotations of trigger words from distant\nsupervision and then carries out argument annotation. Next, a voting phase\nconsolidates the annotation preferences across different LLMs. Finally, we\ncreate the EEMT dataset, the largest EE dataset to date, featuring over 200,000\nsamples, 3,465 event types, and 6,297 role types. For the second challenge, we\npropose an LLM-based Partitioning EE method called LLM-PEE. To overcome the\nlimited context length of LLMs, LLM-PEE first recalls candidate event types and\nthen splits them into multiple partitions for LLMs to extract events. The\nresults in the supervised setting show that LLM-PEE outperforms the\nstate-of-the-art methods by 5.4 in event detection and 6.1 in argument\nextraction. In the zero-shot setting, LLM-PEE achieves up to 12.9 improvement\ncompared to mainstream LLMs, demonstrating its strong generalization\ncapabilities.\n","authors":["Wenxuan Liu","Zixuan Li","Long Bai","Yuxin Zuo","Daozhu Xu","Xiaolong Jin","Jiafeng Guo","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2503.02628v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2410.04070v6","updated":"2025-03-04T13:51:14Z","published":"2024-10-05T08:00:55Z","title":"PAD: Personalized Alignment of LLMs at Decoding-Time","summary":"  Aligning with personalized preferences, which vary significantly across\ncultural, educational, and political differences, poses a significant challenge\ndue to the computational costs and data demands of traditional alignment\nmethods. In response, this paper presents Personalized Alignment at\nDecoding-time (PAD), a novel framework designed to align LLM outputs with\ndiverse personalized preferences during the inference phase, eliminating the\nneed for additional training. By introducing a unique personalized reward\nmodeling strategy, this framework decouples the text generation process from\npersonalized preferences, facilitating the generation of generalizable\ntoken-level personalized rewards. The PAD algorithm leverages these rewards to\nguide the decoding process, dynamically tailoring the base model's predictions\nto personalized preferences. Extensive experimental results demonstrate that\nPAD not only outperforms existing training-based alignment methods in terms of\naligning with diverse preferences but also shows significant generalizability\nto preferences unseen during training and scalability across different base\nmodels. This work advances the capability of LLMs to meet user needs in\nreal-time applications, presenting a substantial step forward in personalized\nLLM alignment.\n","authors":["Ruizhe Chen","Xiaotian Zhang","Meng Luo","Wenhao Chai","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2410.04070v6.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2408.02835v3","updated":"2025-03-04T13:41:49Z","published":"2024-08-05T21:12:12Z","title":"Training a multilayer dynamical spintronic network with standard machine\n  learning tools to perform time series classification","summary":"  The ability to process time-series at low energy cost is critical for many\napplications. Recurrent neural network, which can perform such tasks, are\ncomputationally expensive when implementing in software on conventional\ncomputers. Here we propose to implement a recurrent neural network in hardware\nusing spintronic oscillators as dynamical neurons. Using numerical simulations,\nwe build a multi-layer network and demonstrate that we can use backpropagation\nthrough time (BPTT) and standard machine learning tools to train this network.\nLeveraging the transient dynamics of the spintronic oscillators, we solve the\nsequential digits classification task with $89.83\\pm2.91~\\%$ accuracy, as good\nas the equivalent software network. We devise guidelines on how to choose the\ntime constant of the oscillators as well as hyper-parameters of the network to\nadapt to different input time scales.\n","authors":["Erwan Plouet","Dédalo Sanz-Hernández","Aymeric Vecchiola","Julie Grollier","Frank Mizrahi"],"pdf_url":"https://arxiv.org/pdf/2408.02835v3.pdf","comment":"7 pages, 4 figures"},{"id":"http://arxiv.org/abs/2503.02612v1","updated":"2025-03-04T13:32:40Z","published":"2025-03-04T13:32:40Z","title":"Reinforcement Learning-based Threat Assessment","summary":"  In some game scenarios, due to the uncertainty of the number of enemy units\nand the priority of various attributes, the evaluation of the threat level of\nenemy units as well as the screening has been a challenging research topic, and\nthe core difficulty lies in how to reasonably set the priority of different\nattributes in order to achieve quantitative evaluation of the threat. In this\npaper, we innovatively transform the problem of threat assessment into a\nreinforcement learning problem, and through systematic reinforcement learning\ntraining, we successfully construct an efficient neural network evaluator. The\nevaluator can not only comprehensively integrate the multidimensional attribute\nfeatures of the enemy, but also effectively combine our state information, thus\nrealizing a more accurate and scientific threat assessment.\n","authors":["Wuzhou Sun","Siyi Li","Qingxiang Zou","Zixing Liao"],"pdf_url":"https://arxiv.org/pdf/2503.02612v1.pdf","comment":"10 pages,9 figures"},{"id":"http://arxiv.org/abs/2503.02597v1","updated":"2025-03-04T13:18:33Z","published":"2025-03-04T13:18:33Z","title":"Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual\n  Attention for Multimodal LLMs","summary":"  Recent Multimodal Large Language Models (MLLMs) have demonstrated significant\nprogress in perceiving and reasoning over multimodal inquiries, ushering in a\nnew research era for foundation models. However, vision-language misalignment\nin MLLMs has emerged as a critical challenge, where the textual responses\ngenerated by these models are not factually aligned with the given text-image\ninputs. Existing efforts to address vision-language misalignment have focused\non developing specialized vision-language connectors or leveraging visual\ninstruction tuning from diverse domains. In this paper, we tackle this issue\nfrom a fundamental yet unexplored perspective by revisiting the core\narchitecture of MLLMs. Most MLLMs are typically built on decoder-only LLMs\nconsisting of a causal attention mechanism, which limits the ability of earlier\nmodalities (e.g., images) to incorporate information from later modalities\n(e.g., text). To address this problem, we propose AKI, a novel MLLM that\nunlocks causal attention into modality-mutual attention (MMA) to enable image\ntokens to attend to text tokens. This simple yet effective design allows AKI to\nachieve superior performance in 12 multimodal understanding benchmarks (+7.2%\non average) without introducing additional parameters and increasing training\ntime. Our MMA design is intended to be generic, allowing for application across\nvarious modalities, and scalable to accommodate diverse multimodal scenarios.\nThe code is publicly available at https://github.com/sony/aki, and we will\nrelease our AKI-4B model to encourage further advancements in MLLMs across\nvarious directions.\n","authors":["Wei-Yao Wang","Zhao Wang","Helen Suzuki","Yoshiyuki Kobayashi"],"pdf_url":"https://arxiv.org/pdf/2503.02597v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.14997v2","updated":"2025-03-04T13:17:59Z","published":"2024-10-19T06:12:31Z","title":"Improving Pronunciation and Accent Conversion through Knowledge\n  Distillation And Synthetic Ground-Truth from Native TTS","summary":"  Previous approaches on accent conversion (AC) mainly aimed at making\nnon-native speech sound more native while maintaining the original content and\nspeaker identity. However, non-native speakers sometimes have pronunciation\nissues, which can make it difficult for listeners to understand them. Hence, we\ndeveloped a new AC approach that not only focuses on accent conversion but also\nimproves pronunciation of non-native accented speaker. By providing the\nnon-native audio and the corresponding transcript, we generate the ideal\nground-truth audio with native-like pronunciation with original duration and\nprosody. This ground-truth data aids the model in learning a direct mapping\nbetween accented and native speech. We utilize the end-to-end VITS framework to\nachieve high-quality waveform reconstruction for the AC task. As a result, our\nsystem not only produces audio that closely resembles native accents and while\nretaining the original speaker's identity but also improve pronunciation, as\ndemonstrated by evaluation results.\n","authors":["Tuan Nam Nguyen","Seymanur Akti","Ngoc Quan Pham","Alexander Waibel"],"pdf_url":"https://arxiv.org/pdf/2410.14997v2.pdf","comment":"accepted at ICASSP 2025"},{"id":"http://arxiv.org/abs/2503.02595v1","updated":"2025-03-04T13:17:50Z","published":"2025-03-04T13:17:50Z","title":"StageDesigner: Artistic Stage Generation for Scenography via Theater\n  Scripts","summary":"  In this work, we introduce StageDesigner, the first comprehensive framework\nfor artistic stage generation using large language models combined with\nlayout-controlled diffusion models. Given the professional requirements of\nstage scenography, StageDesigner simulates the workflows of seasoned artists to\ngenerate immersive 3D stage scenes. Specifically, our approach is divided into\nthree primary modules: Script Analysis, which extracts thematic and spatial\ncues from input scripts; Foreground Generation, which constructs and arranges\nessential 3D objects; and Background Generation, which produces a harmonious\nbackground aligned with the narrative atmosphere and maintains spatial\ncoherence by managing occlusions between foreground and background elements.\nFurthermore, we introduce the StagePro-V1 dataset, a dedicated dataset with 276\nunique stage scenes spanning different historical styles and annotated with\nscripts, images, and detailed 3D layouts, specifically tailored for this task.\nFinally, evaluations using both standard and newly proposed metrics, along with\nextensive user studies, demonstrate the effectiveness of StageDesigner. Project\ncan be found at: https://deadsmither5.github.io/2025/01/03/StageDesigner/\n","authors":["Zhaoxing Gan","Mengtian Li","Ruhua Chen","Zhongxia Ji","Sichen Guo","Huanling Hu","Guangnan Ye","Zuo Hu"],"pdf_url":"https://arxiv.org/pdf/2503.02595v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.12309v2","updated":"2025-03-04T13:09:19Z","published":"2023-10-18T20:18:05Z","title":"A Unifying Framework for Learning Argumentation Semantics","summary":"  Argumentation is a very active research field of Artificial Intelligence\nconcerned with the representation and evaluation of arguments used in dialogues\nbetween humans and/or artificial agents. Acceptability semantics of formal\nargumentation systems define the criteria for the acceptance or rejection of\narguments. Several software systems, known as argumentation solvers, have been\ndeveloped to compute the accepted/rejected arguments using such criteria. These\ninclude systems that learn to identify the accepted arguments using\nnon-interpretable methods. In this paper we present a novel framework, which\nuses an Inductive Logic Programming approach to learn the acceptability\nsemantics for several abstract and structured argumentation frameworks in an\ninterpretable way. Through an empirical evaluation we show that our framework\noutperforms existing argumentation solvers, thus opening up new future research\ndirections in the area of formal argumentation and human-machine dialogues.\n","authors":["Zlatina Mileva","Antonis Bikakis","Fabio Aurelio D'Asaro","Mark Law","Alessandra Russo"],"pdf_url":"https://arxiv.org/pdf/2310.12309v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.13152v4","updated":"2025-03-04T13:07:09Z","published":"2024-05-21T18:45:18Z","title":"Interpretable Interaction Modeling for Trajectory Prediction via Agent\n  Selection and Physical Coefficient","summary":"  A thorough understanding of the interaction between the target agent and\nsurrounding agents is a prerequisite for accurate trajectory prediction.\nAlthough many methods have been explored, they assign correlation coefficients\nto surrounding agents in a purely learning-based manner. In this study, we\npresent ASPILin, which manually selects interacting agents and replaces the\nattention scores in Transformer with a newly computed physical correlation\ncoefficient, enhancing the interpretability of interaction modeling.\nSurprisingly, these simple modifications can significantly improve prediction\nperformance and substantially reduce computational costs. We intentionally\nsimplified our model in other aspects, such as map encoding. Remarkably,\nexperiments conducted on the INTERACTION, highD, and CitySim datasets\ndemonstrate that our method is efficient and straightforward, outperforming\nother state-of-the-art methods.\n","authors":["Shiji Huang","Lei Ye","Min Chen","Wenhai Luo","Dihong Wang","Chenqi Xu","Deyuan Liang"],"pdf_url":"https://arxiv.org/pdf/2405.13152v4.pdf","comment":"code:https://github.com/kkk00714/ASPILin"},{"id":"http://arxiv.org/abs/2503.02582v1","updated":"2025-03-04T13:04:48Z","published":"2025-03-04T13:04:48Z","title":"Playing games with Large language models: Randomness and strategy","summary":"  Playing games has a long history of describing intricate interactions in\nsimplified forms. In this paper we explore if large language models (LLMs) can\nplay games, investigating their capabilities for randomisation and strategic\nadaptation through both simultaneous and sequential game interactions. We focus\non GPT-4o-Mini-2024-08-17 and test two games between LLMs: Rock Paper Scissors\n(RPS) and games of strategy (Prisoners Dilemma PD). LLMs are often described as\nstochastic parrots, and while they may indeed be parrots, our results suggest\nthat they are not very stochastic in the sense that their outputs - when\nprompted to be random - are often very biased. Our research reveals that LLMs\nappear to develop loss aversion strategies in repeated games, with RPS\nconverging to stalemate conditions while PD shows systematic shifts between\ncooperative and competitive outcomes based on prompt design. We detail\nprogrammatic tools for independent agent interactions and the Agentic AI\nchallenges faced in implementation. We show that LLMs can indeed play games,\njust not very well. These results have implications for the use of LLMs in\nmulti-agent LLM systems and showcase limitations in current approaches to model\noutput for strategic decision-making.\n","authors":["Alicia Vidler","Toby Walsh"],"pdf_url":"https://arxiv.org/pdf/2503.02582v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2407.03157v2","updated":"2025-03-04T13:01:07Z","published":"2024-07-03T14:34:03Z","title":"Let the Code LLM Edit Itself When You Edit the Code","summary":"  In this work, we investigate a typical scenario in code generation where a\ndeveloper edits existing code in real time and requests a code assistant, e.g.,\na large language model, to re-predict the next token or next line on the fly.\nNaively, the LLM needs to re-encode the entire KV cache to provide an accurate\nprediction. However, this process is computationally expensive, especially when\nthe sequence length is long. Simply encoding the edited subsequence and\nintegrating it to the original KV cache meets the temporal confusion problem,\nleading to significantly worse performance. We address this efficiency and\naccuracy trade-off by introducing \\underline{\\textbf{Positional\n\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\npositional encoding, PIE first removes the rotary matrices in the Key cache\nthat introduce temporal confusion and then reapplies the correct rotary\nmatrices. This process ensures that positional relationships between tokens are\ncorrect and requires only a single round of matrix multiplication. We validate\nthe effectiveness of PIE through extensive experiments on the RepoBench-C-8k\ndataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\nOur evaluation includes three real-world coding tasks: code insertion, code\ndeletion, and multi-place code editing. Results demonstrate that PIE reduces\ncomputational overhead by over 85% compared to the standard full recomputation\napproach across all model sizes and tasks while well approximating the model\nperformance.\n","authors":["Zhenyu He","Jun Zhang","Shengjie Luo","Jingjing Xu","Zhi Zhang","Di He"],"pdf_url":"https://arxiv.org/pdf/2407.03157v2.pdf","comment":"ICLR 2025 Camera Ready"},{"id":"http://arxiv.org/abs/2503.02574v1","updated":"2025-03-04T12:55:07Z","published":"2025-03-04T12:55:07Z","title":"LLM-Safety Evaluations Lack Robustness","summary":"  In this paper, we argue that current safety alignment research efforts for\nlarge language models are hindered by many intertwined sources of noise, such\nas small datasets, methodological inconsistencies, and unreliable evaluation\nsetups. This can, at times, make it impossible to evaluate and compare attacks\nand defenses fairly, thereby slowing progress. We systematically analyze the\nLLM safety evaluation pipeline, covering dataset curation, optimization\nstrategies for automated red-teaming, response generation, and response\nevaluation using LLM judges. At each stage, we identify key issues and\nhighlight their practical impact. We also propose a set of guidelines for\nreducing noise and bias in evaluations of future attack and defense papers.\nLastly, we offer an opposing perspective, highlighting practical reasons for\nexisting limitations. We believe that addressing the outlined problems in\nfuture research will improve the field's ability to generate easily comparable\nresults and make measurable progress.\n","authors":["Tim Beyer","Sophie Xhonneux","Simon Geisler","Gauthier Gidel","Leo Schwinn","Stephan Günnemann"],"pdf_url":"https://arxiv.org/pdf/2503.02574v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02572v1","updated":"2025-03-04T12:54:05Z","published":"2025-03-04T12:54:05Z","title":"RaceVLA: VLA-based Racing Drone Navigation with Human-like Behaviour","summary":"  RaceVLA presents an innovative approach for autonomous racing drone\nnavigation by leveraging Visual-Language-Action (VLA) to emulate human-like\nbehavior. This research explores the integration of advanced algorithms that\nenable drones to adapt their navigation strategies based on real-time\nenvironmental feedback, mimicking the decision-making processes of human\npilots. The model, fine-tuned on a collected racing drone dataset, demonstrates\nstrong generalization despite the complexity of drone racing environments.\nRaceVLA outperforms OpenVLA in motion (75.0 vs 60.0) and semantic\ngeneralization (45.5 vs 36.3), benefiting from the dynamic camera and\nsimplified motion tasks. However, visual (79.6 vs 87.0) and physical (50.0 vs\n76.7) generalization were slightly reduced due to the challenges of maneuvering\nin dynamic environments with varying object sizes. RaceVLA also outperforms\nRT-2 across all axes - visual (79.6 vs 52.0), motion (75.0 vs 55.0), physical\n(50.0 vs 26.7), and semantic (45.5 vs 38.8), demonstrating its robustness for\nreal-time adjustments in complex environments. Experiments revealed an average\nvelocity of 1.04 m/s, with a maximum speed of 2.02 m/s, and consistent\nmaneuverability, demonstrating RaceVLA's ability to handle high-speed scenarios\neffectively. These findings highlight the potential of RaceVLA for\nhigh-performance navigation in competitive racing contexts. The RaceVLA\ncodebase, pretrained weights, and dataset are available at this http URL:\nhttps://racevla.github.io/\n","authors":["Valerii Serpiva","Artem Lykov","Artyom Myshlyaev","Muhammad Haris Khan","Ali Alridha Abdulkarim","Oleg Sautenkov","Dzmitry Tsetserukou"],"pdf_url":"https://arxiv.org/pdf/2503.02572v1.pdf","comment":"6 pages, 6 figures. Submitted to IROS 2025"},{"id":"http://arxiv.org/abs/2502.00196v2","updated":"2025-03-04T12:36:10Z","published":"2025-01-31T22:26:33Z","title":"DermaSynth: Rich Synthetic Image-Text Pairs Using Open Access\n  Dermatology Datasets","summary":"  A major barrier to developing vision large language models (LLMs) in\ndermatology is the lack of large image--text pairs dataset. We introduce\nDermaSynth, a dataset comprising of 92,020 synthetic image--text pairs curated\nfrom 45,205 images (13,568 clinical and 35,561 dermatoscopic) for\ndermatology-related clinical tasks. Leveraging state-of-the-art LLMs, using\nGemini 2.0, we used clinically related prompts and self-instruct method to\ngenerate diverse and rich synthetic texts. Metadata of the datasets were\nincorporated into the input prompts by targeting to reduce potential\nhallucinations. The resulting dataset builds upon open access dermatological\nimage repositories (DERM12345, BCN20000, PAD-UFES-20, SCIN, and HIBA) that have\npermissive CC-BY-4.0 licenses. We also fine-tuned a preliminary\nLlama-3.2-11B-Vision-Instruct model, DermatoLlama 1.0, on 5,000 samples. We\nanticipate this dataset to support and accelerate AI research in dermatology.\nData and code underlying this work are accessible at\nhttps://github.com/abdurrahimyilmaz/DermaSynth.\n","authors":["Abdurrahim Yilmaz","Furkan Yuceyalcin","Ece Gokyayla","Donghee Choi","Ozan Erdem","Ali Anil Demircali","Rahmetullah Varol","Ufuk Gorkem Kirabali","Gulsum Gencoglan","Joram M. Posma","Burak Temelkuran"],"pdf_url":"https://arxiv.org/pdf/2502.00196v2.pdf","comment":"12 pages, 4 figures"},{"id":"http://arxiv.org/abs/2412.06097v2","updated":"2025-03-04T12:32:31Z","published":"2024-12-08T22:57:41Z","title":"Order Theory in the Context of Machine Learning","summary":"  The paper ``Tropical Geometry of Deep Neural Networks'' by L. Zhang et al.\nintroduces an equivalence between integer-valued neural networks (IVNN) with\n$\\text{ReLU}_{t}$ and tropical rational functions, which come with a map to\npolytopes. Here, IVNN refers to a network with integer weights but real biases,\nand $\\text{ReLU}_{t}$ is defined as $\\text{ReLU}_{t}(x)=\\max(x,t)$ for\n$t\\in\\mathbb{R}\\cup\\{-\\infty\\}$.\n  For every poset with $n$ points, there exists a corresponding order polytope,\ni.e., a convex polytope in the unit cube $[0,1]^n$ whose coordinates obey the\ninequalities of the poset. We study neural networks whose associated polytope\nis an order polytope. We then explain how posets with four points induce neural\nnetworks that can be interpreted as $2\\times 2$ convolutional filters. These\nposet filters can be added to any neural network, not only IVNN.\n  Similarly to maxout, poset pooling filters update the weights of the neural\nnetwork during backpropagation with more precision than average pooling, max\npooling, or mixed pooling, without the need to train extra parameters. We\nreport experiments that support our statements.\n  We also define the structure of algebra over the operad of posets on poset\nneural networks and tropical polynomials. This formalism allows us to study the\ncomposition of poset neural network arquitectures and the effect on their\ncorresponding Newton polytopes, via the introduction of the generalization of\ntwo operations on polytopes: the Minkowski sum and the convex envelope.\n","authors":["Eric Dolores-Cuenca","Aldo Guzman-Saenz","Sangil Kim","Susana Lopez-Moreno","Jose Mendoza-Cortes"],"pdf_url":"https://arxiv.org/pdf/2412.06097v2.pdf","comment":"We added experiments with ImageNet 100, and improved the exposition\n  of the theory developed. Added examples. Poster presentation in NeurIPS WIML\n  2024, Talk in JMM 2025 section: Applied category theory II"},{"id":"http://arxiv.org/abs/2503.00493v2","updated":"2025-03-04T12:32:13Z","published":"2025-03-01T13:44:50Z","title":"LLaSE-G1: Incentivizing Generalization Capability for LLaMA-based Speech\n  Enhancement","summary":"  Recent advancements in language models (LMs) have demonstrated strong\ncapabilities in semantic understanding and contextual modeling, which have\nflourished in generative speech enhancement (SE). However, many LM-based SE\napproaches primarily focus on semantic information, often neglecting the\ncritical role of acoustic information, which leads to acoustic inconsistency\nafter enhancement and limited generalization across diverse SE tasks. In this\npaper, we introduce LLaSE-G1, a LLaMA-based language model that incentivizes\ngeneralization capabilities for speech enhancement. LLaSE-G1 offers the\nfollowing key contributions: First, to mitigate acoustic inconsistency,\nLLaSE-G1 employs continuous representations from WavLM as input and predicts\nspeech tokens from X-Codec2, maximizing acoustic preservation. Second, to\npromote generalization capability, LLaSE-G1 introduces dual-channel inputs and\noutputs, unifying multiple SE tasks without requiring task-specific IDs. Third,\nLLaSE-G1 outperforms prior task-specific discriminative and generative SE\nmodels, demonstrating scaling effects at test time and emerging capabilities\nfor unseen SE tasks. Additionally, we release our code and models to support\nfurther research in this area.\n","authors":["Boyi Kang","Xinfa Zhu","Zihan Zhang","Zhen Ye","Mingshuai Liu","Ziqian Wang","Yike Zhu","Guobin Ma","Jun Chen","Longshuai Xiao","Chao Weng","Wei Xue","Lei Xie"],"pdf_url":"https://arxiv.org/pdf/2503.00493v2.pdf","comment":"13 pages, 2 figures, 8 tables"},{"id":"http://arxiv.org/abs/2503.02552v1","updated":"2025-03-04T12:25:01Z","published":"2025-03-04T12:25:01Z","title":"World Models for Anomaly Detection during Model-Based Reinforcement\n  Learning Inference","summary":"  Learning-based controllers are often purposefully kept out of real-world\napplications due to concerns about their safety and reliability. We explore how\nstate-of-the-art world models in Model-Based Reinforcement Learning can be\nutilized beyond the training phase to ensure a deployed policy only operates\nwithin regions of the state-space it is sufficiently familiar with. This is\nachieved by continuously monitoring discrepancies between a world model's\npredictions and observed system behavior during inference. It allows for\ntriggering appropriate measures, such as an emergency stop, once an error\nthreshold is surpassed. This does not require any task-specific knowledge and\nis thus universally applicable. Simulated experiments on established robot\ncontrol tasks show the effectiveness of this method, recognizing changes in\nlocal robot geometry and global gravitational magnitude. Real-world experiments\nusing an agile quadcopter further demonstrate the benefits of this approach by\ndetecting unexpected forces acting on the vehicle. These results indicate how\neven in new and adverse conditions, safe and reliable operation of otherwise\nunpredictable learning-based controllers can be achieved.\n","authors":["Fabian Domberg","Georg Schildbach"],"pdf_url":"https://arxiv.org/pdf/2503.02552v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02549v1","updated":"2025-03-04T12:20:06Z","published":"2025-03-04T12:20:06Z","title":"Federated nnU-Net for Privacy-Preserving Medical Image Segmentation","summary":"  The nnU-Net framework has played a crucial role in medical image segmentation\nand has become the gold standard in multitudes of applications targeting\ndifferent diseases, organs, and modalities. However, so far it has been used\nprimarily in a centralized approach where the data collected from hospitals are\nstored in one center and used to train the nnU-Net. This centralized approach\nhas various limitations, such as leakage of sensitive patient information and\nviolation of patient privacy. Federated learning is one of the approaches to\ntrain a segmentation model in a decentralized manner that helps preserve\npatient privacy. In this paper, we propose FednnU-Net, a federated learning\nextension of nnU-Net. We introduce two novel federated learning methods to the\nnnU-Net framework - Federated Fingerprint Extraction (FFE) and Asymmetric\nFederated Averaging (AsymFedAvg) - and experimentally show their consistent\nperformance for breast, cardiac and fetal segmentation using 6 datasets\nrepresenting samples from 18 institutions. Additionally, to further promote\nresearch and deployment of decentralized training in privacy constrained\ninstitutions, we make our plug-n-play framework public. The source-code is\navailable at https://github.com/faildeny/FednnUNet .\n","authors":["Grzegorz Skorupko","Fotios Avgoustidis","Carlos Martín-Isla","Lidia Garrucho","Dimitri A. Kessler","Esmeralda Ruiz Pujadas","Oliver Díaz","Maciej Bobowicz","Katarzyna Gwoździewicz","Xavier Bargalló","Paulius Jaruševičius","Kaisar Kushibar","Karim Lekadir"],"pdf_url":"https://arxiv.org/pdf/2503.02549v1.pdf","comment":"In review"},{"id":"http://arxiv.org/abs/2503.00401v2","updated":"2025-03-04T12:04:26Z","published":"2025-03-01T08:29:59Z","title":"Smoothing Grounding and Reasoning for MLLM-Powered GUI Agents with\n  Query-Oriented Pivot Tasks","summary":"  Perception-enhanced pre-training, particularly through grounding techniques,\nis widely adopted to enhance the performance of graphical user interface (GUI)\nagents. However, in resource-constrained scenarios, the format discrepancy\nbetween coordinate-oriented grounding and action-oriented reasoning limits the\neffectiveness of grounding for reasoning tasks. To address this challenge, we\npropose a query-oriented pivot approach called query inference, which serves as\na bridge between GUI grounding and reasoning. By inferring potential user\nqueries from a screenshot and its associated element coordinates, query\ninference improves the understanding of coordinates while aligning more closely\nwith reasoning tasks. Experimental results show that query inference\noutperforms previous grounding techniques under the same training data scale.\nNotably, query inference achieves comparable or even better performance to\nlarge-scale grounding-enhanced OS-Atlas with less than 0.1% of training data.\nFurthermore, we explore the impact of reasoning formats and demonstrate that\nintegrating additional semantic information into the input further boosts\nreasoning performance. The code is publicly available at\nhttps://github.com/ZrW00/GUIPivot.\n","authors":["Zongru Wu","Pengzhou Cheng","Zheng Wu","Tianjie Ju","Zhuosheng Zhang","Gongshen Liu"],"pdf_url":"https://arxiv.org/pdf/2503.00401v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02537v1","updated":"2025-03-04T12:03:26Z","published":"2025-03-04T12:03:26Z","title":"RectifiedHR: Enable Efficient High-Resolution Image Generation via\n  Energy Rectification","summary":"  Diffusion models have achieved remarkable advances in various image\ngeneration tasks. However, their performance notably declines when generating\nimages at resolutions higher than those used during the training period.\nDespite the existence of numerous methods for producing high-resolution images,\nthey either suffer from inefficiency or are hindered by complex operations. In\nthis paper, we propose RectifiedHR, an efficient and straightforward solution\nfor training-free high-resolution image generation. Specifically, we introduce\nthe noise refresh strategy, which theoretically only requires a few lines of\ncode to unlock the model's high-resolution generation ability and improve\nefficiency. Additionally, we first observe the phenomenon of energy decay that\nmay cause image blurriness during the high-resolution image generation process.\nTo address this issue, we propose an Energy Rectification strategy, where\nmodifying the hyperparameters of the classifier-free guidance effectively\nimproves the generation performance. Our method is entirely training-free and\nboasts a simple implementation logic. Through extensive comparisons with\nnumerous baseline methods, our RectifiedHR demonstrates superior effectiveness\nand efficiency.\n","authors":["Zhen Yang","Guibao Shen","Liang Hou","Mushui Liu","Luozhou Wang","Xin Tao","Pengfei Wan","Di Zhang","Ying-Cong Chen"],"pdf_url":"https://arxiv.org/pdf/2503.02537v1.pdf","comment":"Project Page: https://zhenyangcs.github.io/RectifiedHR-Diffusion/"},{"id":"http://arxiv.org/abs/2412.06860v2","updated":"2025-03-04T11:47:27Z","published":"2024-12-09T02:36:38Z","title":"Balancing Efficiency and Effectiveness: An LLM-Infused Approach for\n  Optimized CTR Prediction","summary":"  Click-Through Rate (CTR) prediction is essential in online advertising, where\nsemantic information plays a pivotal role in shaping user decisions and\nenhancing CTR effectiveness. Capturing and modeling deep semantic information,\nsuch as a user's preference for \"H\\\"aagen-Dazs' HEAVEN strawberry light ice\ncream\" due to its health-conscious and premium attributes, is challenging.\nTraditional semantic modeling often overlooks these intricate details at the\nuser and item levels. To bridge this gap, we introduce a novel approach that\nmodels deep semantic information end-to-end, leveraging the comprehensive world\nknowledge capabilities of Large Language Models (LLMs). Our proposed\nLLM-infused CTR prediction framework(Multi-level Deep Semantic Information\nInfused CTR model via Distillation, MSD) is designed to uncover deep semantic\ninsights by utilizing LLMs to extract and distill critical information into a\nsmaller, more efficient model, enabling seamless end-to-end training and\ninference. Importantly, our framework is carefully designed to balance\nefficiency and effectiveness, ensuring that the model not only achieves high\nperformance but also operates with optimal resource utilization. Online A/B\ntests conducted on the Meituan sponsored-search system demonstrate that our\nmethod significantly outperforms baseline models in terms of Cost Per Mile\n(CPM) and CTR, validating its effectiveness, scalability, and balanced approach\nin real-world applications.\n","authors":["Guoxiao Zhang","Yi Wei","Yadong Zhang","Huajian Feng","Qiang Liu"],"pdf_url":"https://arxiv.org/pdf/2412.06860v2.pdf","comment":"5 pages, 4 figures,4 tables"},{"id":"http://arxiv.org/abs/2503.01747v2","updated":"2025-03-04T11:30:30Z","published":"2025-03-03T17:15:17Z","title":"Position: Don't use the CLT in LLM evals with fewer than a few hundred\n  datapoints","summary":"  Rigorous statistical evaluations of large language models (LLMs), including\nvalid error bars and significance testing, are essential for meaningful and\nreliable performance assessment. Currently, when such statistical measures are\nreported, they typically rely on the Central Limit Theorem (CLT). In this\nposition paper, we argue that while CLT-based methods for uncertainty\nquantification are appropriate when benchmarks consist of thousands of\nexamples, they fail to provide adequate uncertainty estimates for LLM\nevaluations that rely on smaller, highly specialized benchmarks. In these\nsmall-data settings, we demonstrate that CLT-based methods perform very poorly,\nusually dramatically underestimating uncertainty (i.e. producing error bars\nthat are too small). We give recommendations for alternative frequentist and\nBayesian methods that are both easy to implement and more appropriate in these\nincreasingly common scenarios. We provide a simple Python library for these\nBayesian methods at https://github.com/sambowyer/bayes_evals .\n","authors":["Sam Bowyer","Laurence Aitchison","Desi R. Ivanova"],"pdf_url":"https://arxiv.org/pdf/2503.01747v2.pdf","comment":"36 pages, 37 figures"},{"id":"http://arxiv.org/abs/2503.02512v1","updated":"2025-03-04T11:20:19Z","published":"2025-03-04T11:20:19Z","title":"LTL Verification of Memoryful Neural Agents","summary":"  We present a framework for verifying Memoryful Neural Multi-Agent Systems\n(MN-MAS) against full Linear Temporal Logic (LTL) specifications. In MN-MAS,\nagents interact with a non-deterministic, partially observable environment.\nExamples of MN-MAS include multi-agent systems based on feed-forward and\nrecurrent neural networks or state-space models. Different from previous\napproaches, we support the verification of both bounded and unbounded LTL\nspecifications. We leverage well-established bounded model checking techniques,\nincluding lasso search and invariant synthesis, to reduce the verification\nproblem to that of constraint solving. To solve these constraints, we develop\nefficient methods based on bound propagation, mixed-integer linear programming,\nand adaptive splitting. We evaluate the effectiveness of our algorithms in\nsingle and multi-agent environments from the Gymnasium and PettingZoo\nlibraries, verifying unbounded specifications for the first time and improving\nthe verification time for bounded specifications by an order of magnitude\ncompared to the SoA.\n","authors":["Mehran Hosseini","Alessio Lomuscio","Nicola Paoletti"],"pdf_url":"https://arxiv.org/pdf/2503.02512v1.pdf","comment":"11 pages, 2 figures, accepted at AAMAS 2025 conference"},{"id":"http://arxiv.org/abs/2503.02505v1","updated":"2025-03-04T11:16:46Z","published":"2025-03-04T11:16:46Z","title":"ROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment","summary":"  We aim to develop a goal specification method that is semantically clear,\nspatially sensitive, and intuitive for human users to guide agent interactions\nin embodied environments. Specifically, we propose a novel cross-view goal\nalignment framework that allows users to specify target objects using\nsegmentation masks from their own camera views rather than the agent's\nobservations. We highlight that behavior cloning alone fails to align the\nagent's behavior with human intent when the human and agent camera views differ\nsignificantly. To address this, we introduce two auxiliary objectives:\ncross-view consistency loss and target visibility loss, which explicitly\nenhance the agent's spatial reasoning ability. According to this, we develop\nROCKET-2, a state-of-the-art agent trained in Minecraft, achieving an\nimprovement in the efficiency of inference 3x to 6x. We show ROCKET-2 can\ndirectly interpret goals from human camera views for the first time, paving the\nway for better human-agent interaction.\n","authors":["Shaofei Cai","Zhancun Mu","Anji Liu","Yitao Liang"],"pdf_url":"https://arxiv.org/pdf/2503.02505v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02497v1","updated":"2025-03-04T11:04:35Z","published":"2025-03-04T11:04:35Z","title":"PennyLang: Pioneering LLM-Based Quantum Code Generation with a Novel\n  PennyLane-Centric Dataset","summary":"  Large Language Models (LLMs) offer remarkable capabilities in code\ngeneration, natural language processing, and domain-specific reasoning. Their\npotential in aiding quantum software development remains underexplored,\nparticularly for the PennyLane framework-a leading platform for hybrid\nquantum-classical computing. To address this gap, we introduce a novel,\nhigh-quality dataset comprising 3,347 PennyLane-specific code samples of\nquantum circuits and their contextual descriptions, specifically curated to\ntrain/fine-tune LLM-based quantum code assistance. Our key contributions are\nthreefold: (1) the automatic creation and open-source release of a\ncomprehensive PennyLane dataset leveraging quantum computing textbooks,\nofficial documentation, and open-source repositories; (2) the development of a\nsystematic methodology for data refinement, annotation, and formatting to\noptimize LLM training efficiency; and (3) a thorough evaluation, based on a\nRetrieval-Augmented Generation (RAG) framework, demonstrating the effectiveness\nof our dataset in streamlining PennyLane code generation and improving quantum\ndevelopment workflows. Compared to existing efforts that predominantly focus on\nQiskit, our dataset significantly broadens the spectrum of quantum frameworks\ncovered in AI-driven code assistance. By bridging this gap and providing\nreproducible dataset-creation methodologies, we aim to advance the field of\nAI-assisted quantum programming, making quantum computing more accessible to\nboth newcomers and experienced developers.\n","authors":["Haider Asif","Abdul Basit","Nouhaila Innan","Muhammad Kashif","Alberto Marchisio","Muhammad Shafique"],"pdf_url":"https://arxiv.org/pdf/2503.02497v1.pdf","comment":"10 pages, 8 figures, 6 tables, submitted for review under IJCNN 2025"},{"id":"http://arxiv.org/abs/2503.02495v1","updated":"2025-03-04T11:01:25Z","published":"2025-03-04T11:01:25Z","title":"Union of Experts: Adapting Hierarchical Routing to Equivalently\n  Decomposed Transformer","summary":"  Mixture-of-Experts (MoE) enhances model performance while maintaining\ncomputational efficiency, making it well-suited for large-scale applications.\nHowever, expert in exist MoE paradigm works as an individual, thereby lacking\nhigh-quality expert interactions. Moreover, they have not been effectively\nextended to attention block, which constrains further efficiency improvements.\nTo tackle these issues, we propose Union-of-Experts (UoE), which decomposes\ntransformer into an equitant group of experts, and then implement dynamic\nrouting on input data and experts. Our approach advances MoE design with three\nkey innovations: (1) We conducted equitant expert decomposition on both MLP\nblocks and attention blocks based on matrix partition in tensor parallelism.\n(2) We developed two routing paradigms: patch wise data selection and expert\nselection, to apply routing across different levels. (3) We design the\narchitecture of UoE model, including Selective Multi-Head Attention (SMHA) and\nUnion-of-MLP-Experts (UoME). (4) We develop parallel implementation of UoE's\nrouting and computation operation, and optimize efficiency based on the\nhardware processing analysis. The experiments demonstrate that the model\nemployed with UoE surpass Full Attention, state-of-art MoEs and efficient\ntransformers in several tasks across image and natural language domains. The\nsource codes are available at https://github.com/YujiaoYang-work/UoE.\n","authors":["Yujiao Yang","Jing Lian","Linhui Li"],"pdf_url":"https://arxiv.org/pdf/2503.02495v1.pdf","comment":"17 pages, 6 figures, 5 tables"},{"id":"http://arxiv.org/abs/2503.02484v1","updated":"2025-03-04T10:48:44Z","published":"2025-03-04T10:48:44Z","title":"ERetinex: Event Camera Meets Retinex Theory for Low-Light Image\n  Enhancement","summary":"  Low-light image enhancement aims to restore the under-exposure image captured\nin dark scenarios. Under such scenarios, traditional frame-based cameras may\nfail to capture the structure and color information due to the exposure time\nlimitation. Event cameras are bio-inspired vision sensors that respond to\npixel-wise brightness changes asynchronously. Event cameras' high dynamic range\nis pivotal for visual perception in extreme low-light scenarios, surpassing\ntraditional cameras and enabling applications in challenging dark environments.\nIn this paper, inspired by the success of the retinex theory for traditional\nframe-based low-light image restoration, we introduce the first methods that\ncombine the retinex theory with event cameras and propose a novel retinex-based\nlow-light image restoration framework named ERetinex. Among our contributions,\nthe first is developing a new approach that leverages the high temporal\nresolution data from event cameras with traditional image information to\nestimate scene illumination accurately. This method outperforms traditional\nimage-only techniques, especially in low-light environments, by providing more\nprecise lighting information. Additionally, we propose an effective fusion\nstrategy that combines the high dynamic range data from event cameras with the\ncolor information of traditional images to enhance image quality. Through this\nfusion, we can generate clearer and more detail-rich images, maintaining the\nintegrity of visual information even under extreme lighting conditions. The\nexperimental results indicate that our proposed method outperforms\nstate-of-the-art (SOTA) methods, achieving a gain of 1.0613 dB in PSNR while\nreducing FLOPS by \\textbf{84.28}\\%.\n","authors":["Xuejian Guo","Zhiqiang Tian","Yuehang Wang","Siqi Li","Yu Jiang","Shaoyi Du","Yue Gao"],"pdf_url":"https://arxiv.org/pdf/2503.02484v1.pdf","comment":"Accepted to ICRA 2025"},{"id":"http://arxiv.org/abs/2503.02476v1","updated":"2025-03-04T10:39:42Z","published":"2025-03-04T10:39:42Z","title":"BioD2C: A Dual-level Semantic Consistency Constraint Framework for\n  Biomedical VQA","summary":"  Biomedical visual question answering (VQA) has been widely studied and has\ndemonstrated significant application value and potential in fields such as\nassistive medical diagnosis. Despite their success, current biomedical VQA\nmodels perform multimodal information interaction only at the model level\nwithin large language models (LLMs), leading to suboptimal multimodal semantic\nalignment when dealing with complex tasks. To address this issue, we propose\nBioD2C: a novel Dual-level Semantic Consistency Constraint Framework for\nBiomedical VQA, which achieves dual-level semantic interaction alignment at\nboth the model and feature levels, enabling the model to adaptively learn\nvisual features based on the question. Specifically, we firstly integrate\ntextual features into visual features via an image-text fusion mechanism as\nfeature-level semantic interaction, obtaining visual features conditioned on\nthe given text; and then introduce a text-queue-based cross-modal soft semantic\nloss function to further align the image semantics with the question semantics.\nSpecifically, in this work, we establish a new dataset, BioVGQ, to address\ninherent biases in prior datasets by filtering manually-altered images and\naligning question-answer pairs with multimodal context, and train our model on\nthis dataset. Extensive experimental results demonstrate that BioD2C achieves\nstate-of-the-art (SOTA) performance across multiple downstream datasets,\nshowcasing its robustness, generalizability, and potential to advance\nbiomedical VQA research.\n","authors":["Zhengyang Ji","Shang Gao","Li Liu","Yifan Jia","Yutao Yue"],"pdf_url":"https://arxiv.org/pdf/2503.02476v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.05650v3","updated":"2025-03-04T10:22:01Z","published":"2024-09-09T14:16:27Z","title":"Replay Consolidation with Label Propagation for Continual Object\n  Detection","summary":"  Continual Learning (CL) aims to learn new data while remembering previously\nacquired knowledge. In contrast to CL for image classification, CL for Object\nDetection faces additional challenges such as the missing annotations problem.\nIn this scenario, images from previous tasks may contain instances of unknown\nclasses that could reappear as labeled in future tasks, leading to task\ninterference in replay-based approaches. Consequently, most approaches in the\nliterature have focused on distillation-based techniques, which are effective\nwhen there is a significant class overlap between tasks. In our work, we\npropose an alternative to distillation-based approaches with a novel approach\ncalled Replay Consolidation with Label Propagation for Object Detection\n(RCLPOD). RCLPOD enhances the replay memory by improving the quality of the\nstored samples through a technique that promotes class balance while also\nimproving the quality of the ground truth associated with these samples through\na technique called label propagation. RCLPOD outperforms existing techniques on\nwell-established benchmarks such as VOC and COC. Moreover, our approach is\ndeveloped to work with modern architectures like YOLOv8, making it suitable for\ndynamic, real-world applications such as autonomous driving and robotics, where\ncontinuous learning and resource efficiency are essential.\n","authors":["Riccardo De Monte","Davide Dalle Pezze","Marina Ceccon","Francesco Pasti","Francesco Paissan","Elisabetta Farella","Gian Antonio Susto","Nicola Bellotto"],"pdf_url":"https://arxiv.org/pdf/2409.05650v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02457v1","updated":"2025-03-04T10:06:41Z","published":"2025-03-04T10:06:41Z","title":"Don't Get Too Excited -- Eliciting Emotions in LLMs","summary":"  This paper investigates the challenges of affect control in large language\nmodels (LLMs), focusing on their ability to express appropriate emotional\nstates during extended dialogues. We evaluated state-of-the-art open-weight\nLLMs to assess their affective expressive range in terms of arousal and\nvalence. Our study employs a novel methodology combining LLM-based sentiment\nanalysis with multiturn dialogue simulations between LLMs. We quantify the\nmodels' capacity to express a wide spectrum of emotions and how they fluctuate\nduring interactions. Our findings reveal significant variations among LLMs in\ntheir ability to maintain consistent affect, with some models demonstrating\nmore stable emotional trajectories than others. Furthermore, we identify key\nchallenges in affect control, including difficulties in producing and\nmaintaining extreme emotional states and limitations in adapting affect to\nchanging conversational contexts. These findings have important implications\nfor the development of more emotionally intelligent AI systems and highlight\nthe need for improved affect modelling in LLMs.\n","authors":["Gino Franco Fazzi","Julie Skoven Hinge","Stefan Heinrich","Paolo Burelli"],"pdf_url":"https://arxiv.org/pdf/2503.02457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02453v1","updated":"2025-03-04T10:00:05Z","published":"2025-03-04T10:00:05Z","title":"Sparse Meets Dense: Unified Generative Recommendations with Cascaded\n  Sparse-Dense Representations","summary":"  Generative models have recently gained attention in recommendation systems by\ndirectly predicting item identifiers from user interaction sequences. However,\nexisting methods suffer from significant information loss due to the separation\nof stages such as quantization and sequence modeling, hindering their ability\nto achieve the modeling precision and accuracy of sequential dense retrieval\ntechniques. Integrating generative and dense retrieval methods remains a\ncritical challenge. To address this, we introduce the Cascaded Organized\nBi-Represented generAtive retrieval (COBRA) framework, which innovatively\nintegrates sparse semantic IDs and dense vectors through a cascading process.\nOur method alternates between generating these representations by first\ngenerating sparse IDs, which serve as conditions to aid in the generation of\ndense vectors. End-to-end training enables dynamic refinement of dense\nrepresentations, capturing both semantic insights and collaborative signals\nfrom user-item interactions. During inference, COBRA employs a coarse-to-fine\nstrategy, starting with sparse ID generation and refining them into dense\nvectors via the generative model. We further propose BeamFusion, an innovative\napproach combining beam search with nearest neighbor scores to enhance\ninference flexibility and recommendation diversity. Extensive experiments on\npublic datasets and offline tests validate our method's robustness. Online A/B\ntests on a real-world advertising platform with over 200 million daily users\ndemonstrate substantial improvements in key metrics, highlighting COBRA's\npractical advantages.\n","authors":["Yuhao Yang","Zhi Ji","Zhaopeng Li","Yi Li","Zhonglin Mo","Yue Ding","Kai Chen","Zijian Zhang","Jie Li","Shuanglong Li","Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2503.02453v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19811v3","updated":"2025-03-04T09:54:37Z","published":"2025-02-27T06:36:45Z","title":"Comet: Fine-grained Computation-communication Overlapping for\n  Mixture-of-Experts","summary":"  Mixture-of-experts (MoE) has been extensively employed to scale large\nlanguage models to trillion-plus parameters while maintaining a fixed\ncomputational cost. The development of large MoE models in the distributed\nscenario encounters the problem of large communication overhead. The\ninter-device communication of a MoE layer can occupy 47% time of the entire\nmodel execution with popular models and frameworks. Therefore, existing methods\nsuggest the communication in a MoE layer to be pipelined with the computation\nfor overlapping. However, these coarse grained overlapping schemes introduce a\nnotable impairment of computational efficiency and the latency concealing is\nsub-optimal.\n  To this end, we present COMET, an optimized MoE system with fine-grained\ncommunication-computation overlapping. Leveraging data dependency analysis and\ntask rescheduling, COMET achieves precise fine-grained overlapping of\ncommunication and computation. Through adaptive workload assignment, COMET\neffectively eliminates fine-grained communication bottlenecks and enhances its\nadaptability across various scenarios. Our evaluation shows that COMET\naccelerates the execution of a single MoE layer by $1.96\\times$ and for\nend-to-end execution, COMET delivers a $1.71\\times$ speedup on average. COMET\nhas been adopted in the production environment of clusters with\nten-thousand-scale of GPUs, achieving savings of millions of GPU hours.\n","authors":["Shulai Zhang","Ningxin Zheng","Haibin Lin","Ziheng Jiang","Wenlei Bao","Chengquan Jiang","Qi Hou","Weihao Cui","Size Zheng","Li-Wen Chang","Quan Chen","Xin Liu"],"pdf_url":"https://arxiv.org/pdf/2502.19811v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.04285v3","updated":"2025-03-04T09:25:47Z","published":"2023-10-06T14:37:22Z","title":"Assessing Robustness via Score-Based Adversarial Image Generation","summary":"  Most adversarial attacks and defenses focus on perturbations within small\n$\\ell_p$-norm constraints. However, $\\ell_p$ threat models cannot capture all\nrelevant semantics-preserving perturbations, and hence, the scope of robustness\nevaluations is limited. In this work, we introduce Score-Based Adversarial\nGeneration (ScoreAG), a novel framework that leverages the advancements in\nscore-based generative models to generate unrestricted adversarial examples\nthat overcome the limitations of $\\ell_p$-norm constraints. Unlike traditional\nmethods, ScoreAG maintains the core semantics of images while generating\nadversarial examples, either by transforming existing images or synthesizing\nnew ones entirely from scratch. We further exploit the generative capability of\nScoreAG to purify images, empirically enhancing the robustness of classifiers.\nOur extensive empirical evaluation demonstrates that ScoreAG improves upon the\nmajority of state-of-the-art attacks and defenses across multiple benchmarks.\nThis work highlights the importance of investigating adversarial examples\nbounded by semantics rather than $\\ell_p$-norm constraints. ScoreAG represents\nan important step towards more encompassing robustness assessments.\n","authors":["Marcel Kollovieh","Lukas Gosch","Marten Lienen","Yan Scholten","Leo Schwinn","Stephan Günnemann"],"pdf_url":"https://arxiv.org/pdf/2310.04285v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16779v3","updated":"2025-03-04T09:24:06Z","published":"2025-02-24T02:14:19Z","title":"Unposed Sparse Views Room Layout Reconstruction in the Age of Pretrain\n  Model","summary":"  Room layout estimation from multiple-perspective images is poorly\ninvestigated due to the complexities that emerge from multi-view geometry,\nwhich requires muti-step solutions such as camera intrinsic and extrinsic\nestimation, image matching, and triangulation. However, in 3D reconstruction,\nthe advancement of recent 3D foundation models such as DUSt3R has shifted the\nparadigm from the traditional multi-step structure-from-motion process to an\nend-to-end single-step approach. To this end, we introduce Plane-DUSt3R, a\nnovel method for multi-view room layout estimation leveraging the 3D foundation\nmodel DUSt3R. Plane-DUSt3R incorporates the DUSt3R framework and fine-tunes on\na room layout dataset (Structure3D) with a modified objective to estimate\nstructural planes. By generating uniform and parsimonious results, Plane-DUSt3R\nenables room layout estimation with only a single post-processing step and 2D\ndetection results. Unlike previous methods that rely on single-perspective or\npanorama image, Plane-DUSt3R extends the setting to handle multiple-perspective\nimages. Moreover, it offers a streamlined, end-to-end solution that simplifies\nthe process and reduces error accumulation. Experimental results demonstrate\nthat Plane-DUSt3R not only outperforms state-of-the-art methods on the\nsynthetic dataset but also proves robust and effective on in the wild data with\ndifferent image styles such as cartoon. Our code is available at:\nhttps://github.com/justacar/Plane-DUSt3R\n","authors":["Yaxuan Huang","Xili Dai","Jianan Wang","Xianbiao Qi","Yixing Yuan","Xiangyu Yue"],"pdf_url":"https://arxiv.org/pdf/2502.16779v3.pdf","comment":"Accepted by ICLR 2025. Github\n  page:https://github.com/justacar/Plane-DUSt3R"},{"id":"http://arxiv.org/abs/2308.15905v2","updated":"2025-03-04T09:20:42Z","published":"2023-08-30T09:15:41Z","title":"Thermodynamic Computing via Autonomous Quantum Thermal Machines","summary":"  We develop a physics-based model for classical computation based on\nautonomous quantum thermal machines. These machines consist of few interacting\nquantum bits (qubits) connected to several environments at different\ntemperatures. Heat flows through the machine are here exploited for computing.\nThe process starts by setting the temperatures of the environments according to\nthe logical input. The machine evolves, eventually reaching a non-equilibrium\nsteady state, from which the output of the computation can be determined via\nthe temperature of an auxilliary finite-size reservoir. Such a machine, which\nwe term a ``thermodynamic neuron'', can implement any linearly-separable\nfunction, and we discuss explicitly the cases of NOT, 3-MAJORITY and NOR gates.\nIn turn, we show that a network of thermodynamic neurons can perform any\ndesired function. We discuss the close connection between our model and\nartificial neurons (perceptrons), and argue that our model provides an\nalternative physics-based analogue implementation of neural networks, and more\ngenerally a platform for thermodynamic computing.\n","authors":["Patryk Lipka-Bartosik","Martí Perarnau-Llobet","Nicolas Brunner"],"pdf_url":"https://arxiv.org/pdf/2308.15905v2.pdf","comment":"15 + 5 pages. Published version"},{"id":"http://arxiv.org/abs/2411.02316v4","updated":"2025-03-04T09:10:59Z","published":"2024-11-04T17:40:39Z","title":"Evaluating Creative Short Story Generation in Humans and Large Language\n  Models","summary":"  Story-writing is a fundamental aspect of human imagination, relying heavily\non creativity to produce narratives that are novel, effective, and surprising.\nWhile large language models (LLMs) have demonstrated the ability to generate\nhigh-quality stories, their creative story-writing capabilities remain\nunder-explored. In this work, we conduct a systematic analysis of creativity in\nshort story generation across 60 LLMs and 60 people using a five-sentence\ncreative story-writing task. We use measures to automatically evaluate model-\nand human-generated stories across several dimensions of creativity, including\nnovelty, surprise, diversity, and linguistic complexity. We also collect\ncreativity ratings and Turing Test classifications from non-expert and expert\nhuman raters and LLMs. Automated metrics show that LLMs generate stylistically\ncomplex stories, but tend to fall short in terms of novelty, surprise and\ndiversity when compared to average human writers. Expert ratings generally\ncoincide with automated metrics. However, LLMs and non-experts rate LLM stories\nto be more creative than human-generated stories. We discuss why and how these\ndifferences in ratings occur, and their implications for both human and\nartificial creativity.\n","authors":["Mete Ismayilzada","Claire Stevenson","Lonneke van der Plas"],"pdf_url":"https://arxiv.org/pdf/2411.02316v4.pdf","comment":"Submitted to ICCC 2025"},{"id":"http://arxiv.org/abs/2503.02420v1","updated":"2025-03-04T09:05:01Z","published":"2025-03-04T09:05:01Z","title":"Exploring Model Quantization in GenAI-based Image Inpainting and\n  Detection of Arable Plants","summary":"  Deep learning-based weed control systems often suffer from limited training\ndata diversity and constrained on-board computation, impacting their real-world\nperformance. To overcome these challenges, we propose a framework that\nleverages Stable Diffusion-based inpainting to augment training data\nprogressively in 10% increments -- up to an additional 200%, thus enhancing\nboth the volume and diversity of samples. Our approach is evaluated on two\nstate-of-the-art object detection models, YOLO11(l) and RT-DETR(l), using the\nmAP50 metric to assess detection performance. We explore quantization\nstrategies (FP16 and INT8) for both the generative inpainting and detection\nmodels to strike a balance between inference speed and accuracy. Deployment of\nthe downstream models on the Jetson Orin Nano demonstrates the practical\nviability of our framework in resource-constrained environments, ultimately\nimproving detection accuracy and computational efficiency in intelligent weed\nmanagement systems.\n","authors":["Sourav Modak","Ahmet Oğuz Saltık","Anthony Stein"],"pdf_url":"https://arxiv.org/pdf/2503.02420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01507v2","updated":"2025-03-04T08:47:15Z","published":"2025-03-03T13:22:37Z","title":"Compare different SG-Schemes based on large least square problems","summary":"  This study reviews popular stochastic gradient-based schemes based on large\nleast-square problems. These schemes, often called optimizers in machine\nlearning, play a crucial role in finding better model parameters. Hence, this\nstudy focuses on viewing such optimizers with different hyper-parameters and\nanalyzing them based on least square problems. Codes that produced results in\nthis work are available on\nhttps://github.com/q-viper/gradients-based-methods-on-large-least-square.\n","authors":["Ramkrishna Acharya"],"pdf_url":"https://arxiv.org/pdf/2503.01507v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.00897v2","updated":"2025-03-04T08:46:27Z","published":"2025-03-02T13:43:53Z","title":"A Simple and Effective Reinforcement Learning Method for Text-to-Image\n  Diffusion Fine-tuning","summary":"  Reinforcement learning ( RL)-based fine-tuning has emerged as a powerful\napproach for aligning diffusion models with black-box objectives. Proximal\npolicy optimization (PPO) is the most popular choice of method for policy\noptimization. While effective in terms of performance, PPO is highly sensitive\nto hyper-parameters and involves substantial computational overhead. REINFORCE,\non the other hand, mitigates some computational complexities such as high\nmemory overhead and sensitive hyper-parameter tuning, but has suboptimal\nperformance due to high-variance and sample inefficiency. While the variance of\nthe REINFORCE can be reduced by sampling multiple actions per input prompt and\nusing a baseline correction term, it still suffers from sample inefficiency. To\naddress these challenges, we systematically analyze the\nefficiency-effectiveness trade-off between REINFORCE and PPO, and propose\nleave-one-out PPO ( LOOP), a novel RL for diffusion fine-tuning method. LOOP\ncombines variance reduction techniques from REINFORCE, such as sampling\nmultiple actions per input prompt and a baseline correction term, with the\nrobustness and sample efficiency of PPO via clipping and importance sampling.\nOur results demonstrate that LOOP effectively improves diffusion models on\nvarious black-box objectives, and achieves a better balance between\ncomputational efficiency and performance.\n","authors":["Shashank Gupta","Chaitanya Ahuja","Tsung-Yu Lin","Sreya Dutta Roy","Harrie Oosterhuis","Maarten de Rijke","Satya Narayan Shukla"],"pdf_url":"https://arxiv.org/pdf/2503.00897v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02403v1","updated":"2025-03-04T08:44:30Z","published":"2025-03-04T08:44:30Z","title":"AutoEval: A Practical Framework for Autonomous Evaluation of Mobile\n  Agents","summary":"  Accurate and systematic evaluation of mobile agents can significantly advance\ntheir development and real-world applicability. However, existing benchmarks\nfor mobile agents lack practicality and scalability due to the extensive manual\neffort required to define task reward signals and implement corresponding\nevaluation codes. To this end, we propose AutoEval, an autonomous agent\nevaluation framework that tests a mobile agent without any manual effort.\nFirst, we design a Structured Substate Representation to describe the UI state\nchanges while agent execution, such that task reward signals can be\nautomatically generated. Second, we utilize a Judge System that can\nautonomously evaluate agents' performance given the automatically generated\ntask reward signals. By providing only a task description, our framework\nevaluates agents with fine-grained performance feedback to that task without\nany extra manual effort. We implement a prototype of our framework and validate\nthe automatically generated task reward signals, finding over 93% coverage to\nhuman-annotated reward signals. Moreover, to prove the effectiveness of our\nautonomous Judge System, we manually verify its judge results and demonstrate\nthat it achieves 94% accuracy. Finally, we evaluate the state-of-the-art mobile\nagents using our framework, providing detailed insights into their performance\ncharacteristics and limitations.\n","authors":["Jiahui Sun","Zhichao Hua","Yubin Xia"],"pdf_url":"https://arxiv.org/pdf/2503.02403v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02399v1","updated":"2025-03-04T08:41:45Z","published":"2025-03-04T08:41:45Z","title":"VisAgent: Narrative-Preserving Story Visualization Framework","summary":"  Story visualization is the transformation of narrative elements into image\nsequences. While existing research has primarily focused on visual contextual\ncoherence, the deeper narrative essence of stories often remains overlooked.\nThis limitation hinders the practical application of these approaches, as\ngenerated images frequently fail to capture the intended meaning and nuances of\nthe narrative fully. To address these challenges, we propose VisAgent, a\ntraining-free multi-agent framework designed to comprehend and visualize\npivotal scenes within a given story. By considering story distillation,\nsemantic consistency, and contextual coherence, VisAgent employs an agentic\nworkflow. In this workflow, multiple specialized agents collaborate to: (i)\nrefine layered prompts based on the narrative structure and (ii) seamlessly\nintegrate \\gt{generated} elements, including refined prompts, scene elements,\nand subject placement, into the final image. The empirically validated\neffectiveness confirms the framework's suitability for practical story\nvisualization applications.\n","authors":["Seungkwon Kim","GyuTae Park","Sangyeon Kim","Seung-Hun Nam"],"pdf_url":"https://arxiv.org/pdf/2503.02399v1.pdf","comment":"Accepted to ICASSP 2025. Equal contribution from first two authors"},{"id":"http://arxiv.org/abs/2503.02398v1","updated":"2025-03-04T08:41:40Z","published":"2025-03-04T08:41:40Z","title":"PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence","summary":"  Recommendation agents leverage large language models for user modeling LLM UM\nto construct textual personas guiding alignment with real users. However\nexisting LLM UM methods struggle with long user generated content UGC due to\ncontext limitations and performance degradation. To address this sampling\nstrategies prioritize relevance or recency are often applied yet they\ninevitably neglect the diverse user interests embedded within the discarded\nbehaviors resulting in incomplete modeling and degraded profiling quality.\nFurthermore relevance based sampling requires real time retrieval forcing the\nuser modeling process to operate online which introduces significant latency\noverhead. In this paper we propose PersonaX an agent agnostic LLM UM framework\nthat tackles these challenges through sub behavior sequence SBS selection and\noffline multi persona construction. PersonaX extracts compact SBS segments\noffline to capture diverse user interests generating fine grained textual\npersonas that are cached for efficient online retrieval. This approach ensures\nthat the user persona used for prompting remains highly relevant to the current\ncontext while eliminating the need for online user modeling. For SBS selection\nwe ensure both efficiency length less than five and high representational\nquality by balancing prototypicality and diversity within the sampled data.\nExtensive experiments validate the effectiveness and versatility of PersonaX in\nhigh quality user profiling. Utilizing only 30 to 50 percent of the behavioral\ndata with a sequence length of 480 integrating PersonaX with AgentCF yields an\nabsolute performance improvement of 3 to 11 percent while integration with\nAgent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic\nframework sets a new benchmark for scalable user modeling paving the way for\nmore accurate and efficient LLM driven recommendation agents.\n","authors":["Yunxiao Shi","Wujiang Xu","Zeqi Zhang","Xing Zi","Qiang Wu","Min Xu"],"pdf_url":"https://arxiv.org/pdf/2503.02398v1.pdf","comment":"draft paper"},{"id":"http://arxiv.org/abs/2503.02397v1","updated":"2025-03-04T08:40:42Z","published":"2025-03-04T08:40:42Z","title":"A Binary Classification Social Network Dataset for Graph Machine\n  Learning","summary":"  Social networks have a vast range of applications with graphs. The available\nbenchmark datasets are citation, co-occurrence, e-commerce networks, etc, with\nclasses ranging from 3 to 15. However, there is no benchmark classification\nsocial network dataset for graph machine learning. This paper fills the gap and\npresents the Binary Classification Social Network Dataset (\\textit{BiSND}),\ndesigned for graph machine learning applications to predict binary classes. We\npresent the BiSND in \\textit{tabular and graph} formats to verify its\nrobustness across classical and advanced machine learning. We employ a diverse\nset of classifiers, including four traditional machine learning algorithms\n(Decision Trees, K-Nearest Neighbour, Random Forest, XGBoost), one Deep Neural\nNetwork (multi-layer perceptrons), one Graph Neural Network (Graph\nConvolutional Network), and three state-of-the-art Graph Contrastive Learning\nmethods (BGRL, GRACE, DAENS). Our findings reveal that BiSND is suitable for\nclassification tasks, with F1-scores ranging from 67.66 to 70.15, indicating\npromising avenues for future enhancements.\n","authors":["Adnan Ali","Jinglong Li","Huanhuan Chen","AlMotasem Bellah Al Ajlouni"],"pdf_url":"https://arxiv.org/pdf/2503.02397v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.14851v4","updated":"2025-03-04T08:38:34Z","published":"2024-04-23T09:05:37Z","title":"From Matching to Generation: A Survey on Generative Information\n  Retrieval","summary":"  Information Retrieval (IR) systems are crucial tools for users to access\ninformation, which have long been dominated by traditional methods relying on\nsimilarity matching. With the advancement of pre-trained language models,\ngenerative information retrieval (GenIR) emerges as a novel paradigm,\nattracting increasing attention. Based on the form of information provided to\nusers, current research in GenIR can be categorized into two aspects:\n\\textbf{(1) Generative Document Retrieval} (GR) leverages the generative\nmodel's parameters for memorizing documents, enabling retrieval by directly\ngenerating relevant document identifiers without explicit indexing. \\textbf{(2)\nReliable Response Generation} employs language models to directly generate\ninformation users seek, breaking the limitations of traditional IR in terms of\ndocument granularity and relevance matching while offering flexibility,\nefficiency, and creativity to meet practical needs. This paper aims to\nsystematically review the latest research progress in GenIR. We will summarize\nthe advancements in GR regarding model training and structure, document\nidentifier, incremental learning, etc., as well as progress in reliable\nresponse generation in aspects of internal knowledge memorization, external\nknowledge augmentation, etc. We also review the evaluation, challenges and\nfuture developments in GenIR systems. This review aims to offer a comprehensive\nreference for researchers, encouraging further development in the GenIR field.\nGithub Repository: https://github.com/RUC-NLPIR/GenIR-Survey\n","authors":["Xiaoxi Li","Jiajie Jin","Yujia Zhou","Yuyao Zhang","Peitian Zhang","Yutao Zhu","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2404.14851v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11639v2","updated":"2025-03-04T08:38:14Z","published":"2025-02-17T10:33:24Z","title":"Neural Interpretable Reasoning","summary":"  We formalize a novel modeling framework for achieving interpretability in\ndeep learning, anchored in the principle of inference equivariance. While the\ndirect verification of interpretability scales exponentially with the number of\nvariables of the system, we show that this complexity can be mitigated by\ntreating interpretability as a Markovian property and employing neural\nre-parametrization techniques. Building on these insights, we propose a new\nmodeling paradigm -- neural generation and interpretable execution -- that\nenables scalable verification of equivariance. This paradigm provides a general\napproach for designing Neural Interpretable Reasoners that are not only\nexpressive but also transparent.\n","authors":["Pietro Barbiero","Giuseppe Marra","Gabriele Ciravegna","David Debot","Francesco De Santis","Michelangelo Diligenti","Mateo Espinosa Zarlenga","Francesco Giannini"],"pdf_url":"https://arxiv.org/pdf/2502.11639v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.04309v3","updated":"2025-03-04T08:37:43Z","published":"2024-05-07T13:33:50Z","title":"Non-rigid Structure-from-Motion: Temporally-smooth Procrustean Alignment\n  and Spatially-variant Deformation Modeling","summary":"  Even though Non-rigid Structure-from-Motion (NRSfM) has been extensively\nstudied and great progress has been made, there are still key challenges that\nhinder their broad real-world applications: 1) the inherent motion/rotation\nambiguity requires either explicit camera motion recovery with extra constraint\nor complex Procrustean Alignment; 2) existing low-rank modeling of the global\nshape can over-penalize drastic deformations in the 3D shape sequence. This\npaper proposes to resolve the above issues from a spatial-temporal modeling\nperspective. First, we propose a novel Temporally-smooth Procrustean Alignment\nmodule that estimates 3D deforming shapes and adjusts the camera motion by\naligning the 3D shape sequence consecutively. Our new alignment module remedies\nthe requirement of complex reference 3D shape during alignment, which is more\nconductive to non-isotropic deformation modeling. Second, we propose a\nspatial-weighted approach to enforce the low-rank constraint adaptively at\ndifferent locations to accommodate drastic spatially-variant deformation\nreconstruction better. Our modeling outperform existing low-rank based methods,\nand extensive experiments across different datasets validate the effectiveness\nof our method.\n","authors":["Jiawei Shi","Hui Deng","Yuchao Dai"],"pdf_url":"https://arxiv.org/pdf/2405.04309v3.pdf","comment":"Accepted by CVPR 2024; The new version adds additional experiments\n  and corrects typos"},{"id":"http://arxiv.org/abs/2502.11107v2","updated":"2025-03-04T08:37:41Z","published":"2025-02-16T12:50:20Z","title":"Revisiting Weak-to-Strong Generalization in Theory and Practice: Reverse\n  KL vs. Forward KL","summary":"  As large language models advance toward superhuman performance, ensuring\ntheir alignment with human values and abilities grows increasingly complex.\nWeak-to-strong generalization offers a promising approach by leveraging\npredictions from weaker models to guide stronger systems, but its effectiveness\ncould be constrained by the inherent noise and inaccuracies in these weak\npredictions. To address this, we propose a theoretically grounded approach that\nreplaces forward KL divergence-whose mass-covering behavior risks overfitting\nto imperfect weak signals-with reverse KL divergence. Reverse KL divergence's\nzero-forcing effect prioritizes high-confidence predictions, effectively\nmitigating the influence of unreliable weak supervision. Theoretically, we\nextend existing bounds and derive tighter lower bounds for both forward and\nreverse KL divergence, establishing that reverse KL achieves at least\ncomparable guarantees to forward KL. Notably, when a sufficiently pre-trained\nstrong model is fine-tuned on the last linear layer, reverse KL guarantees that\nit outperforms its weak supervisor by the magnitude of their disagreement.\nEmpirically, we demonstrate that reverse KL and reverse cross-entropy enable\nstrong models to successfully outperform those trained with forward KL and\nstandard cross-entropy across most settings, highlighting the practical\nadvantages of these reverse losses.\n","authors":["Wei Yao","Wenkai Yang","Ziqiao Wang","Yankai Lin","Yong Liu"],"pdf_url":"https://arxiv.org/pdf/2502.11107v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16033v2","updated":"2025-03-04T08:23:58Z","published":"2025-02-22T01:52:37Z","title":"Multimodal Inconsistency Reasoning (MMIR): A New Benchmark for\n  Multimodal Reasoning Models","summary":"  Existing Multimodal Large Language Models (MLLMs) are predominantly trained\nand tested on consistent visual-textual inputs, leaving open the question of\nwhether they can handle inconsistencies in real-world, layout-rich content. To\nbridge this gap, we propose the Multimodal Inconsistency Reasoning (MMIR)\nbenchmark to assess MLLMs' ability to detect and reason about semantic\nmismatches in artifacts such as webpages, presentation slides, and posters.\nMMIR comprises 534 challenging samples, each containing synthetically injected\nerrors across five reasoning-heavy categories: Factual Contradiction, Identity\nMisattribution, Contextual Mismatch, Quantitative Discrepancy, and\nTemporal/Spatial Incoherence. We evaluate six state-of-the-art MLLMs, showing\nthat models with dedicated multimodal reasoning capabilities, such as o1,\nsubstantially outperform their counterparts while open-source models remain\nparticularly vulnerable to inconsistency errors. Detailed error analyses\nfurther show that models excel in detecting pairwise inconsistencies but\nstruggle with inconsistencies confined to single elements in complex layouts.\nProbing experiments reveal that single-modality prompting, including\nChain-of-Thought (CoT) and Set-of-Mark (SoM) methods, yields marginal gains,\nrevealing a key bottleneck in cross-modal reasoning. Our findings highlight the\nneed for advanced multimodal reasoning and point to future research on\nmultimodal inconsistency.\n","authors":["Qianqi Yan","Yue Fan","Hongquan Li","Shan Jiang","Yang Zhao","Xinze Guan","Ching-Chen Kuo","Xin Eric Wang"],"pdf_url":"https://arxiv.org/pdf/2502.16033v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02382v1","updated":"2025-03-04T08:18:46Z","published":"2025-03-04T08:18:46Z","title":"An Efficient and Precise Training Data Construction Framework for\n  Process-supervised Reward Model in Mathematical Reasoning","summary":"  Enhancing the mathematical reasoning capabilities of Large Language Models\n(LLMs) is of great scientific and practical significance. Researchers typically\nemploy process-supervised reward models (PRMs) to guide the reasoning process,\neffectively improving the models' reasoning abilities. However, existing\nmethods for constructing process supervision training data, such as manual\nannotation and per-step Monte Carlo estimation, are often costly or suffer from\npoor quality. To address these challenges, this paper introduces a framework\ncalled EpicPRM, which annotates each intermediate reasoning step based on its\nquantified contribution and uses an adaptive binary search algorithm to enhance\nboth annotation precision and efficiency. Using this approach, we efficiently\nconstruct a high-quality process supervision training dataset named Epic50k,\nconsisting of 50k annotated intermediate steps. Compared to other publicly\navailable datasets, the PRM trained on Epic50k demonstrates significantly\nsuperior performance. Getting Epic50k at https://github.com/xiaolizh1/EpicPRM.\n","authors":["Wei Sun","Qianlong Du","Fuwei Cui","Jiajun Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.02382v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.16783v3","updated":"2025-03-04T07:56:00Z","published":"2024-06-24T16:45:13Z","title":"M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in\n  Large Language Models","summary":"  Instruction finetuning (IFT) is critical for aligning Large Language Models\n(LLMs) to follow instructions. While many effective IFT datasets have been\nintroduced recently, they predominantly focus on high-resource languages like\nEnglish. To better align LLMs across a broad spectrum of languages and tasks,\nwe propose a fully synthetic, novel taxonomy (Evol) guided Multilingual,\nMulti-turn instruction finetuning dataset, called M2Lingual. It is constructed\nby first selecting a diverse set of seed examples and then utilizing the\nproposed Evol taxonomy to convert these seeds into complex and challenging\nmulti-turn instructions. We demonstrate the effectiveness of M2Lingual by\ntraining LLMs of varying sizes and showcasing the enhanced performance across a\ndiverse set of languages. We contribute the 2 step Evol taxonomy with the\nguided generation code: https://github.com/ServiceNow/M2Lingual, as well as the\nfirst fully synthetic, general and task-oriented, multi-turn, multilingual\ndataset built with Evol - M2Lingual:\nhttps://huggingface.co/datasets/ServiceNow-AI/ M2Lingual - containing 182K\ntotal IFT pairs, covering 70 languages and 17+ NLP tasks.\n","authors":["Rishabh Maheshwary","Vikas Yadav","Hoang Nguyen","Khyati Mahajan","Sathwik Tejaswi Madhusudhan"],"pdf_url":"https://arxiv.org/pdf/2406.16783v3.pdf","comment":"39 pages"},{"id":"http://arxiv.org/abs/2406.12296v2","updated":"2025-03-04T07:52:08Z","published":"2024-06-18T06:00:18Z","title":"Generative Artificial Intelligence-Guided User Studies: An Application\n  for Air Taxi Services","summary":"  User studies are crucial for meeting user needs. In user studies, real\nexperimental scenarios and participants are constructed and recruited. However,\nemerging and unfamiliar studies face limitations, including safety concerns and\niterative efficiency. To address these challenges, this study utilises a\nGenerative Artificial Intelligence (GenAI) to create GenAI-generated scenarios\nfor user experience (UX). By recruiting real users to evaluate this experience,\nwe can collect feedback that enables rapid iteration in the early design phase.\nThe air taxi is particularly representative of these challenges and has been\nchosen as the case study for this research. The key contribution was designing\nan Air Taxi Journey (ATJ) using Large Language Models (LLMs) and AI image and\nvideo generators. Based on the GPT-4-generated scripts, key visuals were\ncreated for the air taxi, and the ATJ was evaluated by 72 participants.\nFurthermore, the LLMs demonstrated the ability to identify and suggest\nenvironments that significantly improve participants' willingness toward air\ntaxis. Education level and gender significantly influenced participants' the\ndifference in willingness and their satisfaction with the ATJ. Satisfaction\nwith the ATJ serves as a mediator, significantly influencing participants'\nwillingness to take air taxis. Our study confirms the capability of GenAI to\nsupport user studies, providing a feasible approach and valuable insights for\ndesigning air taxi UX in the early design phase.\n","authors":["Shengdi Xiao","Jingjing Li","Tatsuki Fushimi","Yoichi Ochiai"],"pdf_url":"https://arxiv.org/pdf/2406.12296v2.pdf","comment":"39 pages, 6 main figures, 10 appendix figures"},{"id":"http://arxiv.org/abs/2309.14362v2","updated":"2025-03-04T07:50:39Z","published":"2023-09-23T10:37:57Z","title":"Diversifying Question Generation over Knowledge Base via External\n  Natural Questions","summary":"  Previous methods on knowledge base question generation (KBQG) primarily focus\non enhancing the quality of a single generated question. Recognizing the\nremarkable paraphrasing ability of humans, we contend that diverse texts should\nconvey the same semantics through varied expressions. The above insights make\ndiversifying question generation an intriguing task, where the first challenge\nis evaluation metrics for diversity. Current metrics inadequately assess the\nabove diversity since they calculate the ratio of unique n-grams in the\ngenerated question itself, which leans more towards measuring duplication\nrather than true diversity. Accordingly, we devise a new diversity evaluation\nmetric, which measures the diversity among top-k generated questions for each\ninstance while ensuring their relevance to the ground truth. Clearly, the\nsecond challenge is how to enhance diversifying question generation. To address\nthis challenge, we introduce a dual model framework interwoven by two selection\nstrategies to generate diverse questions leveraging external natural questions.\nThe main idea of our dual framework is to extract more diverse expressions and\nintegrate them into the generation model to enhance diversifying question\ngeneration. Extensive experiments on widely used benchmarks for KBQG\ndemonstrate that our proposed approach generates highly diverse questions and\nimproves the performance of question answering tasks.\n","authors":["Shasha Guo","Jing Zhang","Xirui Ke","Cuiping Li","Hong Chen"],"pdf_url":"https://arxiv.org/pdf/2309.14362v2.pdf","comment":"13 pages, 2 figures"},{"id":"http://arxiv.org/abs/2503.02369v1","updated":"2025-03-04T07:50:32Z","published":"2025-03-04T07:50:32Z","title":"JPDS-NN: Reinforcement Learning-Based Dynamic Task Allocation for\n  Agricultural Vehicle Routing Optimization","summary":"  The Entrance Dependent Vehicle Routing Problem (EDVRP) is a variant of the\nVehicle Routing Problem (VRP) where the scale of cities influences routing\noutcomes, necessitating consideration of their entrances. This paper addresses\nEDVRP in agriculture, focusing on multi-parameter vehicle planning for\nirregularly shaped fields. To address the limitations of traditional methods,\nsuch as heuristic approaches, which often overlook field geometry and entrance\nconstraints, we propose a Joint Probability Distribution Sampling Neural\nNetwork (JPDS-NN) to effectively solve the EDVRP. The network uses an\nencoder-decoder architecture with graph transformers and attention mechanisms\nto model routing as a Markov Decision Process, and is trained via reinforcement\nlearning for efficient and rapid end-to-end planning. Experimental results\nindicate that JPDS-NN reduces travel distances by 48.4-65.4%, lowers fuel\nconsumption by 14.0-17.6%, and computes two orders of magnitude faster than\nbaseline methods, while demonstrating 15-25% superior performance in dynamic\narrangement scenarios. Ablation studies validate the necessity of\ncross-attention and pre-training. The framework enables scalable, intelligent\nrouting for large-scale farming under dynamic constraints.\n","authors":["Yixuan Fan","Haotian Xu","Mengqiao Liu","Qing Zhuo","Tao Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.02369v1.pdf","comment":"8 pages, 7 figures, submitted to IROS 2025"},{"id":"http://arxiv.org/abs/2503.02365v1","updated":"2025-03-04T07:45:45Z","published":"2025-03-04T07:45:45Z","title":"EchoQA: A Large Collection of Instruction Tuning Data for Echocardiogram\n  Reports","summary":"  We introduce a novel question-answering (QA) dataset using echocardiogram\nreports sourced from the Medical Information Mart for Intensive Care database.\nThis dataset is specifically designed to enhance QA systems in cardiology,\nconsisting of 771,244 QA pairs addressing a wide array of cardiac abnormalities\nand their severity. We compare large language models (LLMs), including\nopen-source and biomedical-specific models for zero-shot evaluation, and\nclosed-source models for zero-shot and three-shot evaluation. Our results show\nthat fine-tuning LLMs improves performance across various QA metrics,\nvalidating the value of our dataset. Clinicians also qualitatively evaluate the\nbest-performing model to assess the LLM responses for correctness. Further, we\nconduct fine-grained fairness audits to assess the bias-performance trade-off\nof LLMs across various social determinants of health. Our objective is to\npropel the field forward by establishing a benchmark for LLM AI agents aimed at\nsupporting clinicians with cardiac differential diagnoses, thereby reducing the\ndocumentation burden that contributes to clinician burnout and enabling\nhealthcare professionals to focus more on patient care.\n","authors":["Lama Moukheiber","Mira Moukheiber","Dana Moukheiiber","Hyung-Chul Lee"],"pdf_url":"https://arxiv.org/pdf/2503.02365v1.pdf","comment":"NeurIPS SafeGenAI 2024"},{"id":"http://arxiv.org/abs/2412.01550v2","updated":"2025-03-04T07:37:01Z","published":"2024-12-02T14:37:57Z","title":"SeqAfford: Sequential 3D Affordance Reasoning via Multimodal Large\n  Language Model","summary":"  3D affordance segmentation aims to link human instructions to touchable\nregions of 3D objects for embodied manipulations. Existing efforts typically\nadhere to single-object, single-affordance paradigms, where each affordance\ntype or explicit instruction strictly corresponds to a specific affordance\nregion and are unable to handle long-horizon tasks. Such a paradigm cannot\nactively reason about complex user intentions that often imply sequential\naffordances. In this paper, we introduce the Sequential 3D Affordance Reasoning\ntask, which extends the traditional paradigm by reasoning from cumbersome user\nintentions and then decomposing them into a series of segmentation maps. Toward\nthis, we construct the first instruction-based affordance segmentation\nbenchmark that includes reasoning over both single and sequential affordances,\ncomprising 180K instruction-point cloud pairs. Based on the benchmark, we\npropose our model, SeqAfford, to unlock the 3D multi-modal large language model\nwith additional affordance segmentation abilities, which ensures reasoning with\nworld knowledge and fine-grained affordance grounding in a cohesive framework.\nWe further introduce a multi-granular language-point integration module to\nendow 3D dense prediction. Extensive experimental evaluations show that our\nmodel excels over well-established methods and exhibits open-world\ngeneralization with sequential reasoning abilities.\n","authors":["Chunlin Yu","Hanqing Wang","Ye Shi","Haoyang Luo","Sibei Yang","Jingyi Yu","Jingya Wang"],"pdf_url":"https://arxiv.org/pdf/2412.01550v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02360v1","updated":"2025-03-04T07:34:06Z","published":"2025-03-04T07:34:06Z","title":"BdSLW401: Transformer-Based Word-Level Bangla Sign Language Recognition\n  Using Relative Quantization Encoding (RQE)","summary":"  Sign language recognition (SLR) for low-resource languages like Bangla\nsuffers from signer variability, viewpoint variations, and limited annotated\ndatasets. In this paper, we present BdSLW401, a large-scale, multi-view,\nword-level Bangla Sign Language (BdSL) dataset with 401 signs and 102,176 video\nsamples from 18 signers in front and lateral views. To improve\ntransformer-based SLR, we introduce Relative Quantization Encoding (RQE), a\nstructured embedding approach anchoring landmarks to physiological reference\npoints and quantize motion trajectories. RQE improves attention allocation by\ndecreasing spatial variability, resulting in 44.3% WER reduction in WLASL100,\n21.0% in SignBD-200, and significant gains in BdSLW60 and SignBD-90. However,\nfixed quantization becomes insufficient on large-scale datasets (e.g.,\nWLASL2000), indicating the need for adaptive encoding strategies. Further,\nRQE-SF, an extended variant that stabilizes shoulder landmarks, achieves\nimprovements in pose consistency at the cost of small trade-offs in lateral\nview recognition. The attention graphs prove that RQE improves model\ninterpretability by focusing on the major articulatory features (fingers,\nwrists) and the more distinctive frames instead of global pose changes.\nIntroducing BdSLW401 and demonstrating the effectiveness of RQE-enhanced\nstructured embeddings, this work advances transformer-based SLR for\nlow-resource languages and sets a benchmark for future research in this area.\n","authors":["Husne Ara Rubaiyeat","Njayou Youssouf","Md Kamrul Hasan","Hasan Mahmud"],"pdf_url":"https://arxiv.org/pdf/2503.02360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.05874v2","updated":"2025-03-04T07:29:52Z","published":"2025-01-10T11:17:15Z","title":"VideoRAG: Retrieval-Augmented Generation over Video Corpus","summary":"  Retrieval-Augmented Generation (RAG) is a powerful strategy for improving the\nfactual accuracy of models by retrieving external knowledge relevant to queries\nand incorporating it into the generation process. However, existing approaches\nprimarily focus on text, with some recent advancements considering images, and\nthey largely overlook videos, a rich source of multimodal knowledge capable of\nrepresenting contextual details more effectively than any other modality. While\nvery recent studies explore the use of videos in response generation, they\neither predefine query-associated videos without retrieval or convert videos\ninto textual descriptions losing multimodal richness. To tackle these, we\nintroduce VideoRAG, a framework that not only dynamically retrieves videos\nbased on their relevance with queries but also utilizes both visual and textual\ninformation. The operation of VideoRAG is powered by recent Large Video\nLanguage Models (LVLMs), which enable the direct processing of video content to\nrepresent it for retrieval and the seamless integration of retrieved videos\njointly with queries for response generation. Also, inspired by that the\ncontext size of LVLMs may not be sufficient to process all frames in extremely\nlong videos and not all frames are equally important, we introduce a video\nframe selection mechanism to extract the most informative subset of frames,\nalong with a strategy to extract textual information from videos (as it can aid\nthe understanding of video content) when their subtitles are not available. We\nexperimentally validate the effectiveness of VideoRAG, showcasing that it is\nsuperior to relevant baselines. Code is available at\nhttps://github.com/starsuzi/VideoRAG.\n","authors":["Soyeong Jeong","Kangsan Kim","Jinheon Baek","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2501.05874v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02358v1","updated":"2025-03-04T07:29:03Z","published":"2025-03-04T07:29:03Z","title":"Are Large Vision Language Models Good Game Players?","summary":"  Large Vision Language Models (LVLMs) have demonstrated remarkable abilities\nin understanding and reasoning about both visual and textual information.\nHowever, existing evaluation methods for LVLMs, primarily based on benchmarks\nlike Visual Question Answering and image captioning, often fail to capture the\nfull scope of LVLMs' capabilities. These benchmarks are limited by issues such\nas inadequate assessment of detailed visual perception, data contamination, and\na lack of focus on multi-turn reasoning. To address these challenges, we\npropose \\method{}, a game-based evaluation framework designed to provide a\ncomprehensive assessment of LVLMs' cognitive and reasoning skills in structured\nenvironments. \\method{} uses a set of games to evaluate LVLMs on four core\ntasks: Perceiving, Question Answering, Rule Following, and End-to-End Playing,\nwith each target task designed to assess specific abilities, including visual\nperception, reasoning, decision-making, etc. Based on this framework, we\nconduct extensive experiments that explore the limitations of current LVLMs,\nsuch as handling long structured outputs and perceiving detailed and dense\nelements. Code and data are publicly available at\nhttps://github.com/xinke-wang/LVLM-Playground.\n","authors":["Xinyu Wang","Bohan Zhuang","Qi Wu"],"pdf_url":"https://arxiv.org/pdf/2503.02358v1.pdf","comment":"ICLR2025"},{"id":"http://arxiv.org/abs/2503.02354v1","updated":"2025-03-04T07:25:05Z","published":"2025-03-04T07:25:05Z","title":"CoServe: Efficient Collaboration-of-Experts (CoE) Model Inference with\n  Limited Memory","summary":"  Large language models like GPT-4 are resource-intensive, but recent\nadvancements suggest that smaller, specialized experts can outperform the\nmonolithic models on specific tasks. The Collaboration-of-Experts (CoE)\napproach integrates multiple expert models, improving the accuracy of generated\nresults and offering great potential for precision-critical applications, such\nas automatic circuit board quality inspection. However, deploying CoE serving\nsystems presents challenges to memory capacity due to the large number of\nexperts required, which can lead to significant performance overhead from\nfrequent expert switching across different memory and storage tiers.\n  We propose CoServe, an efficient CoE model serving system on heterogeneous\nCPU and GPU with limited memory. CoServe reduces unnecessary expert switching\nby leveraging expert dependency, a key property of CoE inference. CoServe\nintroduces a dependency-aware request scheduler and dependency-aware expert\nmanagement for efficient inference. It also introduces an offline profiler to\nautomatically find optimal resource allocation on various processors and\ndevices. In real-world intelligent manufacturing workloads, CoServe achieves\n4.5$\\times$ to 12$\\times$ higher throughput compared to state-of-the-art\nsystems.\n","authors":["Jiashun Suo","Xiaojian Liao","Limin Xiao","Li Ruan","Jinquan Wang","Xiao Su","Zhisheng Huo"],"pdf_url":"https://arxiv.org/pdf/2503.02354v1.pdf","comment":"Accepted to ASPLOS '25"},{"id":"http://arxiv.org/abs/2503.02351v1","updated":"2025-03-04T07:20:42Z","published":"2025-03-04T07:20:42Z","title":"MindSimulator: Exploring Brain Concept Localization via Synthetic FMRI","summary":"  Concept-selective regions within the human cerebral cortex exhibit\nsignificant activation in response to specific visual stimuli associated with\nparticular concepts. Precisely localizing these regions stands as a crucial\nlong-term goal in neuroscience to grasp essential brain functions and\nmechanisms. Conventional experiment-driven approaches hinge on manually\nconstructed visual stimulus collections and corresponding brain activity\nrecordings, constraining the support and coverage of concept localization.\nAdditionally, these stimuli often consist of concept objects in unnatural\ncontexts and are potentially biased by subjective preferences, thus prompting\nconcerns about the validity and generalizability of the identified regions. To\naddress these limitations, we propose a data-driven exploration approach. By\nsynthesizing extensive brain activity recordings, we statistically localize\nvarious concept-selective regions. Our proposed MindSimulator leverages\nadvanced generative technologies to learn the probability distribution of brain\nactivity conditioned on concept-oriented visual stimuli. This enables the\ncreation of simulated brain recordings that reflect real neural response\npatterns. Using the synthetic recordings, we successfully localize several\nwell-studied concept-selective regions and validate them against empirical\nfindings, achieving promising prediction accuracy. The feasibility opens\navenues for exploring novel concept-selective regions and provides prior\nhypotheses for future neuroscience research.\n","authors":["Guangyin Bao","Qi Zhang","Zixuan Gong","Zhuojia Wu","Duoqian Miao"],"pdf_url":"https://arxiv.org/pdf/2503.02351v1.pdf","comment":"23 pages, ICLR 2025"},{"id":"http://arxiv.org/abs/2409.14106v4","updated":"2025-03-04T07:17:48Z","published":"2024-09-21T11:19:15Z","title":"Advancing Molecular Graph-Text Pre-training via Fine-grained Alignment","summary":"  Understanding molecular structure and related knowledge is crucial for\nscientific research. Recent studies integrate molecular graphs with their\ntextual descriptions to enhance molecular representation learning. However,\nthey focus on the whole molecular graph and neglect frequently occurring\nsubgraphs, known as motifs, which are essential for determining molecular\nproperties. Without such fine-grained knowledge, these models struggle to\ngeneralize to unseen molecules and tasks that require motif-level insights. To\nbridge this gap, we propose FineMolTex, a novel Fine-grained Molecular\ngraph-Text pre-training framework to jointly learn coarse-grained\nmolecule-level knowledge and fine-grained motif-level knowledge. Specifically,\nFineMolTex consists of two pre-training tasks: a contrastive alignment task for\ncoarse-grained matching and a masked multi-modal modeling task for fine-grained\nmatching. In particular, the latter predicts the labels of masked motifs and\nwords, which are selected based on their importance. By leveraging insights\nfrom both modalities, FineMolTex is able to understand the fine-grained\nmatching between motifs and words. Finally, we conduct extensive experiments\nacross three downstream tasks, achieving up to 238% improvement in the\ntext-based molecule editing task. Additionally, our case studies reveal that\nFineMolTex successfully captures fine-grained knowledge, potentially offering\nvaluable insights for drug discovery and catalyst design.\n","authors":["Yibo Li","Yuan Fang","Mengmei Zhang","Chuan Shi"],"pdf_url":"https://arxiv.org/pdf/2409.14106v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.22839v2","updated":"2025-03-04T07:13:20Z","published":"2024-10-30T09:18:31Z","title":"Danoliteracy of Generative Large Language Models","summary":"  The language technology moonshot moment of Generative Large Language Models\n(GLLMs) was not limited to English: These models brought a surge of\ntechnological applications, investments, and hype to low-resource languages as\nwell. However, the capabilities of these models in languages such as Danish\nwere, until recently, difficult to verify beyond qualitative demonstrations due\nto a lack of applicable evaluation corpora. We present a GLLM benchmark to\nevaluate \\emph{Danoliteracy}, a measure of Danish language and cultural\ncompetency across eight diverse scenarios such as Danish citizenship tests and\nabstractive social media question answering. This limited-size benchmark was\nfound to produce a robust ranking that correlates to human feedback at $\\rho\n\\sim 0.8$ with GPT-4 and Claude Opus models achieving the highest rankings.\nAnalyzing these model results across scenarios, we find one strong underlying\nfactor explaining $95\\%$ of scenario performance variance for GLLMs in Danish,\nsuggesting a $g$ factor of model consistency in language adaptation.\n","authors":["Søren Vejlgaard Holm","Lars Kai Hansen","Martin Carsten Nielsen"],"pdf_url":"https://arxiv.org/pdf/2410.22839v2.pdf","comment":"16 pages, 13 figures, Accepted to NoDaLiDa/Baltic-HLT 2025"},{"id":"http://arxiv.org/abs/2502.15859v3","updated":"2025-03-04T07:10:54Z","published":"2025-02-21T10:16:56Z","title":"AI Governance InternationaL Evaluation Index (AGILE Index)","summary":"  The rapid advancement of Artificial Intelligence (AI) technology is\nprofoundly transforming human society and concurrently presenting a series of\nethical, legal, and social issues. The effective governance of AI has become a\ncrucial global concern. Since 2022, the extensive deployment of generative AI,\nparticularly large language models, marked a new phase in AI governance.\nContinuous efforts are being made by the international community in actively\naddressing the novel challenges posed by these AI developments. As consensus on\ninternational governance continues to be established and put into action, the\npractical importance of conducting a global assessment of the state of AI\ngovernance is progressively coming to light. In this context, we initiated the\ndevelopment of the AI Governance InternationaL Evaluation Index (AGILE Index).\nAdhering to the design principle, \"the level of governance should match the\nlevel of development,\" the inaugural evaluation of the AGILE Index commences\nwith an exploration of four foundational pillars: the development level of AI,\nthe AI governance environment, the AI governance instruments, and the AI\ngovernance effectiveness. It covers 39 indicators across 18 dimensions to\ncomprehensively assess the AI governance level of 14 representative countries\nglobally. The index is utilized to delve into the status of AI governance to\ndate in 14 countries for the first batch of evaluation. The aim is to depict\nthe current state of AI governance in these countries through data scoring,\nassist them in identifying their governance stage and uncovering governance\nissues, and ultimately offer insights for the enhancement of their AI\ngovernance systems.\n","authors":["Yi Zeng","Enmeng Lu","Xin Guan","Cunqing Huangfu","Zizhe Ruan","Ammar Younas","Kang Sun","Xuan Tang","Yuwei Wang","Hongjie Suo","Dongqi Liang","Zhengqiang Han","Aorigele Bao","Xiaoyang Guo","Jin Wang","Jiawei Xie","Yao Liang"],"pdf_url":"https://arxiv.org/pdf/2502.15859v3.pdf","comment":"Evaluation Report. 85 pages, 30 Figures"},{"id":"http://arxiv.org/abs/2503.02345v1","updated":"2025-03-04T07:08:47Z","published":"2025-03-04T07:08:47Z","title":"CQ CNN: A Hybrid Classical Quantum Convolutional Neural Network for\n  Alzheimer's Disease Detection Using Diffusion Generated and U Net Segmented\n  3D MRI","summary":"  The detection of Alzheimer disease (AD) from clinical MRI data is an active\narea of research in medical imaging. Recent advances in quantum computing,\nparticularly the integration of parameterized quantum circuits (PQCs) with\nclassical machine learning architectures, offer new opportunities to develop\nmodels that may outperform traditional methods. However, quantum machine\nlearning (QML) remains in its early stages and requires further experimental\nanalysis to better understand its behavior and limitations. In this paper, we\npropose an end to end hybrid classical quantum convolutional neural network (CQ\nCNN) for AD detection using clinically formatted 3D MRI data. Our approach\ninvolves developing a framework to make 3D MRI data usable for machine\nlearning, designing and training a brain tissue segmentation model (Skull Net),\nand training a diffusion model to generate synthetic images for the minority\nclass. Our converged models exhibit potential quantum advantages, achieving\nhigher accuracy in fewer epochs than classical models. The proposed beta8 3\nqubit model achieves an accuracy of 97.50%, surpassing state of the art (SOTA)\nmodels while requiring significantly fewer computational resources. In\nparticular, the architecture employs only 13K parameters (0.48 MB), reducing\nthe parameter count by more than 99.99% compared to current SOTA models.\nFurthermore, the diffusion-generated data used to train our quantum models, in\nconjunction with real samples, preserve clinical structural standards,\nrepresenting a notable first in the field of QML. We conclude that CQCNN\narchitecture like models, with further improvements in gradient optimization\ntechniques, could become a viable option and even a potential alternative to\nclassical models for AD detection, especially in data limited and resource\nconstrained clinical settings.\n","authors":["Mominul Islam","Mohammad Junayed Hasan","M. R. C. Mahdy"],"pdf_url":"https://arxiv.org/pdf/2503.02345v1.pdf","comment":"Application of hybrid quantum-classical machine learning for (early\n  stage) disease detection"},{"id":"http://arxiv.org/abs/2402.16281v2","updated":"2025-03-04T07:06:42Z","published":"2024-02-26T03:54:32Z","title":"RobKiNet: Robotic Kinematics Informed Neural Network for Optimal Robot\n  Configuration Prediction","summary":"  Task and Motion Planning (TAMP) is essential for robots to interact with the\nworld and accomplish complex tasks. The TAMP problem involves a critical gap:\nexploring the robot's configuration parameters (such as chassis position and\nrobotic arm joint angles) within continuous space to ensure that task-level\nglobal constraints are met while also enhancing the efficiency of subsequent\nmotion planning. Existing methods still have significant room for improvement\nin terms of efficiency. Recognizing that robot kinematics is a key factor in\nmotion planning, we propose a framework called the Robotic Kinematics Informed\nNeural Network (RobKiNet) as a bridge between task and motion layers. RobKiNet\nintegrates kinematic knowledge into neural networks to train models capable of\nefficient configuration prediction. We designed a Chassis Motion Predictor(CMP)\nand a Full Motion Predictor(FMP) using RobKiNet, which employed two entirely\ndifferent sets of forward and inverse kinematics constraints to achieve loosely\ncoupled control and whole-body control, respectively. Experiments demonstrate\nthat CMP and FMP can predict configuration parameters with 96.67% and 98%\naccuracy, respectively. That means that the corresponding motion planning can\nachieve a speedup of 24.24x and 153x compared to random sampling. Furthermore,\nRobKiNet demonstrates remarkable data efficiency. CMP only requires 1/71 and\nFMP only requires 1/15052 of the training data for the same prediction accuracy\ncompared to other deep learning methods. These results demonstrate the great\npotential of RoboKiNet in robot applications.\n","authors":["Yanlong Peng","Zhigang Wang","Yisheng Zhang","Pengxu Chang","Ziwen He","Kai Gu","Hongshen Zhang","Ming Chen"],"pdf_url":"https://arxiv.org/pdf/2402.16281v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.03905v2","updated":"2025-03-04T07:06:35Z","published":"2024-12-05T06:21:31Z","title":"Integrating Various Software Artifacts for Better LLM-based Bug\n  Localization and Program Repair","summary":"  LLMs have garnered considerable attention for their potential to streamline\nAutomated Program Repair (APR). LLM-based approaches can either insert the\ncorrect code or directly generate patches when provided with buggy methods.\nHowever, most of LLM-based APR methods rely on a single type of software\ninformation, without fully leveraging different software artifacts. Despite\nthis, many LLM-based approaches do not explore which specific types of\ninformation best assist in APR. Addressing this gap is crucial for advancing\nLLM-based APR techniques. We propose DEVLoRe to use issue content (description\nand message) and stack error traces to localize buggy methods, then rely on\ndebug information in buggy methods and issue content and stack error to\nlocalize buggy lines and generate plausible patches which can pass all unit\ntests. The results show that while issue content is particularly effective in\nassisting LLMs with fault localization and program repair, different types of\nsoftware artifacts complement each other. By incorporating different artifacts,\nDEVLoRe successfully locates 49.3% and 47.6% of single and non-single buggy\nmethods and generates 56.0% and 14.5% plausible patches for the Defects4J v2.0\ndataset, respectively. This outperforms current state-of-the-art APR methods.\nThe source code and experimental results of this work for replication are\navailable at https://github.com/XYZboom/DEVLoRe.\n","authors":["Qiong Feng","Xiaotian Ma","Jiayi Sheng","Ziyuan Feng","Wei Song","Peng Liang"],"pdf_url":"https://arxiv.org/pdf/2412.03905v2.pdf","comment":"22 pages, 11 images, 9 tables, Manuscript submitted to a journal\n  (2024)"},{"id":"http://arxiv.org/abs/2503.02341v1","updated":"2025-03-04T07:04:55Z","published":"2025-03-04T07:04:55Z","title":"GRADEO: Towards Human-Like Evaluation for Text-to-Video Generation via\n  Multi-Step Reasoning","summary":"  Recent great advances in video generation models have demonstrated their\npotential to produce high-quality videos, bringing challenges to effective\nevaluation. Unlike human evaluation, existing automated evaluation metrics lack\nhigh-level semantic understanding and reasoning capabilities for video, thus\nmaking them infeasible and unexplainable. To fill this gap, we curate\nGRADEO-Instruct, a multi-dimensional T2V evaluation instruction tuning dataset,\nincluding 3.3k videos from over 10 existing video generation models and\nmulti-step reasoning assessments converted by 16k human annotations. We then\nintroduce GRADEO, one of the first specifically designed video evaluation\nmodels, which grades AI-generated videos for explainable scores and assessments\nthrough multi-step reasoning. Experiments show that our method aligns better\nwith human evaluations than existing methods. Furthermore, our benchmarking\nreveals that current video generation models struggle to produce content that\naligns with human reasoning and complex real-world scenarios. The models,\ndatasets, and codes will be released soon.\n","authors":["Zhun Mou","Bin Xia","Zhengchao Huang","Wenming Yang","Jiaya Jia"],"pdf_url":"https://arxiv.org/pdf/2503.02341v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.15061v2","updated":"2025-03-04T07:00:07Z","published":"2025-01-25T03:46:35Z","title":"PolaFormer: Polarity-aware Linear Attention for Vision Transformers","summary":"  Linear attention has emerged as a promising alternative to softmax-based\nattention, leveraging kernelized feature maps to reduce complexity from\nquadratic to linear in sequence length. However, the non-negative constraint on\nfeature maps and the relaxed exponential function used in approximation lead to\nsignificant information loss compared to the original query-key dot products,\nresulting in less discriminative attention maps with higher entropy. To address\nthe missing interactions driven by negative values in query-key pairs, we\npropose a polarity-aware linear attention mechanism that explicitly models both\nsame-signed and opposite-signed query-key interactions, ensuring comprehensive\ncoverage of relational information. Furthermore, to restore the spiky\nproperties of attention maps, we provide a theoretical analysis proving the\nexistence of a class of element-wise functions (with positive first and second\nderivatives) that can reduce entropy in the attention distribution. For\nsimplicity, and recognizing the distinct contributions of each dimension, we\nemploy a learnable power function for rescaling, allowing strong and weak\nattention signals to be effectively separated. Extensive experiments\ndemonstrate that the proposed PolaFormer improves performance on various vision\ntasks, enhancing both expressiveness and efficiency by up to 4.6%.\n","authors":["Weikang Meng","Yadan Luo","Xin Li","Dongmei Jiang","Zheng Zhang"],"pdf_url":"https://arxiv.org/pdf/2501.15061v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18168v4","updated":"2025-03-04T06:59:18Z","published":"2025-02-25T13:00:05Z","title":"SECURA: Sigmoid-Enhanced CUR Decomposition with Uninterrupted Retention\n  and Low-Rank Adaptation in Large Language Models","summary":"  With the rapid development of large language models (LLMs), fully fine-tuning\n(FT) these models is becoming increasingly infeasible due to high computational\ndemands. Moreover, FT also increases the risk of catastrophic forgetting. As an\nalternative, Low-Rank Adaptation (LoRA) has been proposed. By fine-tuning only\na small subset of parameters, LoRA achieves performance similar to FT while\nsignificantly reducing resource requirements. However, since LoRA inherits FT's\ndesign, the issue of catastrophic forgetting still remains. To address these\nlimitations, we propose SECURA: Sigmoid-Enhanced CUR Decomposition LoRA, a\nnovel PEFT variant designed to mitigate catastrophic forgetting while improving\nfine-tuning performance. Our method introduces a novel normalization technique,\nSigmoid-based Magnitude Norm (S-MagNorm), which enhances parameter retention\nand fine-tuning efficiency. SECURA has been evaluated on a diverse range of\ntasks, including mathematical problem-solving (GSM8K), complex\nquestion-answering (CNNDM), translation (NewsDE), and complex multiple-choice\nreasoning (LogiQA). Experimental results demonstrate that it achieves an\naverage fine-tuning improvement of 3.59% across four MCQ tasks and 2.51% across\nfive QA tasks on Gemma2 2B, Qwen2 1.5B, Qwen2 7B, Llama3 8B, and Llama3.1 8B,\noutperforming DoRA. Additionally, SECURA demonstrates superior knowledge\nretention capabilities, achieving state-of-the-art performance in 16 continual\nlearning tests and maintaining more than 70% accuracy on LLMs' basic knowledge\ncompared to Experience Replay (ER), sequential learning (SEQ), EWC, I-LoRA, and\nCUR-LoRA.\n","authors":["Yuxuan Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.18168v4.pdf","comment":"New work on PEFT for LLMs, introducing S-MagNorm and CABR-LoRA to\n  enhance fine-tuning performance and knowledge retention. In v4, we renamed\n  Sigmoid-based Magnitude Normalization to S-MagNorm for clarity and added a\n  gradient comparison between SECURA and CABR-LoRA to highlight their\n  contributions"},{"id":"http://arxiv.org/abs/2503.02338v1","updated":"2025-03-04T06:59:01Z","published":"2025-03-04T06:59:01Z","title":"Enhancing the Product Quality of the Injection Process Using eXplainable\n  Artificial Intelligence","summary":"  The injection molding process is a traditional technique for making products\nin various industries such as electronics and automobiles via solidifying\nliquid resin into certain molds. Although the process is not related to\ncreating the main part of engines or semiconductors, this manufacturing\nmethodology sets the final form of the products. Re-cently, research has\ncontinued to reduce the defect rate of the injection molding process. This\nstudy proposes an optimal injection molding process control system to reduce\nthe defect rate of injection molding products with XAI (eXplainable Artificial\nIntelligence) ap-proaches. Boosting algorithms (XGBoost and LightGBM) are used\nas tree-based classifiers for predicting whether each product is normal or\ndefective. The main features to control the process for improving the product\nare extracted by SHapley Additive exPlanations, while the individual\nconditional expectation analyzes the optimal control range of these extracted\nfeatures. To validate the methodology presented in this work, the actual\ninjection molding AI manufacturing dataset provided by KAMP (Korea AI\nManufacturing Platform) is employed for the case study. The results reveal that\nthe defect rate decreases from 1.00% (Original defect rate) to 0.21% with\nXGBoost and 0.13% with LightGBM, respectively.\n","authors":["Jisoo Hong","Yongmin Hong","Jung-Woo Baek","Sung-Woo Kang"],"pdf_url":"https://arxiv.org/pdf/2503.02338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.05109v4","updated":"2025-03-04T06:51:36Z","published":"2024-08-09T14:59:36Z","title":"A Survey of NL2SQL with Large Language Models: Where are we, and where\n  are we going?","summary":"  Translating users' natural language queries (NL) into SQL queries (i.e.,\nNL2SQL, a.k.a., Text-to-SQL) can significantly reduce barriers to accessing\nrelational databases and support various commercial applications. The\nperformance of NL2SQL has been greatly enhanced with the emergence of Large\nLanguage Models (LLMs). In this survey, we provide a comprehensive review of\nNL2SQL techniques powered by LLMs, covering its entire lifecycle from the\nfollowing four aspects: (1) Model: NL2SQL translation techniques that tackle\nnot only NL ambiguity and under-specification, but also properly map NL with\ndatabase schema and instances; (2) Data: From the collection of training data,\ndata synthesis due to training data scarcity, to NL2SQL benchmarks; (3)\nEvaluation: Evaluating NL2SQL methods from multiple angles using different\nmetrics and granularities; and (4) Error Analysis: analyzing NL2SQL errors to\nfind the root cause and guiding NL2SQL models to evolve. Moreover, we provide a\nrule of thumb for developing NL2SQL solutions. Finally, we discuss the research\nchallenges and open problems of NL2SQL in the LLMs era.\n","authors":["Xinyu Liu","Shuyu Shen","Boyan Li","Peixian Ma","Runzhi Jiang","Yuxin Zhang","Ju Fan","Guoliang Li","Nan Tang","Yuyu Luo"],"pdf_url":"https://arxiv.org/pdf/2408.05109v4.pdf","comment":"20 pages, 11 figures, 2 tables"},{"id":"http://arxiv.org/abs/2503.01220v2","updated":"2025-03-04T06:50:03Z","published":"2025-03-03T06:37:30Z","title":"Tera-MIND: Tera-scale mouse brain simulation via spatial mRNA-guided\n  diffusion","summary":"  Holistic 3D modeling of molecularly defined brain structures is crucial for\nunderstanding complex brain functions. Emerging tissue profiling technologies\nenable the construction of a comprehensive atlas of the mammalian brain with\nsub-cellular resolution and spatially resolved gene expression data. However,\nsuch tera-scale volumetric datasets present significant computational\nchallenges in understanding complex brain functions within their native 3D\nspatial context. Here, we propose the novel generative approach\n$\\textbf{Tera-MIND}$, which can simulate $\\textbf{Tera}$-scale $\\textbf{M}$ouse\nbra$\\textbf{IN}$s in 3D using a patch-based and boundary-aware\n$\\textbf{D}$iffusion model. Taking spatial transcriptomic data as the\nconditional input, we generate virtual mouse brains with comprehensive cellular\nmorphological detail at teravoxel scale. Through the lens of 3D $gene$-$gene$\nself-attention, we identify spatial molecular interactions for key\ntranscriptomic pathways in the murine brain, exemplified by glutamatergic and\ndopaminergic neuronal systems. Importantly, these $in$-$silico$ biological\nfindings are consistent and reproducible across three tera-scale virtual mouse\nbrains. Therefore, Tera-MIND showcases a promising path toward efficient and\ngenerative simulations of whole organ systems for biomedical research. Project\nwebsite: https://musikisomorphie.github.io/Tera-MIND.html\n","authors":["Jiqing Wu","Ingrid Berg","Yawei Li","Ender Konukoglu","Viktor H. Koelzer"],"pdf_url":"https://arxiv.org/pdf/2503.01220v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02334v1","updated":"2025-03-04T06:45:54Z","published":"2025-03-04T06:45:54Z","title":"BiasICL: In-Context Learning and Demographic Biases of Vision Language\n  Models","summary":"  Vision language models (VLMs) show promise in medical diagnosis, but their\nperformance across demographic subgroups when using in-context learning (ICL)\nremains poorly understood. We examine how the demographic composition of\ndemonstration examples affects VLM performance in two medical imaging tasks:\nskin lesion malignancy prediction and pneumothorax detection from chest\nradiographs. Our analysis reveals that ICL influences model predictions through\nmultiple mechanisms: (1) ICL allows VLMs to learn subgroup-specific disease\nbase rates from prompts and (2) ICL leads VLMs to make predictions that perform\ndifferently across demographic groups, even after controlling for\nsubgroup-specific disease base rates. Our empirical results inform\nbest-practices for prompting current VLMs (specifically examining demographic\nsubgroup performance, and matching base rates of labels to target distribution\nat a bulk level and within subgroups), while also suggesting next steps for\nimproving our theoretical understanding of these models.\n","authors":["Sonnet Xu","Joseph Janizek","Yixing Jiang","Roxana Daneshjou"],"pdf_url":"https://arxiv.org/pdf/2503.02334v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02333v1","updated":"2025-03-04T06:45:17Z","published":"2025-03-04T06:45:17Z","title":"Examining the Mental Health Impact of Misinformation on Social Media\n  Using a Hybrid Transformer-Based Approach","summary":"  Social media has significantly reshaped interpersonal communication,\nfostering connectivity while also enabling the proliferation of misinformation.\nThe unchecked spread of false narratives has profound effects on mental health,\ncontributing to increased stress, anxiety, and misinformation-driven paranoia.\nThis study presents a hybrid transformer-based approach using a RoBERTa-LSTM\nclassifier to detect misinformation, assess its impact on mental health, and\nclassify disorders linked to misinformation exposure. The proposed models\ndemonstrate accuracy rates of 98.4, 87.8, and 77.3 in detecting misinformation,\nmental health implications, and disorder classification, respectively.\nFurthermore, Pearson's Chi-Squared Test for Independence (p-value = 0.003871)\nvalidates the direct correlation between misinformation and deteriorating\nmental well-being. This study underscores the urgent need for better\nmisinformation management strategies to mitigate its psychological\nrepercussions. Future research could explore broader datasets incorporating\nlinguistic, demographic, and cultural variables to deepen the understanding of\nmisinformation-induced mental health distress.\n","authors":["Sarvesh Arora","Sarthak Arora","Deepika Kumar","Vallari Agrawal","Vedika Gupta","Dipit Vasdev"],"pdf_url":"https://arxiv.org/pdf/2503.02333v1.pdf","comment":"20 pages"},{"id":"http://arxiv.org/abs/2011.13986v3","updated":"2025-03-04T06:42:03Z","published":"2020-11-27T20:40:45Z","title":"Reflective-Net: Learning from Explanations","summary":"  We examine whether data generated by explanation techniques, which promote a\nprocess of self-reflection, can improve classifier performance. Our work is\nbased on the idea that humans have the ability to make quick, intuitive\ndecisions as well as to reflect on their own thinking and learn from\nexplanations. To the best of our knowledge, this is the first time that the\npotential of mimicking this process by using explanations generated by\nexplainability methods has been explored. We found that combining explanations\nwith traditional labeled data leads to significant improvements in\nclassification accuracy and training efficiency across multiple image\nclassification datasets and convolutional neural network architectures. It is\nworth noting that during training, we not only used explanations for the\ncorrect or predicted class, but also for other classes. This serves multiple\npurposes, including allowing for reflection on potential outcomes and enriching\nthe data through augmentation.\n","authors":["Johannes Schneider","Michalis Vlachos"],"pdf_url":"https://arxiv.org/pdf/2011.13986v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05500v2","updated":"2025-03-04T06:34:37Z","published":"2024-10-07T21:12:32Z","title":"Residual Kolmogorov-Arnold Network for Enhanced Deep Learning","summary":"  Despite their immense success, deep neural networks (CNNs) are costly to\ntrain, while modern architectures can retain hundreds of convolutional layers\nin network depth. Standard convolutional operations are fundamentally limited\nby their linear nature along with fixed activations, where multiple layers are\nneeded to learn complex patterns, making this approach computationally\ninefficient and prone to optimization difficulties. As a result, we introduce\nRKAN (Residual Kolmogorov-Arnold Network), which could be easily implemented\ninto stages of traditional networks, such as ResNet. The module also integrates\npolynomial feature transformation that provides the expressive power of many\nconvolutional layers through learnable, non-linear feature refinement. Our\nproposed RKAN module offers consistent improvements over the base models on\nvarious well-known benchmark datasets, such as CIFAR-100, Food-101, and\nImageNet.\n","authors":["Ray Congrui Yu","Sherry Wu","Jiang Gui"],"pdf_url":"https://arxiv.org/pdf/2410.05500v2.pdf","comment":"Code is available at https://github.com/withray/residualKAN.git"},{"id":"http://arxiv.org/abs/2503.02324v1","updated":"2025-03-04T06:32:30Z","published":"2025-03-04T06:32:30Z","title":"PromptCoT: Synthesizing Olympiad-level Problems for Mathematical\n  Reasoning in Large Language Models","summary":"  The ability of large language models to solve complex mathematical problems\nhas progressed significantly, particularly for tasks requiring advanced\nreasoning. However, the scarcity of sufficiently challenging problems,\nparticularly at the Olympiad level, hinders further advancements. In this work,\nwe introduce PromptCoT, a novel approach for automatically generating\nhigh-quality Olympiad-level math problems. The proposed method synthesizes\ncomplex problems based on mathematical concepts and the rationale behind\nproblem construction, emulating the thought processes of experienced problem\ndesigners. We provide a theoretical analysis demonstrating that an optimal\nrationale should maximize both the likelihood of rationale generation given the\nassociated concepts and the likelihood of problem generation conditioned on\nboth the rationale and the concepts. Our method is evaluated on standard\nbenchmarks including GSM8K, MATH-500, and AIME2024, where it consistently\noutperforms existing problem generation methods. Furthermore, we demonstrate\nthat PromptCoT exhibits superior data scalability, consistently maintaining\nhigh performance as the dataset size increases, outperforming the baselines.\nThe implementation is available at https://github.com/zhaoxlpku/PromptCoT.\n","authors":["Xueliang Zhao","Wei Wu","Jian Guan","Lingpeng Kong"],"pdf_url":"https://arxiv.org/pdf/2503.02324v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2412.17256v2","updated":"2025-03-04T06:29:50Z","published":"2024-12-23T03:58:34Z","title":"B-STaR: Monitoring and Balancing Exploration and Exploitation in\n  Self-Taught Reasoners","summary":"  In the absence of extensive human-annotated data for complex reasoning tasks,\nself-improvement -- where models are trained on their own outputs -- has\nemerged as a primary method for enhancing performance. However, the critical\nfactors underlying the mechanism of these iterative self-improving methods\nremain poorly understood, such as under what conditions self-improvement is\neffective, and what are the bottlenecks in the current iterations. In this\nwork, we identify and propose methods to monitor two pivotal factors in this\niterative process: (1) the model's ability to generate sufficiently diverse\nresponses (exploration); and (2) the effectiveness of external rewards in\ndistinguishing high-quality candidates from lower-quality ones (exploitation).\nUsing mathematical reasoning as a case study, we begin with a quantitative\nanalysis to track the dynamics of exploration and exploitation, discovering\nthat a model's exploratory capabilities rapidly deteriorate over iterations,\nand the effectiveness of exploiting external rewards diminishes as well.\nMotivated by these findings, we introduce B-STaR, a Self-Taught Reasoning\nframework that autonomously adjusts configurations across iterations to Balance\nexploration and exploitation, thereby optimizing the self-improving\neffectiveness based on the current policy model and available rewards. Our\nexperiments on mathematical reasoning, coding, and commonsense reasoning\ndemonstrate that B-STaR not only enhances the model's exploratory capabilities\nthroughout training but also achieves a more effective balance between\nexploration and exploitation, leading to superior performance.\n","authors":["Weihao Zeng","Yuzhen Huang","Lulu Zhao","Yijun Wang","Zifei Shan","Junxian He"],"pdf_url":"https://arxiv.org/pdf/2412.17256v2.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2503.00032v2","updated":"2025-03-04T06:26:41Z","published":"2025-02-25T00:59:27Z","title":"Detecting LLM-Generated Korean Text through Linguistic Feature Analysis","summary":"  The rapid advancement of large language models (LLMs) increases the\ndifficulty of distinguishing between human-written and LLM-generated text.\nDetecting LLM-generated text is crucial for upholding academic integrity,\npreventing plagiarism, protecting copyrights, and ensuring ethical research\npractices. Most prior studies on detecting LLM-generated text focus primarily\non English text. However, languages with distinct morphological and syntactic\ncharacteristics require specialized detection approaches. Their unique\nstructures and usage patterns can hinder the direct application of methods\nprimarily designed for English. Among such languages, we focus on Korean, which\nhas relatively flexible spacing rules, a rich morphological system, and less\nfrequent comma usage compared to English. We introduce KatFish, the first\nbenchmark dataset for detecting LLM-generated Korean text. The dataset consists\nof text written by humans and generated by four LLMs across three genres.\n  By examining spacing patterns, part-of-speech diversity, and comma usage, we\nilluminate the linguistic differences between human-written and LLM-generated\nKorean text. Building on these observations, we propose KatFishNet, a detection\nmethod specifically designed for the Korean language. KatFishNet achieves an\naverage of 19.78% higher AUROC compared to the best-performing existing\ndetection method. Our code and data are available at\nhttps://github.com/Shinwoo-Park/detecting_llm_generated_korean_text_through_linguistic_analysis.\n","authors":["Shinwoo Park","Shubin Kim","Do-Kyung Kim","Yo-Sub Han"],"pdf_url":"https://arxiv.org/pdf/2503.00032v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07273v2","updated":"2025-03-04T06:26:07Z","published":"2025-02-11T05:40:42Z","title":"Variational Learning Induces Adaptive Label Smoothing","summary":"  We show that variational learning naturally induces an adaptive label\nsmoothing where label noise is specialized for each example. Such\nlabel-smoothing is useful to handle examples with labeling errors and\ndistribution shifts, but designing a good adaptivity strategy is not always\neasy. We propose to skip this step and simply use the natural adaptivity\ninduced during the optimization of a variational objective. We show empirical\nresults where a variational algorithm called IVON outperforms traditional label\nsmoothing and yields adaptivity strategies similar to those of an existing\napproach. By connecting Bayesian methods to label smoothing, our work provides\na new way to handle overconfident predictions.\n","authors":["Sin-Han Yang","Zhedong Liu","Gian Maria Marconi","Mohammad Emtiyaz Khan"],"pdf_url":"https://arxiv.org/pdf/2502.07273v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02318v1","updated":"2025-03-04T06:18:34Z","published":"2025-03-04T06:18:34Z","title":"Audio-Reasoner: Improving Reasoning Capability in Large Audio Language\n  Models","summary":"  Recent advancements in multimodal reasoning have largely overlooked the audio\nmodality. We introduce Audio-Reasoner, a large-scale audio language model for\ndeep reasoning in audio tasks. We meticulously curated a large-scale and\ndiverse multi-task audio dataset with simple annotations. Then, we leverage\nclosed-source models to conduct secondary labeling, QA generation, along with\nstructured COT process. These datasets together form a high-quality reasoning\ndataset with 1.2 million reasoning-rich samples, which we name CoTA. Following\ninference scaling principles, we train Audio-Reasoner on CoTA, enabling it to\nachieve great logical capabilities in audio reasoning. Experiments show\nstate-of-the-art performance across key benchmarks, including MMAU-mini\n(+25.42%), AIR-Bench chat/foundation(+14.57%/+10.13%), and MELD (+8.01%). Our\nfindings stress the core of structured CoT training in advancing audio\nreasoning.\n","authors":["Zhifei Xie","Mingbao Lin","Zihang Liu","Pengcheng Wu","Shuicheng Yan","Chunyan Miao"],"pdf_url":"https://arxiv.org/pdf/2503.02318v1.pdf","comment":"Technical report, in process"},{"id":"http://arxiv.org/abs/2503.02311v1","updated":"2025-03-04T06:13:53Z","published":"2025-03-04T06:13:53Z","title":"Target Return Optimizer for Multi-Game Decision Transformer","summary":"  Achieving autonomous agents with robust generalization capabilities across\ndiverse games and tasks remains one of the ultimate goals in AI research.\nRecent advancements in transformer-based offline reinforcement learning,\nexemplified by the MultiGame Decision Transformer [Lee et al., 2022], have\nshown remarkable performance across various games or tasks. However, these\napproaches depend heavily on human expertise, presenting substantial challenges\nfor practical deployment, particularly in scenarios with limited prior\ngame-specific knowledge. In this paper, we propose an algorithm called\nMulti-Game Target Return Optimizer (MTRO) to autonomously determine\ngame-specific target returns within the Multi-Game Decision Transformer\nframework using solely offline datasets. MTRO addresses the existing\nlimitations by automating the target return configuration process, leveraging\nenvironmental reward information extracted from offline datasets. Notably, MTRO\ndoes not require additional training, enabling seamless integration into\nexisting Multi-Game Decision Transformer architectures. Our experimental\nevaluations on Atari games demonstrate that MTRO enhances the performance of RL\npolicies across a wide array of games, underscoring its potential to advance\nthe field of autonomous agent development.\n","authors":["Kensuke Tatematsu","Akifumi Wachi"],"pdf_url":"https://arxiv.org/pdf/2503.02311v1.pdf","comment":"10 pages"},{"id":"http://arxiv.org/abs/2503.02303v1","updated":"2025-03-04T06:04:54Z","published":"2025-03-04T06:04:54Z","title":"Flexible Prefrontal Control over Hippocampal Episodic Memory for\n  Goal-Directed Generalization","summary":"  Many tasks require flexibly modifying perception and behavior based on\ncurrent goals. Humans can retrieve episodic memories from days to years ago,\nusing them to contextualize and generalize behaviors across novel but\nstructurally related situations. The brain's ability to control episodic\nmemories based on task demands is often attributed to interactions between the\nprefrontal cortex (PFC) and hippocampus (HPC). We propose a reinforcement\nlearning model that incorporates a PFC-HPC interaction mechanism for\ngoal-directed generalization. In our model, the PFC learns to generate\nquery-key representations to encode and retrieve goal-relevant episodic\nmemories, modulating HPC memories top-down based on current task demands.\nMoreover, the PFC adapts its encoding and retrieval strategies dynamically when\nfaced with multiple goals presented in a blocked, rather than interleaved,\nmanner. Our results show that: (1) combining working memory with selectively\nretrieved episodic memory allows transfer of decisions among similar\nenvironments or situations, (2) top-down control from PFC over HPC improves\nlearning of arbitrary structural associations between events for generalization\nto novel environments compared to a bottom-up sensory-driven approach, and (3)\nthe PFC encodes generalizable representations during both encoding and\nretrieval of goal-relevant memories, whereas the HPC exhibits event-specific\nrepresentations. Together, these findings highlight the importance of\ngoal-directed prefrontal control over hippocampal episodic memory for\ndecision-making in novel situations and suggest a computational mechanism by\nwhich PFC-HPC interactions enable flexible behavior.\n","authors":["Yicong Zheng","Nora Wolf","Charan Ranganath","Randall C. O'Reilly","Kevin L. McKee"],"pdf_url":"https://arxiv.org/pdf/2503.02303v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00396v3","updated":"2025-03-04T05:51:53Z","published":"2024-06-01T10:45:41Z","title":"Stochastic Resetting Mitigates Latent Gradient Bias of SGD from Label\n  Noise","summary":"  Giving up and starting over may seem wasteful in many situations such as\nsearching for a target or training deep neural networks (DNNs). Our study,\nthough, demonstrates that resetting from a checkpoint can significantly improve\ngeneralization performance when training DNNs with noisy labels. In the\npresence of noisy labels, DNNs initially learn the general patterns of the data\nbut then gradually memorize the corrupted data, leading to overfitting. By\ndeconstructing the dynamics of stochastic gradient descent (SGD), we identify\nthe behavior of a latent gradient bias induced by noisy labels, which harms\ngeneralization. To mitigate this negative effect, we apply the stochastic\nresetting method to SGD, inspired by recent developments in the field of\nstatistical physics achieving efficient target searches. We first theoretically\nidentify the conditions where resetting becomes beneficial, and then we\nempirically validate our theory, confirming the significant improvements\nachieved by resetting. We further demonstrate that our method is both easy to\nimplement and compatible with other methods for handling noisy labels.\nAdditionally, this work offers insights into the learning dynamics of DNNs from\nan interpretability perspective, expanding the potential to analyze training\nmethods through the lens of statistical physics.\n","authors":["Youngkyoung Bae","Yeongwoo Song","Hawoong Jeong"],"pdf_url":"https://arxiv.org/pdf/2406.00396v3.pdf","comment":"30 pages, 14 figures"},{"id":"http://arxiv.org/abs/2503.02296v1","updated":"2025-03-04T05:39:24Z","published":"2025-03-04T05:39:24Z","title":"Memorize or Generalize? Evaluating LLM Code Generation with Evolved\n  Questions","summary":"  Large Language Models (LLMs) are known to exhibit a memorization phenomenon\nin code generation: instead of truly understanding the underlying principles of\na programming problem, they tend to memorize the original prompt and its\nsolution together in the training. Consequently, when facing variants of the\noriginal problem, their answers very likely resemble the memorized solutions\nand fail to generalize. In this paper, we investigate this phenomenon by\ndesigning three evolution strategies to create variants: mutation,\nparaphrasing, and code-rewriting. By comparing the performance and AST\nsimilarity of the LLM-generated codes before and after these three evolutions,\nwe develop a memorization score that positively correlates with the level of\nmemorization. As expected, as supervised fine-tuning goes on, the memorization\nscore rises before overfitting, suggesting more severe memorization. We\ndemonstrate that common mitigation approaches, such as prompt translation and\nusing evolved variants as data augmentation in supervised learning and\nreinforcement learning, either compromise the performance or fail to alleviate\nthe memorization issue. Therefore, memorization remains a significant challenge\nin LLM code generation, highlighting the need for a more effective solution.\n","authors":["Wentao Chen","Lizhe Zhang","Li Zhong","Letian Peng","Zilong Wang","Jingbo Shang"],"pdf_url":"https://arxiv.org/pdf/2503.02296v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01450v2","updated":"2025-03-04T05:23:52Z","published":"2025-03-03T11:59:03Z","title":"POPGym Arcade: Parallel Pixelated POMDPs","summary":"  We introduce POPGym Arcade, a benchmark consisting of 7 pixel-based\nenvironments each with three difficulties, utilizing a single observation and\naction space. Each environment offers both fully observable and partially\nobservable variants, enabling counterfactual studies on partial observability.\nPOPGym Arcade utilizes JIT compilation on hardware accelerators to achieve\nsubstantial speedups over CPU-bound environments. Moreover, this enables\nPodracer-style architectures to further increase hardware utilization and\ntraining speed. We evaluate memory models on our environments using a Podracer\nvariant of Q learning, and examine the results. Finally, we generate memory\nsaliency maps, uncovering how memories propagate through policies. Our library\nis available at https://github.com/bolt-research/popgym_arcade.\n","authors":["Zekang Wang","Zhe He","Edan Toledo","Steven Morad"],"pdf_url":"https://arxiv.org/pdf/2503.01450v2.pdf","comment":null}]},"2025-03-05T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2503.02820v2","updated":"2025-03-05T14:44:53Z","published":"2025-03-04T17:47:27Z","title":"Integral Forms in Matrix Lie Groups","summary":"  Matrix Lie groups provide a language for describing motion in such fields as\nrobotics, computer vision, and graphics. When using these tools, we are often\nfaced with turning infinite-series expressions into more compact finite series\n(e.g., the Euler-Rodriques formula), which can sometimes be onerous. In this\npaper, we identify some useful integral forms in matrix Lie group expressions\nthat offer a more streamlined pathway for computing compact analytic results.\nMoreover, we present some recursive structures in these integral forms that\nshow many of these expressions are interrelated. Key to our approach is that we\nare able to apply the minimal polynomial for a Lie algebra quite early in the\nprocess to keep expressions compact throughout the derivations. With the series\napproach, the minimal polynomial is usually applied at the end, making it hard\nto recognize common analytic expressions in the result. We show that our\nintegral method can reproduce several series-derived results from the\nliterature.\n","authors":["Timothy D Barfoot"],"pdf_url":"https://arxiv.org/pdf/2503.02820v2.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2503.01202v3","updated":"2025-03-05T03:11:07Z","published":"2025-03-03T05:55:30Z","title":"A Multi-Sensor Fusion Approach for Rapid Orthoimage Generation in\n  Large-Scale UAV Mapping","summary":"  Rapid generation of large-scale orthoimages from Unmanned Aerial Vehicles\n(UAVs) has been a long-standing focus of research in the field of aerial\nmapping. A multi-sensor UAV system, integrating the Global Positioning System\n(GPS), Inertial Measurement Unit (IMU), 4D millimeter-wave radar and camera,\ncan provide an effective solution to this problem. In this paper, we utilize\nmulti-sensor data to overcome the limitations of conventional orthoimage\ngeneration methods in terms of temporal performance, system robustness, and\ngeographic reference accuracy. A prior-pose-optimized feature matching method\nis introduced to enhance matching speed and accuracy, reducing the number of\nrequired features and providing precise references for the Structure from\nMotion (SfM) process. The proposed method exhibits robustness in low-texture\nscenes like farmlands, where feature matching is difficult. Experiments show\nthat our approach achieves accurate feature matching orthoimage generation in a\nshort time. The proposed drone system effectively aids in farmland detection\nand management.\n","authors":["Jialei He","Zhihao Zhan","Zhituo Tu","Xiang Zhu","Jie Yuan"],"pdf_url":"https://arxiv.org/pdf/2503.01202v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02412v2","updated":"2025-03-05T05:19:22Z","published":"2025-03-04T08:53:38Z","title":"SEB-Naver: A SE(2)-based Local Navigation Framework for Car-like Robots\n  on Uneven Terrain","summary":"  Autonomous navigation of car-like robots on uneven terrain poses unique\nchallenges compared to flat terrain, particularly in traversability assessment\nand terrain-associated kinematic modelling for motion planning. This paper\nintroduces SEB-Naver, a novel SE(2)-based local navigation framework designed\nto overcome these challenges. First, we propose an efficient traversability\nassessment method for SE(2) grids, leveraging GPU parallel computing to enable\nreal-time updates and maintenance of local maps. Second, inspired by\ndifferential flatness, we present an optimization-based trajectory planning\nmethod that integrates terrain-associated kinematic models, significantly\nimproving both planning efficiency and trajectory quality. Finally, we unify\nthese components into SEB-Naver, achieving real-time terrain assessment and\ntrajectory optimization. Extensive simulations and real-world experiments\ndemonstrate the effectiveness and efficiency of our approach. The code is at\nhttps://github.com/ZJU-FAST-Lab/seb_naver.\n","authors":["Xiaoying Li","Long Xu","Xiaolin Huang","Donglai Xue","Zhihao Zhang","Zhichao Han","Chao Xu","Yanjun Cao","Fei Gao"],"pdf_url":"https://arxiv.org/pdf/2503.02412v2.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2503.00397v3","updated":"2025-03-05T08:09:16Z","published":"2025-03-01T08:18:11Z","title":"Floorplan-SLAM: A Real-Time, High-Accuracy, and Long-Term Multi-Session\n  Point-Plane SLAM for Efficient Floorplan Reconstruction","summary":"  Floorplan reconstruction provides structural priors essential for reliable\nindoor robot navigation and high-level scene understanding. However, existing\napproaches either require time-consuming offline processing with a complete\nmap, or rely on expensive sensors and substantial computational resources. To\naddress the problems, we propose Floorplan-SLAM, which incorporates floorplan\nreconstruction tightly into a multi-session SLAM system by seamlessly\ninteracting with plane extraction, pose estimation, and back-end optimization,\nachieving real-time, high-accuracy, and long-term floorplan reconstruction\nusing only a stereo camera. Specifically, we present a robust plane extraction\nalgorithm that operates in a compact plane parameter space and leverages\nspatially complementary features to accurately detect planar structures, even\nin weakly textured scenes. Furthermore, we propose a floorplan reconstruction\nmodule tightly coupled with the SLAM system, which uses continuously optimized\nplane landmarks and poses to formulate and solve a novel optimization problem,\nthereby enabling real-time incremental floorplan reconstruction. Note that by\nleveraging the map merging capability of multi-session SLAM, our method\nsupports long-term floorplan reconstruction across multiple sessions without\nredundant data collection. Experiments on the VECtor and the self-collected\ndatasets indicate that Floorplan-SLAM significantly outperforms\nstate-of-the-art methods in terms of plane extraction robustness, pose\nestimation accuracy, and floorplan reconstruction fidelity and speed, achieving\nreal-time performance at 25-45 FPS without GPU acceleration, which reduces the\nfloorplan reconstruction time for a 1000 square meters scene from over 10 hours\nto just 9.44 minutes.\n","authors":["Haolin Wang","Zeren Lv","Hao Wei","Haijiang Zhu","Yihong Wu"],"pdf_url":"https://arxiv.org/pdf/2503.00397v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.10341v5","updated":"2025-03-05T06:53:17Z","published":"2024-07-14T21:41:29Z","title":"Affordance-Guided Reinforcement Learning via Visual Prompting","summary":"  Robots equipped with reinforcement learning (RL) have the potential to learn\na wide range of skills solely from a reward signal. However, obtaining a robust\nand dense reward signal for general manipulation tasks remains a challenge.\nExisting learning-based approaches require significant data, such as human\ndemonstrations of success and failure, to learn task-specific reward functions.\nRecently, there is also a growing adoption of large multi-modal foundation\nmodels for robotics that can perform visual reasoning in physical contexts and\ngenerate coarse robot motions for manipulation tasks. Motivated by this range\nof capability, in this work, we present Keypoint-based Affordance Guidance for\nImprovements (KAGI), a method leveraging rewards shaped by vision-language\nmodels (VLMs) for autonomous RL. State-of-the-art VLMs have demonstrated\nimpressive reasoning about affordances through keypoints in zero-shot, and we\nuse these to define dense rewards that guide autonomous robotic learning. On\nreal-world manipulation tasks specified by natural language descriptions, KAGI\nimproves the sample efficiency of autonomous RL and enables successful task\ncompletion in 30K online fine-tuning steps. Additionally, we demonstrate the\nrobustness of KAGI to reductions in the number of in-domain demonstrations used\nfor pre-training, reaching similar performance in 45K online fine-tuning steps.\nProject website: https://sites.google.com/view/affordance-guided-rl\n","authors":["Olivia Y. Lee","Annie Xie","Kuan Fang","Karl Pertsch","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2407.10341v5.pdf","comment":"8 pages, 6 figures. Robotics: Science and Systems (RSS) 2024, Task\n  Specification for General-Purpose Intelligent Robots & Lifelong Robot\n  Learning Workshops"},{"id":"http://arxiv.org/abs/2503.01676v2","updated":"2025-03-05T01:27:57Z","published":"2025-03-03T15:49:18Z","title":"Perceptual Motor Learning with Active Inference Framework for Robust\n  Lateral Control","summary":"  This paper presents a novel Perceptual Motor Learning (PML) framework\nintegrated with Active Inference (AIF) to enhance lateral control in Highly\nAutomated Vehicles (HAVs). PML, inspired by human motor learning, emphasizes\nthe seamless integration of perception and action, enabling efficient\ndecision-making in dynamic environments. Traditional autonomous driving\napproaches--including modular pipelines, imitation learning, and reinforcement\nlearning--struggle with adaptability, generalization, and computational\nefficiency. In contrast, PML with AIF leverages a generative model to minimize\nprediction error (\"surprise\") and actively shape vehicle control based on\nlearned perceptual-motor representations. Our approach unifies deep learning\nwith active inference principles, allowing HAVs to perform lane-keeping\nmaneuvers with minimal data and without extensive retraining across different\nenvironments. Extensive experiments in the CARLA simulator demonstrate that PML\nwith AIF enhances adaptability without increasing computational overhead while\nachieving performance comparable to conventional methods. These findings\nhighlight the potential of PML-driven active inference as a robust alternative\nfor real-world autonomous driving applications.\n","authors":["Elahe Delavari","John Moore","Junho Hong","Jaerock Kwon"],"pdf_url":"https://arxiv.org/pdf/2503.01676v2.pdf","comment":"This work has been submitted to IROS 2025 and is currently under\n  review. arXiv admin note: text overlap with arXiv:2407.07684"},{"id":"http://arxiv.org/abs/2503.01582v2","updated":"2025-03-05T02:02:19Z","published":"2025-03-03T14:23:37Z","title":"Category-level Meta-learned NeRF Priors for Efficient Object Mapping","summary":"  In 3D object mapping, category-level priors enable efficient object\nreconstruction and canonical pose estimation, requiring only a single prior per\nsemantic category (e.g., chair, book, laptop). Recently, DeepSDF has\npredominantly been used as a category-level shape prior, but it struggles to\nreconstruct sharp geometry and is computationally expensive. In contrast, NeRFs\ncapture fine details but have yet to be effectively integrated with\ncategory-level priors in a real-time multi-object mapping framework. To bridge\nthis gap, we introduce PRENOM, a Prior-based Efficient Neural Object Mapper\nthat integrates category-level priors with object-level NeRFs to enhance\nreconstruction efficiency while enabling canonical object pose estimation.\nPRENOM gets to know objects on a first-name basis by meta-learning on synthetic\nreconstruction tasks generated from open-source shape datasets. To account for\nobject category variations, it employs a multi-objective genetic algorithm to\noptimize the NeRF architecture for each category, balancing reconstruction\nquality and training time. Additionally, prior-based probabilistic ray sampling\ndirects sampling toward expected object regions, accelerating convergence and\nimproving reconstruction quality under constrained resources. Experimental\nresults on a low-end GPU highlight the ability of PRENOM to achieve\nhigh-quality reconstructions while maintaining computational feasibility.\nSpecifically, comparisons with prior-free NeRF-based approaches on a synthetic\ndataset show a 21% lower Chamfer distance, demonstrating better reconstruction\nquality. Furthermore, evaluations against other approaches using shape priors\non a noisy real-world dataset indicate a 13% improvement averaged across all\nreconstruction metrics, and comparable pose and size estimation accuracy, while\nbeing trained for 5x less time.\n","authors":["Saad Ejaz","Hriday Bavle","Laura Ribeiro","Holger Voos","Jose Luis Sanchez-Lopez"],"pdf_url":"https://arxiv.org/pdf/2503.01582v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14562v4","updated":"2025-03-05T18:55:03Z","published":"2024-09-22T19:00:53Z","title":"DROP: Dexterous Reorientation via Online Planning","summary":"  Achieving human-like dexterity is a longstanding challenge in robotics, in\npart due to the complexity of planning and control for contact-rich systems. In\nreinforcement learning (RL), one popular approach has been to use\nmassively-parallelized, domain-randomized simulations to learn a policy offline\nover a vast array of contact conditions, allowing robust sim-to-real transfer.\nInspired by recent advances in real-time parallel simulation, this work\nconsiders instead the viability of online planning methods for contact-rich\nmanipulation by studying the well-known in-hand cube reorientation task. We\npropose a simple architecture that employs a sampling-based predictive\ncontroller and vision-based pose estimator to search for contact-rich control\nactions online. We conduct thorough experiments to assess the real-world\nperformance of our method, architectural design choices, and key factors for\nrobustness, demonstrating that our simple sampling-based approach achieves\nperformance comparable to prior RL-based works. Supplemental material:\nhttps://caltech-amber.github.io/drop.\n","authors":["Albert H. Li","Preston Culbertson","Vince Kurtz","Aaron D. Ames"],"pdf_url":"https://arxiv.org/pdf/2409.14562v4.pdf","comment":"Extended version, updated appendix. Accepted to ICRA 2025"},{"id":"http://arxiv.org/abs/2503.03734v1","updated":"2025-03-05T18:44:48Z","published":"2025-03-05T18:44:48Z","title":"OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature\n  Extraction","summary":"  Vision-Language-Action (VLA) models aim to predict robotic actions based on\nvisual observations and language instructions. Existing approaches require\nfine-tuning pre-trained visionlanguage models (VLMs) as visual and language\nfeatures are independently fed into downstream policies, degrading the\npre-trained semantic alignments. We propose OTTER, a novel VLA architecture\nthat leverages these existing alignments through explicit, text-aware visual\nfeature extraction. Instead of processing all visual features, OTTER\nselectively extracts and passes only task-relevant visual features that are\nsemantically aligned with the language instruction to the policy transformer.\nThis allows OTTER to keep the pre-trained vision-language encoders frozen.\nThereby, OTTER preserves and utilizes the rich semantic understanding learned\nfrom large-scale pre-training, enabling strong zero-shot generalization\ncapabilities. In simulation and real-world experiments, OTTER significantly\noutperforms existing VLA models, demonstrating strong zeroshot generalization\nto novel objects and environments. Video, code, checkpoints, and dataset:\nhttps://ottervla.github.io/.\n","authors":["Huang Huang","Fangchen Liu","Letian Fu","Tingfan Wu","Mustafa Mukadam","Jitendra Malik","Ken Goldberg","Pieter Abbeel"],"pdf_url":"https://arxiv.org/pdf/2503.03734v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03726v1","updated":"2025-03-05T18:28:32Z","published":"2025-03-05T18:28:32Z","title":"Active 6D Pose Estimation for Textureless Objects using Multi-View RGB\n  Frames","summary":"  Estimating the 6D pose of textureless objects from RBG images is an important\nproblem in robotics. Due to appearance ambiguities, rotational symmetries, and\nsevere occlusions, single-view based 6D pose estimators are still unable to\nhandle a wide range of objects, motivating research towards multi-view pose\nestimation and next-best-view prediction that addresses these limitations. In\nthis work, we propose a comprehensive active perception framework for\nestimating the 6D poses of textureless objects using only RGB images. Our\napproach is built upon a key idea: decoupling the 6D pose estimation into a\nsequential two-step process can greatly improve both accuracy and efficiency.\nFirst, we estimate the 3D translation of each object, resolving scale and depth\nambiguities inherent to RGB images. These estimates are then used to simplify\nthe subsequent task of determining the 3D orientation, which we achieve through\ncanonical scale template matching. Building on this formulation, we then\nintroduce an active perception strategy that predicts the next best camera\nviewpoint to capture an RGB image, effectively reducing object pose uncertainty\nand enhancing pose accuracy. We evaluate our method on the public ROBI dataset\nas well as on a transparent object dataset that we created. When evaluated\nusing the same camera viewpoints, our multi-view pose estimation significantly\noutperforms state-of-the-art approaches. Furthermore, by leveraging our\nnext-best-view strategy, our method achieves high object pose accuracy with\nsubstantially fewer viewpoints than heuristic-based policies.\n","authors":["Jun Yang","Wenjie Xue","Sahar Ghavidel","Steven L. Waslander"],"pdf_url":"https://arxiv.org/pdf/2503.03726v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03707v1","updated":"2025-03-05T17:58:16Z","published":"2025-03-05T17:58:16Z","title":"Curating Demonstrations using Online Experience","summary":"  Many robot demonstration datasets contain heterogeneous demonstrations of\nvarying quality. This heterogeneity may benefit policy pre-training, but can\nhinder robot performance when used with a final imitation learning objective.\nIn particular, some strategies in the data may be less reliable than others or\nmay be underrepresented in the data, leading to poor performance when such\nstrategies are sampled at test time. Moreover, such unreliable or\nunderrepresented strategies can be difficult even for people to discern, and\nsifting through demonstration datasets is time-consuming and costly. On the\nother hand, policy performance when trained on such demonstrations can reflect\nthe reliability of different strategies. We thus propose for robots to\nself-curate based on online robot experience (Demo-SCORE). More specifically,\nwe train and cross-validate a classifier to discern successful policy roll-outs\nfrom unsuccessful ones and use the classifier to filter heterogeneous\ndemonstration datasets. Our experiments in simulation and the real world show\nthat Demo-SCORE can effectively identify suboptimal demonstrations without\nmanual curation. Notably, Demo-SCORE achieves over 15-35% higher absolute\nsuccess rate in the resulting policy compared to the base policy trained with\nall original demonstrations.\n","authors":["Annie S. Chen","Alec M. Lessing","Yuejiang Liu","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2503.03707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18086v2","updated":"2025-03-05T17:37:48Z","published":"2024-11-27T06:47:09Z","title":"DMVC-Tracker: Distributed Multi-Agent Trajectory Planning for Target\n  Tracking Using Dynamic Buffered Voronoi and Inter-Visibility Cells","summary":"  This letter presents a distributed trajectory planning method for multi-agent\naerial tracking. The proposed method uses a Dynamic Buffered Voronoi Cell\n(DBVC) and a Dynamic Inter-Visibility Cell (DIVC) to formulate the distributed\ntrajectory generation. Specifically, the DBVC and the DIVC are time-variant\nspaces that prevent mutual collisions and occlusions among agents, while\nenabling them to maintain suitable distances from the moving target. We combine\nthe DBVC and the DIVC with an efficient Bernstein polynomial motion\nprimitive-based tracking generation method, which has been refined into a less\nconservative approach than in our previous work. The proposed algorithm can\ncompute each agent's trajectory within several milliseconds on an Intel i7\ndesktop. We validate the tracking performance in challenging scenarios,\nincluding environments with dozens of obstacles.\n","authors":["Yunwoo Lee","Jungwon Park","H. Jin Kim"],"pdf_url":"https://arxiv.org/pdf/2411.18086v2.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.03662v1","updated":"2025-03-05T16:52:21Z","published":"2025-03-05T16:52:21Z","title":"Adaptive Negative Damping Control for User-Dependent Multi-Terrain\n  Walking Assistance with a Hip Exoskeleton","summary":"  Hip exoskeletons are known for their versatility in assisting users across\nvaried scenarios. However, current assistive strategies often lack the\nflexibility to accommodate for individual walking patterns and adapt to diverse\nlocomotion environments. In this work, we present a novel control strategy that\nadapts the mechanical impedance of the human-exoskeleton system. We design the\nhip assistive torques as an adaptive virtual negative damping, which is able to\ninject energy into the system while allowing the users to remain in control and\ncontribute voluntarily to the movements. Experiments with five healthy subjects\ndemonstrate that our controller reduces the metabolic cost of walking compared\nto free walking (average reduction of 7.2%), and it preserves the lower-limbs\nkinematics. Additionally, our method achieves minimal power losses from the\nexoskeleton across the entire gait cycle (less than 2% negative mechanical\npower out of the total power), ensuring synchronized action with the users'\nmovements. Moreover, we use Bayesian Optimization to adapt the assistance\nstrength and allow for seamless adaptation and transitions across multi-terrain\nenvironments. Our strategy achieves efficient power transmission under all\nconditions. Our approach demonstrates an individualized, adaptable, and\nstraightforward controller for hip exoskeletons, advancing the development of\nviable, adaptive, and user-dependent control laws.\n","authors":["Giulia Ramella","Auke Ijspeert","Mohamed Bouri"],"pdf_url":"https://arxiv.org/pdf/2503.03662v1.pdf","comment":"Copyright 2025 IEEE. Personal use of this material is permitted.\n  Permission from IEEE must be obtained for all other uses, in any current or\n  future media, including reprinting/republishing this material for advertising\n  or promotional purposes, creating new collective works, for resale or\n  redistribution to servers or lists, or reuse of any copyrighted component of\n  this work in other works"},{"id":"http://arxiv.org/abs/2403.17667v2","updated":"2025-03-05T16:29:01Z","published":"2024-03-26T12:57:05Z","title":"Learning Goal-Directed Object Pushing in Cluttered Scenes with\n  Location-Based Attention","summary":"  In complex scenarios where typical pick-and-place techniques are\ninsufficient, often non-prehensile manipulation can ensure that a robot is able\nto fulfill its task. However, non-prehensile manipulation is challenging due to\nits underactuated nature with hybrid-dynamics, where a robot needs to reason\nabout an object's long-term behavior and contact-switching, while being robust\nto contact uncertainty. The presence of clutter in the workspace further\ncomplicates this task, introducing the need to include more advanced spatial\nanalysis to avoid unwanted collisions. Building upon prior work on\nreinforcement learning with multimodal categorical exploration for planar\npushing, we propose to incorporate location-based attention to enable robust\nmanipulation in cluttered scenes. Unlike previous approaches addressing this\nobstacle avoiding pushing task, our framework requires no predefined global\npaths and considers the desired target orientation of the manipulated object.\nExperimental results in simulation as well as with a real KUKA iiwa robot arm\ndemonstrate that our learned policy manipulates objects successfully while\navoiding collisions through complex obstacle configurations, including dynamic\nobstacles, to reach the desired target pose.\n","authors":["Nils Dengler","Juan Del Aguila Ferrandis","João Moura","Sethu Vijayakumar","Maren Bennewitz"],"pdf_url":"https://arxiv.org/pdf/2403.17667v2.pdf","comment":"Submitted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS)2025"},{"id":"http://arxiv.org/abs/2502.20900v2","updated":"2025-03-05T16:23:09Z","published":"2025-02-28T09:57:20Z","title":"DexGraspVLA: A Vision-Language-Action Framework Towards General\n  Dexterous Grasping","summary":"  Dexterous grasping remains a fundamental yet challenging problem in robotics.\nA general-purpose robot must be capable of grasping diverse objects in\narbitrary scenarios. However, existing research typically relies on specific\nassumptions, such as single-object settings or limited environments, leading to\nconstrained generalization. Our solution is DexGraspVLA, a hierarchical\nframework that utilizes a pre-trained Vision-Language model as the high-level\ntask planner and learns a diffusion-based policy as the low-level Action\ncontroller. The key insight lies in iteratively transforming diverse language\nand visual inputs into domain-invariant representations, where imitation\nlearning can be effectively applied due to the alleviation of domain shift.\nThus, it enables robust generalization across a wide range of real-world\nscenarios. Notably, our method achieves a 90+% success rate under thousands of\nunseen object, lighting, and background combinations in a ``zero-shot''\nenvironment. Empirical analysis further confirms the consistency of internal\nmodel behavior across environmental variations, thereby validating our design\nand explaining its generalization performance. We hope our work can be a step\nforward in achieving general dexterous grasping. Our demo and code can be found\nat https://dexgraspvla.github.io/.\n","authors":["Yifan Zhong","Xuchuan Huang","Ruochong Li","Ceyao Zhang","Yitao Liang","Yaodong Yang","Yuanpei Chen"],"pdf_url":"https://arxiv.org/pdf/2502.20900v2.pdf","comment":"21 pages, 10 figures"},{"id":"http://arxiv.org/abs/2503.03633v1","updated":"2025-03-05T16:14:36Z","published":"2025-03-05T16:14:36Z","title":"Motion Planning and Control with Unknown Nonlinear Dynamics through\n  Predicted Reachability","summary":"  Autonomous motion planning under unknown nonlinear dynamics presents\nsignificant challenges. An agent needs to continuously explore the system\ndynamics to acquire its properties, such as reachability, in order to guide\nsystem navigation adaptively. In this paper, we propose a hybrid\nplanning-control framework designed to compute a feasible trajectory toward a\ntarget. Our approach involves partitioning the state space and approximating\nthe system by a piecewise affine (PWA) system with constrained control inputs.\nBy abstracting the PWA system into a directed weighted graph, we incrementally\nupdate the existence of its edges via affine system identification and reach\ncontrol theory, introducing a predictive reachability condition by exploiting\nprior information of the unknown dynamics. Heuristic weights are assigned to\nedges based on whether their existence is certain or remains indeterminate.\nConsequently, we propose a framework that adaptively collects and analyzes data\nduring mission execution, continually updates the predictive graph, and\nsynthesizes a controller online based on the graph search outcomes. We\ndemonstrate the efficacy of our approach through simulation scenarios involving\na mobile robot operating in unknown terrains, with its unknown dynamics\nabstracted as a single integrator model.\n","authors":["Zhiquan Zhang","Gokul Puthumanaillam","Manav Vora","Melkior Ornik"],"pdf_url":"https://arxiv.org/pdf/2503.03633v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03629v1","updated":"2025-03-05T16:09:30Z","published":"2025-03-05T16:09:30Z","title":"TeraSim: Uncovering Unknown Unsafe Events for Autonomous Vehicles\n  through Generative Simulation","summary":"  Traffic simulation is essential for autonomous vehicle (AV) development,\nenabling comprehensive safety evaluation across diverse driving conditions.\nHowever, traditional rule-based simulators struggle to capture complex human\ninteractions, while data-driven approaches often fail to maintain long-term\nbehavioral realism or generate diverse safety-critical events. To address these\nchallenges, we propose TeraSim, an open-source, high-fidelity traffic\nsimulation platform designed to uncover unknown unsafe events and efficiently\nestimate AV statistical performance metrics, such as crash rates. TeraSim is\ndesigned for seamless integration with third-party physics simulators and\nstandalone AV stacks, to construct a complete AV simulation system.\nExperimental results demonstrate its effectiveness in generating diverse\nsafety-critical events involving both static and dynamic agents, identifying\nhidden deficiencies in AV systems, and enabling statistical performance\nevaluation. These findings highlight TeraSim's potential as a practical tool\nfor AV safety assessment, benefiting researchers, developers, and policymakers.\nThe code is available at https://github.com/mcity/TeraSim.\n","authors":["Haowei Sun","Xintao Yan","Zhijie Qiao","Haojie Zhu","Yihao Sun","Jiawei Wang","Shengyin Shen","Darian Hogue","Rajanikant Ananta","Derek Johnson","Greg Stevens","Greg McGuire","Yifan Wei","Wei Zheng","Yong Sun","Yasuo Fukai","Henry X. Liu"],"pdf_url":"https://arxiv.org/pdf/2503.03629v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.06615v5","updated":"2025-03-05T16:07:20Z","published":"2024-09-10T16:11:57Z","title":"One-Shot Imitation under Mismatched Execution","summary":"  Human demonstrations as prompts are a powerful way to program robots to do\nlong-horizon manipulation tasks. However, translating these demonstrations into\nrobot-executable actions presents significant challenges due to execution\nmismatches in movement styles and physical capabilities. Existing methods\neither depend on human-robot paired data, which is infeasible to scale, or rely\nheavily on frame-level visual similarities that often break down in practice.\nTo address these challenges, we propose RHyME, a novel framework that\nautomatically aligns human and robot task executions using optimal transport\ncosts. Given long-horizon robot demonstrations, RHyME synthesizes semantically\nequivalent human videos by retrieving and composing short-horizon human clips.\nThis approach facilitates effective policy training without the need for paired\ndata. RHyME successfully imitates a range of cross-embodiment demonstrators,\nboth in simulation and with a real human hand, achieving over 50\\% increase in\ntask success compared to previous methods. We release our code and datasets at\nhttps://portal-cornell.github.io/rhyme/.\n","authors":["Kushal Kedia","Prithwish Dan","Angela Chao","Maximus Adrian Pace","Sanjiban Choudhury"],"pdf_url":"https://arxiv.org/pdf/2409.06615v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.16720v2","updated":"2025-03-05T15:35:47Z","published":"2024-09-25T08:09:52Z","title":"Dashing for the Golden Snitch: Multi-Drone Time-Optimal Motion Planning\n  with Multi-Agent Reinforcement Learning","summary":"  Recent innovations in autonomous drones have facilitated time-optimal flight\nin single-drone configurations, and enhanced maneuverability in multi-drone\nsystems by applying optimal control and learning-based methods. However, few\nstudies have achieved time-optimal motion planning for multi-drone systems,\nparticularly during highly agile maneuvers or in dynamic scenarios. This paper\npresents a decentralized policy network using multi-agent reinforcement\nlearning for time-optimal multi-drone flight. To strike a balance between\nflight efficiency and collision avoidance, we introduce a soft collision-free\nmechanism inspired by optimization-based methods. By customizing PPO in a\ncentralized training, decentralized execution (CTDE) fashion, we unlock higher\nefficiency and stability in training while ensuring lightweight implementation.\nExtensive simulations show that, despite slight performance trade-offs compared\nto single-drone systems, our multi-drone approach maintains near-time-optimal\nperformance with a low collision rate. Real-world experiments validate our\nmethod, with two quadrotors using the same network as in simulation achieving a\nmaximum speed of 13.65 m/s and a maximum body rate of 13.4 rad/s in a 5.5 m *\n5.5 m * 2.0 m space across various tracks, relying entirely on onboard\ncomputation.\n","authors":["Xian Wang","Jin Zhou","Yuanli Feng","Jiahao Mei","Jiming Chen","Shuo Li"],"pdf_url":"https://arxiv.org/pdf/2409.16720v2.pdf","comment":"v2: 7 pages, 6 figures; terminology corrected, algorithmic and\n  equation descriptions revised, references added"},{"id":"http://arxiv.org/abs/2503.03599v1","updated":"2025-03-05T15:32:38Z","published":"2025-03-05T15:32:38Z","title":"REGRACE: A Robust and Efficient Graph-based Re-localization Algorithm\n  using Consistency Evaluation","summary":"  Loop closures are essential for correcting odometry drift and creating\nconsistent maps, especially in the context of large-scale navigation. Current\nmethods using dense point clouds for accurate place recognition do not scale\nwell due to computationally expensive scan-to-scan comparisons. Alternative\nobject-centric approaches are more efficient but often struggle with\nsensitivity to viewpoint variation. In this work, we introduce REGRACE, a novel\napproach that addresses these challenges of scalability and perspective\ndifference in re-localization by using LiDAR-based submaps. We introduce\nrotation-invariant features for each labeled object and enhance them with\nneighborhood context through a graph neural network. To identify potential\nrevisits, we employ a scalable bag-of-words approach, pooling one learned\nglobal feature per submap. Additionally, we define a revisit with geometrical\nconsistency cues rather than embedding distance, allowing us to recognize\nfar-away loop closures. Our evaluations demonstrate that REGRACE achieves\nsimilar results compared to state-of-the-art place recognition and registration\nbaselines while being twice as fast.\n","authors":["Débora N. P. Oliveira","Joshua Knights","Sebastián Barbas Laina","Simon Boche","Wolfram Burgard","Stefan Leutenegger"],"pdf_url":"https://arxiv.org/pdf/2503.03599v1.pdf","comment":"Submitted to IROS2025"},{"id":"http://arxiv.org/abs/2503.03579v1","updated":"2025-03-05T15:13:54Z","published":"2025-03-05T15:13:54Z","title":"A Generative System for Robot-to-Human Handovers: from Intent Inference\n  to Spatial Configuration Imagery","summary":"  We propose a novel system for robot-to-human object handover that emulates\nhuman coworker interactions. Unlike most existing studies that focus primarily\non grasping strategies and motion planning, our system focus on 1. inferring\nhuman handover intents, 2. imagining spatial handover configuration. The first\none integrates multimodal perception-combining visual and verbal cues-to infer\nhuman intent. The second one using a diffusion-based model to generate the\nhandover configuration, involving the spacial relationship among robot's\ngripper, the object, and the human hand, thereby mimicking the cognitive\nprocess of motor imagery. Experimental results demonstrate that our approach\neffectively interprets human cues and achieves fluent, human-like handovers,\noffering a promising solution for collaborative robotics. Code, videos, and\ndata are available at: https://i3handover.github.io.\n","authors":["Hanxin Zhang","Abdulqader Dhafer","Zhou Daniel Hao","Hongbiao Dong"],"pdf_url":"https://arxiv.org/pdf/2503.03579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00757v2","updated":"2025-03-05T15:13:25Z","published":"2024-10-01T14:52:05Z","title":"Collaborative motion planning for multi-manipulator systems through\n  Reinforcement Learning and Dynamic Movement Primitives","summary":"  Robotic tasks often require multiple manipulators to enhance task efficiency\nand speed, but this increases complexity in terms of collaboration, collision\navoidance, and the expanded state-action space. To address these challenges, we\npropose a multi-level approach combining Reinforcement Learning (RL) and\nDynamic Movement Primitives (DMP) to generate adaptive, real-time trajectories\nfor new tasks in dynamic environments using a demonstration library. This\nmethod ensures collision-free trajectory generation and efficient collaborative\nmotion planning. We validate the approach through experiments in the PyBullet\nsimulation environment with UR5e robotic manipulators.\n","authors":["Siddharth Singh","Tian Xu","Qing Chang"],"pdf_url":"https://arxiv.org/pdf/2410.00757v2.pdf","comment":"7 pages, 7 figures, conference submission"},{"id":"http://arxiv.org/abs/2503.03574v1","updated":"2025-03-05T15:01:56Z","published":"2025-03-05T15:01:56Z","title":"Olympus: A Jumping Quadruped for Planetary Exploration Utilizing\n  Reinforcement Learning for In-Flight Attitude Control","summary":"  Exploring planetary bodies with lower gravity, such as the moon and Mars,\nallows legged robots to utilize jumping as an efficient form of locomotion thus\ngiving them a valuable advantage over traditional rovers for exploration.\nMotivated by this fact, this paper presents the design, simulation, and\nlearning-based \"in-flight\" attitude control of Olympus, a jumping legged robot\ntailored to the gravity of Mars. First, the design requirements are outlined\nfollowed by detailing how simulation enabled optimizing the robot's design -\nfrom its legs to the overall configuration - towards high vertical jumping,\nforward jumping distance, and in-flight attitude reorientation. Subsequently,\nthe reinforcement learning policy used to track desired in-flight attitude\nmaneuvers is presented. Successfully crossing the sim2real gap, extensive\nexperimental studies of attitude reorientation tests are demonstrated.\n","authors":["Jørgen Anker Olsen","Grzegorz Malczyk","Kostas Alexis"],"pdf_url":"https://arxiv.org/pdf/2503.03574v1.pdf","comment":"7 pages, 6 figures, Accepted to the IEEE International Conference on\n  Robotics and Automation (ICRA) 2025"},{"id":"http://arxiv.org/abs/2409.16215v2","updated":"2025-03-05T14:49:21Z","published":"2024-09-24T16:21:27Z","title":"Tiny Robotics Dataset and Benchmark for Continual Object Detection","summary":"  Detecting objects in mobile robotics is crucial for numerous applications,\nfrom autonomous navigation to inspection. However, robots often need to operate\nin different domains from those they were trained in, requiring them to adjust\nto these changes. Tiny mobile robots, subject to size, power, and computational\nconstraints, encounter even more difficulties in running and adapting these\nalgorithms. Such adaptability, though, is crucial for real-world deployment,\nwhere robots must operate effectively in dynamic and unpredictable settings. In\nthis work, we introduce a novel benchmark to evaluate the continual learning\ncapabilities of object detection systems in tiny robotic platforms. Our\ncontributions include: (i) Tiny Robotics Object Detection~(TiROD), a\ncomprehensive dataset collected using the onboard camera of a small mobile\nrobot, designed to test object detectors across various domains and classes;\n(ii) a benchmark of different continual learning strategies on this dataset\nusing NanoDet, a lightweight object detector. Our results highlight key\nchallenges in developing robust and efficient continual learning strategies for\nobject detectors in tiny robotics.\n","authors":["Francesco Pasti","Riccardo De Monte","Davide Dalle Pezze","Gian Antonio Susto","Nicola Bellotto"],"pdf_url":"https://arxiv.org/pdf/2409.16215v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03556v1","updated":"2025-03-05T14:44:53Z","published":"2025-03-05T14:44:53Z","title":"Afford-X: Generalizable and Slim Affordance Reasoning for Task-oriented\n  Manipulation","summary":"  Object affordance reasoning, the ability to infer object functionalities\nbased on physical properties, is fundamental for task-oriented planning and\nactivities in both humans and Artificial Intelligence (AI). This capability,\nrequired for planning and executing daily activities in a task-oriented manner,\nrelies on commonsense knowledge of object physics and functionalities,\nextending beyond simple object recognition. Current computational models for\naffordance reasoning from perception lack generalizability, limiting their\napplicability in novel scenarios. Meanwhile, comprehensive Large Language\nModels (LLMs) with emerging reasoning capabilities are challenging to deploy on\nlocal devices for task-oriented manipulations. Here, we introduce LVIS-Aff, a\nlarge-scale dataset comprising 1,496 tasks and 119k images, designed to enhance\nthe generalizability of affordance reasoning from perception. Utilizing this\ndataset, we develop Afford-X, an end-to-end trainable affordance reasoning\nmodel that incorporates Verb Attention and Bi-Fusion modules to improve\nmulti-modal understanding. This model achieves up to a 12.1% performance\nimprovement over the best-reported results from non-LLM methods, while also\ndemonstrating a 1.2% enhancement compared to our previous conference paper.\nAdditionally, it maintains a compact 187M parameter size and infers nearly 50\ntimes faster than the GPT-4V API. Our work demonstrates the potential for\nefficient, generalizable affordance reasoning models that can be deployed on\nlocal devices for task-oriented manipulations. We showcase Afford-X's\neffectiveness in enabling task-oriented manipulations for robots across various\ntasks and environments, underscoring its efficiency and broad implications for\nadvancing robotics and AI systems in real-world applications.\n","authors":["Xiaomeng Zhu","Yuyang Li","Leiyao Cui","Pengfei Li","Huan-ang Gao","Yixin Zhu","Hao Zhao"],"pdf_url":"https://arxiv.org/pdf/2503.03556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.00133v2","updated":"2025-03-05T14:40:48Z","published":"2025-02-28T19:22:13Z","title":"A Magnetic-Actuated Vision-Based Whisker Array for Contact Perception\n  and Grasping","summary":"  Tactile sensing and the manipulation of delicate objects are critical\nchallenges in robotics. This study presents a vision-based magnetic-actuated\nwhisker array sensor that integrates these functions. The sensor features eight\nwhiskers arranged circularly, supported by an elastomer membrane and actuated\nby electromagnets and permanent magnets. A camera tracks whisker movements,\nenabling high-resolution tactile feedback. The sensor's performance was\nevaluated through object classification and grasping experiments. In the\nclassification experiment, the sensor approached objects from four directions\nand accurately identified five distinct objects with a classification accuracy\nof 99.17% using a Multi-Layer Perceptron model. In the grasping experiment, the\nsensor tested configurations of eight, four, and two whiskers, achieving the\nhighest success rate of 87% with eight whiskers. These results highlight the\nsensor's potential for precise tactile sensing and reliable manipulation.\n","authors":["Zhixian Hu","Juan Wachs","Yu She"],"pdf_url":"https://arxiv.org/pdf/2503.00133v2.pdf","comment":"Accepted by IEEE International Conference on Robotics and Automation\n  (ICRA) 2025"},{"id":"http://arxiv.org/abs/2503.03535v1","updated":"2025-03-05T14:18:39Z","published":"2025-03-05T14:18:39Z","title":"Unified Human Localization and Trajectory Prediction with Monocular\n  Vision","summary":"  Conventional human trajectory prediction models rely on clean curated data,\nrequiring specialized equipment or manual labeling, which is often impractical\nfor robotic applications. The existing predictors tend to overfit to clean\nobservation affecting their robustness when used with noisy inputs. In this\nwork, we propose MonoTransmotion (MT), a Transformer-based framework that uses\nonly a monocular camera to jointly solve localization and prediction tasks. Our\nframework has two main modules: Bird's Eye View (BEV) localization and\ntrajectory prediction. The BEV localization module estimates the position of a\nperson using 2D human poses, enhanced by a novel directional loss for smoother\nsequential localizations. The trajectory prediction module predicts future\nmotion from these estimates. We show that by jointly training both tasks with\nour unified framework, our method is more robust in real-world scenarios made\nof noisy inputs. We validate our MT network on both curated and non-curated\ndatasets. On the curated dataset, MT achieves around 12% improvement over\nbaseline models on BEV localization and trajectory prediction. On real-world\nnon-curated dataset, experimental results indicate that MT maintains similar\nperformance levels, highlighting its robustness and generalization capability.\nThe code is available at https://github.com/vita-epfl/MonoTransmotion.\n","authors":["Po-Chien Luan","Yang Gao","Celine Demonsant","Alexandre Alahi"],"pdf_url":"https://arxiv.org/pdf/2503.03535v1.pdf","comment":"ICRA 2025"},{"id":"http://arxiv.org/abs/2409.16502v2","updated":"2025-03-05T14:11:44Z","published":"2024-09-24T23:18:32Z","title":"GSplatLoc: Grounding Keypoint Descriptors into 3D Gaussian Splatting for\n  Improved Visual Localization","summary":"  Although various visual localization approaches exist, such as scene\ncoordinate regression and camera pose regression, these methods often struggle\nwith optimization complexity or limited accuracy. To address these challenges,\nwe explore the use of novel view synthesis techniques, particularly 3D Gaussian\nSplatting (3DGS), which enables the compact encoding of both 3D geometry and\nscene appearance. We propose a two-stage procedure that integrates dense and\nrobust keypoint descriptors from the lightweight XFeat feature extractor into\n3DGS, enhancing performance in both indoor and outdoor environments. The coarse\npose estimates are directly obtained via 2D-3D correspondences between the 3DGS\nrepresentation and query image descriptors. In the second stage, the initial\npose estimate is refined by minimizing the rendering-based photometric warp\nloss. Benchmarking on widely used indoor and outdoor datasets demonstrates\nimprovements over recent neural rendering-based localization methods, such as\nNeRFMatch and PNeRFLoc.\n","authors":["Gennady Sidorov","Malik Mohrat","Denis Gridusov","Ruslan Rakhimov","Sergey Kolyubin"],"pdf_url":"https://arxiv.org/pdf/2409.16502v2.pdf","comment":"Project website at https://gsplatloc.github.io/"},{"id":"http://arxiv.org/abs/2503.03511v1","updated":"2025-03-05T13:57:37Z","published":"2025-03-05T13:57:37Z","title":"NeuGrasp: Generalizable Neural Surface Reconstruction with Background\n  Priors for Material-Agnostic Object Grasp Detection","summary":"  Robotic grasping in scenes with transparent and specular objects presents\ngreat challenges for methods relying on accurate depth information. In this\npaper, we introduce NeuGrasp, a neural surface reconstruction method that\nleverages background priors for material-agnostic grasp detection. NeuGrasp\nintegrates transformers and global prior volumes to aggregate multi-view\nfeatures with spatial encoding, enabling robust surface reconstruction in\nnarrow and sparse viewing conditions. By focusing on foreground objects through\nresidual feature enhancement and refining spatial perception with an\noccupancy-prior volume, NeuGrasp excels in handling objects with transparent\nand specular surfaces. Extensive experiments in both simulated and real-world\nscenarios show that NeuGrasp outperforms state-of-the-art methods in grasping\nwhile maintaining comparable reconstruction quality. More details are available\nat https://neugrasp.github.io/.\n","authors":["Qingyu Fan","Yinghao Cai","Chao Li","Wenzhe He","Xudong Zheng","Tao Lu","Bin Liang","Shuo Wang"],"pdf_url":"https://arxiv.org/pdf/2503.03511v1.pdf","comment":"7 pages, 5 figures. IEEE International Conference on Robotics and\n  Automation (ICRA) 2025"},{"id":"http://arxiv.org/abs/2503.03509v1","updated":"2025-03-05T13:57:05Z","published":"2025-03-05T13:57:05Z","title":"A Benchmark for Optimal Multi-Modal Multi-Robot Multi-Goal Path Planning\n  with Given Robot Assignment","summary":"  In many industrial robotics applications, multiple robots are working in a\nshared workspace to complete a set of tasks as quickly as possible. Such\nsettings can be treated as multi-modal multi-robot multi-goal path planning\nproblems, where each robot has to reach an ordered sequence of goals. Existing\napproaches to this type of problem solve this using prioritization or assume\nsynchronous completion of tasks, and are thus neither optimal nor complete. We\nformalize this problem as a single path planning problem and introduce a\nbenchmark encompassing a diverse range of problem instances including scenarios\nwith various robots, planning horizons, and collaborative tasks such as\nhandovers. Along with the benchmark, we adapt an RRT* and a PRM* planner to\nserve as a baseline for the planning problems. Both planners work in the\ncomposite space of all robots and introduce the required changes to work in our\nsetting. Unlike existing approaches, our planner and formulation is not\nrestricted to discretized 2D workspaces, supports a changing environment, and\nworks for heterogeneous robot teams over multiple modes with different\nconstraints, and multiple goals. Videos and code for the benchmark and the\nplanners is available at https://vhartman.github.io/mrmg-planning/.\n","authors":["Valentin N. Hartmann","Tirza Heinle","Stelian Coros"],"pdf_url":"https://arxiv.org/pdf/2503.03509v1.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2501.18516v2","updated":"2025-03-05T13:54:04Z","published":"2025-01-30T17:28:11Z","title":"Learn from the Past: Language-conditioned Object Rearrangement with\n  Large Language Models","summary":"  Object manipulation for rearrangement into a specific goal state is a\nsignificant task for collaborative robots. Accurately determining object\nplacement is a key challenge, as misalignment can increase task complexity and\nthe risk of collisions, affecting the efficiency of the rearrangement process.\nMost current methods heavily rely on pre-collected datasets to train the model\nfor predicting the goal position. As a result, these methods are restricted to\nspecific instructions, which limits their broader applicability and\ngeneralisation. In this paper, we propose a framework of flexible\nlanguage-conditioned object rearrangement based on the Large Language Model\n(LLM). Our approach mimics human reasoning by making use of successful past\nexperiences as a reference to infer the best strategies to achieve a current\ndesired goal position. Based on LLM's strong natural language comprehension and\ninference ability, our method generalises to handle various everyday objects\nand free-form language instructions in a zero-shot manner. Experimental results\ndemonstrate that our methods can effectively execute the robotic rearrangement\ntasks, even those involving long sequences of orders.\n","authors":["Guanqun Cao","Ryan Mckenna","Erich Graf","John Oyekan"],"pdf_url":"https://arxiv.org/pdf/2501.18516v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00508v3","updated":"2025-03-05T13:41:46Z","published":"2024-11-01T10:48:03Z","title":"CLIP-RT: Learning Language-Conditioned Robotic Policies from Natural\n  Language Supervision","summary":"  Teaching robots desired skills in real-world environments remains\nchallenging, especially for non-experts. A key bottleneck is that collecting\nrobotic data often requires expertise or specialized hardware, limiting\naccessibility and scalability. We posit that natural language offers an\nintuitive and accessible interface for robot learning. To this end, we study\ntwo aspects: (1) enabling non-experts to collect robotic data through natural\nlanguage supervision (e.g., \"move the arm to the right\") and (2) learning\nrobotic policies directly from this supervision. Specifically, we introduce a\ndata collection framework that collects robot demonstrations based on natural\nlanguage supervision and further augments these demonstrations. We then present\nCLIP-RT, a vision-language-action (VLA) model that learns language-conditioned\nvisuomotor policies from this supervision. CLIP-RT adapts the pretrained CLIP\nmodels and learns to predict language-based motion primitives via contrastive\nimitation learning. We train CLIP-RT on the Open X-Embodiment dataset and\nfinetune it on in-domain data collected by our framework to learn diverse\nskills. CLIP-RT demonstrates strong capabilities in learning novel manipulation\nskills, outperforming the state-of-the-art model, OpenVLA (7B parameters), by\n24% in average success rates, while using 7x fewer parameters (1B). We further\nobserve that CLIP-RT shows significant improvements in few-shot generalization.\nFinally, through collaboration with humans or large pretrained models, we\ndemonstrate that CLIP-RT can further improve its generalization on challenging\nrobotic tasks.\n","authors":["Gi-Cheon Kang","Junghyun Kim","Kyuhwan Shim","Jun Ki Lee","Byoung-Tak Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.00508v3.pdf","comment":"27 pages"},{"id":"http://arxiv.org/abs/2503.03481v1","updated":"2025-03-05T13:17:19Z","published":"2025-03-05T13:17:19Z","title":"Coordinated Trajectories for Non-stop Flying Carriers Holding a\n  Cable-Suspended Load","summary":"  Multirotor UAVs have been typically considered for aerial manipulation, but\ntheir scarce endurance prevents long-lasting manipulation tasks. This work\ndemonstrates that the non-stop flights of three or more carriers are compatible\nwith holding a constant pose of a cable-suspended load, thus potentially\nenabling aerial manipulation with energy-efficient non-stop carriers. It also\npresents an algorithm for generating the coordinated non-stop trajectories. The\nproposed method builds upon two pillars: (1)~the choice of $n$ special linearly\nindependent directions of internal forces within the $3n-6$-dimensional\nnullspace of the grasp matrix of the load, chosen as the edges of a Hamiltonian\ncycle on the graph that connects the cable attachment points on the load.\nAdjacent pairs of directions are used to generate $n$ forces evolving on\ndistinct 2D affine subspaces, despite the attachment points being generically\nin 3D; (2)~the construction of elliptical trajectories within these subspaces\nby mapping, through appropriate graph coloring, each edge of the Hamiltonian\ncycle to a periodic coordinate while ensuring that no adjacent coordinates\nexhibit simultaneous zero derivatives. Combined with conditions for load\nstatics and attachment point positions, these choices ensure that each of the\n$n$ force trajectories projects onto the corresponding cable constraint sphere\nwith non-zero tangential velocity, enabling perpetual motion of the carriers\nwhile the load is still. The theoretical findings are validated through\nsimulations and laboratory experiments with non-stopping multirotor UAVs.\n","authors":["Chiara Gabellieri","Antonio Franchi"],"pdf_url":"https://arxiv.org/pdf/2503.03481v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03480v1","updated":"2025-03-05T13:16:55Z","published":"2025-03-05T13:16:55Z","title":"SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via\n  Safe Reinforcement Learning","summary":"  Vision-language-action models (VLAs) have shown great potential as generalist\nrobot policies. However, these models pose urgent safety challenges during\ndeployment, including the risk of physical harm to the environment, the robot\nitself, and humans. How can safety be explicitly incorporated into VLAs? In\nthis work, we propose SafeVLA, a novel algorithm designed to integrate safety\ninto VLAs, ensuring the protection of the environment, robot hardware and\nhumans in real-world settings. SafeVLA effectively balances safety and task\nperformance by employing large-scale constrained learning within simulated\nenvironments. We demonstrate that SafeVLA outperforms the current\nstate-of-the-art method in both safety and task performance, achieving average\nimprovements of 83.58% and 3.85%, respectively, in simulation. By prioritizing\nsafety, our approach eliminates high-risk behaviors and reduces the upper bound\nof unsafe behaviors to 1/35 of that in the current state-of-the-art, thereby\nsignificantly mitigating long-tail risks. Furthermore, the learned safety\nconstraints generalize to diverse, unseen scenarios, including multiple\nout-of-distribution perturbations and tasks. Our data, models and newly\nproposed benchmark environment are available at\nhttps://sites.google.com/view/pku-safevla.\n","authors":["Borong Zhang","Yuhao Zhang","Jiaming Ji","Yingshan Lei","Josef Dai","Yuanpei Chen","Yaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2503.03480v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2503.03476v1","updated":"2025-03-05T13:12:49Z","published":"2025-03-05T13:12:49Z","title":"Continuous Control of Diverse Skills in Quadruped Robots Without\n  Complete Expert Datasets","summary":"  Learning diverse skills for quadruped robots presents significant challenges,\nsuch as mastering complex transitions between different skills and handling\ntasks of varying difficulty. Existing imitation learning methods, while\nsuccessful, rely on expensive datasets to reproduce expert behaviors. Inspired\nby introspective learning, we propose Progressive Adversarial Self-Imitation\nSkill Transition (PASIST), a novel method that eliminates the need for complete\nexpert datasets. PASIST autonomously explores and selects high-quality\ntrajectories based on predefined target poses instead of demonstrations,\nleveraging the Generative Adversarial Self-Imitation Learning (GASIL)\nframework. To further enhance learning, We develop a skill selection module to\nmitigate mode collapse by balancing the weights of skills with varying levels\nof difficulty. Through these methods, PASIST is able to reproduce skills\ncorresponding to the target pose while achieving smooth and natural transitions\nbetween them. Evaluations on both simulation platforms and the Solo 8 robot\nconfirm the effectiveness of PASIST, offering an efficient alternative to\nexpert-driven learning.\n","authors":["Jiaxin Tu","Xiaoyi Wei","Yueqi Zhang","Taixian Hou","Xiaofei Gao","Zhiyan Dong","Peng Zhai","Lihua Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.03476v1.pdf","comment":"Accepted by ICRA 2025"},{"id":"http://arxiv.org/abs/2503.03464v1","updated":"2025-03-05T12:54:54Z","published":"2025-03-05T12:54:54Z","title":"Generative Artificial Intelligence in Robotic Manipulation: A Survey","summary":"  This survey provides a comprehensive review on recent advancements of\ngenerative learning models in robotic manipulation, addressing key challenges\nin the field. Robotic manipulation faces critical bottlenecks, including\nsignificant challenges in insufficient data and inefficient data acquisition,\nlong-horizon and complex task planning, and the multi-modality reasoning\nability for robust policy learning performance across diverse environments. To\ntackle these challenges, this survey introduces several generative model\nparadigms, including Generative Adversarial Networks (GANs), Variational\nAutoencoders (VAEs), diffusion models, probabilistic flow models, and\nautoregressive models, highlighting their strengths and limitations. The\napplications of these models are categorized into three hierarchical layers:\nthe Foundation Layer, focusing on data generation and reward generation; the\nIntermediate Layer, covering language, code, visual, and state generation; and\nthe Policy Layer, emphasizing grasp generation and trajectory generation. Each\nlayer is explored in detail, along with notable works that have advanced the\nstate of the art. Finally, the survey outlines future research directions and\nchallenges, emphasizing the need for improved efficiency in data utilization,\nbetter handling of long-horizon tasks, and enhanced generalization across\ndiverse robotic scenarios. All the related resources, including research\npapers, open-source data, and projects, are collected for the community in\nhttps://github.com/GAI4Manipulation/AwesomeGAIManipulation\n","authors":["Kun Zhang","Peng Yun","Jun Cen","Junhao Cai","Didi Zhu","Hangjie Yuan","Chao Zhao","Tao Feng","Michael Yu Wang","Qifeng Chen","Jia Pan","Bo Yang","Hua Chen"],"pdf_url":"https://arxiv.org/pdf/2503.03464v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03449v1","updated":"2025-03-05T12:26:48Z","published":"2025-03-05T12:26:48Z","title":"Tiny Lidars for Manipulator Self-Awareness: Sensor Characterization and\n  Initial Localization Experiments","summary":"  For several tasks, ranging from manipulation to inspection, it is beneficial\nfor robots to localize a target object in their surroundings. In this paper, we\npropose an approach that utilizes coarse point clouds obtained from\nminiaturized VL53L5CX Time-of-Flight (ToF) sensors (tiny lidars) to localize a\ntarget object in the robot's workspace. We first conduct an experimental\ncampaign to calibrate the dependency of sensor readings on relative range and\norientation to targets. We then propose a probabilistic sensor model that is\nvalidated in an object pose estimation task using a Particle Filter (PF). The\nresults show that the proposed sensor model improves the performance of the\nlocalization of the target object with respect to two baselines: one that\nassumes measurements are free from uncertainty and one in which the confidence\nis provided by the sensor datasheet.\n","authors":["Giammarco Caroleo","Alessandro Albini","Daniele De Martini","Timothy D. Barfoot","Perla Maiolino"],"pdf_url":"https://arxiv.org/pdf/2503.03449v1.pdf","comment":"7 pages, 6 figures, 3 tables, conference submission"},{"id":"http://arxiv.org/abs/2409.16972v2","updated":"2025-03-05T12:17:56Z","published":"2024-09-25T14:32:59Z","title":"Efficient Submap-based Autonomous MAV Exploration using Visual-Inertial\n  SLAM Configurable for LiDARs or Depth Cameras","summary":"  Autonomous exploration of unknown space is an essential component for the\ndeployment of mobile robots in the real world. Safe navigation is crucial for\nall robotics applications and requires accurate and consistent maps of the\nrobot's surroundings. To achieve full autonomy and allow deployment in a wide\nvariety of environments, the robot must rely on on-board state estimation which\nis prone to drift over time. We propose a Micro Aerial Vehicle (MAV)\nexploration framework based on local submaps to allow retaining global\nconsistency by applying loop-closure corrections to the relative submap poses.\nTo enable large-scale exploration we efficiently compute global,\nenvironment-wide frontiers from the local submap frontiers and use a\nsampling-based next-best-view exploration planner. Our method seamlessly\nsupports using either a LiDAR sensor or a depth camera, making it suitable for\ndifferent kinds of MAV platforms. We perform comparative evaluations in\nsimulation against a state-of-the-art submap-based exploration framework to\nshowcase the efficiency and reconstruction quality of our approach. Finally, we\ndemonstrate the applicability of our method to real-world MAVs, one equipped\nwith a LiDAR and the other with a depth camera. Video available at\nhttps://youtu.be/Uf5fwmYcuq4 .\n","authors":["Sotiris Papatheodorou","Simon Boche","Sebastián Barbas Laina","Stefan Leutenegger"],"pdf_url":"https://arxiv.org/pdf/2409.16972v2.pdf","comment":"In proceedings of the IEEE International Conference on Robotics and\n  Automation, 2025. 7 pages, 8 figures, for the accompanying video see\n  https://youtu.be/Uf5fwmYcuq4"},{"id":"http://arxiv.org/abs/2503.03412v1","updated":"2025-03-05T11:43:11Z","published":"2025-03-05T11:43:11Z","title":"REACT: Real-time Efficient Attribute Clustering and Transfer for\n  Updatable 3D Scene Graph","summary":"  Modern-day autonomous robots need high-level map representations to perform\nsophisticated tasks. Recently, 3D scene graphs (3DSGs) have emerged as a\npromising alternative to traditional grid maps, blending efficient memory use\nand rich feature representation. However, most efforts to apply them have been\nlimited to static worlds. This work introduces REACT, a framework that\nefficiently performs real-time attribute clustering and transfer to relocalize\nobject nodes in a 3DSG. REACT employs a novel method for comparing object\ninstances using an embedding model trained on triplet loss, facilitating\ninstance clustering and matching. Experimental results demonstrate that REACT\nis able to relocalize objects while maintaining computational efficiency. The\nREACT framework's source code will be available as an open-source project,\npromoting further advancements in reusable and updatable 3DSGs.\n","authors":["Phuoc Nguyen","Francesco Verdoja","Ville Kyrki"],"pdf_url":"https://arxiv.org/pdf/2503.03412v1.pdf","comment":"Submitted to IROS 2025"},{"id":"http://arxiv.org/abs/2404.04589v4","updated":"2025-03-05T10:53:24Z","published":"2024-04-06T10:57:57Z","title":"ARS548_ros. An ARS 548 RDI radar driver for ROS","summary":"  The ARS 548 RDI Radar is a premium model of the fifth generation of 77 GHz\nlong range radar sensors with new RF antenna arrays, which offer digital beam\nforming. This radar measures independently the distance, speed and angle of\nobjects without any reflectors in one measurement cycle based on Pulse\nCompression with New Frequency Modulation. Unfortunately, to the best of our\nknowledge, there are no open source drivers available for Linux systems to\nenable users to analyze the data acquired by the sensor. In this paper, we\npresent a driver that can interpret the data from the ARS 548 RDI sensor and\nmake it available over the Robot Operating System versions 1 and 2 (ROS and\nROS2). Thus, these data can be stored, represented, and analyzed using the\npowerful tools offered by ROS. Besides, our driver offers advanced object\nfeatures provided by the sensor, such as relative estimated velocity and\nacceleration of each object, its orientation and angular velocity. We focus on\nthe configuration of the sensor and the use of our driver including its\nfiltering and representation tools. Besides, we offer a video tutorial to help\nin its configuration process. Finally, a dataset acquired with this sensor and\nan Ouster OS1-32 LiDAR sensor, to have baseline measurements, is available, so\nthat the user can check the correctness of our driver.\n","authors":["Fernando Fernández-Calatayud","Lucía Coto-Elena","David Alejo","José J. Carpio-Jiménez","Fernando Caballero","Luis Merino"],"pdf_url":"https://arxiv.org/pdf/2404.04589v4.pdf","comment":"20 pages, 6 figures and 23 references"},{"id":"http://arxiv.org/abs/2503.03373v1","updated":"2025-03-05T10:49:28Z","published":"2025-03-05T10:49:28Z","title":"Direct Sparse Odometry with Continuous 3D Gaussian Maps for Indoor\n  Environments","summary":"  Accurate localization is essential for robotics and augmented reality\napplications such as autonomous navigation. Vision-based methods combining\nprior maps aim to integrate LiDAR-level accuracy with camera cost efficiency\nfor robust pose estimation. Existing approaches, however, often depend on\nunreliable interpolation procedures when associating discrete point cloud maps\nwith dense image pixels, which inevitably introduces depth errors and degrades\npose estimation accuracy. We propose a monocular visual odometry framework\nutilizing a continuous 3D Gaussian map, which directly assigns geometrically\nconsistent depth values to all extracted high-gradient points without\ninterpolation. Evaluations on two public datasets demonstrate superior tracking\naccuracy compared to existing methods. We have released the source code of this\nwork for the development of the community.\n","authors":["Jie Deng","Fengtian Lang","Zikang Yuan","Xin Yang"],"pdf_url":"https://arxiv.org/pdf/2503.03373v1.pdf","comment":"7 pages,5 figures"},{"id":"http://arxiv.org/abs/2503.03338v1","updated":"2025-03-05T10:12:22Z","published":"2025-03-05T10:12:22Z","title":"Navigating Intelligence: A Survey of Google OR-Tools and Machine\n  Learning for Global Path Planning in Autonomous Vehicles","summary":"  We offer a new in-depth investigation of global path planning (GPP) for\nunmanned ground vehicles, an autonomous mining sampling robot named ROMIE. GPP\nis essential for ROMIE's optimal performance, which is translated into solving\nthe traveling salesman problem, a complex graph theory challenge that is\ncrucial for determining the most effective route to cover all sampling\nlocations in a mining field. This problem is central to enhancing ROMIE's\noperational efficiency and competitiveness against human labor by optimizing\ncost and time. The primary aim of this research is to advance GPP by\ndeveloping, evaluating, and improving a cost-efficient software and web\napplication. We delve into an extensive comparison and analysis of Google\noperations research (OR)-Tools optimization algorithms. Our study is driven by\nthe goal of applying and testing the limits of OR-Tools capabilities by\nintegrating Reinforcement Learning techniques for the first time. This enables\nus to compare these methods with OR-Tools, assessing their computational\neffectiveness and real-world application efficiency. Our analysis seeks to\nprovide insights into the effectiveness and practical application of each\ntechnique. Our findings indicate that Q-Learning stands out as the optimal\nstrategy, demonstrating superior efficiency by deviating only 1.2% on average\nfrom the optimal solutions across our datasets.\n","authors":["Alexandre Benoit","Pedram Asef"],"pdf_url":"https://arxiv.org/pdf/2503.03338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03282v1","updated":"2025-03-05T09:07:13Z","published":"2025-03-05T09:07:13Z","title":"Supervised Visual Docking Network for Unmanned Surface Vehicles Using\n  Auto-labeling in Real-world Water Environments","summary":"  Unmanned Surface Vehicles (USVs) are increasingly applied to water operations\nsuch as environmental monitoring and river-map modeling. It faces a significant\nchallenge in achieving precise autonomous docking at ports or stations, still\nrelying on remote human control or external positioning systems for accuracy\nand safety which limits the full potential of human-out-of-loop deployment for\nUSVs.This paper introduces a novel supervised learning pipeline with the\nauto-labeling technique for USVs autonomous visual docking. Firstly, we\ndesigned an auto-labeling data collection pipeline that appends relative pose\nand image pair to the dataset. This step does not require conventional manual\nlabeling for supervised learning. Secondly, the Neural Dock Pose Estimator\n(NDPE) is proposed to achieve relative dock pose prediction without the need\nfor hand-crafted feature engineering, camera calibration, and peripheral\nmarkers. Moreover, The NDPE can accurately predict the relative dock pose in\nreal-world water environments, facilitating the implementation of\nPosition-Based Visual Servo (PBVS) and low-level motion controllers for\nefficient and autonomous docking.Experiments show that the NDPE is robust to\nthe disturbance of the distance and the USV velocity. The effectiveness of our\nproposed solution is tested and validated in real-world water environments,\nreflecting its capability to handle real-world autonomous docking tasks.\n","authors":["Yijie Chu","Ziniu Wu","Yong Yue","Eng Gee Lim","Paolo Paoletti","Xiaohui Zhu"],"pdf_url":"https://arxiv.org/pdf/2503.03282v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03262v1","updated":"2025-03-05T08:38:51Z","published":"2025-03-05T08:38:51Z","title":"Trajectory Prediction for Autonomous Driving: Progress, Limitations, and\n  Future Directions","summary":"  As the potential for autonomous vehicles to be integrated on a large scale\ninto modern traffic systems continues to grow, ensuring safe navigation in\ndynamic environments is crucial for smooth integration. To guarantee safety and\nprevent collisions, autonomous vehicles must be capable of accurately\npredicting the trajectories of surrounding traffic agents. Over the past\ndecade, significant efforts from both academia and industry have been dedicated\nto designing solutions for precise trajectory forecasting. These efforts have\nproduced a diverse range of approaches, raising questions about the differences\nbetween these methods and whether trajectory prediction challenges have been\nfully addressed. This paper reviews a substantial portion of recent trajectory\nprediction methods and devises a taxonomy to classify existing solutions. A\ngeneral overview of the prediction pipeline is also provided, covering input\nand output modalities, modeling features, and prediction paradigms discussed in\nthe literature. In addition, the paper discusses active research areas within\ntrajectory prediction, addresses the posed research questions, and highlights\nthe remaining research gaps and challenges.\n","authors":["Nadya Abdel Madjid","Abdulrahman Ahmad","Murad Mebrahtu","Yousef Babaa","Abdelmoamen Nasser","Sumbal Malik","Bilal Hassan","Naoufel Werghi","Jorge Dias","Majid Khonji"],"pdf_url":"https://arxiv.org/pdf/2503.03262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.00692v2","updated":"2025-03-05T08:21:44Z","published":"2025-03-02T02:08:06Z","title":"Learning Perceptive Humanoid Locomotion over Challenging Terrain","summary":"  Humanoid robots are engineered to navigate terrains akin to those encountered\nby humans, which necessitates human-like locomotion and perceptual abilities.\nCurrently, the most reliable controllers for humanoid motion rely exclusively\non proprioception, a reliance that becomes both dangerous and unreliable when\ncoping with rugged terrain. Although the integration of height maps into\nperception can enable proactive gait planning, robust utilization of this\ninformation remains a significant challenge, especially when exteroceptive\nperception is noisy. To surmount these challenges, we propose a solution based\non a teacher-student distillation framework. In this paradigm, an oracle policy\naccesses noise-free data to establish an optimal reference policy, while the\nstudent policy not only imitates the teacher's actions but also simultaneously\ntrains a world model with a variational information bottleneck for sensor\ndenoising and state estimation. Extensive evaluations demonstrate that our\napproach markedly enhances performance in scenarios characterized by unreliable\nterrain estimations. Moreover, we conducted rigorous testing in both\nchallenging urban settings and off-road environments, the model successfully\ntraverse 2 km of varied terrain without external intervention.\n","authors":["Wandong Sun","Baoshi Cao","Long Chen","Yongbo Su","Yang Liu","Zongwu Xie","Hong Liu"],"pdf_url":"https://arxiv.org/pdf/2503.00692v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03254v1","updated":"2025-03-05T08:13:56Z","published":"2025-03-05T08:13:56Z","title":"SCORE: Saturated Consensus Relocalization in Semantic Line Maps","summary":"  This is the arxiv version for our paper submitted to IEEE/RSJ IROS 2025. We\npropose a scene-agnostic and light-weight visual relocalization framework that\nleverages semantically labeled 3D lines as a compact map representation. In our\nframework, the robot localizes itself by capturing a single image, extracting\n2D lines, associating them with semantically similar 3D lines in the map, and\nsolving a robust perspective-n-line problem. To address the extremely high\noutlier ratios~(exceeding 99.5\\%) caused by one-to-many ambiguities in semantic\nmatching, we introduce the Saturated Consensus Maximization~(Sat-CM)\nformulation, which enables accurate pose estimation when the classic Consensus\nMaximization framework fails. We further propose a fast global solver to the\nformulated Sat-CM problems, leveraging rigorous interval analysis results to\nensure both accuracy and computational efficiency. Additionally, we develop a\npipeline for constructing semantic 3D line maps using posed depth images. To\nvalidate the effectiveness of our framework, which integrates our innovations\nin robust estimation and practical engineering insights, we conduct extensive\nexperiments on the ScanNet++ dataset.\n","authors":["Haodong Jiang","Xiang Zheng","Yanglin Zhang","Qingcheng Zeng","Yiqian Li","Ziyang Hong","Junfeng Wu"],"pdf_url":"https://arxiv.org/pdf/2503.03254v1.pdf","comment":"11 pages, 14 figurs, arxiv version for paper submitted to IROS 2025"},{"id":"http://arxiv.org/abs/2503.03252v1","updated":"2025-03-05T08:11:59Z","published":"2025-03-05T08:11:59Z","title":"STORM: Spatial-Temporal Iterative Optimization for Reliable Multicopter\n  Trajectory Generation","summary":"  Efficient and safe trajectory planning plays a critical role in the\napplication of quadrotor unmanned aerial vehicles. Currently, the inherent\ntrade-off between constraint compliance and computational efficiency\nenhancement in UAV trajectory optimization problems has not been sufficiently\naddressed. To enhance the performance of UAV trajectory optimization, we\npropose a spatial-temporal iterative optimization framework. Firstly, B-splines\nare utilized to represent UAV trajectories, with rigorous safety assurance\nachieved through strict enforcement of constraints on control points.\nSubsequently, a set of QP-LP subproblems via spatial-temporal decoupling and\nconstraint linearization is derived. Finally, an iterative optimization\nstrategy incorporating guidance gradients is employed to obtain\nhigh-performance UAV trajectories in different scenarios. Both simulation and\nreal-world experimental results validate the efficiency and high-performance of\nthe proposed optimization framework in generating safe and fast trajectories.\nOur source codes will be released for community reference at\nhttps://hitsz-mas.github.io/STORM\n","authors":["Jinhao Zhang","Zhexuan Zhou","Wenlong Xia","Youmin Gong","Jie Mei"],"pdf_url":"https://arxiv.org/pdf/2503.03252v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.13916v2","updated":"2025-03-05T07:44:52Z","published":"2024-11-21T08:06:13Z","title":"Joint-repositionable Inner-wireless Planar Snake Robot","summary":"  Bio-inspired multi-joint snake robots offer the advantages of terrain\nadaptability due to their limbless structure and high flexibility. However, a\nseries of dozens of motor units in typical multiple-joint snake robots results\nin a heavy body structure and hundreds of watts of high power consumption. This\npaper presents a joint-repositionable, inner-wireless snake robot that enables\nmulti-joint-like locomotion using a low-powered underactuated mechanism. The\nsnake robot, consisting of a series of flexible passive links, can dynamically\nchange its joint coupling configuration by repositioning motor-driven joint\nunits along rack gears inside the robot. Additionally, a soft robot skin\nwirelessly powers the internal joint units, avoiding the risk of wire tangling\nand disconnection caused by the movable joint units. The combination of the\njoint-repositionable mechanism and the wireless-charging-enabled soft skin\nachieves a high degree of bending, along with a lightweight structure of 1.3 kg\nand energy-efficient wireless power transmission of 7.6 watts.\n","authors":["Ayato Kanada","Ryo Takahashi","Keito Hayashi","Ryusuke Hosaka","Wakako Yukita","Yasutaka Nakashima","Tomoyuki Yokota","Takao Someya","Mitsuhiro Kamezaki","Yoshihiro Kawahara","Motoji Yamamoto"],"pdf_url":"https://arxiv.org/pdf/2411.13916v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03234v1","updated":"2025-03-05T07:24:00Z","published":"2025-03-05T07:24:00Z","title":"Social Gesture Recognition in spHRI: Leveraging Fabric-Based Tactile\n  Sensing on Humanoid Robots","summary":"  Humans are able to convey different messages using only touch. Equipping\nrobots with the ability to understand social touch adds another modality in\nwhich humans and robots can communicate. In this paper, we present a social\ngesture recognition system using a fabric-based, large-scale tactile sensor\nintegrated onto the arms of a humanoid robot. We built a social gesture dataset\nusing multiple participants and extracted temporal features for classification.\nBy collecting real-world data on a humanoid robot, our system provides valuable\ninsights into human-robot social touch, further advancing the development of\nspHRI systems for more natural and effective communication.\n","authors":["Dakarai Crowder","Kojo Vandyck","Xiping Sun","James McCann","Wenzhen Yuan"],"pdf_url":"https://arxiv.org/pdf/2503.03234v1.pdf","comment":"Accepted to ICRA 25. 8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2409.20154v6","updated":"2025-03-05T07:14:04Z","published":"2024-09-30T10:02:42Z","title":"GravMAD: Grounded Spatial Value Maps Guided Action Diffusion for\n  Generalized 3D Manipulation","summary":"  Robots' ability to follow language instructions and execute diverse 3D\nmanipulation tasks is vital in robot learning. Traditional imitation\nlearning-based methods perform well on seen tasks but struggle with novel,\nunseen ones due to variability. Recent approaches leverage large foundation\nmodels to assist in understanding novel tasks, thereby mitigating this issue.\nHowever, these methods lack a task-specific learning process, which is\nessential for an accurate understanding of 3D environments, often leading to\nexecution failures. In this paper, we introduce GravMAD, a sub-goal-driven,\nlanguage-conditioned action diffusion framework that combines the strengths of\nimitation learning and foundation models. Our approach breaks tasks into\nsub-goals based on language instructions, allowing auxiliary guidance during\nboth training and inference. During training, we introduce Sub-goal Keypose\nDiscovery to identify key sub-goals from demonstrations. Inference differs from\ntraining, as there are no demonstrations available, so we use pre-trained\nfoundation models to bridge the gap and identify sub-goals for the current\ntask. In both phases, GravMaps are generated from sub-goals, providing GravMAD\nwith more flexible 3D spatial guidance compared to fixed 3D positions.\nEmpirical evaluations on RLBench show that GravMAD significantly outperforms\nstate-of-the-art methods, with a 28.63% improvement on novel tasks and a 13.36%\ngain on tasks encountered during training. Evaluations on real-world robotic\ntasks further show that GravMAD can reason about real-world tasks, associate\nthem with relevant visual information, and generalize to novel tasks. These\nresults demonstrate GravMAD's strong multi-task learning and generalization in\n3D manipulation. Video demonstrations are available at:\nhttps://gravmad.github.io.\n","authors":["Yangtao Chen","Zixuan Chen","Junhui Yin","Jing Huo","Pinzhuo Tian","Jieqi Shi","Yang Gao"],"pdf_url":"https://arxiv.org/pdf/2409.20154v6.pdf","comment":"ICLR 2025. The first two authors contributed equally"},{"id":"http://arxiv.org/abs/2503.03230v1","updated":"2025-03-05T07:03:15Z","published":"2025-03-05T07:03:15Z","title":"OpenGV 2.0: Motion prior-assisted calibration and SLAM with\n  vehicle-mounted surround-view systems","summary":"  The present paper proposes optimization-based solutions to visual SLAM with a\nvehicle-mounted surround-view camera system. Owing to their original use-case,\nsuch systems often only contain a single camera facing into either direction\nand very limited overlap between fields of view. Our novelty consist of three\noptimization modules targeting at practical online calibration of exterior\norientations from simple two-view geometry, reliable front-end initialization\nof relative displacements, and accurate back-end optimization using a\ncontinuous-time trajectory model. The commonality between the proposed modules\nis given by the fact that all three of them exploit motion priors that are\nrelated to the inherent non-holonomic characteristics of passenger vehicle\nmotion. In contrast to prior related art, the proposed modules furthermore\nexcel in terms of bypassing partial unobservabilities in the transformation\nvariables that commonly occur for Ackermann-motion. As a further contribution,\nthe modules are built into a novel surround-view camera SLAM system that\nspecifically targets deployment on Ackermann vehicles operating in urban\nenvironments. All modules are studied in the context of in-depth ablation\nstudies, and the practical validity of the entire framework is supported by a\nsuccessful application to challenging, large-scale publicly available online\ndatasets. Note that upon acceptance, the entire framework is scheduled for\nopen-source release as part of an extension of the OpenGV library.\n","authors":["Kun Huang","Yifu Wang","Si'ao Zhang","Zhirui Wang","Zhanpeng Ouyang","Zhenghua Yu","Laurent Kneip"],"pdf_url":"https://arxiv.org/pdf/2503.03230v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19908v2","updated":"2025-03-05T06:36:27Z","published":"2025-02-27T09:26:22Z","title":"CarPlanner: Consistent Auto-regressive Trajectory Planning for\n  Large-scale Reinforcement Learning in Autonomous Driving","summary":"  Trajectory planning is vital for autonomous driving, ensuring safe and\nefficient navigation in complex environments. While recent learning-based\nmethods, particularly reinforcement learning (RL), have shown promise in\nspecific scenarios, RL planners struggle with training inefficiencies and\nmanaging large-scale, real-world driving scenarios. In this paper, we introduce\n\\textbf{CarPlanner}, a \\textbf{C}onsistent \\textbf{a}uto-\\textbf{r}egressive\n\\textbf{Planner} that uses RL to generate multi-modal trajectories. The\nauto-regressive structure enables efficient large-scale RL training, while the\nincorporation of consistency ensures stable policy learning by maintaining\ncoherent temporal consistency across time steps. Moreover, CarPlanner employs a\ngeneration-selection framework with an expert-guided reward function and an\ninvariant-view module, simplifying RL training and enhancing policy\nperformance. Extensive analysis demonstrates that our proposed RL framework\neffectively addresses the challenges of training efficiency and performance\nenhancement, positioning CarPlanner as a promising solution for trajectory\nplanning in autonomous driving. To the best of our knowledge, we are the first\nto demonstrate that the RL-based planner can surpass both IL- and rule-based\nstate-of-the-arts (SOTAs) on the challenging large-scale real-world dataset\nnuPlan. Our proposed CarPlanner surpasses RL-, IL-, and rule-based SOTA\napproaches within this demanding dataset.\n","authors":["Dongkun Zhang","Jiaming Liang","Ke Guo","Sha Lu","Qi Wang","Rong Xiong","Zhenwei Miao","Yue Wang"],"pdf_url":"https://arxiv.org/pdf/2502.19908v2.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2403.20001v2","updated":"2025-03-05T06:21:32Z","published":"2024-03-29T06:28:01Z","title":"Adaptive Energy Regularization for Autonomous Gait Transition and\n  Energy-Efficient Quadruped Locomotion","summary":"  In reinforcement learning for legged robot locomotion, crafting effective\nreward strategies is crucial. Pre-defined gait patterns and complex reward\nsystems are widely used to stabilize policy training. Drawing from the natural\nlocomotion behaviors of humans and animals, which adapt their gaits to minimize\nenergy consumption, we propose a simplified, energy-centric reward strategy to\nfoster the development of energy-efficient locomotion across various speeds in\nquadruped robots. By implementing an adaptive energy reward function and\nadjusting the weights based on velocity, we demonstrate that our approach\nenables ANYmal-C and Unitree Go1 robots to autonomously select appropriate\ngaits, such as four-beat walking at lower speeds and trotting at higher speeds,\nresulting in improved energy efficiency and stable velocity tracking compared\nto previous methods using complex reward designs and prior gait knowledge. The\neffectiveness of our policy is validated through simulations in the IsaacGym\nsimulation environment and on real robots, demonstrating its potential to\nfacilitate stable and adaptive locomotion.\n","authors":["Boyuan Liang","Lingfeng Sun","Xinghao Zhu","Bike Zhang","Ziyin Xiong","Yixiao Wang","Chenran Li","Koushil Sreenath","Masayoshi Tomizuka"],"pdf_url":"https://arxiv.org/pdf/2403.20001v2.pdf","comment":"7 pages, 7 figures"},{"id":"http://arxiv.org/abs/2211.07945v4","updated":"2025-03-05T06:17:58Z","published":"2022-11-15T07:07:38Z","title":"Geometric Impedance Control on SE(3) for Robotic Manipulators","summary":"  After its introduction, impedance control has been utilized as a primary\ncontrol scheme for robotic manipulation tasks that involve interaction with\nunknown environments. While impedance control has been extensively studied, the\ngeometric structure of SE(3) for the robotic manipulator itself and its use in\nformulating a robotic task has not been adequately addressed. In this paper, we\npropose a differential geometric approach to impedance control. Given a\nleft-invariant error metric in SE(3), the corresponding error vectors in\nposition and velocity are first derived. We then propose the impedance control\nschemes that adequately account for the geometric structure of the manipulator\nin SE(3) based on a left-invariant potential function. The closed-loop\nstabilities for the proposed control schemes are verified using Lyapunov\nfunction-based analysis. The proposed control design clearly outperformed a\nconventional impedance control approach when tracking challenging trajectory\nprofiles.\n","authors":["Joohwan Seo","Nikhil Potu Surya Prakash","Alexander Rose","Jongeun Choi","Roberto Horowitz"],"pdf_url":"https://arxiv.org/pdf/2211.07945v4.pdf","comment":"Presented at IFAC World Congress 2023, Yokohama, Japan"},{"id":"http://arxiv.org/abs/2409.15866v3","updated":"2025-03-05T05:55:45Z","published":"2024-09-24T08:40:04Z","title":"Online Planning for Multi-UAV Pursuit-Evasion in Unknown Environments\n  Using Deep Reinforcement Learning","summary":"  Multi-UAV pursuit-evasion, where pursuers aim to capture evaders, poses a key\nchallenge for UAV swarm intelligence. Multi-agent reinforcement learning (MARL)\nhas demonstrated potential in modeling cooperative behaviors, but most RL-based\napproaches remain constrained to simplified simulations with limited dynamics\nor fixed scenarios. Previous attempts to deploy RL policy to real-world\npursuit-evasion are largely restricted to two-dimensional scenarios, such as\nground vehicles or UAVs at fixed altitudes. In this paper, we address multi-UAV\npursuit-evasion by considering UAV dynamics and physical constraints. We\nintroduce an evader prediction-enhanced network to tackle partial observability\nin cooperative strategy learning. Additionally, we propose an adaptive\nenvironment generator within MARL training, enabling higher exploration\nefficiency and better policy generalization across diverse scenarios.\nSimulations show our method significantly outperforms all baselines in\nchallenging scenarios, generalizing to unseen scenarios with a 100% capture\nrate. Finally, we derive a feasible policy via a two-stage reward refinement\nand deploy the policy on real quadrotors in a zero-shot manner. To our\nknowledge, this is the first work to derive and deploy an RL-based policy using\ncollective thrust and body rates control commands for multi-UAV pursuit-evasion\nin unknown environments. The open-source code and videos are available at\nhttps://sites.google.com/view/pursuit-evasion-rl.\n","authors":["Jiayu Chen","Chao Yu","Guosheng Li","Wenhao Tang","Shilong Ji","Xinyi Yang","Botian Xu","Huazhong Yang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2409.15866v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03208v1","updated":"2025-03-05T05:53:08Z","published":"2025-03-05T05:53:08Z","title":"Embodied Escaping: End-to-End Reinforcement Learning for Robot\n  Navigation in Narrow Environment","summary":"  Autonomous navigation is a fundamental task for robot vacuum cleaners in\nindoor environments. Since their core function is to clean entire areas, robots\ninevitably encounter dead zones in cluttered and narrow scenarios. Existing\nplanning methods often fail to escape due to complex environmental constraints,\nhigh-dimensional search spaces, and high difficulty maneuvers. To address these\nchallenges, this paper proposes an embodied escaping model that leverages\nreinforcement learning-based policy with an efficient action mask for dead zone\nescaping. To alleviate the issue of the sparse reward in training, we introduce\na hybrid training policy that improves learning efficiency. In handling\nredundant and ineffective action options, we design a novel action\nrepresentation to reshape the discrete action space with a uniform turning\nradius. Furthermore, we develop an action mask strategy to select valid action\nquickly, balancing precision and efficiency. In real-world experiments, our\nrobot is equipped with a Lidar, IMU, and two-wheel encoders. Extensive\nquantitative and qualitative experiments across varying difficulty levels\ndemonstrate that our robot can consistently escape from challenging dead zones.\nMoreover, our approach significantly outperforms compared path planning and\nreinforcement learning methods in terms of success rate and collision\navoidance.\n","authors":["Han Zheng","Jiale Zhang","Mingyang Jiang","Peiyuan Liu","Danni Liu","Tong Qin","Ming Yang"],"pdf_url":"https://arxiv.org/pdf/2503.03208v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03200v1","updated":"2025-03-05T05:36:26Z","published":"2025-03-05T05:36:26Z","title":"Transformer-Based Spatio-Temporal Association of Apple Fruitlets","summary":"  In this paper, we present a transformer-based method to spatio-temporally\nassociate apple fruitlets in stereo-images collected on different days and from\ndifferent camera poses. State-of-the-art association methods in agriculture are\ndedicated towards matching larger crops using either high-resolution point\nclouds or temporally stable features, which are both difficult to obtain for\nsmaller fruit in the field. To address these challenges, we propose a\ntransformer-based architecture that encodes the shape and position of each\nfruitlet, and propagates and refines these features through a series of\ntransformer encoder layers with alternating self and cross-attention. We\ndemonstrate that our method is able to achieve an F1-score of 92.4% on data\ncollected in a commercial apple orchard and outperforms all baselines and\nablations.\n","authors":["Harry Freeman","George Kantor"],"pdf_url":"https://arxiv.org/pdf/2503.03200v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03196v1","updated":"2025-03-05T05:30:22Z","published":"2025-03-05T05:30:22Z","title":"SpiritSight Agent: Advanced GUI Agent with One Look","summary":"  Graphical User Interface (GUI) agents show amazing abilities in assisting\nhuman-computer interaction, automating human user's navigation on digital\ndevices. An ideal GUI agent is expected to achieve high accuracy, low latency,\nand compatibility for different GUI platforms. Recent vision-based approaches\nhave shown promise by leveraging advanced Vision Language Models (VLMs). While\nthey generally meet the requirements of compatibility and low latency, these\nvision-based GUI agents tend to have low accuracy due to their limitations in\nelement grounding. To address this issue, we propose $\\textbf{SpiritSight}$, a\nvision-based, end-to-end GUI agent that excels in GUI navigation tasks across\nvarious GUI platforms. First, we create a multi-level, large-scale,\nhigh-quality GUI dataset called $\\textbf{GUI-Lasagne}$ using scalable methods,\nempowering SpiritSight with robust GUI understanding and grounding\ncapabilities. Second, we introduce the $\\textbf{Universal Block Parsing (UBP)}$\nmethod to resolve the ambiguity problem in dynamic high-resolution of visual\ninputs, further enhancing SpiritSight's ability to ground GUI objects. Through\nthese efforts, SpiritSight agent outperforms other advanced methods on diverse\nGUI benchmarks, demonstrating its superior capability and compatibility in GUI\nnavigation tasks. Models are available at\n$\\href{https://huggingface.co/SenseLLM/SpiritSight-Agent-8B}{this\\ URL}$.\n","authors":["Zhiyuan Huang","Ziming Cheng","Junting Pan","Zhaohui Hou","Mingjie Zhan"],"pdf_url":"https://arxiv.org/pdf/2503.03196v1.pdf","comment":"Paper accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2503.03192v1","updated":"2025-03-05T05:17:15Z","published":"2025-03-05T05:17:15Z","title":"Distributed Certifiably Correct Range-Aided SLAM","summary":"  Reliable simultaneous localization and mapping (SLAM) algorithms are\nnecessary for safety-critical autonomous navigation. In the\ncommunication-constrained multi-agent setting, navigation systems increasingly\nuse point-to-point range sensors as they afford measurements with low bandwidth\nrequirements and known data association. The state estimation problem for these\nsystems takes the form of range-aided (RA) SLAM. However, distributed\nalgorithms for solving the RA-SLAM problem lack formal guarantees on the\nquality of the returned estimate. To this end, we present the first distributed\nalgorithm for RA-SLAM that can efficiently recover certifiably globally optimal\nsolutions. Our algorithm, distributed certifiably correct RA-SLAM (DCORA),\nachieves this via the Riemannian Staircase method, where computational\nprocedures developed for distributed certifiably correct pose graph\noptimization are generalized to the RA-SLAM problem. We demonstrate DCORA's\nefficacy on real-world multi-agent datasets by achieving absolute trajectory\nerrors comparable to those of a state-of-the-art centralized certifiably\ncorrect RA-SLAM algorithm. Additionally, we perform a parametric study on the\nstructure of the RA-SLAM problem using synthetic data, revealing how common\nparameters affect DCORA's performance.\n","authors":["Alexander Thoms","Alan Papalia","Jared Velasquez","David M. Rosen","Sriram Narasimhan"],"pdf_url":"https://arxiv.org/pdf/2503.03192v1.pdf","comment":"8 pages, 3 figures, accepted to 2025 International Conference on\n  Robotics and Automation"},{"id":"http://arxiv.org/abs/2502.00931v3","updated":"2025-03-05T04:11:08Z","published":"2025-02-02T21:44:15Z","title":"VL-Nav: Real-time Vision-Language Navigation with Spatial Reasoning","summary":"  Vision-language navigation in unknown environments is crucial for mobile\nrobots. In scenarios such as household assistance and rescue, mobile robots\nneed to understand a human command, such as \"find a person wearing black\". We\npresent a novel vision-language navigation (VL-Nav) system that integrates\nefficient spatial reasoning on low-power robots. Unlike prior methods that rely\non a single image-level feature similarity to guide a robot, our method\nintegrates pixel-wise vision-language features with curiosity-driven\nexploration. This approach enables robust navigation to human-instructed\ninstances across diverse environments. We deploy VL-Nav on a four-wheel mobile\nrobot and evaluate its performance through comprehensive navigation tasks in\nboth indoor and outdoor environments, spanning different scales and semantic\ncomplexities. Remarkably, VL-Nav operates at a real-time frequency of 30 Hz\nwith a Jetson Orin NX, highlighting its ability to conduct efficient\nvision-language navigation. Results show that VL-Nav achieves an overall\nsuccess rate of 86.3%, outperforming previous methods by 44.15%.\n","authors":["Yi Du","Taimeng Fu","Zhuoqun Chen","Bowen Li","Shaoshu Su","Zhipeng Zhao","Chen Wang"],"pdf_url":"https://arxiv.org/pdf/2502.00931v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10802v3","updated":"2025-03-05T03:56:36Z","published":"2024-09-17T00:39:14Z","title":"Bayesian Optimal Experimental Design for Robot Kinematic Calibration","summary":"  This paper develops a Bayesian optimal experimental design for robot\nkinematic calibration on ${\\mathbb{S}^3 \\!\\times\\! \\mathbb{R}^3}$. Our method\nbuilds upon a Gaussian process approach that incorporates a geometry-aware\nkernel based on Riemannian Mat\\'ern kernels over ${\\mathbb{S}^3}$. To learn the\nforward kinematics errors via Bayesian optimization with a Gaussian process, we\ndefine a geodesic distance-based objective function. Pointwise values of this\nfunction are sampled via noisy measurements taken using fiducial markers on the\nend-effector using a camera and computed pose with the nominal kinematics. The\ncorrected Denavit-Hartenberg parameters are obtained using an efficient\nquadratic program that operates on the collected data sets. The effectiveness\nof the proposed method is demonstrated via simulations and calibration\nexperiments on NASA's ocean world lander autonomy testbed (OWLAT).\n","authors":["Ersin Das","Thomas Touma","Joel W. Burdick"],"pdf_url":"https://arxiv.org/pdf/2409.10802v3.pdf","comment":"ICRA 2025"},{"id":"http://arxiv.org/abs/2503.03145v1","updated":"2025-03-05T03:37:45Z","published":"2025-03-05T03:37:45Z","title":"Causality-Based Reinforcement Learning Method for Multi-Stage Robotic\n  Tasks","summary":"  Deep reinforcement learning has made significant strides in various robotic\ntasks. However, employing deep reinforcement learning methods to tackle\nmulti-stage tasks still a challenge. Reinforcement learning algorithms often\nencounter issues such as redundant exploration, getting stuck in dead ends, and\nprogress reversal in multi-stage tasks. To address this, we propose a method\nthat integrates causal relationships with reinforcement learning for\nmulti-stage tasks. Our approach enables robots to automatically discover the\ncausal relationships between their actions and the rewards of the tasks and\nconstructs the action space using only causal actions, thereby reducing\nredundant exploration and progress reversal. By integrating correct causal\nrelationships using the causal policy gradient method into the learning\nprocess, our approach can enhance the performance of reinforcement learning\nalgorithms in multi-stage robotic tasks.\n","authors":["Jiechao Deng","Ning Tan"],"pdf_url":"https://arxiv.org/pdf/2503.03145v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01206v2","updated":"2025-03-05T03:30:03Z","published":"2025-03-03T06:02:53Z","title":"Action Tokenizer Matters in In-Context Imitation Learning","summary":"  In-context imitation learning (ICIL) is a new paradigm that enables robots to\ngeneralize from demonstrations to unseen tasks without retraining. A\nwell-structured action representation is the key to capturing demonstration\ninformation effectively, yet action tokenizer (the process of discretizing and\nencoding actions) remains largely unexplored in ICIL. In this work, we first\nsystematically evaluate existing action tokenizer methods in ICIL and reveal a\ncritical limitation: while they effectively encode action trajectories, they\nfail to preserve temporal smoothness, which is crucial for stable robotic\nexecution. To address this, we propose LipVQ-VAE, a variational autoencoder\nthat enforces the Lipschitz condition in the latent action space via weight\nnormalization. By propagating smoothness constraints from raw action inputs to\na quantized latent codebook, LipVQ-VAE generates more stable and smoother\nactions. When integrating into ICIL, LipVQ-VAE improves performance by more\nthan 5.3% in high-fidelity simulators, with real-world experiments confirming\nits ability to produce smoother, more reliable trajectories. Code and\ncheckpoints will be released.\n","authors":["An Dinh Vuong","Minh Nhat Vu","Dong An","Ian Reid"],"pdf_url":"https://arxiv.org/pdf/2503.01206v2.pdf","comment":"7 pages, 6 figures"},{"id":"http://arxiv.org/abs/2409.14754v3","updated":"2025-03-05T03:11:56Z","published":"2024-09-23T07:08:44Z","title":"CushionCatch: A Compliant Catching Mechanism for Mobile Manipulators via\n  Combined Optimization and Learning","summary":"  Catching flying objects with a cushioning process is a skill commonly\nperformed by humans, yet it remains a significant challenge for robots. In this\npaper, we present a framework that combines optimization and learning to\nachieve compliant catching on mobile manipulators (CCMM). First, we propose a\nhigh-level capture planner for mobile manipulators (MM) that calculates the\noptimal capture point and joint configuration. Next, the pre-catching (PRC)\nplanner ensures the robot reaches the target joint configuration as quickly as\npossible. To learn compliant catching strategies, we propose a network that\nleverages the strengths of LSTM for capturing temporal dependencies and\npositional encoding for spatial context (P-LSTM). This network is designed to\neffectively learn compliant strategies from human demonstrations. Following\nthis, the post-catching (POC) planner tracks the compliant sequence output by\nthe P-LSTM while avoiding potential collisions due to structural differences\nbetween humans and robots. We validate the CCMM framework through both\nsimulated and real-world ball-catching scenarios, achieving a success rate of\n98.70% in simulation, 92.59% in real-world tests, and a 28.7% reduction in\nimpact torques. The open source code will be released for the reference of the\ncommunity.\n","authors":["Bingjie Chen","Keyu Fan","Qi Yang","Yi Cheng","Houde Liu","Kangkang Dong","Chongkun Xia","Liang Han","Bin Liang"],"pdf_url":"https://arxiv.org/pdf/2409.14754v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18047v2","updated":"2025-03-05T03:08:12Z","published":"2024-09-26T16:48:21Z","title":"HARMONIC: Cognitive and Control Collaboration in Human-Robotic Teams","summary":"  This paper introduces HARMONIC, a cognitive-robotic architecture that\nintegrates the OntoAgent cognitive framework with general-purpose robot control\nsystems applied to human-robot teaming (HRT). We also present a cognitive\nstrategy for robots that incorporates metacognition, natural language\ncommunication, and explainability capabilities required for collaborative\npartnerships in HRT. Through simulation experiments involving a joint search\ntask performed by a heterogeneous team of a UGV, a drone, and a human operator,\nwe demonstrate the system's ability to coordinate actions between robots with\nheterogeneous capabilities, adapt to complex scenarios, and facilitate natural\nhuman-robot communication. Evaluation results show that robots using the\nOntoAgent architecture within the HARMONIC framework can reason about plans,\ngoals, and team member attitudes while providing clear explanations for their\ndecisions, which are essential prerequisites for realistic human-robot teaming.\n","authors":["Sanjay Oruganti","Sergei Nirenburg","Marjorie McShane","Jesse English","Michael K. Roberts","Christian Arndt","Sahithi Kamireddy"],"pdf_url":"https://arxiv.org/pdf/2409.18047v2.pdf","comment":"Submitted to IROS 2025"},{"id":"http://arxiv.org/abs/2503.03125v1","updated":"2025-03-05T02:43:52Z","published":"2025-03-05T02:43:52Z","title":"Don't Shake the Wheel: Momentum-Aware Planning in End-to-End Autonomous\n  Driving","summary":"  End-to-end autonomous driving frameworks enable seamless integration of\nperception and planning but often rely on one-shot trajectory prediction, which\nmay lead to unstable control and vulnerability to occlusions in single-frame\nperception. To address this, we propose the Momentum-Aware Driving (MomAD)\nframework, which introduces trajectory momentum and perception momentum to\nstabilize and refine trajectory predictions. MomAD comprises two core\ncomponents: (1) Topological Trajectory Matching (TTM) employs Hausdorff\nDistance to select the optimal planning query that aligns with prior paths to\nensure coherence;(2) Momentum Planning Interactor (MPI) cross-attends the\nselected planning query with historical queries to expand static and dynamic\nperception files. This enriched query, in turn, helps regenerate long-horizon\ntrajectory and reduce collision risks. To mitigate noise arising from dynamic\nenvironments and detection errors, we introduce robust instance denoising\nduring training, enabling the planning model to focus on critical signals and\nimprove its robustness. We also propose a novel Trajectory Prediction\nConsistency (TPC) metric to quantitatively assess planning stability.\nExperiments on the nuScenes dataset demonstrate that MomAD achieves superior\nlong-term consistency (>=3s) compared to SOTA methods. Moreover, evaluations on\nthe curated Turning-nuScenes shows that MomAD reduces the collision rate by 26%\nand improves TPC by 0.97m (33.45%) over a 6s prediction horizon, while\nclosedloop on Bench2Drive demonstrates an up to 16.3% improvement in success\nrate.\n","authors":["Ziying Song","Caiyan Jia","Lin Liu","Hongyu Pan","Yongchang Zhang","Junming Wang","Xingyu Zhang","Shaoqing Xu","Lei Yang","Yadan Luo"],"pdf_url":"https://arxiv.org/pdf/2503.03125v1.pdf","comment":"16 pages, 8 figures"},{"id":"http://arxiv.org/abs/2412.18292v2","updated":"2025-03-05T02:00:30Z","published":"2024-12-24T09:00:31Z","title":"Enhancing Multi-Robot Semantic Navigation Through Multimodal\n  Chain-of-Thought Score Collaboration","summary":"  Understanding how humans cooperatively utilize semantic knowledge to explore\nunfamiliar environments and decide on navigation directions is critical for\nhouse service multi-robot systems. Previous methods primarily focused on\nsingle-robot centralized planning strategies, which severely limited\nexploration efficiency. Recent research has considered decentralized planning\nstrategies for multiple robots, assigning separate planning models to each\nrobot, but these approaches often overlook communication costs. In this work,\nwe propose Multimodal Chain-of-Thought Co-Navigation (MCoCoNav), a modular\napproach that utilizes multimodal Chain-of-Thought to plan collaborative\nsemantic navigation for multiple robots. MCoCoNav combines visual perception\nwith Vision Language Models (VLMs) to evaluate exploration value through\nprobabilistic scoring, thus reducing time costs and achieving stable outputs.\nAdditionally, a global semantic map is used as a communication bridge,\nminimizing communication overhead while integrating observational results.\nGuided by scores that reflect exploration trends, robots utilize this map to\nassess whether to explore new frontier points or revisit history nodes.\nExperiments on HM3D_v0.2 and MP3D demonstrate the effectiveness of our\napproach. Our code is available at https://github.com/FrankZxShen/MCoCoNav.git.\n","authors":["Zhixuan Shen","Haonan Luo","Kexun Chen","Fengmao Lv","Tianrui Li"],"pdf_url":"https://arxiv.org/pdf/2412.18292v2.pdf","comment":"16 pages, 10 figures, Extended Version of accepted AAAI 2025 Paper"},{"id":"http://arxiv.org/abs/2405.17859v3","updated":"2025-03-05T01:48:25Z","published":"2024-05-28T06:16:57Z","title":"Adapting Pre-Trained Vision Models for Novel Instance Detection and\n  Segmentation","summary":"  Novel Instance Detection and Segmentation (NIDS) aims at detecting and\nsegmenting novel object instances given a few examples of each instance. We\npropose a unified, simple, yet effective framework (NIDS-Net) comprising object\nproposal generation, embedding creation for both instance templates and\nproposal regions, and embedding matching for instance label assignment.\nLeveraging recent advancements in large vision methods, we utilize Grounding\nDINO and Segment Anything Model (SAM) to obtain object proposals with accurate\nbounding boxes and masks. Central to our approach is the generation of\nhigh-quality instance embeddings. We utilized foreground feature averages of\npatch embeddings from the DINOv2 ViT backbone, followed by refinement through a\nweight adapter mechanism that we introduce.\n  We show experimentally that our weight adapter can adjust the embeddings\nlocally within their feature space and effectively limit overfitting in the\nfew-shot setting. Furthermore, the weight adapter optimizes weights to enhance\nthe distinctiveness of instance embeddings during similarity computation. This\nmethodology enables a straightforward matching strategy that results in\nsignificant performance gains. Our framework surpasses current state-of-the-art\nmethods, demonstrating notable improvements in four detection datasets. In the\nsegmentation tasks on seven core datasets of the BOP challenge, our method\noutperforms the leading published RGB methods and remains competitive with the\nbest RGB-D method. We have also verified our method using real-world images\nfrom a Fetch robot and a RealSense camera. Project Page:\nhttps://irvlutd.github.io/NIDSNet/\n","authors":["Yangxiao Lu","Jishnu Jaykumar P","Yunhui Guo","Nicholas Ruozzi","Yu Xiang"],"pdf_url":"https://arxiv.org/pdf/2405.17859v3.pdf","comment":"Project Page: https://irvlutd.github.io/NIDSNet/"},{"id":"http://arxiv.org/abs/2503.03100v1","updated":"2025-03-05T01:32:56Z","published":"2025-03-05T01:32:56Z","title":"Car-STAGE: Automated framework for large-scale high-dimensional\n  simulated time-series data generation based on user-defined criteria","summary":"  Generating large-scale sensing datasets through photo-realistic simulation is\nan important aspect of many robotics applications such as autonomous driving.\nIn this paper, we consider the problem of synchronous data collection from the\nopen-source CARLA simulator using multiple sensors attached to vehicle based on\nuser-defined criteria. We propose a novel, one-step framework that we refer to\nas Car-STAGE, based on CARLA simulator, to generate data using a graphical user\ninterface (GUI) defining configuration parameters to data collection without\nany user intervention. This framework can utilize the user-defined\nconfiguration parameters such as choice of maps, number and configurations of\nsensors, environmental and lighting conditions etc. to run the simulation in\nthe background, collecting high-dimensional sensor data from diverse sensors\nsuch as RGB Camera, LiDAR, Radar, Depth Camera, IMU Sensor, GNSS Sensor,\nSemantic Segmentation Camera, Instance Segmentation Camera, and Optical Flow\nCamera along with the ground-truths of the individual actors and storing the\nsensor data as well as ground-truth labels in a local or cloud-based database.\nThe framework uses multiple threads where a main thread runs the server, a\nworker thread deals with queue and frame number and the rest of the threads\nprocesses the sensor data. The other way we derive speed up over the native\nimplementation is by memory mapping the raw binary data into the disk and then\nconverting the data into known formats at the end of data collection. We show\nthat using these techniques, we gain a significant speed up over frames, under\nan increasing set of sensors and over the number of spawned objects.\n","authors":["Asma A. Almutairi","David J. LeBlanc","Arpan Kusari"],"pdf_url":"https://arxiv.org/pdf/2503.03100v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09795v2","updated":"2025-03-05T01:09:24Z","published":"2025-02-13T22:10:21Z","title":"Vision-based Geo-Localization of Future Mars Rotorcraft in Challenging\n  Illumination Conditions","summary":"  Planetary exploration using aerial assets has the potential for unprecedented\nscientific discoveries on Mars. While NASA's Mars helicopter Ingenuity proved\nflight in Martian atmosphere is possible, future Mars rotocrafts will require\nadvanced navigation capabilities for long-range flights. One such critical\ncapability is Map-based Localization (MbL) which registers an onboard image to\na reference map during flight in order to mitigate cumulative drift from visual\nodometry. However, significant illumination differences between rotocraft\nobservations and a reference map prove challenging for traditional MbL systems,\nrestricting the operational window of the vehicle. In this work, we investigate\na new MbL system and propose Geo-LoFTR, a geometry-aided deep learning model\nfor image registration that is more robust under large illumination differences\nthan prior models. The system is supported by a custom simulation framework\nthat uses real orbital maps to produce large amounts of realistic images of the\nMartian terrain. Comprehensive evaluations show that our proposed system\noutperforms prior MbL efforts in terms of localization accuracy under\nsignificant lighting and scale variations. Furthermore, we demonstrate the\nvalidity of our approach across a simulated Martian day.\n","authors":["Dario Pisanti","Robert Hewitt","Roland Brockers","Georgios Georgakis"],"pdf_url":"https://arxiv.org/pdf/2502.09795v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.14262v3","updated":"2025-03-05T00:50:23Z","published":"2024-09-21T23:06:14Z","title":"GND: Global Navigation Dataset with Multi-Modal Perception and\n  Multi-Category Traversability in Outdoor Campus Environments","summary":"  Navigating large-scale outdoor environments requires complex reasoning in\nterms of geometric structures, environmental semantics, and terrain\ncharacteristics, which are typically captured by onboard sensors such as LiDAR\nand cameras. While current mobile robots can navigate such environments using\npre-defined, high-precision maps based on hand-crafted rules catered for the\nspecific environment, they lack commonsense reasoning capabilities that most\nhumans possess when navigating unknown outdoor spaces. To address this gap, we\nintroduce the Global Navigation Dataset (GND), a large-scale dataset that\nintegrates multi-modal sensory data, including 3D LiDAR point clouds and RGB\nand 360-degree images, as well as multi-category traversability maps\n(pedestrian walkways, vehicle roadways, stairs, off-road terrain, and\nobstacles) from ten university campuses. These environments encompass a variety\nof parks, urban settings, elevation changes, and campus layouts of different\nscales. The dataset covers approximately 2.7km2 and includes at least 350\nbuildings in total. We also present a set of novel applications of GND to\nshowcase its utility to enable global robot navigation, such as map-based\nglobal navigation, mapless navigation, and global place recognition.\n","authors":["Jing Liang","Dibyendu Das","Daeun Song","Md Nahid Hasan Shuvo","Mohammad Durrani","Karthik Taranath","Ivan Penskiy","Dinesh Manocha","Xuesu Xiao"],"pdf_url":"https://arxiv.org/pdf/2409.14262v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03081v1","updated":"2025-03-05T00:44:12Z","published":"2025-03-05T00:44:12Z","title":"AirExo-2: Scaling up Generalizable Robotic Imitation Learning with\n  Low-Cost Exoskeletons","summary":"  Scaling up imitation learning for real-world applications requires efficient\nand cost-effective demonstration collection methods. Current teleoperation\napproaches, though effective, are expensive and inefficient due to the\ndependency on physical robot platforms. Alternative data sources like\nin-the-wild demonstrations can eliminate the need for physical robots and offer\nmore scalable solutions. However, existing in-the-wild data collection devices\nhave limitations: handheld devices offer restricted in-hand camera observation,\nwhile whole-body devices often require fine-tuning with robot data due to\naction inaccuracies. In this paper, we propose AirExo-2, a low-cost exoskeleton\nsystem for large-scale in-the-wild demonstration collection. By introducing the\ndemonstration adaptor to transform the collected in-the-wild demonstrations\ninto pseudo-robot demonstrations, our system addresses key challenges in\nutilizing in-the-wild demonstrations for downstream imitation learning in\nreal-world environments. Additionally, we present RISE-2, a generalizable\npolicy that integrates 2D and 3D perceptions, outperforming previous imitation\nlearning policies in both in-domain and out-of-domain tasks, even with limited\ndemonstrations. By leveraging in-the-wild demonstrations collected and\ntransformed by the AirExo-2 system, without the need for additional robot\ndemonstrations, RISE-2 achieves comparable or superior performance to policies\ntrained with teleoperated data, highlighting the potential of AirExo-2 for\nscalable and generalizable imitation learning. Project page:\nhttps://airexo.tech/airexo2\n","authors":["Hongjie Fang","Chenxi Wang","Yiming Wang","Jingjing Chen","Shangning Xia","Jun Lv","Zihao He","Xiyan Yi","Yunhan Guo","Xinyu Zhan","Lixin Yang","Weiming Wang","Cewu Lu","Hao-Shu Fang"],"pdf_url":"https://arxiv.org/pdf/2503.03081v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01962v2","updated":"2025-03-05T00:41:20Z","published":"2024-10-02T19:10:23Z","title":"LS-HAR: Language Supervised Human Action Recognition with Salient\n  Fusion, Construction Sites as a Use-Case","summary":"  Detecting human actions is a crucial task for autonomous robots and vehicles,\noften requiring the integration of various data modalities for improved\naccuracy. In this study, we introduce a novel approach to Human Action\nRecognition (HAR) using language supervision named LS-HAR based on skeleton and\nvisual cues. Our method leverages a language model to guide the feature\nextraction process in the skeleton encoder. Specifically, we employ learnable\nprompts for the language model conditioned on the skeleton modality to optimize\nfeature representation. Furthermore, we propose a fusion mechanism that\ncombines dual-modality features using a salient fusion module, incorporating\nattention and transformer mechanisms to address the modalities' high\ndimensionality. This fusion process prioritizes informative video frames and\nbody joints, enhancing the recognition accuracy of human actions. Additionally,\nwe introduce a new dataset tailored for real-world robotic applications in\nconstruction sites, featuring visual, skeleton, and depth data modalities,\nnamed VolvoConstAct. This dataset serves to facilitate the training and\nevaluation of machine learning models to instruct autonomous construction\nmachines for performing necessary tasks in real-world construction sites. To\nevaluate our approach, we conduct experiments on our dataset as well as three\nwidely used public datasets: NTU-RGB+D, NTU-RGB+D 120, and NW-UCLA. Results\nreveal that our proposed method achieves promising performance across all\ndatasets, demonstrating its robustness and potential for various applications.\nThe code, dataset, and demonstration of real-machine experiments are available\nat: https://mmahdavian.github.io/ls_har/\n","authors":["Mohammad Mahdavian","Mohammad Loni","Ted Samuelsson","Mo Chen"],"pdf_url":"https://arxiv.org/pdf/2410.01962v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03077v1","updated":"2025-03-05T00:33:43Z","published":"2025-03-05T00:33:43Z","title":"MochiSwarm: A testbed for robotic blimps in realistic environments","summary":"  Testing aerial robots in tasks such as pickup-and-delivery and surveillance\nsignificantly benefits from high energy efficiency and scalability of the\ndeployed robotic system. This paper presents MochiSwarm, an open-source testbed\nof light-weight robotic blimps, ready for multi-robot operation without\nexternal localization. We introduce the system design in hardware, software,\nand perception, which capitalizes on modularity, low cost, and light weight.\nThe hardware allows for rapid modification, which enables the integration of\nadditional sensors to enhance autonomy for different scenarios. The software\nframework supports different actuation models and communication between the\nbase station and multiple blimps. The detachable perception module allows\nindependent blimps to perform tasks that involve detection and autonomous\nactuation. We showcase a differential-drive module as an example, of which the\nautonomy is enabled by visual servoing using the perception module. A case\nstudy of pickup-and-delivery tasks with up to 12 blimps highlights the autonomy\nof the MochiSwarm without external infrastructures.\n","authors":["Jiawei Xu","Thong Vu","Diego S. D'Antonio","David Saldaña"],"pdf_url":"https://arxiv.org/pdf/2503.03077v1.pdf","comment":"Acepted for publication at ICRA2025"},{"id":"http://arxiv.org/abs/2407.17457v3","updated":"2025-03-05T00:32:49Z","published":"2024-07-24T17:50:00Z","title":"CSCPR: Cross-Source-Context Indoor RGB-D Place Recognition","summary":"  We extend our previous work, PoCo, and present a new algorithm,\nCross-Source-Context Place Recognition (CSCPR), for RGB-D indoor place\nrecognition that integrates global retrieval and reranking into an end-to-end\nmodel and keeps the consistency of using Context-of-Clusters (CoCs) for feature\nprocessing. Unlike prior approaches that primarily focus on the RGB domain for\nplace recognition reranking, CSCPR is designed to handle the RGB-D data. We\napply the CoCs to handle cross-sourced and cross-scaled RGB-D point clouds and\nintroduce two novel modules for reranking: the Self-Context Cluster (SCC) and\nthe Cross Source Context Cluster (CSCC), which enhance feature representation\nand match query-database pairs based on local features, respectively. We also\nrelease two new datasets, ScanNetIPR and ARKitIPR. Our experiments demonstrate\nthat CSCPR significantly outperforms state-of-the-art models on these datasets\nby at least 29.27% in Recall@1 on the ScanNet-PR dataset and 43.24% in the new\ndatasets. Code and datasets will be released.\n","authors":["Jing Liang","Zhuo Deng","Zheming Zhou","Min Sun","Omid Ghasemalizadeh","Cheng-Hao Kuo","Arnie Sen","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2407.17457v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03074v1","updated":"2025-03-05T00:27:32Z","published":"2025-03-05T00:27:32Z","title":"BEVDriver: Leveraging BEV Maps in LLMs for Robust Closed-Loop Driving","summary":"  Autonomous driving has the potential to set the stage for more efficient\nfuture mobility, requiring the research domain to establish trust through safe,\nreliable and transparent driving. Large Language Models (LLMs) possess\nreasoning capabilities and natural language understanding, presenting the\npotential to serve as generalized decision-makers for ego-motion planning that\ncan interact with humans and navigate environments designed for human drivers.\nWhile this research avenue is promising, current autonomous driving approaches\nare challenged by combining 3D spatial grounding and the reasoning and language\ncapabilities of LLMs. We introduce BEVDriver, an LLM-based model for end-to-end\nclosed-loop driving in CARLA that utilizes latent BEV features as perception\ninput. BEVDriver includes a BEV encoder to efficiently process multi-view\nimages and 3D LiDAR point clouds. Within a common latent space, the BEV\nfeatures are propagated through a Q-Former to align with natural language\ninstructions and passed to the LLM that predicts and plans precise future\ntrajectories while considering navigation instructions and critical scenarios.\nOn the LangAuto benchmark, our model reaches up to 18.9% higher performance on\nthe Driving Score compared to SoTA methods.\n","authors":["Katharina Winter","Mark Azer","Fabian B. Flohr"],"pdf_url":"https://arxiv.org/pdf/2503.03074v1.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2503.03071v1","updated":"2025-03-05T00:21:23Z","published":"2025-03-05T00:21:23Z","title":"Physically-Feasible Reactive Synthesis for Terrain-Adaptive Locomotion\n  via Trajectory Optimization and Symbolic Repair","summary":"  We propose an integrated planning framework for quadrupedal locomotion over\ndynamically changing, unforeseen terrains. Existing approaches either rely on\nheuristics for instantaneous foothold selection--compromising safety and\nversatility--or solve expensive trajectory optimization problems with complex\nterrain features and long time horizons. In contrast, our framework leverages\nreactive synthesis to generate correct-by-construction controllers at the\nsymbolic level, and mixed-integer convex programming (MICP) for dynamic and\nphysically feasible footstep planning for each symbolic transition. We use a\nhigh-level manager to reduce the large state space in synthesis by\nincorporating local environment information, improving synthesis scalability.\nTo handle specifications that cannot be met due to dynamic infeasibility, and\nto minimize costly MICP solves, we leverage a symbolic repair process to\ngenerate only necessary symbolic transitions. During online execution,\nre-running the MICP with real-world terrain data, along with runtime symbolic\nrepair, bridges the gap between offline synthesis and online execution. We\ndemonstrate, in simulation, our framework's capabilities to discover missing\nlocomotion skills and react promptly in safety-critical environments, such as\nscattered stepping stones and rebars.\n","authors":["Ziyi Zhou","Qian Meng","Hadas Kress-Gazit","Ye Zhao"],"pdf_url":"https://arxiv.org/pdf/2503.03071v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.18347v2","updated":"2025-03-05T00:20:00Z","published":"2024-12-24T11:14:55Z","title":"The Constitutional Filter: Bayesian Estimation of Compliant Agents","summary":"  Predicting agents impacted by legal policies, physical limitations, and\noperational preferences is inherently difficult. In recent years,\nneuro-symbolic methods have emerged, integrating machine learning and symbolic\nreasoning models into end-to-end learnable systems. Hereby, a promising avenue\nfor expressing high-level constraints over multi-modal input data in robotics\nhas opened up. This work introduces an approach for Bayesian estimation of\nagents expected to comply with a human-interpretable neuro-symbolic model we\ncall its Constitution. Hence, we present the Constitutional Filter (CoFi),\nleading to improved tracking of agents by leveraging expert knowledge,\nincorporating deep learning architectures, and accounting for environmental\nuncertainties. CoFi extends the general, recursive Bayesian estimation setting,\nensuring compatibility with a vast landscape of established techniques such as\nParticle Filters. To underpin the advantages of CoFi, we evaluate its\nperformance on real-world marine traffic data. Beyond improved performance, we\nshow how CoFi can learn to trust and adapt to the level of compliance of an\nagent, recovering baseline performance even if the assumed Constitution clashes\nwith reality.\n","authors":["Simon Kohaut","Felix Divo","Benedict Flade","Devendra Singh Dhami","Julian Eggert","Kristian Kersting"],"pdf_url":"https://arxiv.org/pdf/2412.18347v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03973v1","updated":"2025-03-05T23:48:32Z","published":"2025-03-05T23:48:32Z","title":"Equivariant Filter Design for Range-only SLAM","summary":"  Range-only Simultaneous Localisation and Mapping (RO-SLAM) is of interest due\nto its practical applications in ultra-wideband (UWB) and Bluetooth Low Energy\n(BLE) localisation in terrestrial and aerial applications and acoustic beacon\nlocalisation in submarine applications. In this work, we consider a mobile\nrobot equipped with an inertial measurement unit (IMU) and a range sensor that\nmeasures distances to a collection of fixed landmarks. We derive an equivariant\nfilter (EqF) for the RO-SLAM problem based on a symmetry Lie group that is\ncompatible with the range measurements. The proposed filter does not require\nbootstrapping or initialisation of landmark positions, and demonstrates\nrobustness to the no-prior situation. The filter is demonstrated on a\nreal-world dataset, and it is shown to significantly outperform a\nstate-of-the-art EKF alternative in terms of both accuracy and robustness.\n","authors":["Yixiao Ge","Arthur Pearce","Pieter van Goor","Robert Mahony"],"pdf_url":"https://arxiv.org/pdf/2503.03973v1.pdf","comment":"11 pages, 5 figures, accepted for presentation at IEEE International\n  Conference on Robotics and Automation 2025"},{"id":"http://arxiv.org/abs/2406.04560v2","updated":"2025-03-05T23:42:19Z","published":"2024-06-07T00:28:12Z","title":"meSch: Multi-Agent Energy-Aware Scheduling for Task Persistence","summary":"  This paper develops a scheduling protocol for a team of autonomous robots\nthat operate on long-term persistent tasks. The proposed framework, called\nmeSch, accounts for the limited battery capacity of the robots and ensures that\nthe robots return to charge their batteries one at a time at the single\ncharging station. The protocol is applicable to general nonlinear robot models\nunder certain assumptions, does not require robots to be deployed at different\ntimes, and can handle robots with different discharge rates. We further\nconsider the case when the charging station is mobile and its state information\nis subject to uncertainty. The feasibility of the algorithm in terms of\nensuring persistent charging is given under certain assumptions, while the\nefficacy of meSch is validated through simulation and hardware experiments.\n","authors":["Kaleb Ben Naveed","An Dang","Rahul Kumar","Dimitra Panagou"],"pdf_url":"https://arxiv.org/pdf/2406.04560v2.pdf","comment":"Submitted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025"},{"id":"http://arxiv.org/abs/2211.09619v4","updated":"2025-03-05T23:25:34Z","published":"2022-11-17T16:12:45Z","title":"Introduction to Online Control","summary":"  This text presents an introduction to an emerging paradigm in control of\ndynamical systems and differentiable reinforcement learning called online\nnonstochastic control. The new approach applies techniques from online convex\noptimization and convex relaxations to obtain new methods with provable\nguarantees for classical settings in optimal and robust control.\n  The primary distinction between online nonstochastic control and other\nframeworks is the objective. In optimal control, robust control, and other\ncontrol methodologies that assume stochastic noise, the goal is to perform\ncomparably to an offline optimal strategy. In online nonstochastic control,\nboth the cost functions as well as the perturbations from the assumed dynamical\nmodel are chosen by an adversary. Thus the optimal policy is not defined a\npriori. Rather, the target is to attain low regret against the best policy in\nhindsight from a benchmark class of policies.\n  This objective suggests the use of the decision making framework of online\nconvex optimization as an algorithmic methodology. The resulting methods are\nbased on iterative mathematical optimization algorithms, and are accompanied by\nfinite-time regret and computational complexity guarantees.\n","authors":["Elad Hazan","Karan Singh"],"pdf_url":"https://arxiv.org/pdf/2211.09619v4.pdf","comment":"Draft; comments/suggestions welcome at\n  nonstochastic.control@gmail.com"},{"id":"http://arxiv.org/abs/2409.09523v2","updated":"2025-03-05T23:23:09Z","published":"2024-09-14T20:29:11Z","title":"Lab2Car: A Versatile Wrapper for Deploying Experimental Planners in\n  Complex Real-world Environments","summary":"  Human-level autonomous driving is an ever-elusive goal, with planning and\ndecision making -- the cognitive functions that determine driving behavior --\nposing the greatest challenge. Despite a proliferation of promising approaches,\nprogress is stifled by the difficulty of deploying experimental planners in\nnaturalistic settings. In this work, we propose Lab2Car, an optimization-based\nwrapper that can take a trajectory sketch from an arbitrary motion planner and\nconvert it to a safe, comfortable, dynamically feasible trajectory that the car\ncan follow. This allows motion planners that do not provide such guarantees to\nbe safely tested and optimized in real-world environments. We demonstrate the\nversatility of Lab2Car by using it to deploy a machine learning (ML) planner\nand a classical planner on self-driving cars in Las Vegas. The resulting\nsystems handle challenging scenarios, such as cut-ins, overtaking, and\nyielding, in complex urban environments like casino pick-up/drop-off areas. Our\nwork paves the way for quickly deploying and evaluating candidate motion\nplanners in realistic settings, ensuring rapid iteration and accelerating\nprogress towards human-level autonomy.\n","authors":["Marc Heim","Francisco Suarez-Ruiz","Ishraq Bhuiyan","Bruno Brito","Momchil S. Tomov"],"pdf_url":"https://arxiv.org/pdf/2409.09523v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2503.03957v1","updated":"2025-03-05T23:08:43Z","published":"2025-03-05T23:08:43Z","title":"Enhancing Autonomous Driving Safety with Collision Scenario Integration","summary":"  Autonomous vehicle safety is crucial for the successful deployment of\nself-driving cars. However, most existing planning methods rely heavily on\nimitation learning, which limits their ability to leverage collision data\neffectively. Moreover, collecting collision or near-collision data is\ninherently challenging, as it involves risks and raises ethical and practical\nconcerns. In this paper, we propose SafeFusion, a training framework to learn\nfrom collision data. Instead of over-relying on imitation learning, SafeFusion\nintegrates safety-oriented metrics during training to enable collision\navoidance learning. In addition, to address the scarcity of collision data, we\npropose CollisionGen, a scalable data generation pipeline to generate diverse,\nhigh-quality scenarios using natural language prompts, generative models, and\nrule-based filtering. Experimental results show that our approach improves\nplanning performance in collision-prone scenarios by 56\\% over previous\nstate-of-the-art planners while maintaining effectiveness in regular driving\nsituations. Our work provides a scalable and effective solution for advancing\nthe safety of autonomous driving systems.\n","authors":["Zi Wang","Shiyi Lan","Xinglong Sun","Nadine Chang","Zhenxin Li","Zhiding Yu","Jose M. Alvarez"],"pdf_url":"https://arxiv.org/pdf/2503.03957v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03947v1","updated":"2025-03-05T22:25:54Z","published":"2025-03-05T22:25:54Z","title":"COARSE: Collaborative Pseudo-Labeling with Coarse Real Labels for\n  Off-Road Semantic Segmentation","summary":"  Autonomous off-road navigation faces challenges due to diverse, unstructured\nenvironments, requiring robust perception with both geometric and semantic\nunderstanding. However, scarce densely labeled semantic data limits\ngeneralization across domains. Simulated data helps, but introduces domain\nadaptation issues. We propose COARSE, a semi-supervised domain adaptation\nframework for off-road semantic segmentation, leveraging sparse, coarse\nin-domain labels and densely labeled out-of-domain data. Using pretrained\nvision transformers, we bridge domain gaps with complementary pixel-level and\npatch-level decoders, enhanced by a collaborative pseudo-labeling strategy on\nunlabeled data. Evaluations on RUGD and Rellis-3D datasets show significant\nimprovements of 9.7\\% and 8.4\\% respectively, versus only using coarse data.\nTests on real-world off-road vehicle data in a multi-biome setting further\ndemonstrate COARSE's applicability.\n","authors":["Aurelio Noca","Xianmei Lei","Jonathan Becktor","Jeffrey Edlund","Anna Sabel","Patrick Spieler","Curtis Padgett","Alexandre Alahi","Deegan Atha"],"pdf_url":"https://arxiv.org/pdf/2503.03947v1.pdf","comment":"preprint, 8 pages"},{"id":"http://arxiv.org/abs/2403.12853v3","updated":"2025-03-05T22:11:38Z","published":"2024-03-19T15:57:32Z","title":"FlexiFly: Interfacing the Physical World with Foundation Models\n  Empowered by Reconfigurable Drone Systems","summary":"  Foundation models (FM) have shown immense human-like capabilities for\ngenerating digital media. However, foundation models that can freely sense,\ninteract, and actuate the physical domain is far from being realized. This is\ndue to 1) requiring dense deployments of sensors to fully cover and analyze\nlarge spaces, while 2) events often being localized to small areas, making it\ndifficult for FMs to pinpoint relevant areas of interest relevant to the\ncurrent task. We propose FlexiFly, a platform that enables FMs to ``zoom in''\nand analyze relevant areas with higher granularity to better understand the\nphysical environment and carry out tasks. FlexiFly accomplishes by introducing\n1) a novel image segmentation technique that aids in identifying relevant\nlocations and 2) a modular and reconfigurable sensing and actuation drone\nplatform that FMs can actuate to ``zoom in'' with relevant sensors and\nactuators. We demonstrate through real smart home deployments that FlexiFly\nenables FMs and LLMs to complete diverse tasks up to $85\\%$ more successfully.\nFlexiFly is critical step towards FMs and LLMs that can naturally interface\nwith the physical world.\n","authors":["Minghui Zhao","Junxi Xia","Kaiyuan Hou","Yanchen Liu","Stephen Xia","Xiaofan Jiang"],"pdf_url":"https://arxiv.org/pdf/2403.12853v3.pdf","comment":"This paper is accepted by ACM SenSys 2025. The published version is\n  https://doi.org/10.1145/3715014.3722081 in ACM Digital Library"},{"id":"http://arxiv.org/abs/2503.03921v1","updated":"2025-03-05T21:42:46Z","published":"2025-03-05T21:42:46Z","title":"CREStE: Scalable Mapless Navigation with Internet Scale Priors and\n  Counterfactual Guidance","summary":"  We address the long-horizon mapless navigation problem: enabling robots to\ntraverse novel environments without relying on high-definition maps or precise\nwaypoints that specify exactly where to navigate. Achieving this requires\novercoming two major challenges -- learning robust, generalizable perceptual\nrepresentations of the environment without pre-enumerating all possible\nnavigation factors and forms of perceptual aliasing and utilizing these learned\nrepresentations to plan human-aligned navigation paths. Existing solutions\nstruggle to generalize due to their reliance on hand-curated object lists that\noverlook unforeseen factors, end-to-end learning of navigation features from\nscarce large-scale robot datasets, and handcrafted reward functions that scale\npoorly to diverse scenarios. To overcome these limitations, we propose CREStE,\nthe first method that learns representations and rewards for addressing the\nfull mapless navigation problem without relying on large-scale robot datasets\nor manually curated features. CREStE leverages visual foundation models trained\non internet-scale data to learn continuous bird's-eye-view representations\ncapturing elevation, semantics, and instance-level features. To utilize learned\nrepresentations for planning, we propose a counterfactual-based loss and active\nlearning procedure that focuses on the most salient perceptual cues by querying\nhumans for counterfactual trajectory annotations in challenging scenes. We\nevaluate CREStE in kilometer-scale navigation tasks across six distinct urban\nenvironments. CREStE significantly outperforms all state-of-the-art approaches\nwith 70% fewer human interventions per mission, including a 2-kilometer mission\nin an unseen environment with just 1 intervention; showcasing its robustness\nand effectiveness for long-horizon mapless navigation. For videos and\nadditional materials, see https://amrl.cs.utexas.edu/creste .\n","authors":["Arthur Zhang","Harshit Sikchi","Amy Zhang","Joydeep Biswas"],"pdf_url":"https://arxiv.org/pdf/2503.03921v1.pdf","comment":"19 pages, 10 figures, 5 tables"},{"id":"http://arxiv.org/abs/2503.03912v1","updated":"2025-03-05T21:25:03Z","published":"2025-03-05T21:25:03Z","title":"GO-VMP: Global Optimization for View Motion Planning in Fruit Mapping","summary":"  Automating labor-intensive tasks such as crop monitoring with robots is\nessential for enhancing production and conserving resources. However,\nautonomously monitoring horticulture crops remains challenging due to their\ncomplex structures, which often result in fruit occlusions. Existing view\nplanning methods attempt to reduce occlusions but either struggle to achieve\nadequate coverage or incur high robot motion costs. We introduce a global\noptimization approach for view motion planning that aims to minimize robot\nmotion costs while maximizing fruit coverage. To this end, we leverage coverage\nconstraints derived from the set covering problem (SCP) within a shortest\nHamiltonian path problem (SHPP) formulation. While both SCP and SHPP are\nwell-established, their tailored integration enables a unified framework that\ncomputes a global view path with minimized motion while ensuring full coverage\nof selected targets. Given the NP-hard nature of the problem, we employ a\nregion-prior-based selection of coverage targets and a sparse graph structure\nto achieve effective optimization outcomes within a limited time. Experiments\nin simulation demonstrate that our method detects more fruits, enhances surface\ncoverage, and achieves higher volume accuracy than the motion-efficient\nbaseline with a moderate increase in motion cost, while significantly reducing\nmotion costs compared to the coverage-focused baseline. Real-world experiments\nfurther confirm the practical applicability of our approach.\n","authors":["Allen Isaac Jose","Sicong Pan","Tobias Zaenker","Rohit Menon","Sebastian Houben","Maren Bennewitz"],"pdf_url":"https://arxiv.org/pdf/2503.03912v1.pdf","comment":"Allen Isaac Jose and Sicong Pan have equal contribution. Submitted to\n  IROS 2025"},{"id":"http://arxiv.org/abs/2503.03911v1","updated":"2025-03-05T21:23:15Z","published":"2025-03-05T21:23:15Z","title":"Safe LLM-Controlled Robots with Formal Guarantees via Reachability\n  Analysis","summary":"  The deployment of Large Language Models (LLMs) in robotic systems presents\nunique safety challenges, particularly in unpredictable environments. Although\nLLMs, leveraging zero-shot learning, enhance human-robot interaction and\ndecision-making capabilities, their inherent probabilistic nature and lack of\nformal guarantees raise significant concerns for safety-critical applications.\nTraditional model-based verification approaches often rely on precise system\nmodels, which are difficult to obtain for real-world robotic systems and may\nnot be fully trusted due to modeling inaccuracies, unmodeled dynamics, or\nenvironmental uncertainties. To address these challenges, this paper introduces\na safety assurance framework for LLM-controlled robots based on data-driven\nreachability analysis, a formal verification technique that ensures all\npossible system trajectories remain within safe operational limits. Our\nframework specifically investigates the problem of instructing an LLM to\nnavigate the robot to a specified goal and assesses its ability to generate\nlow-level control actions that successfully guide the robot safely toward that\ngoal. By leveraging historical data to construct reachable sets of states for\nthe robot-LLM system, our approach provides rigorous safety guarantees against\nunsafe behaviors without relying on explicit analytical models. We validate the\nframework through experimental case studies in autonomous navigation and task\nplanning, demonstrating its effectiveness in mitigating risks associated with\nLLM-generated commands. This work advances the integration of formal methods\ninto LLM-based robotics, offering a principled and practical approach to\nensuring safety in next-generation autonomous systems.\n","authors":["Ahmad Hafez","Alireza Naderi Akhormeh","Amr Hegazy","Amr Alanwar"],"pdf_url":"https://arxiv.org/pdf/2503.03911v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03897v1","updated":"2025-03-05T20:55:16Z","published":"2025-03-05T20:55:16Z","title":"Endpoint-Explicit Differential Dynamic Programming via Exact Resolution","summary":"  We introduce a novel method for handling endpoint constraints in constrained\ndifferential dynamic programming (DDP). Unlike existing approaches, our method\nguarantees quadratic convergence and is exact, effectively managing rank\ndeficiencies in both endpoint and stagewise equality constraints. It is\napplicable to both forward and inverse dynamics formulations, making it\nparticularly well-suited for model predictive control (MPC) applications and\nfor accelerating optimal control (OC) solvers. We demonstrate the efficacy of\nour approach across a broad range of robotics problems and provide a\nuser-friendly open-source implementation within CROCODDYL.\n","authors":["Maria Parilli","Sergi Martinez","Carlos Mastalli"],"pdf_url":"https://arxiv.org/pdf/2503.03897v1.pdf","comment":"7 pages, IEEE ICRA paper"},{"id":"http://arxiv.org/abs/2503.03890v1","updated":"2025-03-05T20:46:30Z","published":"2025-03-05T20:46:30Z","title":"LensDFF: Language-enhanced Sparse Feature Distillation for Efficient\n  Few-Shot Dexterous Manipulation","summary":"  Learning dexterous manipulation from few-shot demonstrations is a significant\nyet challenging problem for advanced, human-like robotic systems. Dense\ndistilled feature fields have addressed this challenge by distilling rich\nsemantic features from 2D visual foundation models into the 3D domain. However,\ntheir reliance on neural rendering models such as Neural Radiance Fields (NeRF)\nor Gaussian Splatting results in high computational costs. In contrast,\nprevious approaches based on sparse feature fields either suffer from\ninefficiencies due to multi-view dependencies and extensive training or lack\nsufficient grasp dexterity. To overcome these limitations, we propose\nLanguage-ENhanced Sparse Distilled Feature Field (LensDFF), which efficiently\ndistills view-consistent 2D features onto 3D points using our novel\nlanguage-enhanced feature fusion strategy, thereby enabling single-view\nfew-shot generalization. Based on LensDFF, we further introduce a few-shot\ndexterous manipulation framework that integrates grasp primitives into the\ndemonstrations to generate stable and highly dexterous grasps. Moreover, we\npresent a real2sim grasp evaluation pipeline for efficient grasp assessment and\nhyperparameter tuning. Through extensive simulation experiments based on the\nreal2sim pipeline and real-world experiments, our approach achieves competitive\ngrasping performance, outperforming state-of-the-art approaches.\n","authors":["Qian Feng","David S. Martinez Lema","Jianxiang Feng","Zhaopeng Chen","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2503.03890v1.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2503.03889v1","updated":"2025-03-05T20:43:49Z","published":"2025-03-05T20:43:49Z","title":"Pretrained LLMs as Real-Time Controllers for Robot Operated Serial\n  Production Line","summary":"  The manufacturing industry is undergoing a transformative shift, driven by\ncutting-edge technologies like 5G, AI, and cloud computing. Despite these\nadvancements, effective system control, which is crucial for optimizing\nproduction efficiency, remains a complex challenge due to the intricate,\nknowledge-dependent nature of manufacturing processes and the reliance on\ndomain-specific expertise. Conventional control methods often demand heavy\ncustomization, considerable computational resources, and lack transparency in\ndecision-making. In this work, we investigate the feasibility of using Large\nLanguage Models (LLMs), particularly GPT-4, as a straightforward, adaptable\nsolution for controlling manufacturing systems, specifically, mobile robot\nscheduling. We introduce an LLM-based control framework to assign mobile robots\nto different machines in robot assisted serial production lines, evaluating its\nperformance in terms of system throughput. Our proposed framework outperforms\ntraditional scheduling approaches such as First-Come-First-Served (FCFS),\nShortest Processing Time (SPT), and Longest Processing Time (LPT). While it\nachieves performance that is on par with state-of-the-art methods like\nMulti-Agent Reinforcement Learning (MARL), it offers a distinct advantage by\ndelivering comparable throughput without the need for extensive retraining.\nThese results suggest that the proposed LLM-based solution is well-suited for\nscenarios where technical expertise, computational resources, and financial\ninvestment are limited, while decision transparency and system scalability are\ncritical concerns.\n","authors":["Muhammad Waseem","Kshitij Bhatta","Chen Li","Qing Chang"],"pdf_url":"https://arxiv.org/pdf/2503.03889v1.pdf","comment":"20 pages, 7 figures"},{"id":"http://arxiv.org/abs/2409.14342v2","updated":"2025-03-05T20:19:05Z","published":"2024-09-22T07:01:18Z","title":"Adapting Gait Frequency for Posture-regulating Humanoid Push-recovery\n  via Hierarchical Model Predictive Control","summary":"  Current humanoid push-recovery strategies often use whole-body motion, yet\nthey tend to overlook posture regulation. For instance, in manipulation tasks,\nthe upper body may need to stay upright and have minimal recovery displacement.\nThis paper introduces a novel approach to enhancing humanoid push-recovery\nperformance under unknown disturbances and regulating body posture by tailoring\nthe recovery stepping strategy. We propose a hierarchical-MPC-based scheme that\nanalyzes and detects instability in the prediction window and quickly recovers\nthrough adapting gait frequency. Our approach integrates a high-level nonlinear\nMPC, a posture-aware gait frequency adaptation planner, and a low-level convex\nlocomotion MPC. The planners predict the center of mass (CoM) state\ntrajectories that can be assessed for precursors of potential instability and\nposture deviation. In simulation, we demonstrate improved maximum recoverable\nimpulse by 131% on average compared with baseline approaches. In hardware\nexperiments, a 125 ms advancement in recovery stepping timing/reflex has been\nobserved with the proposed approach. We also demonstrate improved push-recovery\nperformance and minimized body attitude change under 0.2 rad.\n","authors":["Junheng Li","Zhanhao Le","Junchao Ma","Quan Nguyen"],"pdf_url":"https://arxiv.org/pdf/2409.14342v2.pdf","comment":"7 pages, 6 figures, accepted to ICRA 2025"},{"id":"http://arxiv.org/abs/2406.05881v3","updated":"2025-03-05T19:34:08Z","published":"2024-06-09T18:40:24Z","title":"LGR2: Language Guided Reward Relabeling for Accelerating Hierarchical\n  Reinforcement Learning","summary":"  Developing interactive systems that utilize natural language instructions to\nsolve complex robotic control tasks has long been a goal of the robotics\ncommunity. While Large Language Models (LLMs) excel at logical reasoning,\nin-context learning, and code generation, translating high-level instructions\ninto low-level robotic actions still remains challenging. Furthermore, solving\nsuch tasks often requires acquiring policies to execute diverse subtasks and\nintegrating them to achieve the final objective. Hierarchical Reinforcement\nLearning (HRL) offers a promising solution for solving such tasks by enabling\ntemporal abstraction and improved exploration. However, HRL suffers from\nnon-stationarity caused by the changing lower-level behaviour, which hinders\neffective policy learning. We propose LGR2, a novel HRL framework that\nmitigates non-stationarity in HRL by using language-guided higher-level rewards\nthat remain unaffected by the changing lower-level policy behaviour. To analyze\nthe efficacy of our approach, we perform empirical analysis to demonstrate that\nLGR2 effectively mitigates non-stationarity in HRL and attains success rates\nexceeding 70% in challenging, sparsely-rewarded robotic navigation and\nmanipulation environments, where other baselines typically fail to show\nsignificant progress. Finally, we perform real-world robotic experiments on\ncomplex tasks and demonstrate that LGR2 consistently outperforms the baselines.\n","authors":["Utsav Singh","Pramit Bhattacharyya","Vinay P. Namboodiri"],"pdf_url":"https://arxiv.org/pdf/2406.05881v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.09213v2","updated":"2025-03-05T19:29:52Z","published":"2024-10-11T19:35:05Z","title":"iFANnpp: Nuclear Power Plant Digital Twin for Robots and Autonomous\n  Intelligence","summary":"  Robotics has gained significant attention in the nuclear industry due its\nprecision and ability to automate tasks. However, the increasing complexity of\nrobots has led to a growing demand for advanced simulation and control methods\nto predict robot behavior and optimize plant performance, motivating the use of\ndigital twins for robotic applications. Most existing digital twins only\naddress parts of systems and do not offer a total design of a nuclear power\nplant. Furthermore, they are often designed for specific algorithms or tasks,\nmaking them unsuitable for broader research applications or projects. In\nresponse, this work proposes a comprehensive nuclear power plant digital twin\ndesigned to improve real-time monitoring, operational efficiency, and\npredictive maintenance. The full nuclear power plant is modeled in Unreal\nEngine 5 to incorporate the complexities and various phenomena. The\nhigh-resolution simulation environment is integrated with a Generic Pressurized\nWater Reactor Simulator, a high-fidelity physics-driven software, to create a\nrealistic model of a nuclear power plant and a real-time updated virtual\nenvironment. The virtual environment provides various features for researchers\nto easily test custom robot algorithms and frameworks, applicable to research\nin the nuclear industry as well as industrial systems in general. The digital\ntwin's performance is presented, and critical research problems are addressed,\nincluding multi-robot task scheduling and robot navigation in\nradiation-affected areas, by leveraging implemented features.\n","authors":["Youndo Do","Marc Zebrowitz","Jackson Stahl","Fan Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.09213v2.pdf","comment":"13 pages, 10 figures; submitted to IEEE Transactions on Automation\n  Science and Engineering"},{"id":"http://arxiv.org/abs/2503.03796v1","updated":"2025-03-05T14:33:18Z","published":"2025-03-05T14:33:18Z","title":"Human Implicit Preference-Based Policy Fine-tuning for Multi-Agent\n  Reinforcement Learning in USV Swarm","summary":"  Multi-Agent Reinforcement Learning (MARL) has shown promise in solving\ncomplex problems involving cooperation and competition among agents, such as an\nUnmanned Surface Vehicle (USV) swarm used in search and rescue, surveillance,\nand vessel protection. However, aligning system behavior with user preferences\nis challenging due to the difficulty of encoding expert intuition into reward\nfunctions. To address the issue, we propose a Reinforcement Learning with Human\nFeedback (RLHF) approach for MARL that resolves credit-assignment challenges\nthrough an Agent-Level Feedback system categorizing feedback into intra-agent,\ninter-agent, and intra-team types. To overcome the challenges of direct human\nfeedback, we employ a Large Language Model (LLM) evaluator to validate our\napproach using feedback scenarios such as region constraints, collision\navoidance, and task allocation. Our method effectively refines USV swarm\npolicies, addressing key challenges in multi-agent systems while maintaining\nfairness and performance consistency.\n","authors":["Hyeonjun Kim","Kanghoon Lee","Junho Park","Jiachen Li","Jinkyoo Park"],"pdf_url":"https://arxiv.org/pdf/2503.03796v1.pdf","comment":"7 pages, 4 figures"}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2503.00735v3","updated":"2025-03-05T11:50:24Z","published":"2025-03-02T05:16:43Z","title":"LADDER: Self-Improving LLMs Through Recursive Problem Decomposition","summary":"  We introduce LADDER (Learning through Autonomous Difficulty-Driven Example\nRecursion), a framework which enables Large Language Models to autonomously\nimprove their problem-solving capabilities through self-guided learning by\nrecursively generating and solving progressively simpler variants of complex\nproblems. Unlike prior approaches that require curated datasets or human\nfeedback, LADDER leverages a model's own capabilities to generate easier\nquestion variants. We demonstrate LADDER's effectiveness in the subject of\nmathematical integration, improving Llama 3.2 3B's accuracy from 1% to 82% on\nundergraduate-level problems and enabling Qwen2.5 7B Deepseek-R1 Distilled to\nachieve 73% on the MIT Integration Bee qualifying examination. We also\nintroduce TTRL (Test-Time Reinforcement Learning), where we perform\nreinforcement learning on variants of test problems at inference time. TTRL\nenables Qwen2.5 7B Deepseek-R1 Distilled to achieve a state-of-the-art score of\n90% on the MIT Integration Bee qualifying examination, surpassing OpenAI o1's\nperformance. These results show how self-directed strategic learning can\nachieve significant capability improvements without relying on architectural\nscaling or human supervision.\n","authors":["Toby Simonds","Akira Yoshiyama"],"pdf_url":"https://arxiv.org/pdf/2503.00735v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02623v2","updated":"2025-03-05T15:23:16Z","published":"2025-03-04T13:48:50Z","title":"Rewarding Doubt: A Reinforcement Learning Approach to Confidence\n  Calibration of Large Language Models","summary":"  A safe and trustworthy use of Large Language Models (LLMs) requires an\naccurate expression of confidence in their answers. We introduce a novel\nReinforcement Learning (RL) approach for LLM calibration that fine-tunes LLMs\nto elicit calibrated confidence estimations in their answers to factual\nquestions. We model the problem as a betting game where the model predicts a\nconfidence score together with every answer, and design a reward function that\npenalizes both over and under-confidence. We prove that under our reward design\nan optimal policy would result in a perfectly calibrated confidence estimation.\nOur experiments demonstrate significantly improved confidence calibration and\ngeneralization to new tasks without re-training, indicating that our approach\nteaches a general confidence awareness. This approach enables the training of\ninherently calibrated LLMs.\n","authors":["Paul Stangel","David Bani-Harouni","Chantal Pellegrini","Ege Özsoy","Kamilia Zaripova","Matthias Keicher","Nassir Navab"],"pdf_url":"https://arxiv.org/pdf/2503.02623v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.01214v3","updated":"2025-03-05T07:02:28Z","published":"2024-07-01T11:59:59Z","title":"Revisiting Random Walks for Learning on Graphs","summary":"  We revisit a simple model class for machine learning on graphs, where a\nrandom walk on a graph produces a machine-readable record, and this record is\nprocessed by a deep neural network to directly make vertex-level or graph-level\npredictions. We call these stochastic machines random walk neural networks\n(RWNNs), and through principled analysis, show that we can design them to be\nisomorphism invariant while capable of universal approximation of graph\nfunctions in probability. A useful finding is that almost any kind of record of\nrandom walks guarantees probabilistic invariance as long as the vertices are\nanonymized. This enables us, for example, to record random walks in plain text\nand adopt a language model to read these text records to solve graph tasks. We\nfurther establish a parallelism to message passing neural networks using tools\nfrom Markov chain theory, and show that over-smoothing in message passing is\nalleviated by construction in RWNNs, while over-squashing manifests as\nprobabilistic under-reaching. We empirically demonstrate RWNNs on a range of\nproblems, verifying our theoretical analysis and demonstrating the use of\nlanguage models for separating strongly regular graphs where 3-WL test fails,\nand transductive classification on arXiv citation network. Code is available at\nhttps://github.com/jw9730/random-walk.\n","authors":["Jinwoo Kim","Olga Zaghen","Ayhan Suleymanzade","Youngmin Ryou","Seunghoon Hong"],"pdf_url":"https://arxiv.org/pdf/2407.01214v3.pdf","comment":"51 pages, 14 figures"},{"id":"http://arxiv.org/abs/2503.01478v3","updated":"2025-03-05T05:24:54Z","published":"2025-03-03T12:37:34Z","title":"SePer: Measure Retrieval Utility Through The Lens Of Semantic Perplexity\n  Reduction","summary":"  Large Language Models (LLMs) have demonstrated improved generation\nperformance by incorporating externally retrieved knowledge, a process known as\nretrieval-augmented generation (RAG). Despite the potential of this approach,\nexisting studies evaluate RAG effectiveness by 1) assessing retrieval and\ngeneration components jointly, which obscures retrieval's distinct\ncontribution, or 2) examining retrievers using traditional metrics such as\nNDCG, which creates a gap in understanding retrieval's true utility in the\noverall generation process. To address the above limitations, in this work, we\nintroduce an automatic evaluation method that measures retrieval quality\nthrough the lens of information gain within the RAG framework. Specifically, we\npropose Semantic Perplexity (SePer), a metric that captures the LLM's internal\nbelief about the correctness of the retrieved information. We quantify the\nutility of retrieval by the extent to which it reduces semantic perplexity\npost-retrieval. Extensive experiments demonstrate that SePer not only aligns\nclosely with human preferences but also offers a more precise and efficient\nevaluation of retrieval utility across diverse RAG scenarios.\n","authors":["Lu Dai","Yijie Xu","Jinhui Ye","Hao Liu","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2503.01478v3.pdf","comment":"ICLR 2025 Spotlight"},{"id":"http://arxiv.org/abs/2503.02368v2","updated":"2025-03-05T09:12:25Z","published":"2025-03-04T07:49:10Z","title":"Iterative Value Function Optimization for Guided Decoding","summary":"  While Reinforcement Learning from Human Feedback (RLHF) has become the\npredominant method for controlling language model outputs, it suffers from high\ncomputational costs and training instability. Guided decoding, especially\nvalue-guided methods, offers a cost-effective alternative by controlling\noutputs without re-training models. However, the accuracy of the value function\nis crucial for value-guided decoding, as inaccuracies can lead to suboptimal\ndecision-making and degraded performance. Existing methods struggle with\naccurately estimating the optimal value function, leading to less effective\ncontrol. We propose Iterative Value Function Optimization, a novel framework\nthat addresses these limitations through two key components: Monte Carlo Value\nEstimation, which reduces estimation variance by exploring diverse\ntrajectories, and Iterative On-Policy Optimization, which progressively\nimproves value estimation through collecting trajectories from value-guided\npolicies. Extensive experiments on text summarization, multi-turn dialogue, and\ninstruction following demonstrate the effectiveness of value-guided decoding\napproaches in aligning language models. These approaches not only achieve\nalignment but also significantly reduce computational costs by leveraging\nprincipled value function optimization for efficient and effective control.\n","authors":["Zhenhua Liu","Lijun Li","Ruizhe Chen","Yuxian Jiang","Tong Zhu","Zhaochen Su","Wenliang Chen","Jing Shao"],"pdf_url":"https://arxiv.org/pdf/2503.02368v2.pdf","comment":"20 pages, 10 figures"},{"id":"http://arxiv.org/abs/2407.17773v3","updated":"2025-03-05T03:07:12Z","published":"2024-07-25T05:02:39Z","title":"KiVA: Kid-inspired Visual Analogies for Testing Large Multimodal Models","summary":"  This paper investigates visual analogical reasoning in large multimodal\nmodels (LMMs) compared to human adults and children. A \"visual analogy\" is an\nabstract rule inferred from one image and applied to another. While benchmarks\nexist for testing visual reasoning in LMMs, they require advanced skills and\nomit basic visual analogies that even young children can make. Inspired by\ndevelopmental psychology, we propose a new benchmark of 4,300 visual\ntransformations of everyday objects to test LMMs on visual analogical reasoning\nand compare them to children (ages three to five) and to adults. We structure\nthe evaluation into three stages: identifying what changed (e.g., color,\nnumber, etc.), how it changed (e.g., added one object), and applying the rule\nto new scenarios. Our findings show that while GPT-o1, GPT-4V, LLaVA-1.5, and\nMANTIS identify the \"what\" effectively, they struggle with quantifying the\n\"how\" and extrapolating this rule to new objects. In contrast, children and\nadults exhibit much stronger analogical reasoning at all three stages.\nAdditionally, the strongest tested model, GPT-o1, performs better in tasks\ninvolving simple surface-level visual attributes like color and size,\ncorrelating with quicker human adult response times. Conversely, more complex\ntasks such as number, rotation, and reflection, which necessitate extensive\ncognitive processing and understanding of extrinsic spatial properties in the\nphysical world, present more significant challenges. Altogether, these findings\nhighlight the limitations of training models on data that primarily consists of\n2D images and text.\n","authors":["Eunice Yiu","Maan Qraitem","Anisa Noor Majhi","Charlie Wong","Yutong Bai","Shiry Ginosar","Alison Gopnik","Kate Saenko"],"pdf_url":"https://arxiv.org/pdf/2407.17773v3.pdf","comment":"10 pages. Project website: https://ey242.github.io/kiva.github.io/.\n  Benchmark and code: https://github.com/ey242/KiVA"},{"id":"http://arxiv.org/abs/2503.03750v1","updated":"2025-03-05T18:59:23Z","published":"2025-03-05T18:59:23Z","title":"The MASK Benchmark: Disentangling Honesty From Accuracy in AI Systems","summary":"  As large language models (LLMs) become more capable and agentic, the\nrequirement for trust in their outputs grows significantly, yet at the same\ntime concerns have been mounting that models may learn to lie in pursuit of\ntheir goals. To address these concerns, a body of work has emerged around the\nnotion of \"honesty\" in LLMs, along with interventions aimed at mitigating\ndeceptive behaviors. However, evaluations of honesty are currently highly\nlimited, with no benchmark combining large scale and applicability to all\nmodels. Moreover, many benchmarks claiming to measure honesty in fact simply\nmeasure accuracy--the correctness of a model's beliefs--in disguise. In this\nwork, we introduce a large-scale human-collected dataset for measuring honesty\ndirectly, allowing us to disentangle accuracy from honesty for the first time.\nAcross a diverse set of LLMs, we find that while larger models obtain higher\naccuracy on our benchmark, they do not become more honest. Surprisingly, while\nmost frontier LLMs obtain high scores on truthfulness benchmarks, we find a\nsubstantial propensity in frontier LLMs to lie when pressured to do so,\nresulting in low honesty scores on our benchmark. We find that simple methods,\nsuch as representation engineering interventions, can improve honesty. These\nresults underscore the growing need for robust evaluations and effective\ninterventions to ensure LLMs remain trustworthy.\n","authors":["Richard Ren","Arunim Agarwal","Mantas Mazeika","Cristina Menghini","Robert Vacareanu","Brad Kenstler","Mick Yang","Isabelle Barrass","Alice Gatti","Xuwang Yin","Eduardo Trevino","Matias Geralnik","Adam Khoja","Dean Lee","Summer Yue","Dan Hendrycks"],"pdf_url":"https://arxiv.org/pdf/2503.03750v1.pdf","comment":"Website: https://www.mask-benchmark.ai"},{"id":"http://arxiv.org/abs/2503.03746v1","updated":"2025-03-05T18:58:44Z","published":"2025-03-05T18:58:44Z","title":"Process-based Self-Rewarding Language Models","summary":"  Large Language Models have demonstrated outstanding performance across\nvarious downstream tasks and have been widely applied in multiple scenarios.\nHuman-annotated preference data is used for training to further improve LLMs'\nperformance, which is constrained by the upper limit of human performance.\nTherefore, Self-Rewarding method has been proposed, where LLMs generate\ntraining data by rewarding their own outputs. However, the existing\nself-rewarding paradigm is not effective in mathematical reasoning scenarios\nand may even lead to a decline in performance. In this work, we propose the\nProcess-based Self-Rewarding pipeline for language models, which introduces\nlong-thought reasoning, step-wise LLM-as-a-Judge, and step-wise preference\noptimization within the self-rewarding paradigm. Our new paradigm successfully\nenhances the performance of LLMs on multiple mathematical reasoning benchmarks\nthrough iterative Process-based Self-Rewarding, demonstrating the immense\npotential of self-rewarding to achieve LLM reasoning that may surpass human\ncapabilities.\n","authors":["Shimao Zhang","Xiao Liu","Xin Zhang","Junxiao Liu","Zheheng Luo","Shujian Huang","Yeyun Gong"],"pdf_url":"https://arxiv.org/pdf/2503.03746v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03743v1","updated":"2025-03-05T18:56:16Z","published":"2025-03-05T18:56:16Z","title":"CHOP: Mobile Operating Assistant with Constrained High-frequency\n  Optimized Subtask Planning","summary":"  The advancement of visual language models (VLMs) has enhanced mobile device\noperations, allowing simulated human-like actions to address user requirements.\nCurrent VLM-based mobile operating assistants can be structured into three\nlevels: task, subtask, and action. The subtask level, linking high-level goals\nwith low-level executable actions, is crucial for task completion but faces two\nchallenges: ineffective subtasks that lower-level agent cannot execute and\ninefficient subtasks that fail to contribute to the completion of the\nhigher-level task. These challenges stem from VLM's lack of experience in\ndecomposing subtasks within GUI scenarios in multi-agent architecture. To\naddress these, we propose a new mobile assistant architecture with constrained\nhigh-frequency o}ptimized planning (CHOP). Our approach overcomes the VLM's\ndeficiency in GUI scenarios planning by using human-planned subtasks as the\nbasis vector. We evaluate our architecture in both English and Chinese contexts\nacross 20 Apps, demonstrating significant improvements in both effectiveness\nand efficiency. Our dataset and code is available at\nhttps://github.com/Yuqi-Zhou/CHOP\n","authors":["Yuqi Zhou","Shuai Wang","Sunhao Dai","Qinglin Jia","Zhaocheng Du","Zhenhua Dong","Jun Xu"],"pdf_url":"https://arxiv.org/pdf/2503.03743v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03733v1","updated":"2025-03-05T18:44:35Z","published":"2025-03-05T18:44:35Z","title":"Rethinking Deep Clustering Paradigms: Self-Supervision Is All You Need","summary":"  The recent advances in deep clustering have been made possible by significant\nprogress in self-supervised and pseudo-supervised learning. However, the\ntrade-off between self-supervision and pseudo-supervision can give rise to\nthree primary issues. The joint training causes Feature Randomness and Feature\nDrift, whereas the independent training causes Feature Randomness and Feature\nTwist. In essence, using pseudo-labels generates random and unreliable\nfeatures. The combination of pseudo-supervision and self-supervision drifts the\nreliable clustering-oriented features. Moreover, moving from self-supervision\nto pseudo-supervision can twist the curved latent manifolds. This paper\naddresses the limitations of existing deep clustering paradigms concerning\nFeature Randomness, Feature Drift, and Feature Twist. We propose a new paradigm\nwith a new strategy that replaces pseudo-supervision with a second round of\nself-supervision training. The new strategy makes the transition between\ninstance-level self-supervision and neighborhood-level self-supervision\nsmoother and less abrupt. Moreover, it prevents the drifting effect that is\ncaused by the strong competition between instance-level self-supervision and\nclustering-level pseudo-supervision. Moreover, the absence of the\npseudo-supervision prevents the risk of generating random features. With this\nnovel approach, our paper introduces a Rethinking of the Deep Clustering\nParadigms, denoted by R-DC. Our model is specifically designed to address three\nprimary challenges encountered in Deep Clustering: Feature Randomness, Feature\nDrift, and Feature Twist. Experimental results conducted on six datasets have\nshown that the two-level self-supervision training yields substantial\nimprovements.\n","authors":["Amal Shaheena","Nairouz Mrabahb","Riadh Ksantinia","Abdulla Alqaddoumia"],"pdf_url":"https://arxiv.org/pdf/2503.03733v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07674v2","updated":"2025-03-05T18:39:05Z","published":"2025-01-13T20:13:59Z","title":"CDS: Data Synthesis Method Guided by Cognitive Diagnosis Theory","summary":"  Large Language Models (LLMs) have achieved significant advancements, but the\nincreasing complexity of tasks and higher performance demands highlight the\nneed for continuous improvement. Some approaches utilize synthetic data\ngenerated by advanced LLMs based on evaluation results to train models.\nHowever, conventional evaluation methods fail to provide detailed, fine-grained\nprofiles of LLMs, limiting their guidance for data synthesis. In this paper, we\nintroduce the Cognitive Diagnostic Synthesis (CDS) method, which incorporates a\ndiagnostic process inspired by Cognitive Diagnosis Theory (CDT) to refine\nevaluation results and characterize model profiles at the knowledge component\nlevel. Based on these diagnostics, we propose two diagnosis-synthesis\nstrategies for weakness-targeted data synthesis. Additionally, we present an\nenhanced data augmentation and selection pipeline to improve the quality and\ndiversity of synthesized data. Our experiments with several open-source models\nshow significant improvements across multiple benchmarks, achieving up to 6.00%\nimprovement in code generation, 13.10% in mathematical reasoning, and 5.43% in\nacademic exams. Code and data are available on GitHub.\n","authors":["Haokun Zhao","Jinyi Han","Jiaqing Liang","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2501.07674v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07132v2","updated":"2025-03-05T18:33:41Z","published":"2025-02-10T23:50:09Z","title":"Interactive Data Harmonization with LLM Agents","summary":"  Data harmonization is an essential task that entails integrating datasets\nfrom diverse sources. Despite years of research in this area, it remains a\ntime-consuming and challenging task due to schema mismatches, varying\nterminologies, and differences in data collection methodologies. This paper\npresents the case for agentic data harmonization as a means to both empower\nexperts to harmonize their data and to streamline the process. We introduce\nHarmonia, a system that combines LLM-based reasoning, an interactive user\ninterface, and a library of data harmonization primitives to automate the\nsynthesis of data harmonization pipelines. We demonstrate Harmonia in a\nclinical data harmonization scenario, where it helps to interactively create\nreusable pipelines that map datasets to a standard format. Finally, we discuss\nchallenges and open problems, and suggest research directions for advancing our\nvision.\n","authors":["Aécio Santos","Eduardo H. M. Pena","Roque Lopez","Juliana Freire"],"pdf_url":"https://arxiv.org/pdf/2502.07132v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03724v1","updated":"2025-03-05T18:24:58Z","published":"2025-03-05T18:24:58Z","title":"Deep Causal Behavioral Policy Learning: Applications to Healthcare","summary":"  We present a deep learning-based approach to studying dynamic clinical\nbehavioral regimes in diverse non-randomized healthcare settings. Our proposed\nmethodology - deep causal behavioral policy learning (DC-BPL) - uses deep\nlearning algorithms to learn the distribution of high-dimensional clinical\naction paths, and identifies the causal link between these action paths and\npatient outcomes. Specifically, our approach: (1) identifies the causal effects\nof provider assignment on clinical outcomes; (2) learns the distribution of\nclinical actions a given provider would take given evolving patient\ninformation; (3) and combines these steps to identify the optimal provider for\na given patient type and emulate that provider's care decisions. Underlying\nthis strategy, we train a large clinical behavioral model (LCBM) on electronic\nhealth records data using a transformer architecture, and demonstrate its\nability to estimate clinical behavioral policies. We propose a novel\ninterpretation of a behavioral policy learned using the LCBM: that it is an\nefficient encoding of complex, often implicit, knowledge used to treat a\npatient. This allows us to learn a space of policies that are critical to a\nwide range of healthcare applications, in which the vast majority of clinical\nknowledge is acquired tacitly through years of practice and only a tiny\nfraction of information relevant to patient care is written down (e.g. in\ntextbooks, studies or standardized guidelines).\n","authors":["Jonas Knecht","Anna Zink","Jonathan Kolstad","Maya Petersen"],"pdf_url":"https://arxiv.org/pdf/2503.03724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.14395v2","updated":"2025-03-05T18:17:28Z","published":"2024-04-22T17:55:56Z","title":"PARAMANU-GANITA: Can Small Math Language Models Rival with Large\n  Language Models on Mathematical Reasoning?","summary":"  In this paper, we study whether domain specific pretraining of small\ngenerative language models (SLM) from scratch with domain specialized tokenizer\nand Chain-of-Thought (CoT) instruction fine-tuning results in competitive\nperformance on mathematical reasoning compared to LLMs? Secondly, whether this\napproach is environmentally sustainable, highly cost efficient? To address\nthese research questions, we present Paramanu-Ganita, a 208 million-parameter\nnovel decoder-only Auto Regressive SLM on mathematics. We performed pretraining\nfrom scratch on 31.5 billion tokens for 170 A100 hours using a context size of\n4096 on a mixed mathematical corpus consisting of web pages, source code,\ntextbooks, CoT templatised StackOverflow QA pairs, and mathematical lecture\nnotes in LaTeX curated by us. We also trained a math and code specialised BPE\ntokenizer. We proposed and performed CoT instruction fine-tuning of\nParamanu-Ganita on the MetaMathQA dataset. Our model Paramanu-Ganita, despite\nbeing 34 times smaller than the 7B LLMs, outperforms generalist LLMs by\napproximately 30% points, and even math-specialised LLMs by 3-23% points in\nGSM8K test accuracy metric. On MATH benchmark, Paramanu-Ganita outperformed the\nvarious models by 6-8% points. On benchmarks like LogiQA, MMLU (high school,\ncollege level), and competitive exams level, AGIEVAL (AQuA-RAT, SAT-Math),\nParamanu-Ganita outperformed others by 1-4%. Our model is available at\nhttps://huggingface.co/gyanai/paramanu-ganita-208M-hf .\n","authors":["Mitodru Niyogi","Arnab Bhattacharya"],"pdf_url":"https://arxiv.org/pdf/2404.14395v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03717v1","updated":"2025-03-05T18:10:11Z","published":"2025-03-05T18:10:11Z","title":"Machine Learning in Biomechanics: Key Applications and Limitations in\n  Walking, Running, and Sports Movements","summary":"  This chapter provides an overview of recent and promising Machine Learning\napplications, i.e. pose estimation, feature estimation, event detection, data\nexploration & clustering, and automated classification, in gait (walking and\nrunning) and sports biomechanics. It explores the potential of Machine Learning\nmethods to address challenges in biomechanical workflows, highlights central\nlimitations, i.e. data and annotation availability and explainability, that\nneed to be addressed, and emphasises the importance of interdisciplinary\napproaches for fully harnessing the potential of Machine Learning in gait and\nsports biomechanics.\n","authors":["Carlo Dindorf","Fabian Horst","Djordje Slijepčević","Bernhard Dumphart","Jonas Dully","Matthias Zeppelzauer","Brian Horsak","Michael Fröhlich"],"pdf_url":"https://arxiv.org/pdf/2503.03717v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.03888v3","updated":"2025-03-05T18:04:40Z","published":"2025-01-07T15:51:49Z","title":"Neural DNF-MT: A Neuro-symbolic Approach for Learning Interpretable and\n  Editable Policies","summary":"  Although deep reinforcement learning has been shown to be effective, the\nmodel's black-box nature presents barriers to direct policy interpretation. To\naddress this problem, we propose a neuro-symbolic approach called neural DNF-MT\nfor end-to-end policy learning. The differentiable nature of the neural DNF-MT\nmodel enables the use of deep actor-critic algorithms for training. At the same\ntime, its architecture is designed so that trained models can be directly\ntranslated into interpretable policies expressed as standard (bivalent or\nprobabilistic) logic programs. Moreover, additional layers can be included to\nextract abstract features from complex observations, acting as a form of\npredicate invention. The logic representations are highly interpretable, and we\nshow how the bivalent representations of deterministic policies can be edited\nand incorporated back into a neural model, facilitating manual intervention and\nadaptation of learned policies. We evaluate our approach on a range of tasks\nrequiring learning deterministic or stochastic behaviours from various forms of\nobservations. Our empirical results show that our neural DNF-MT model performs\nat the level of competing black-box methods whilst providing interpretable\npolicies.\n","authors":["Kexin Gu Baugh","Luke Dickens","Alessandra Russo"],"pdf_url":"https://arxiv.org/pdf/2501.03888v3.pdf","comment":"AAMAS 2025 (with Appendix)"},{"id":"http://arxiv.org/abs/2503.03708v1","updated":"2025-03-05T17:59:19Z","published":"2025-03-05T17:59:19Z","title":"Rethinking Video Tokenization: A Conditioned Diffusion-based Approach","summary":"  Video tokenizers, which transform videos into compact latent representations,\nare key to video generation. Existing video tokenizers are based on the VAE\narchitecture and follow a paradigm where an encoder compresses videos into\ncompact latents, and a deterministic decoder reconstructs the original videos\nfrom these latents. In this paper, we propose a novel\n\\underline{\\textbf{C}}onditioned \\underline{\\textbf{D}}iffusion-based video\n\\underline{\\textbf{T}}okenizer entitled \\textbf{\\ourmethod}, which departs from\nprevious methods by replacing the deterministic decoder with a 3D causal\ndiffusion model. The reverse diffusion generative process of the decoder is\nconditioned on the latent representations derived via the encoder. With a\nfeature caching and sampling acceleration, the framework efficiently\nreconstructs high-fidelity videos of arbitrary lengths. Results show that\n{\\ourmethod} achieves state-of-the-art performance in video reconstruction\ntasks using just a single-step sampling. Even a smaller version of {\\ourmethod}\nstill achieves reconstruction results on par with the top two baselines.\nFurthermore, the latent video generation model trained using {\\ourmethod} also\nshows superior performance.\n","authors":["Nianzu Yang","Pandeng Li","Liming Zhao","Yang Li","Chen-Wei Xie","Yehui Tang","Xudong Lu","Zhihang Liu","Yun Zheng","Yu Liu","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2503.03708v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03707v1","updated":"2025-03-05T17:58:16Z","published":"2025-03-05T17:58:16Z","title":"Curating Demonstrations using Online Experience","summary":"  Many robot demonstration datasets contain heterogeneous demonstrations of\nvarying quality. This heterogeneity may benefit policy pre-training, but can\nhinder robot performance when used with a final imitation learning objective.\nIn particular, some strategies in the data may be less reliable than others or\nmay be underrepresented in the data, leading to poor performance when such\nstrategies are sampled at test time. Moreover, such unreliable or\nunderrepresented strategies can be difficult even for people to discern, and\nsifting through demonstration datasets is time-consuming and costly. On the\nother hand, policy performance when trained on such demonstrations can reflect\nthe reliability of different strategies. We thus propose for robots to\nself-curate based on online robot experience (Demo-SCORE). More specifically,\nwe train and cross-validate a classifier to discern successful policy roll-outs\nfrom unsuccessful ones and use the classifier to filter heterogeneous\ndemonstration datasets. Our experiments in simulation and the real world show\nthat Demo-SCORE can effectively identify suboptimal demonstrations without\nmanual curation. Notably, Demo-SCORE achieves over 15-35% higher absolute\nsuccess rate in the resulting policy compared to the base policy trained with\nall original demonstrations.\n","authors":["Annie S. Chen","Alec M. Lessing","Yuejiang Liu","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2503.03707v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01776v2","updated":"2025-03-05T17:51:09Z","published":"2025-03-03T17:59:48Z","title":"Beyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation","summary":"  Many large-scale systems rely on high-quality deep representations\n(embeddings) to facilitate tasks like retrieval, search, and generative\nmodeling. Matryoshka Representation Learning (MRL) recently emerged as a\nsolution for adaptive embedding lengths, but it requires full model retraining\nand suffers from noticeable performance degradations at short lengths. In this\npaper, we show that sparse coding offers a compelling alternative for achieving\nadaptive representation with minimal overhead and higher fidelity. We propose\nContrastive Sparse Representation (CSR), a method that sparsifies pre-trained\nembeddings into a high-dimensional but selectively activated feature space. By\nleveraging lightweight autoencoding and task-aware contrastive objectives, CSR\npreserves semantic quality while allowing flexible, cost-effective inference at\ndifferent sparsity levels. Extensive experiments on image, text, and multimodal\nbenchmarks demonstrate that CSR consistently outperforms MRL in terms of both\naccuracy and retrieval speed-often by large margins-while also cutting training\ntime to a fraction of that required by MRL. Our results establish sparse coding\nas a powerful paradigm for adaptive representation learning in real-world\napplications where efficiency and fidelity are both paramount. Code is\navailable at https://github.com/neilwen987/CSR_Adaptive_Rep\n","authors":["Tiansheng Wen","Yifei Wang","Zequn Zeng","Zhong Peng","Yudi Su","Xinyang Liu","Bo Chen","Hongwei Liu","Stefanie Jegelka","Chenyu You"],"pdf_url":"https://arxiv.org/pdf/2503.01776v2.pdf","comment":"A novel sparse coding framework designed for learning adaptive\n  representation"},{"id":"http://arxiv.org/abs/2410.08143v2","updated":"2025-03-05T17:50:44Z","published":"2024-10-10T17:30:09Z","title":"DelTA: An Online Document-Level Translation Agent Based on Multi-Level\n  Memory","summary":"  Large language models (LLMs) have achieved reasonable quality improvements in\nmachine translation (MT). However, most current research on MT-LLMs still faces\nsignificant challenges in maintaining translation consistency and accuracy when\nprocessing entire documents. In this paper, we introduce DelTA, a\nDocument-levEL Translation Agent designed to overcome these limitations. DelTA\nfeatures a multi-level memory structure that stores information across various\ngranularities and spans, including Proper Noun Records, Bilingual Summary,\nLong-Term Memory, and Short-Term Memory, which are continuously retrieved and\nupdated by auxiliary LLM-based components. Experimental results indicate that\nDelTA significantly outperforms strong baselines in terms of translation\nconsistency and quality across four open/closed-source LLMs and two\nrepresentative document translation datasets, achieving an increase in\nconsistency scores by up to 4.58 percentage points and in COMET scores by up to\n3.16 points on average. DelTA employs a sentence-by-sentence translation\nstrategy, ensuring no sentence omissions and offering a memory-efficient\nsolution compared to the mainstream method. Furthermore, DelTA improves pronoun\nand context-dependent translation accuracy, and the summary component of the\nagent also shows promise as a tool for query-based summarization tasks. The\ncode and data of our approach are released at\nhttps://github.com/YutongWang1216/DocMTAgent.\n","authors":["Yutong Wang","Jiali Zeng","Xuebo Liu","Derek F. Wong","Fandong Meng","Jie Zhou","Min Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.08143v2.pdf","comment":"Accepted as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2503.03693v1","updated":"2025-03-05T17:43:49Z","published":"2025-03-05T17:43:49Z","title":"ILLC: Iterative Layer-by-Layer Compression for Enhancing Structural\n  Faithfulness in SpArX","summary":"  In the field of Explainable Artificial Intelligence (XAI), argumentative XAI\napproaches have been proposed to represent the internal reasoning process of\ndeep neural networks in a more transparent way by interpreting hidden nodes as\narguements. However, as the number of layers increases, existing compression\nmethods simplify all layers at once, which lead to high accumulative\ninformation loss. To compensate for this, we propose an iterative\nlayer-by-layer compression technique in which each layer is compressed\nseparately and the reduction error in the next layer is immediately compensated\nfor, thereby improving the overall input-output and structural fidelity of the\nmodel. Experiments on the Breast Cancer Diagnosis dataset show that, compared\nto traditional compression, the method reduces input-output and structural\nunfaithfulness, and maintains a more consistent attack-support relationship in\nthe Argumentative Explanation scheme. This is significant because it provides a\nnew way to make complex MLP models more compact while still conveying their\ninternal inference logic without distortion.\n","authors":["Ungsik Kim"],"pdf_url":"https://arxiv.org/pdf/2503.03693v1.pdf","comment":"8 pages, 2 figures"},{"id":"http://arxiv.org/abs/2411.08932v2","updated":"2025-03-05T17:11:13Z","published":"2024-11-13T03:16:18Z","title":"PyGen: A Collaborative Human-AI Approach to Python Package Creation","summary":"  The principles of automation and innovation serve as foundational elements\nfor advancement in contemporary science and technology. Here, we introduce\nPygen, an automation platform designed to empower researchers, technologists,\nand hobbyists to bring abstract ideas to life as core, usable software tools\nwritten in Python. Pygen leverages the immense power of autoregressive large\nlanguage models to augment human creativity during the ideation, iteration, and\ninnovation process. By combining state-of-the-art language models with\nopen-source code generation technologies, Pygen has significantly reduced the\nmanual overhead of tool development. From a user prompt, Pygen automatically\ngenerates Python packages for a complete workflow from concept to package\ngeneration and documentation. The findings of our work show that Pygen\nconsiderably enhances the researcher's productivity by enabling the creation of\nresilient, modular, and well-documented packages for various specialized\npurposes. We employ a prompt enhancement approach to distill the user's package\ndescription into increasingly specific and actionable. While being inherently\nan open-ended task, we have evaluated the generated packages and the\ndocumentation using Human Evaluation, LLM-based evaluation, and CodeBLEU, with\ndetailed results in the results section. Furthermore, we documented our\nresults, analyzed the limitations, and suggested strategies to alleviate them.\nPygen is our vision of ethical automation, a framework that promotes\ninclusivity, accessibility, and collaborative development. This project marks\nthe beginning of a large-scale effort towards creating tools where intelligent\nagents collaborate with humans to improve scientific and technological\ndevelopment substantially.\n  Our code and generated examples are open-sourced at\n[https://github.com/GitsSaikat/Pygen]\n","authors":["Saikat Barua","Mostafizur Rahman","Md Jafor Sadek","Rafiul Islam","Shehnaz Khaled","Md. Shohrab Hossain"],"pdf_url":"https://arxiv.org/pdf/2411.08932v2.pdf","comment":"33 pages, 13 figures"},{"id":"http://arxiv.org/abs/2410.17579v3","updated":"2025-03-05T17:09:46Z","published":"2024-10-23T06:08:45Z","title":"Bonsai: Gradient-free Graph Distillation for Node Classification","summary":"  Graph distillation has emerged as a promising avenue to enable scalable\ntraining of GNNs by compressing the training dataset while preserving essential\ngraph characteristics. Our study uncovers significant shortcomings in current\ngraph distillation techniques. First, the majority of the algorithms\nparadoxically require training on the full dataset to perform distillation.\nSecond, due to their gradient-emulating approach, these methods require fresh\ndistillation for any change in hyperparameters or GNN architecture, limiting\ntheir flexibility and reusability. Finally, they fail to achieve substantial\nsize reduction due to synthesizing fully-connected, edge-weighted graphs. To\naddress these challenges, we present Bonsai, a novel graph distillation method\nempowered by the observation that \\textit{computation trees} form the\nfundamental processing units of message-passing GNNs. Bonsai distills datasets\nby encoding a careful selection of \\textit{exemplar} trees that maximize the\nrepresentation of all computation trees in the training set. This unique\napproach imparts Bonsai as the first linear-time, model-agnostic graph\ndistillation algorithm for node classification that outperforms existing\nbaselines across $6$ real-world datasets on accuracy, while being $22$ times\nfaster on average. Bonsai is grounded in rigorous mathematical guarantees on\nthe adopted approximation strategies making it robust to GNN architectures,\ndatasets, and parameters.\n","authors":["Mridul Gupta","Samyak Jain","Vansh Ramani","Hariprasad Kodamana","Sayan Ranu"],"pdf_url":"https://arxiv.org/pdf/2410.17579v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03669v1","updated":"2025-03-05T17:03:48Z","published":"2025-03-05T17:03:48Z","title":"Attentive Reasoning Queries: A Systematic Method for Optimizing\n  Instruction-Following in Large Language Models","summary":"  We present Attentive Reasoning Queries (ARQs), a novel structured reasoning\napproach that significantly improves instruction-following in Large Language\nModels through domain-specialized reasoning blueprints. While LLMs demonstrate\nremarkable capabilities across diverse tasks, they often fail to maintain\nadherence to complex, use-case-specific instructions during multi-turn\nconversations, presenting challenges for business-critical applications. ARQs\naddress this limitation by guiding LLMs through systematic reasoning steps with\ntargeted queries that reinstate critical instructions and facilitate\nintermediate reasoning throughout the completion process. In extensive testing\nwithin Parlant, our framework for reliable customer-facing agents in which ARQs\nwere born out of necessity, they achieved a 90.2% success rate across 87 test\nscenarios, outperforming both Chain-of-Thought reasoning (86.1%) and direct\nresponse generation (81.5%). ARQs showed particular strength in addressing\npersistent failure modes like guideline re-application and hallucination\nprevention. Our analysis also revealed that ARQs can potentially be more\ncomputationally efficient than free-form reasoning when carefully designed.\nThese findings demonstrate that structured reasoning approaches provide\neffective mechanisms for controlling how LLMs process information and make\ndecisions in complex scenarios.\n","authors":["Bar Karov","Dor Zohar","Yam Marcovitz"],"pdf_url":"https://arxiv.org/pdf/2503.03669v1.pdf","comment":"Supplementary materials, including code, is available on our GitHub:\n  https://github.com/emcie-co/parlant/tree/arqs-a-systematic-method-for-optimizing-instruction-following-in-llms"},{"id":"http://arxiv.org/abs/2503.03664v1","updated":"2025-03-05T16:54:15Z","published":"2025-03-05T16:54:15Z","title":"A Generative Approach to High Fidelity 3D Reconstruction from Text Data","summary":"  The convergence of generative artificial intelligence and advanced computer\nvision technologies introduces a groundbreaking approach to transforming\ntextual descriptions into three-dimensional representations. This research\nproposes a fully automated pipeline that seamlessly integrates text-to-image\ngeneration, various image processing techniques, and deep learning methods for\nreflection removal and 3D reconstruction. By leveraging state-of-the-art\ngenerative models like Stable Diffusion, the methodology translates natural\nlanguage inputs into detailed 3D models through a multi-stage workflow.\n  The reconstruction process begins with the generation of high-quality images\nfrom textual prompts, followed by enhancement by a reinforcement learning agent\nand reflection removal using the Stable Delight model. Advanced image upscaling\nand background removal techniques are then applied to further enhance visual\nfidelity. These refined two-dimensional representations are subsequently\ntransformed into volumetric 3D models using sophisticated machine learning\nalgorithms, capturing intricate spatial relationships and geometric\ncharacteristics. This process achieves a highly structured and detailed output,\nensuring that the final 3D models reflect both semantic accuracy and geometric\nprecision.\n  This approach addresses key challenges in generative reconstruction, such as\nmaintaining semantic coherence, managing geometric complexity, and preserving\ndetailed visual information. Comprehensive experimental evaluations will assess\nreconstruction quality, semantic accuracy, and geometric fidelity across\ndiverse domains and varying levels of complexity. By demonstrating the\npotential of AI-driven 3D reconstruction techniques, this research offers\nsignificant implications for fields such as augmented reality (AR), virtual\nreality (VR), and digital content creation.\n","authors":["Venkat Kumar R","Deepak Saravanan"],"pdf_url":"https://arxiv.org/pdf/2503.03664v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.10102v2","updated":"2025-03-05T16:51:06Z","published":"2024-02-15T16:56:25Z","title":"A privacy-preserving, distributed and cooperative FCM-based learning\n  approach for cancer research","summary":"  Distributed Artificial Intelligence is attracting interest day by day. In\nthis paper, the authors introduce an innovative methodology for distributed\nlearning of Particle Swarm Optimization-based Fuzzy Cognitive Maps in a\nprivacy-preserving way. The authors design a training scheme for collaborative\nFCM learning that offers data privacy compliant with the current regulation.\nThis method is applied to a cancer detection problem, proving that the\nperformance of the model is improved by the Federated Learning process, and\nobtaining similar results to the ones that can be found in the literature.\n","authors":["Jose L. Salmeron","Irina Arévalo"],"pdf_url":"https://arxiv.org/pdf/2402.10102v2.pdf","comment":"Rough Sets: International Joint Conference, IJCRS 2020"},{"id":"http://arxiv.org/abs/2410.16024v2","updated":"2025-03-05T16:49:51Z","published":"2024-10-21T13:58:38Z","title":"SMAC-R1: The Emergence of Intelligence in Decision-Making Tasks","summary":"  StarCraft Multi-Agent Challenge (SMAC) has been one of the most commonly used\nexperimental environments in multi-agent reinforcement learning (MARL), where\nthe specific task is to control a set number of allied units to defeat enemy\nforces. Traditional MARL algorithms often require interacting with the\nenvironment for millions of steps to train a parametric model, of which the\nresulting policies are typically non-interpretable with weak transferability.\nIn this paper, we introduce SMAC-R1 which is based on the Qwen2.5-7B-Base LLM\ndistilled from DeepSeek-Coder-v2.5-236B. Similar to online reinforcement\nlearning after behavior cloning in offline learning process, in our pipeline,\nagents leverage the DeepSeek LLM to generate decision tree code by providing\ntask descriptions, and the agents are further self-reflected using feedback\nfrom the rewards provided by the environment. Based on that, we augment the\ngenerated scripts to fine-tune a small LLM, Qwen2.5-7B-Base, to distill the\ndecision-making ability via Supervised Fine-Tuning (SFT) and enhance the script\ngeneration ability by the Group Relative Policy Optimization (GRPO) algorithm.\nWe conduct experiments in the original 23 SMAC tasks and 10 newly-designed\ntasks to demonstrate that our method can produce high-quality, interpretable\ndecision trees with minimal environmental exploration. Moreover, these scripts\nexhibit strong transferability, successfully applying to homogeneous SMAC\nenvironments without modification. We believe this approach offers a new\ndirection for solving decision-making tasks and domain-specific LLM training\npipelines in the future.\n","authors":["Yue Deng","Weiyu Ma","Yuxin Fan","Ruyi Song","Yin Zhang","Haifeng Zhang","Jian Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.16024v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.07402v2","updated":"2025-03-05T16:48:23Z","published":"2024-09-11T16:42:22Z","title":"What to align in multimodal contrastive learning?","summary":"  Humans perceive the world through multisensory integration, blending the\ninformation of different modalities to adapt their behavior. Contrastive\nlearning offers an appealing solution for multimodal self-supervised learning.\nIndeed, by considering each modality as a different view of the same entity, it\nlearns to align features of different modalities in a shared representation\nspace. However, this approach is intrinsically limited as it only learns shared\nor redundant information between modalities, while multimodal interactions can\narise in other ways. In this work, we introduce CoMM, a Contrastive MultiModal\nlearning strategy that enables the communication between modalities in a single\nmultimodal space. Instead of imposing cross- or intra- modality constraints, we\npropose to align multimodal representations by maximizing the mutual\ninformation between augmented versions of these multimodal features. Our\ntheoretical analysis shows that shared, synergistic and unique terms of\ninformation naturally emerge from this formulation, allowing us to estimate\nmultimodal interactions beyond redundancy. We test CoMM both in a controlled\nand in a series of real-world settings: in the former, we demonstrate that CoMM\neffectively captures redundant, unique and synergistic information between\nmodalities. In the latter, CoMM learns complex multimodal interactions and\nachieves state-of-the-art results on the seven multimodal benchmarks. Code is\navailable at https://github.com/Duplums/CoMM\n","authors":["Benoit Dufumier","Javiera Castillo-Navarro","Devis Tuia","Jean-Philippe Thiran"],"pdf_url":"https://arxiv.org/pdf/2409.07402v2.pdf","comment":"ICLR 2025, 25 pages"},{"id":"http://arxiv.org/abs/2411.00816v2","updated":"2025-03-05T16:36:05Z","published":"2024-10-28T08:10:21Z","title":"CycleResearcher: Improving Automated Research via Automated Review","summary":"  The automation of scientific discovery has been a long-standing goal within\nthe research community, driven by the potential to accelerate knowledge\ncreation. While significant progress has been made using commercial large\nlanguage models (LLMs) as research assistants or idea generators, the\npossibility of automating the entire research process with open-source LLMs\nremains largely unexplored. This paper explores the feasibility of using\nopen-source post-trained LLMs as autonomous agents capable of performing the\nfull cycle of automated research and review, from literature review and\nmanuscript preparation to peer review and paper refinement. Our iterative\npreference training framework consists of CycleResearcher, which conducts\nresearch tasks, and CycleReviewer, which simulates the peer review process,\nproviding iterative feedback via reinforcement learning. To train these models,\nwe develop two new datasets, Review-5k and Research-14k, reflecting real-world\nmachine learning research and peer review dynamics. Our results demonstrate\nthat CycleReviewer achieves promising performance with a 26.89\\% reduction in\nmean absolute error (MAE) compared to individual human reviewers in predicting\npaper scores, indicating the potential of LLMs to effectively assist\nexpert-level research evaluation. In research, the papers generated by the\nCycleResearcher model achieved a score of 5.36 in simulated peer reviews,\nshowing some competitiveness in terms of simulated review scores compared to\nthe preprint level of 5.24 from human experts, while still having room for\nimprovement compared to the accepted paper level of 5.69. This work represents\na significant step toward fully automated scientific inquiry, providing ethical\nsafeguards and exploring AI-driven research capabilities. The code, dataset and\nmodel weight are released at https://wengsyx.github.io/Researcher/\n","authors":["Yixuan Weng","Minjun Zhu","Guangsheng Bao","Hongbo Zhang","Jindong Wang","Yue Zhang","Linyi Yang"],"pdf_url":"https://arxiv.org/pdf/2411.00816v2.pdf","comment":"Accept in ICLR 2025"},{"id":"http://arxiv.org/abs/2503.03655v1","updated":"2025-03-05T16:35:15Z","published":"2025-03-05T16:35:15Z","title":"Improving 6D Object Pose Estimation of metallic Household and Industry\n  Objects","summary":"  6D object pose estimation suffers from reduced accuracy when applied to\nmetallic objects. We set out to improve the state-of-the-art by addressing\nchallenges such as reflections and specular highlights in industrial\napplications. Our novel BOP-compatible dataset, featuring a diverse set of\nmetallic objects (cans, household, and industrial items) under various lighting\nand background conditions, provides additional geometric and visual cues. We\ndemonstrate that these cues can be effectively leveraged to enhance overall\nperformance. To illustrate the usefulness of the additional features, we\nimprove upon the GDRNPP algorithm by introducing an additional keypoint\nprediction and material estimator head in order to improve spatial scene\nunderstanding. Evaluations on the new dataset show improved accuracy for\nmetallic objects, supporting the hypothesis that additional geometric and\nvisual cues can improve learning.\n","authors":["Thomas Pöllabauer","Michael Gasser","Tristan Wirth","Sarah Berkei","Volker Knauthe","Arjan Kuijper"],"pdf_url":"https://arxiv.org/pdf/2503.03655v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03654v1","updated":"2025-03-05T16:32:47Z","published":"2025-03-05T16:32:47Z","title":"Improving Neutral Point of View Text Generation through\n  Parameter-Efficient Reinforcement Learning and a Small-Scale High-Quality\n  Dataset","summary":"  This paper describes the construction of a dataset and the evaluation of\ntraining methods to improve generative large language models' (LLMs) ability to\nanswer queries on sensitive topics with a Neutral Point of View (NPOV), i.e.,\nto provide significantly more informative, diverse and impartial answers. The\ndataset, the SHQ-NPOV dataset, comprises 300 high-quality, human-written\nquadruplets: a query on a sensitive topic, an answer, an NPOV rating, and a set\nof links to source texts elaborating the various points of view. The first key\ncontribution of this paper is a new methodology to create such datasets through\niterative rounds of human peer-critique and annotator training, which we\nrelease alongside the dataset. The second key contribution is the\nidentification of a highly effective training regime for parameter-efficient\nreinforcement learning (PE-RL) to improve NPOV generation. We compare and\nextensively evaluate PE-RL and multiple baselines-including LoRA finetuning (a\nstrong baseline), SFT and RLHF.\n  PE-RL not only improves on overall NPOV quality compared to the strongest\nbaseline ($97.06\\%\\rightarrow 99.08\\%$), but also scores much higher on\nfeatures linguists identify as key to separating good answers from the best\nanswers ($60.25\\%\\rightarrow 85.21\\%$ for presence of supportive details,\n$68.74\\%\\rightarrow 91.43\\%$ for absence of oversimplification). A qualitative\nanalysis corroborates this. Finally, our evaluation finds no statistical\ndifferences between results on topics that appear in the training dataset and\nthose on separated evaluation topics, which provides strong evidence that our\napproach to training PE-RL exhibits very effective out of topic generalization.\n","authors":["Jessica Hoffmann","Christiane Ahlheim","Zac Yu","Aria Walfrand","Jarvis Jin","Marie Tano","Ahmad Beirami","Erin van Liemt","Nithum Thain","Hakim Sidahmed","Lucas Dixon"],"pdf_url":"https://arxiv.org/pdf/2503.03654v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20900v2","updated":"2025-03-05T16:23:09Z","published":"2025-02-28T09:57:20Z","title":"DexGraspVLA: A Vision-Language-Action Framework Towards General\n  Dexterous Grasping","summary":"  Dexterous grasping remains a fundamental yet challenging problem in robotics.\nA general-purpose robot must be capable of grasping diverse objects in\narbitrary scenarios. However, existing research typically relies on specific\nassumptions, such as single-object settings or limited environments, leading to\nconstrained generalization. Our solution is DexGraspVLA, a hierarchical\nframework that utilizes a pre-trained Vision-Language model as the high-level\ntask planner and learns a diffusion-based policy as the low-level Action\ncontroller. The key insight lies in iteratively transforming diverse language\nand visual inputs into domain-invariant representations, where imitation\nlearning can be effectively applied due to the alleviation of domain shift.\nThus, it enables robust generalization across a wide range of real-world\nscenarios. Notably, our method achieves a 90+% success rate under thousands of\nunseen object, lighting, and background combinations in a ``zero-shot''\nenvironment. Empirical analysis further confirms the consistency of internal\nmodel behavior across environmental variations, thereby validating our design\nand explaining its generalization performance. We hope our work can be a step\nforward in achieving general dexterous grasping. Our demo and code can be found\nat https://dexgraspvla.github.io/.\n","authors":["Yifan Zhong","Xuchuan Huang","Ruochong Li","Ceyao Zhang","Yitao Liang","Yaodong Yang","Yuanpei Chen"],"pdf_url":"https://arxiv.org/pdf/2502.20900v2.pdf","comment":"21 pages, 10 figures"},{"id":"http://arxiv.org/abs/2501.17755v2","updated":"2025-03-05T16:20:03Z","published":"2025-01-29T16:48:13Z","title":"AI Governance through Markets","summary":"  This paper argues that market governance mechanisms should be considered a\nkey approach in the governance of artificial intelligence (AI), alongside\ntraditional regulatory frameworks. While current governance approaches have\npredominantly focused on regulation, we contend that market-based mechanisms\noffer effective incentives for responsible AI development. We examine four\nemerging vectors of market governance: insurance, auditing, procurement, and\ndue diligence, demonstrating how these mechanisms can affirm the relationship\nbetween AI risk and financial risk while addressing capital allocation\ninefficiencies. While we do not claim that market forces alone can adequately\nprotect societal interests, we maintain that standardised AI disclosures and\nmarket mechanisms can create powerful incentives for safe and responsible AI\ndevelopment. This paper urges regulators, economists, and machine learning\nresearchers to investigate and implement market-based approaches to AI\ngovernance.\n","authors":["Philip Moreira Tomei","Rupal Jain","Matija Franklin"],"pdf_url":"https://arxiv.org/pdf/2501.17755v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02102v2","updated":"2025-03-05T16:18:33Z","published":"2025-03-03T22:37:03Z","title":"Provable Benefits of Task-Specific Prompts for In-context Learning","summary":"  The in-context learning capabilities of modern language models have motivated\na deeper mathematical understanding of sequence models. A line of recent work\nhas shown that linear attention models can emulate projected gradient descent\niterations to implicitly learn the task vector from the data provided in the\ncontext window. In this work, we consider a novel setting where the global task\ndistribution can be partitioned into a union of conditional task distributions.\nWe then examine the use of task-specific prompts and prediction heads for\nlearning the prior information associated with the conditional task\ndistribution using a one-layer attention model. Our results on loss landscape\nshow that task-specific prompts facilitate a covariance-mean decoupling where\nprompt-tuning explains the conditional mean of the distribution whereas the\nvariance is learned/explained through in-context learning. Incorporating\ntask-specific head further aids this process by entirely decoupling estimation\nof mean and variance components. This covariance-mean perspective similarly\nexplains how jointly training prompt and attention weights can provably help\nover fine-tuning after pretraining.\n","authors":["Xiangyu Chang","Yingcong Li","Muti Kara","Samet Oymak","Amit K. Roy-Chowdhury"],"pdf_url":"https://arxiv.org/pdf/2503.02102v2.pdf","comment":"Proceedings of the 28th International Conference on Artificial\n  Intelligence and Statistics (AISTATS) 2025"},{"id":"http://arxiv.org/abs/2410.12893v2","updated":"2025-03-05T16:16:01Z","published":"2024-10-16T12:24:42Z","title":"MIRROR: A Novel Approach for the Automated Evaluation of Open-Ended\n  Question Generation","summary":"  Automatic question generation is a critical task that involves evaluating\nquestion quality by considering factors such as engagement, pedagogical value,\nand the ability to stimulate critical thinking. These aspects require\nhuman-like understanding and judgment, which automated systems currently lack.\nHowever, human evaluations are costly and impractical for large-scale samples\nof generated questions. Therefore, we propose a novel system, MIRROR (Multi-LLM\nIterative Review and Response for Optimized Rating), which leverages large\nlanguage models (LLMs) to automate the evaluation process for questions\ngenerated by automated question generation systems. We experimented with\nseveral state-of-the-art LLMs, such as GPT-4, Gemini, and Llama2-70b. We\nobserved that the scores of human evaluation metrics, namely relevance,\nappropriateness, novelty, complexity, and grammaticality, improved when using\nthe feedback-based approach called MIRROR, tending to be closer to the human\nbaseline scores. Furthermore, we observed that Pearson's correlation\ncoefficient between GPT-4 and human experts improved when using our proposed\nfeedback-based approach, MIRROR, compared to direct prompting for evaluation.\nError analysis shows that our proposed approach, MIRROR, significantly helps to\nimprove relevance and appropriateness.\n","authors":["Aniket Deroy","Subhankar Maity","Sudeshna Sarkar"],"pdf_url":"https://arxiv.org/pdf/2410.12893v2.pdf","comment":"NeurIPS'24 Workshop on Large Foundation Models for Educational\n  Assessment (FM-EduAssess)"},{"id":"http://arxiv.org/abs/2409.06615v5","updated":"2025-03-05T16:07:20Z","published":"2024-09-10T16:11:57Z","title":"One-Shot Imitation under Mismatched Execution","summary":"  Human demonstrations as prompts are a powerful way to program robots to do\nlong-horizon manipulation tasks. However, translating these demonstrations into\nrobot-executable actions presents significant challenges due to execution\nmismatches in movement styles and physical capabilities. Existing methods\neither depend on human-robot paired data, which is infeasible to scale, or rely\nheavily on frame-level visual similarities that often break down in practice.\nTo address these challenges, we propose RHyME, a novel framework that\nautomatically aligns human and robot task executions using optimal transport\ncosts. Given long-horizon robot demonstrations, RHyME synthesizes semantically\nequivalent human videos by retrieving and composing short-horizon human clips.\nThis approach facilitates effective policy training without the need for paired\ndata. RHyME successfully imitates a range of cross-embodiment demonstrators,\nboth in simulation and with a real human hand, achieving over 50\\% increase in\ntask success compared to previous methods. We release our code and datasets at\nhttps://portal-cornell.github.io/rhyme/.\n","authors":["Kushal Kedia","Prithwish Dan","Angela Chao","Maximus Adrian Pace","Sanjiban Choudhury"],"pdf_url":"https://arxiv.org/pdf/2409.06615v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.21028v2","updated":"2025-03-05T15:52:43Z","published":"2025-02-28T13:16:34Z","title":"Measuring and identifying factors of individuals' trust in Large\n  Language Models","summary":"  Large Language Models (LLMs) can engage in human-looking conversational\nexchanges. Although conversations can elicit trust between users and LLMs,\nscarce empirical research has examined trust formation in human-LLM contexts,\nbeyond LLMs' trustworthiness or human trust in AI in general. Here, we\nintroduce the Trust-In-LLMs Index (TILLMI) as a new framework to measure\nindividuals' trust in LLMs, extending McAllister's cognitive and affective\ntrust dimensions to LLM-human interactions. We developed TILLMI as a\npsychometric scale, prototyped with a novel protocol we called LLM-simulated\nvalidity. The LLM-based scale was then validated in a sample of 1,000 US\nrespondents. Exploratory Factor Analysis identified a two-factor structure. Two\nitems were then removed due to redundancy, yielding a final 6-item scale with a\n2-factor structure. Confirmatory Factor Analysis on a separate subsample showed\nstrong model fit ($CFI = .995$, $TLI = .991$, $RMSEA = .046$, $p_{X^2} > .05$).\nConvergent validity analysis revealed that trust in LLMs correlated positively\nwith openness to experience, extraversion, and cognitive flexibility, but\nnegatively with neuroticism. Based on these findings, we interpreted TILLMI's\nfactors as \"closeness with LLMs\" (affective dimension) and \"reliance on LLMs\"\n(cognitive dimension). Younger males exhibited higher closeness with- and\nreliance on LLMs compared to older women. Individuals with no direct experience\nwith LLMs exhibited lower levels of trust compared to LLMs' users. These\nfindings offer a novel empirical foundation for measuring trust in AI-driven\nverbal communication, informing responsible design, and fostering balanced\nhuman-AI collaboration.\n","authors":["Edoardo Sebastiano De Duro","Giuseppe Alessandro Veltri","Hudson Golino","Massimo Stella"],"pdf_url":"https://arxiv.org/pdf/2502.21028v2.pdf","comment":"23 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.03606v1","updated":"2025-03-05T15:42:37Z","published":"2025-03-05T15:42:37Z","title":"Decoupled Recommender Systems: Exploring Alternative Recommender\n  Ecosystem Designs","summary":"  Recommender ecosystems are an emerging subject of research. Such research\nexamines how the characteristics of algorithms, recommendation consumers, and\nitem providers influence system dynamics and long-term outcomes. One\narchitectural possibility that has not yet been widely explored in this line of\nresearch is the consequences of a configuration in which recommendation\nalgorithms are decoupled from the platforms they serve. This is sometimes\ncalled \"the friendly neighborhood algorithm store\" or \"middleware\" model. We\nare particularly interested in how such architectures might offer a range of\ndifferent distributions of utility across consumers, providers, and\nrecommendation platforms. In this paper, we create a model of a recommendation\necosystem that incorporates algorithm choice and examine the outcomes of such a\ndesign.\n","authors":["Anas Buhayh","Elizabeth McKinnie","Robin Burke"],"pdf_url":"https://arxiv.org/pdf/2503.03606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03595v1","updated":"2025-03-05T15:28:50Z","published":"2025-03-05T15:28:50Z","title":"Towards Understanding Text Hallucination of Diffusion Models via Local\n  Generation Bias","summary":"  Score-based diffusion models have achieved incredible performance in\ngenerating realistic images, audio, and video data. While these models produce\nhigh-quality samples with impressive details, they often introduce unrealistic\nartifacts, such as distorted fingers or hallucinated texts with no meaning.\nThis paper focuses on textual hallucinations, where diffusion models correctly\ngenerate individual symbols but assemble them in a nonsensical manner. Through\nexperimental probing, we consistently observe that such phenomenon is\nattributed it to the network's local generation bias. Denoising networks tend\nto produce outputs that rely heavily on highly correlated local regions,\nparticularly when different dimensions of the data distribution are nearly\npairwise independent. This behavior leads to a generation process that\ndecomposes the global distribution into separate, independent distributions for\neach symbol, ultimately failing to capture the global structure, including\nunderlying grammar. Intriguingly, this bias persists across various denoising\nnetwork architectures including MLP and transformers which have the structure\nto model global dependency. These findings also provide insights into\nunderstanding other types of hallucinations, extending beyond text, as a result\nof implicit biases in the denoising models. Additionally, we theoretically\nanalyze the training dynamics for a specific case involving a two-layer MLP\nlearning parity points on a hypercube, offering an explanation of its\nunderlying mechanism.\n","authors":["Rui Lu","Runzhe Wang","Kaifeng Lyu","Xitai Jiang","Gao Huang","Mengdi Wang"],"pdf_url":"https://arxiv.org/pdf/2503.03595v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03594v1","updated":"2025-03-05T15:27:36Z","published":"2025-03-05T15:27:36Z","title":"Small but Mighty: Enhancing Time Series Forecasting with Lightweight\n  LLMs","summary":"  While LLMs have demonstrated remarkable potential in time series forecasting,\ntheir practical deployment remains constrained by excessive computational\ndemands and memory footprints. Existing LLM-based approaches typically suffer\nfrom three critical limitations: Inefficient parameter utilization in handling\nnumerical time series patterns; Modality misalignment between continuous\ntemporal signals and discrete text embeddings; and Inflexibility for real-time\nexpert knowledge integration. We present SMETimes, the first systematic\ninvestigation of sub-3B parameter SLMs for efficient and accurate time series\nforecasting. Our approach centers on three key innovations: A\nstatistically-enhanced prompting mechanism that bridges numerical time series\nwith textual semantics through descriptive statistical features; A adaptive\nfusion embedding architecture that aligns temporal patterns with language model\ntoken spaces through learnable parameters; And a dynamic mixture-of-experts\nframework enabled by SLMs' computational efficiency, adaptively combining base\npredictions with domain-specific models. Extensive evaluations across seven\nbenchmark datasets demonstrate that our 3B-parameter SLM achieves\nstate-of-the-art performance on five primary datasets while maintaining 3.8x\nfaster training and 5.2x lower memory consumption compared to 7B-parameter LLM\nbaselines. Notably, the proposed model exhibits better learning capabilities,\nachieving 12.3% lower MSE than conventional LLM. Ablation studies validate that\nour statistical prompting and cross-modal fusion modules respectively\ncontribute 15.7% and 18.2% error reduction in long-horizon forecasting tasks.\nBy redefining the efficiency-accuracy trade-off landscape, this work\nestablishes SLMs as viable alternatives to resource-intensive LLMs for\npractical time series forecasting. Code and models are available at\nhttps://github.com/xiyan1234567/SMETimes.\n","authors":["Haoran Fan","Bin Li","Yixuan Weng","Shoujun Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.03594v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2503.03592v1","updated":"2025-03-05T15:26:59Z","published":"2025-03-05T15:26:59Z","title":"English K_Quantization of LLMs Does Not Disproportionately Diminish\n  Multilingual Performance","summary":"  For consumer usage of locally deployed LLMs, the GGUF format and\nk_quantization are invaluable tools for maintaining the performance of the\noriginal model while reducing it to sizes deployable with consumer-grade\nhardware. The number of bits dedicated to each weight from the original model\nis reduced based on how important they are thought to be during model\ninference. This importance is arrived at through the application of an\n'importance matrix'-a relatively small text document meant to be representative\nof the LLM's standard use-cases. In the vast majority of quants available\nonline, this document is primarily written in English. It was therefore an open\nquestion whether performance on English language tasks was preserved through\nthe sacrifice of multilingual performance and whether it can be preserved with\nalternate importance matrices. This article investigates these hypotheses by\nquantizing Llama3.3 70B on importance matrices written in three languages\n(English, Norwegian, and Malayalam) and evaluating them on the MixEval dataset\nin both English and Norwegian. All experiments related to k_quantization\nyielded non-significant results (In all cases p > 0.237) indicating that\ncurrent quantization practices do not disproportionately harm multilingual\nperformance.\n","authors":["Karl Audun Borgersen"],"pdf_url":"https://arxiv.org/pdf/2503.03592v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2501.16207v3","updated":"2025-03-05T15:26:49Z","published":"2025-01-27T17:00:56Z","title":"From Informal to Formal -- Incorporating and Evaluating LLMs on Natural\n  Language Requirements to Verifiable Formal Proofs","summary":"  The research in AI-based formal mathematical reasoning has shown an\nunstoppable growth trend. These studies have excelled in mathematical\ncompetitions like IMO and have made significant progress. This paper focuses on\nformal verification, an immediate application scenario of formal reasoning, and\nbreaks it down into sub-tasks. We constructed 18k high-quality\ninstruction-response pairs across five formal specification languages (Coq,\nLean4, Dafny, ACSL, and TLA+) by distilling gpt-4o and evaluated against ten\nopen-sourced LLMs, including recent popular DeepSeek-R1. We also fine-tuned\nseveral 7~8B small models to achieve comparable performance with\nDeepseek-R1-671B. Interestingly, we observed that fine-tuning with formal data\nalso enhances mathematics, reasoning, and coding capabilities. Fine-tuned\nmodels are released at https: //huggingface.co/fm-universe.\n","authors":["Jialun Cao","Yaojie Lu","Meiziniu Li","Haoyang Ma","Haokun Li","Mengda He","Cheng Wen","Le Sun","Hongyu Zhang","Shengchao Qin","Shing-Chi Cheung","Cong Tian"],"pdf_url":"https://arxiv.org/pdf/2501.16207v3.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2501.01999v2","updated":"2025-03-05T15:26:17Z","published":"2025-01-01T07:00:41Z","title":"On the Utility of Equivariance and Symmetry Breaking in Deep Learning\n  Architectures on Point Clouds","summary":"  This paper explores the key factors that influence the performance of models\nworking with point clouds, across different tasks of varying geometric\ncomplexity. In this work, we explore the trade-offs between flexibility and\nweight-sharing introduced by equivariant layers, assessing when equivariance\nboosts or detracts from performance. It is often argued that providing more\ninformation as input improves a model's performance. However, if this\nadditional information breaks certain properties, such as $\\SE(3)$\nequivariance, does it remain beneficial? We identify the key aspects of\nequivariant and non-equivariant architectures that drive success in different\ntasks by benchmarking them on segmentation, regression, and generation tasks\nacross multiple datasets with increasing complexity. We observe a positive\nimpact of equivariance, which becomes more pronounced with increasing task\ncomplexity, even when strict equivariance is not required.\n","authors":["Sharvaree Vadgama","Mohammad Mohaiminul Islam","Domas Buracus","Christian Shewmake","Erik Bekkers"],"pdf_url":"https://arxiv.org/pdf/2501.01999v2.pdf","comment":"19 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.20427v2","updated":"2025-03-05T14:58:33Z","published":"2025-02-27T12:26:25Z","title":"DeePen: Penetration Testing for Audio Deepfake Detection","summary":"  Deepfakes - manipulated or forged audio and video media - pose significant\nsecurity risks to individuals, organizations, and society at large. To address\nthese challenges, machine learning-based classifiers are commonly employed to\ndetect deepfake content. In this paper, we assess the robustness of such\nclassifiers through a systematic penetration testing methodology, which we\nintroduce as DeePen. Our approach operates without prior knowledge of or access\nto the target deepfake detection models. Instead, it leverages a set of\ncarefully selected signal processing modifications - referred to as attacks -\nto evaluate model vulnerabilities. Using DeePen, we analyze both real-world\nproduction systems and publicly available academic model checkpoints,\ndemonstrating that all tested systems exhibit weaknesses and can be reliably\ndeceived by simple manipulations such as time-stretching or echo addition.\nFurthermore, our findings reveal that while some attacks can be mitigated by\nretraining detection systems with knowledge of the specific attack, others\nremain persistently effective. We release all associated code.\n","authors":["Nicolas Müller","Piotr Kawa","Adriana Stan","Thien-Phuc Doan","Souhwan Jung","Wei Herng Choong","Philip Sperl","Konstantin Böttinger"],"pdf_url":"https://arxiv.org/pdf/2502.20427v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.11613v2","updated":"2025-03-05T14:55:49Z","published":"2024-07-16T11:22:34Z","title":"Bringing AI Participation Down to Scale: A Comment on Open AIs\n  Democratic Inputs to AI Project","summary":"  In 2023, Open AIs Democratic Inputs program funded 10 teams to design\nprocedures for public participation in generative AI. In this Perspective, we\nreview the results of the project, drawing on interviews with some of the teams\nand our own experiences conducting participation exercises, we identify several\nshared yet largely unspoken assumptions of the Democratic Inputs program 1.\nthat participation must be scalable 2. that the object of participation is a\nsingle model 3. that there must be a single form of participation 4. that the\ngoal is to extract abstract principles 5. that these principles should have\nconsensus 6. that publics should be representative and encourage alternative\nforms of participation in AI, perhaps not undertaken by tech companies.\n","authors":["David Moats","Chandrima Ganguly"],"pdf_url":"https://arxiv.org/pdf/2407.11613v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03563v1","updated":"2025-03-05T14:51:46Z","published":"2025-03-05T14:51:46Z","title":"A Conceptual Model for Attributions in Event-Centric Knowledge Graphs","summary":"  The use of narratives as a means of fusing information from knowledge graphs\n(KGs) into a coherent line of argumentation has been the subject of recent\ninvestigation. Narratives are especially useful in event-centric knowledge\ngraphs in that they provide a means to connect different real-world events and\ncategorize them by well-known narrations. However, specifically for\ncontroversial events, a problem in information fusion arises, namely, multiple\nviewpoints regarding the validity of certain event aspects, e.g., regarding the\nrole a participant takes in an event, may exist. Expressing those viewpoints in\nKGs is challenging because disputed information provided by different\nviewpoints may introduce inconsistencies. Hence, most KGs only feature a single\nview on the contained information, hampering the effectiveness of narrative\ninformation access. This paper is an extension of our original work and\nintroduces attributions, i.e., parameterized predicates that allow for the\nrepresentation of facts that are only valid in a specific viewpoint. For this,\nwe develop a conceptual model that allows for the representation of\nviewpoint-dependent information. As an extension, we enhance the model by a\nconception of viewpoint-compatibility. Based on this, we deepen our original\ndeliberations on the model's effects on information fusion and provide\nadditional grounding in the literature.\n","authors":["Florian Plötzky","Katarina Britz","Wolf-Tilo Balke"],"pdf_url":"https://arxiv.org/pdf/2503.03563v1.pdf","comment":"Submitted to Data & Knowledge Engineering, 22 pages, 9 figures"},{"id":"http://arxiv.org/abs/2503.03562v1","updated":"2025-03-05T14:49:08Z","published":"2025-03-05T14:49:08Z","title":"Towards Visual Discrimination and Reasoning of Real-World Physical\n  Dynamics: Physics-Grounded Anomaly Detection","summary":"  Humans detect real-world object anomalies by perceiving, interacting, and\nreasoning based on object-conditioned physical knowledge. The long-term goal of\nIndustrial Anomaly Detection (IAD) is to enable machines to autonomously\nreplicate this skill. However, current IAD algorithms are largely developed and\ntested on static, semantically simple datasets, which diverge from real-world\nscenarios where physical understanding and reasoning are essential.To bridge\nthis gap, we introduce the Physics Anomaly Detection (Phys-AD) dataset, the\nfirst large-scale, real-world, physics-grounded video dataset for industrial\nanomaly detection. Collected using a real robot arm and motor, Phys-AD provides\na diverse set of dynamic, semantically rich scenarios. The dataset includes\nmore than 6400 videos across 22 real-world object categories, interacting with\nrobot arms and motors, and exhibits 47 types of anomalies. Anomaly detection in\nPhys-AD requires visual reasoning, combining both physical knowledge and video\ncontent to determine object abnormality.We benchmark state-of-the-art anomaly\ndetection methods under three settings: unsupervised AD, weakly-supervised AD,\nand video-understanding AD, highlighting their limitations in handling\nphysics-grounded anomalies. Additionally, we introduce the Physics Anomaly\nExplanation (PAEval) metric, designed to assess the ability of visual-language\nfoundation models to not only detect anomalies but also provide accurate\nexplanations for their underlying physical causes. Our dataset and benchmark\nwill be publicly available.\n","authors":["Wenqiao Li","Yao Gu","Xintao Chen","Xiaohao Xu","Ming Hu","Xiaonan Huang","Yingna Wu"],"pdf_url":"https://arxiv.org/pdf/2503.03562v1.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2411.13982v2","updated":"2025-03-05T14:45:55Z","published":"2024-11-21T09:47:13Z","title":"Safety Without Semantic Disruptions: Editing-free Safe Image Generation\n  via Context-preserving Dual Latent Reconstruction","summary":"  Training multimodal generative models on large, uncurated datasets can result\nin users being exposed to harmful, unsafe and controversial or\nculturally-inappropriate outputs. While model editing has been proposed to\nremove or filter undesirable concepts in embedding and latent spaces, it can\ninadvertently damage learned manifolds, distorting concepts in close semantic\nproximity. We identify limitations in current model editing techniques, showing\nthat even benign, proximal concepts may become misaligned. To address the need\nfor safe content generation, we leverage safe embeddings and a modified\ndiffusion process with tunable weighted summation in the latent space to\ngenerate safer images. Our method preserves global context without compromising\nthe structural integrity of the learned manifolds. We achieve state-of-the-art\nresults on safe image generation benchmarks and offer intuitive control over\nthe level of model safety. We identify trade-offs between safety and\ncensorship, which presents a necessary perspective in the development of\nethical AI models. We will release our code.\n  Keywords: Text-to-Image Models, Generative AI, Safety, Reliability, Model\nEditing\n","authors":["Jordan Vice","Naveed Akhtar","Mubarak Shah","Richard Hartley","Ajmal Mian"],"pdf_url":"https://arxiv.org/pdf/2411.13982v2.pdf","comment":"This research is supported by the NISDRG project #20100007, funded by\n  the Australian Government"},{"id":"http://arxiv.org/abs/2407.16205v5","updated":"2025-03-05T14:43:33Z","published":"2024-07-23T06:14:41Z","title":"LLMs can be Dangerous Reasoners: Analyzing-based Jailbreak Attack on\n  Large Language Models","summary":"  The rapid development of Large Language Models (LLMs) has brought significant\nadvancements across various tasks. However, despite these achievements, LLMs\nstill exhibit inherent safety vulnerabilities, especially when confronted with\njailbreak attacks. Existing jailbreak methods suffer from two main limitations:\nreliance on complicated prompt engineering and iterative optimization, which\nlead to low attack success rate (ASR) and attack efficiency (AE). In this work,\nwe propose an efficient jailbreak attack method, Analyzing-based Jailbreak\n(ABJ), which leverages the advanced reasoning capability of LLMs to\nautonomously generate harmful content, revealing their underlying safety\nvulnerabilities during complex reasoning process. We conduct comprehensive\nexperiments on ABJ across various open-source and closed-source LLMs. In\nparticular, ABJ achieves high ASR (82.1% on GPT-4o-2024-11-20) with exceptional\nAE among all target LLMs, showcasing its remarkable attack effectiveness,\ntransferability, and efficiency. Our findings underscore the urgent need to\nprioritize and improve the safety of LLMs to mitigate the risks of misuse.\n","authors":["Shi Lin","Hongming Yang","Dingyang Lin","Rongchang Li","Xun Wang","Changting Lin","Wenpeng Xing","Meng Han"],"pdf_url":"https://arxiv.org/pdf/2407.16205v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07115v3","updated":"2025-03-05T14:43:01Z","published":"2025-02-10T23:11:44Z","title":"Online Scheduling for LLM Inference with KV Cache Constraints","summary":"  Large Language Model (LLM) inference, where a trained model generates text\none word at a time in response to user prompts, is a computationally intensive\nprocess requiring efficient scheduling to optimize latency and resource\nutilization. A key challenge in LLM inference is the management of the\nKey-Value (KV) cache, which reduces redundant computations but introduces\nmemory constraints. In this work, we model LLM inference with KV cache\nconstraints theoretically and propose novel batching and scheduling algorithms\nthat minimize inference latency while effectively managing the KV cache's\nmemory.\n  We analyze both semi-online and fully online scheduling models, and our\nresults are threefold. First, we provide a polynomial-time algorithm that\nachieves exact optimality in terms of average latency in the semi-online prompt\narrival model. Second, in the fully online case with a stochastic prompt\narrival, we introduce an efficient online scheduling algorithm with constant\nregret. Third, we prove that no algorithm (deterministic or randomized) can\nachieve a constant competitive ratio in fully online adversarial settings. Our\nempirical evaluations on a public LLM inference dataset, using the Llama-70B\nmodel on A100 GPUs, show that our approach significantly outperforms benchmark\nalgorithms used currently in practice, achieving lower latency while reducing\nenergy consumption. Overall, our results offer a path toward more sustainable\nand cost-effective LLM deployment.\n","authors":["Patrick Jaillet","Jiashuo Jiang","Chara Podimata","Zijie Zhou"],"pdf_url":"https://arxiv.org/pdf/2502.07115v3.pdf","comment":"Will add a lemma in the proof of Theorem 5.3 to make the statement\n  and proof more rigorous"},{"id":"http://arxiv.org/abs/2502.11681v4","updated":"2025-03-05T14:38:19Z","published":"2025-02-17T11:16:19Z","title":"RIDE: Enhancing Large Language Model Alignment through Restyled\n  In-Context Learning Demonstration Exemplars","summary":"  Alignment tuning is crucial for ensuring large language models (LLMs) behave\nethically and helpfully. Current alignment approaches require high-quality\nannotations and significant training resources. This paper proposes a low-cost,\ntuning-free method using in-context learning (ICL) to enhance LLM alignment.\nThrough an analysis of high-quality ICL demos, we identified style as a key\nfactor influencing LLM alignment capabilities and explicitly restyled ICL\nexemplars based on this stylistic framework. Additionally, we combined the\nrestyled demos to achieve a balance between the two conflicting aspects of LLM\nalignment--factuality and safety. We packaged the restyled examples as prompts\nto trigger few-shot learning, improving LLM alignment. Compared to the best\nbaseline approach, with an average score of 5.00 as the maximum, our method\nachieves a maximum 0.10 increase on the Alpaca task (from 4.50 to 4.60), a 0.22\nenhancement on the Just-eval benchmark (from 4.34 to 4.56), and a maximum\nimprovement of 0.32 (from 3.53 to 3.85) on the MT-Bench dataset. We release the\ncode and data at https://github.com/AnonymousCode-ComputerScience/RIDE.\n","authors":["Yuncheng Hua","Lizhen Qu","Zhuang Li","Hao Xue","Flora D. Salim","Gholamreza Haffari"],"pdf_url":"https://arxiv.org/pdf/2502.11681v4.pdf","comment":"38 pages, 2 figures, 20 tables; The paper is under review in ARR"},{"id":"http://arxiv.org/abs/2503.03532v1","updated":"2025-03-05T14:14:25Z","published":"2025-03-05T14:14:25Z","title":"AI-Enabled Conversational Journaling for Advancing Parkinson's Disease\n  Symptom Tracking","summary":"  Journaling plays a crucial role in managing chronic conditions by allowing\npatients to document symptoms and medication intake, providing essential data\nfor long-term care. While valuable, traditional journaling methods often rely\non static, self-directed entries, lacking interactive feedback and real-time\nguidance. This gap can result in incomplete or imprecise information, limiting\nits usefulness for effective treatment. To address this gap, we introduce\nPATRIKA, an AI-enabled prototype designed specifically for people with\nParkinson's disease (PwPD). The system incorporates cooperative conversation\nprinciples, clinical interview simulations, and personalization to create a\nmore effective and user-friendly journaling experience. Through two user\nstudies with PwPD and iterative refinement of PATRIKA, we demonstrate\nconversational journaling's significant potential in patient engagement and\ncollecting clinically valuable information. Our results showed that generating\nprobing questions PATRIKA turned journaling into a bi-directional interaction.\nAdditionally, we offer insights for designing journaling systems for healthcare\nand future directions for promoting sustained journaling.\n","authors":["Mashrur Rashik","Shilpa Sweth","Nishtha Agrawal","Saiyyam Kochar","Kara M Smith","Fateme Rajabiyazdi","Vidya Setlur","Narges Mahyar","Ali Sarvghad"],"pdf_url":"https://arxiv.org/pdf/2503.03532v1.pdf","comment":"To appear in the ACM CHI conference on Human Factors in Computing\n  Systems (CHI), 2025"},{"id":"http://arxiv.org/abs/2409.16502v2","updated":"2025-03-05T14:11:44Z","published":"2024-09-24T23:18:32Z","title":"GSplatLoc: Grounding Keypoint Descriptors into 3D Gaussian Splatting for\n  Improved Visual Localization","summary":"  Although various visual localization approaches exist, such as scene\ncoordinate regression and camera pose regression, these methods often struggle\nwith optimization complexity or limited accuracy. To address these challenges,\nwe explore the use of novel view synthesis techniques, particularly 3D Gaussian\nSplatting (3DGS), which enables the compact encoding of both 3D geometry and\nscene appearance. We propose a two-stage procedure that integrates dense and\nrobust keypoint descriptors from the lightweight XFeat feature extractor into\n3DGS, enhancing performance in both indoor and outdoor environments. The coarse\npose estimates are directly obtained via 2D-3D correspondences between the 3DGS\nrepresentation and query image descriptors. In the second stage, the initial\npose estimate is refined by minimizing the rendering-based photometric warp\nloss. Benchmarking on widely used indoor and outdoor datasets demonstrates\nimprovements over recent neural rendering-based localization methods, such as\nNeRFMatch and PNeRFLoc.\n","authors":["Gennady Sidorov","Malik Mohrat","Denis Gridusov","Ruslan Rakhimov","Sergey Kolyubin"],"pdf_url":"https://arxiv.org/pdf/2409.16502v2.pdf","comment":"Project website at https://gsplatloc.github.io/"},{"id":"http://arxiv.org/abs/2503.03528v1","updated":"2025-03-05T14:11:13Z","published":"2025-03-05T14:11:13Z","title":"AdaSin: Enhancing Hard Sample Metrics with Dual Adaptive Penalty for\n  Face Recognition","summary":"  In recent years, the emergence of deep convolutional neural networks has\npositioned face recognition as a prominent research focus in computer vision.\nTraditional loss functions, such as margin-based, hard-sample mining-based, and\nhybrid approaches, have achieved notable performance improvements, with some\nleveraging curriculum learning to optimize training. However, these methods\noften fall short in effectively quantifying the difficulty of hard samples. To\naddress this, we propose Adaptive Sine (AdaSin) loss function, which introduces\nthe sine of the angle between a sample's embedding feature and its ground-truth\nclass center as a novel difficulty metric. This metric enables precise and\neffective penalization of hard samples. By incorporating curriculum learning,\nthe model dynamically adjusts classification boundaries across different\ntraining stages. Unlike previous adaptive-margin loss functions, AdaSin\nintroduce a dual adaptive penalty, applied to both the positive and negative\ncosine similarities of hard samples. This design imposes stronger constraints,\nenhancing intra-class compactness and inter-class separability. The combination\nof the dual adaptive penalty and curriculum learning is guided by a\nwell-designed difficulty metric. It enables the model to focus more effectively\non hard samples in later training stages, and lead to the extraction of highly\ndiscriminative face features. Extensive experiments across eight benchmarks\ndemonstrate that AdaSin achieves superior accuracy compared to other\nstate-of-the-art methods.\n","authors":["Qiqi Guo","Zhuowen Zheng","Guanghua Yang","Zhiquan Liu","Xiaofan Li","Jianqing Li","Jinyu Tian","Xueyuan Gong"],"pdf_url":"https://arxiv.org/pdf/2503.03528v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.09444v3","updated":"2025-03-05T14:02:10Z","published":"2024-01-31T15:37:12Z","title":"Multimodal Action Quality Assessment","summary":"  Action quality assessment (AQA) is to assess how well an action is performed.\nPrevious works perform modelling by only the use of visual information,\nignoring audio information. We argue that although AQA is highly dependent on\nvisual information, the audio is useful complementary information for improving\nthe score regression accuracy, especially for sports with background music,\nsuch as figure skating and rhythmic gymnastics. To leverage multimodal\ninformation for AQA, i.e., RGB, optical flow and audio information, we propose\na Progressive Adaptive Multimodal Fusion Network (PAMFN) that separately models\nmodality-specific information and mixed-modality information. Our model\nconsists of with three modality-specific branches that independently explore\nmodality-specific information and a mixed-modality branch that progressively\naggregates the modality-specific information from the modality-specific\nbranches. To build the bridge between modality-specific branches and the\nmixed-modality branch, three novel modules are proposed. First, a\nModality-specific Feature Decoder module is designed to selectively transfer\nmodality-specific information to the mixed-modality branch. Second, when\nexploring the interaction between modality-specific information, we argue that\nusing an invariant multimodal fusion policy may lead to suboptimal results, so\nas to take the potential diversity in different parts of an action into\nconsideration. Therefore, an Adaptive Fusion Module is proposed to learn\nadaptive multimodal fusion policies in different parts of an action. This\nmodule consists of several FusionNets for exploring different multimodal fusion\nstrategies and a PolicyNet for deciding which FusionNets are enabled. Third, a\nmodule called Cross-modal Feature Decoder is designed to transfer cross-modal\nfeatures generated by Adaptive Fusion Module to the mixed-modality branch.\n","authors":["Ling-An Zeng","Wei-Shi Zheng"],"pdf_url":"https://arxiv.org/pdf/2402.09444v3.pdf","comment":"IEEE Transactions on Image Processing 2024"},{"id":"http://arxiv.org/abs/2503.03511v1","updated":"2025-03-05T13:57:37Z","published":"2025-03-05T13:57:37Z","title":"NeuGrasp: Generalizable Neural Surface Reconstruction with Background\n  Priors for Material-Agnostic Object Grasp Detection","summary":"  Robotic grasping in scenes with transparent and specular objects presents\ngreat challenges for methods relying on accurate depth information. In this\npaper, we introduce NeuGrasp, a neural surface reconstruction method that\nleverages background priors for material-agnostic grasp detection. NeuGrasp\nintegrates transformers and global prior volumes to aggregate multi-view\nfeatures with spatial encoding, enabling robust surface reconstruction in\nnarrow and sparse viewing conditions. By focusing on foreground objects through\nresidual feature enhancement and refining spatial perception with an\noccupancy-prior volume, NeuGrasp excels in handling objects with transparent\nand specular surfaces. Extensive experiments in both simulated and real-world\nscenarios show that NeuGrasp outperforms state-of-the-art methods in grasping\nwhile maintaining comparable reconstruction quality. More details are available\nat https://neugrasp.github.io/.\n","authors":["Qingyu Fan","Yinghao Cai","Chao Li","Wenzhe He","Xudong Zheng","Tao Lu","Bin Liang","Shuo Wang"],"pdf_url":"https://arxiv.org/pdf/2503.03511v1.pdf","comment":"7 pages, 5 figures. IEEE International Conference on Robotics and\n  Automation (ICRA) 2025"},{"id":"http://arxiv.org/abs/2503.03506v1","updated":"2025-03-05T13:54:13Z","published":"2025-03-05T13:54:13Z","title":"Rethinking Synthetic Data definitions: A privacy driven approach","summary":"  Synthetic data is gaining traction as a cost-effective solution for the\nincreasing data demands of AI development and can be generated either from\nexisting knowledge or derived data captured from real-world events. The source\nof the synthetic data generation and the technique used significantly impacts\nits residual privacy risk and therefore its opportunity for sharing.\nTraditional classification of synthetic data types no longer fit the newer\ngeneration techniques and there is a need to better align the classification\nwith practical needs. We suggest a new way of grouping synthetic data types\nthat better supports privacy evaluations to aid regulatory policymaking. Our\nnovel classification provides flexibility to new advancements like deep\ngenerative methods and offers a more practical framework for future\napplications.\n","authors":["Vibeke Binz Vallevik","Serena Elizabeth Marshall","Aleksandar Babic","Jan Franz Nygaard"],"pdf_url":"https://arxiv.org/pdf/2503.03506v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03505v1","updated":"2025-03-05T13:53:10Z","published":"2025-03-05T13:53:10Z","title":"Parallelized Planning-Acting for Efficient LLM-based Multi-Agent Systems","summary":"  Recent advancements in Large Language Model(LLM)-based Multi-Agent\nSystems(MAS) have demonstrated remarkable potential for tackling complex\ndecision-making tasks. However, existing frameworks inevitably rely on\nserialized execution paradigms, where agents must complete sequential LLM\nplanning before taking action. This fundamental constraint severely limits\nreal-time responsiveness and adaptation, which is crucial in dynamic\nenvironments with ever-changing scenarios. In this paper, we propose a novel\nparallelized planning-acting framework for LLM-based MAS, featuring a\ndual-thread architecture with interruptible execution to enable concurrent\nplanning and acting. Specifically, our framework comprises two core threads:(1)\na planning thread driven by a centralized memory system, maintaining\nsynchronization of environmental states and agent communication to support\ndynamic decision-making; and (2) an acting thread equipped with a comprehensive\nskill library, enabling automated task execution through recursive\ndecomposition. Extensive experiments on challenging Minecraft demonstrate the\neffectiveness of the proposed framework.\n","authors":["Yaoru Li","Shunyu Liu","Tongya Zheng","Mingli Song"],"pdf_url":"https://arxiv.org/pdf/2503.03505v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03503v1","updated":"2025-03-05T13:47:55Z","published":"2025-03-05T13:47:55Z","title":"Collaborative Expert LLMs Guided Multi-Objective Molecular Optimization","summary":"  Molecular optimization is a crucial yet complex and time-intensive process\nthat often acts as a bottleneck for drug development. Traditional methods rely\nheavily on trial and error, making multi-objective optimization both\ntime-consuming and resource-intensive. Current AI-based methods have shown\nlimited success in handling multi-objective optimization tasks, hampering their\npractical utilization. To address this challenge, we present MultiMol, a\ncollaborative large language model (LLM) system designed to guide\nmulti-objective molecular optimization. MultiMol comprises two agents,\nincluding a data-driven worker agent and a literature-guided research agent.\nThe data-driven worker agent is a large language model being fine-tuned to\nlearn how to generate optimized molecules considering multiple objectives,\nwhile the literature-guided research agent is responsible for searching\ntask-related literature to find useful prior knowledge that facilitates\nidentifying the most promising optimized candidates. In evaluations across six\nmulti-objective optimization tasks, MultiMol significantly outperforms existing\nmethods, achieving a 82.30% success rate, in sharp contrast to the 27.50%\nsuccess rate of current strongest methods. To further validate its practical\nimpact, we tested MultiMol on two real-world challenges. First, we enhanced the\nselectivity of Xanthine Amine Congener (XAC), a promiscuous ligand that binds\nboth A1R and A2AR, successfully biasing it towards A1R. Second, we improved the\nbioavailability of Saquinavir, an HIV-1 protease inhibitor with known\nbioavailability limitations. Overall, these results indicate that MultiMol\nrepresents a highly promising approach for multi-objective molecular\noptimization, holding great potential to accelerate the drug development\nprocess and contribute to the advancement of pharmaceutical research.\n","authors":["Jiajun Yu","Yizhen Zheng","Huan Yee Koh","Shirui Pan","Tianyue Wang","Haishuai Wang"],"pdf_url":"https://arxiv.org/pdf/2503.03503v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03502v1","updated":"2025-03-05T13:47:53Z","published":"2025-03-05T13:47:53Z","title":"CURVALID: Geometrically-guided Adversarial Prompt Detection","summary":"  Adversarial prompts capable of jailbreaking large language models (LLMs) and\ninducing undesirable behaviours pose a significant obstacle to their safe\ndeployment. Current mitigation strategies rely on activating built-in defence\nmechanisms or fine-tuning the LLMs, but the fundamental distinctions between\nadversarial and benign prompts are yet to be understood. In this work, we\nintroduce CurvaLID, a novel defense framework that efficiently detects\nadversarial prompts by leveraging their geometric properties. It is agnostic to\nthe type of LLM, offering a unified detection framework across diverse\nadversarial prompts and LLM architectures. CurvaLID builds on the geometric\nanalysis of text prompts to uncover their underlying differences. We\ntheoretically extend the concept of curvature via the Whewell equation into an\n$n$-dimensional word embedding space, enabling us to quantify local geometric\nproperties, including semantic shifts and curvature in the underlying\nmanifolds. Additionally, we employ Local Intrinsic Dimensionality (LID) to\ncapture geometric features of text prompts within adversarial subspaces. Our\nfindings reveal that adversarial prompts differ fundamentally from benign\nprompts in terms of their geometric characteristics. Our results demonstrate\nthat CurvaLID delivers superior detection and rejection of adversarial queries,\npaving the way for safer LLM deployment. The source code can be found at\nhttps://github.com/Cancanxxx/CurvaLID\n","authors":["Canaan Yung","Hanxun Huang","Sarah Monazam Erfani","Christopher Leckie"],"pdf_url":"https://arxiv.org/pdf/2503.03502v1.pdf","comment":"29 Pages, 5 figues"},{"id":"http://arxiv.org/abs/2412.18355v2","updated":"2025-03-05T13:25:09Z","published":"2024-12-24T11:35:40Z","title":"Handling Spatial-Temporal Data Heterogeneity for Federated Continual\n  Learning via Tail Anchor","summary":"  Federated continual learning (FCL) allows each client to continually update\nits knowledge from task streams, enhancing the applicability of federated\nlearning in real-world scenarios. However, FCL needs to address not only\nspatial data heterogeneity between clients but also temporal data heterogeneity\nbetween tasks. In this paper, empirical experiments demonstrate that such\ninput-level heterogeneity significantly affects the model's internal parameters\nand outputs, leading to severe spatial-temporal catastrophic forgetting of\nlocal and previous knowledge. To this end, we propose Federated Tail Anchor\n(FedTA) to mix trainable Tail Anchor with the frozen output features to adjust\ntheir position in the feature space, thereby overcoming parameter-forgetting\nand output-forgetting. Three novel components are also included: Input\nEnhancement for improving the performance of pre-trained models on downstream\ntasks; Selective Input Knowledge Fusion for fusion of heterogeneous local\nknowledge on the server; and Best Global Prototype Selection for finding the\nbest anchor point for each class in the feature space. Extensive experiments\ndemonstrate that FedTA not only outperforms existing FCL methods but also\neffectively preserves the relative positions of features.\n","authors":["Hao Yu","Xin Yang","Le Zhang","Hanlin Gu","Tianrui Li","Lixin Fan","Qiang Yang"],"pdf_url":"https://arxiv.org/pdf/2412.18355v2.pdf","comment":"This paper is accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2502.20475v2","updated":"2025-03-05T13:22:47Z","published":"2025-02-27T19:23:15Z","title":"Promote, Suppress, Iterate: How Language Models Answer One-to-Many\n  Factual Queries","summary":"  To answer one-to-many factual queries (e.g., listing cities of a country), a\nlanguage model (LM) must simultaneously recall knowledge and avoid repeating\nprevious answers. How are these two subtasks implemented and integrated\ninternally? Across multiple datasets and models, we identify a\npromote-then-suppress mechanism: the model first recalls all answers, and then\nsuppresses previously generated ones. Specifically, LMs use both the subject\nand previous answer tokens to perform knowledge recall, with attention\npropagating subject information and MLPs promoting the answers. Then, attention\nattends to and suppresses previous answer tokens, while MLPs amplify the\nsuppression signal. Our mechanism is corroborated by extensive experimental\nevidence: in addition to using early decoding and causal tracing, we analyze\nhow components use different tokens by introducing both Token Lens, which\ndecodes aggregated attention updates from specified tokens, and a knockout\nmethod that analyzes changes in MLP outputs after removing attention to\nspecified tokens. Overall, we provide new insights into how LMs' internal\ncomponents interact with different input tokens to support complex factual\nrecall. Code is available at\nhttps://github.com/Lorenayannnnn/how-lms-answer-one-to-many-factual-queries.\n","authors":["Tianyi Lorena Yan","Robin Jia"],"pdf_url":"https://arxiv.org/pdf/2502.20475v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.02393v3","updated":"2025-03-05T13:19:16Z","published":"2025-01-04T22:30:21Z","title":"Graph-Aware Isomorphic Attention for Adaptive Dynamics in Transformers","summary":"  We present an approach to modifying Transformer architectures by integrating\ngraph-aware relational reasoning into the attention mechanism, merging concepts\nfrom graph neural networks and language modeling. Building on the inherent\nconnection between attention and graph theory, we reformulate the Transformer's\nattention mechanism as a graph operation and propose Graph-Aware Isomorphic\nAttention. This method leverages advanced graph modeling strategies, including\nGraph Isomorphism Networks (GIN) and Principal Neighborhood Aggregation (PNA),\nto enrich the representation of relational structures. Our approach captures\ncomplex dependencies and generalizes across tasks, as evidenced by a reduced\ngeneralization gap and improved learning performance. Additionally, we expand\nthe concept of graph-aware attention to introduce Sparse GIN-Attention, a\nfine-tuning approach that employs sparse GINs. By interpreting attention\nmatrices as sparse adjacency graphs, this technique enhances the adaptability\nof pre-trained foundational models with minimal computational overhead,\nendowing them with graph-aware capabilities. Sparse GIN-Attention fine-tuning\nachieves improved training dynamics and better generalization compared to\nalternative methods like low-rank adaption (LoRA). We discuss latent graph-like\nstructures within traditional attention mechanisms, offering a new lens through\nwhich Transformers can be understood. By evolving Transformers as hierarchical\nGIN models for relational reasoning. This perspective suggests profound\nimplications for foundational model development, enabling the design of\narchitectures that dynamically adapt to both local and global dependencies.\nApplications in bioinformatics, materials science, language modeling, and\nbeyond could benefit from this synthesis of relational and sequential data\nmodeling, setting the stage for interpretable and generalizable modeling\nstrategies.\n","authors":["Markus J. Buehler"],"pdf_url":"https://arxiv.org/pdf/2501.02393v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03480v1","updated":"2025-03-05T13:16:55Z","published":"2025-03-05T13:16:55Z","title":"SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via\n  Safe Reinforcement Learning","summary":"  Vision-language-action models (VLAs) have shown great potential as generalist\nrobot policies. However, these models pose urgent safety challenges during\ndeployment, including the risk of physical harm to the environment, the robot\nitself, and humans. How can safety be explicitly incorporated into VLAs? In\nthis work, we propose SafeVLA, a novel algorithm designed to integrate safety\ninto VLAs, ensuring the protection of the environment, robot hardware and\nhumans in real-world settings. SafeVLA effectively balances safety and task\nperformance by employing large-scale constrained learning within simulated\nenvironments. We demonstrate that SafeVLA outperforms the current\nstate-of-the-art method in both safety and task performance, achieving average\nimprovements of 83.58% and 3.85%, respectively, in simulation. By prioritizing\nsafety, our approach eliminates high-risk behaviors and reduces the upper bound\nof unsafe behaviors to 1/35 of that in the current state-of-the-art, thereby\nsignificantly mitigating long-tail risks. Furthermore, the learned safety\nconstraints generalize to diverse, unseen scenarios, including multiple\nout-of-distribution perturbations and tasks. Our data, models and newly\nproposed benchmark environment are available at\nhttps://sites.google.com/view/pku-safevla.\n","authors":["Borong Zhang","Yuhao Zhang","Jiaming Ji","Yingshan Lei","Josef Dai","Yuanpei Chen","Yaodong Yang"],"pdf_url":"https://arxiv.org/pdf/2503.03480v1.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2503.03462v1","updated":"2025-03-05T12:52:14Z","published":"2025-03-05T12:52:14Z","title":"Open-Source Large Language Models as Multilingual Crowdworkers:\n  Synthesizing Open-Domain Dialogues in Several Languages With No Examples in\n  Targets and No Machine Translation","summary":"  The prevailing paradigm in the domain of Open-Domain Dialogue agents\npredominantly focuses on the English language, encompassing both models and\ndatasets. Furthermore, the financial and temporal investments required for\ncrowdsourcing such datasets for finetuning are substantial, particularly when\nmultiple languages are involved. Fortunately, advancements in Large Language\nModels (LLMs) have unveiled a plethora of possibilities across diverse tasks.\nSpecifically, instruction-tuning has enabled LLMs to execute tasks based on\nnatural language instructions, occasionally surpassing the performance of human\ncrowdworkers. Additionally, these models possess the capability to function in\nvarious languages within a single thread. Consequently, to generate new samples\nin different languages, we propose leveraging these capabilities to replicate\nthe data collection process. We introduce a pipeline for generating Open-Domain\nDialogue data in multiple Target Languages using LLMs, with demonstrations\nprovided in a unique Source Language. By eschewing explicit Machine Translation\nin this approach, we enhance the adherence to language-specific nuances. We\napply this methodology to the PersonaChat dataset. To enhance the openness of\ngenerated dialogues and mimic real life scenarii, we added the notion of speech\nevents corresponding to the type of conversation the speakers are involved in\nand also that of common ground which represents the premises of a conversation.\n","authors":["Ahmed Njifenjou","Virgile Sucal","Bassam Jabaian","Fabrice Lefèvre"],"pdf_url":"https://arxiv.org/pdf/2503.03462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03459v1","updated":"2025-03-05T12:49:44Z","published":"2025-03-05T12:49:44Z","title":"Unified Mind Model: Reimagining Autonomous Agents in the LLM Era","summary":"  Large language models (LLMs) have recently demonstrated remarkable\ncapabilities across domains, tasks, and languages (e.g., ChatGPT and GPT-4),\nreviving the research of general autonomous agents with human-like cognitive\nabilities.Such human-level agents require semantic comprehension and\ninstruction-following capabilities, which exactly fall into the strengths of\nLLMs.Although there have been several initial attempts to build human-level\nagents based on LLMs, the theoretical foundation remains a challenging open\nproblem. In this paper, we propose a novel theoretical cognitive architecture,\nthe Unified Mind Model (UMM), which offers guidance to facilitate the rapid\ncreation of autonomous agents with human-level cognitive abilities.\nSpecifically, our UMM starts with the global workspace theory and further\nleverage LLMs to enable the agent with various cognitive abilities, such as\nmulti-modal perception, planning, reasoning, tool use, learning, memory,\nreflection and motivation. Building upon UMM, we then develop an agent-building\nengine, MindOS, which allows users to quickly create domain-/task-specific\nautonomous agents without any programming effort.\n","authors":["Pengbo Hu","Xiang Ying"],"pdf_url":"https://arxiv.org/pdf/2503.03459v1.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2403.10860v2","updated":"2025-03-05T12:41:05Z","published":"2024-03-16T08:57:00Z","title":"Sim2Real within 5 Minutes: Efficient Domain Transfer with Stylized\n  Gaussian Splatting for Endoscopic Images","summary":"  Robot assisted endoluminal intervention is an emerging technique for both\nbenign and malignant luminal lesions. With vision-based navigation, when\ncombined with pre-operative imaging data as priors, it is possible to recover\nposition and pose of the endoscope without the need of additional sensors. In\npractice, however, aligning pre-operative and intra-operative domains is\ncomplicated by significant texture differences. Although methods such as style\ntransfer can be used to address this issue, they require large datasets from\nboth source and target domains with prolonged training times. This paper\nproposes an efficient domain transfer method based on stylized Gaussian\nsplatting, only requiring a few of real images (10 images) with very fast\ntraining time. Specifically, the transfer process includes two phases. In the\nfirst phase, the 3D models reconstructed from CT scans are represented as\ndifferential Gaussian point clouds. In the second phase, only color appearance\nrelated parameters are optimized to transfer the style and preserve the visual\ncontent. A novel structure consistency loss is applied to latent features and\ndepth levels to enhance the stability of the transferred images. Detailed\nvalidation was performed to demonstrate the performance advantages of the\nproposed method compared to that of the current state-of-the-art, highlighting\nthe potential for intra-operative surgical navigation.\n","authors":["Junyang Wu","Yun Gu","Guang-Zhong Yang"],"pdf_url":"https://arxiv.org/pdf/2403.10860v2.pdf","comment":"Accepted by ICRA 2025"},{"id":"http://arxiv.org/abs/2502.05503v3","updated":"2025-03-05T12:27:57Z","published":"2025-02-08T09:31:26Z","title":"A Physical Coherence Benchmark for Evaluating Video Generation Models\n  via Optical Flow-guided Frame Prediction","summary":"  Recent advances in video generation models demonstrate their potential as\nworld simulators, but they often struggle with videos deviating from physical\nlaws, a key concern overlooked by most text-to-video benchmarks. We introduce a\nbenchmark designed specifically to assess the Physical Coherence of generated\nvideos, PhyCoBench. Our benchmark includes 120 prompts covering 7 categories of\nphysical principles, capturing key physical laws observable in video content.\nWe evaluated four state-of-the-art (SoTA) T2V models on PhyCoBench and\nconducted manual assessments. Additionally, we propose an automated evaluation\nmodel: PhyCoPredictor, a diffusion model that generates optical flow and video\nframes in a cascade manner. Through a consistency evaluation comparing\nautomated and manual sorting, the experimental results show that PhyCoPredictor\ncurrently aligns most closely with human evaluation. Therefore, it can\neffectively evaluate the physical coherence of videos, providing insights for\nfuture model optimization. Our benchmark, including physical coherence prompts,\nthe automatic evaluation tool PhyCoPredictor, and the generated video dataset,\nhas been released on GitHub at https://github.com/Jeckinchen/PhyCoBench.\n","authors":["Yongfan Chen","Xiuwen Zhu","Tianyu Li"],"pdf_url":"https://arxiv.org/pdf/2502.05503v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03444v1","updated":"2025-03-05T12:24:20Z","published":"2025-03-05T12:24:20Z","title":"Taxation Perspectives from Large Language Models: A Case Study on\n  Additional Tax Penalties","summary":"  How capable are large language models (LLMs) in the domain of taxation?\nAlthough numerous studies have explored the legal domain in general, research\ndedicated to taxation remain scarce. Moreover, the datasets used in these\nstudies are either simplified, failing to reflect the real-world complexities,\nor unavailable as open source. To address this gap, we introduce PLAT, a new\nbenchmark designed to assess the ability of LLMs to predict the legitimacy of\nadditional tax penalties. PLAT is constructed to evaluate LLMs' understanding\nof tax law, particularly in cases where resolving the issue requires more than\njust applying related statutes. Our experiments with six LLMs reveal that their\nbaseline capabilities are limited, especially when dealing with conflicting\nissues that demand a comprehensive understanding. However, we found that\nenabling retrieval, self-reasoning, and discussion among multiple agents with\nspecific role assignments, this limitation can be mitigated.\n","authors":["Eunkyung Choi","Young Jin Suh","Hun Park","Wonseok Hwang"],"pdf_url":"https://arxiv.org/pdf/2503.03444v1.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2503.03443v1","updated":"2025-03-05T12:24:12Z","published":"2025-03-05T12:24:12Z","title":"Conceptualizing Uncertainty","summary":"  Uncertainty in machine learning refers to the degree of confidence or lack\nthereof in a model's predictions. While uncertainty quantification methods\nexist, explanations of uncertainty, especially in high-dimensional settings,\nremain an open challenge. Existing work focuses on feature attribution\napproaches which are restricted to local explanations. Understanding\nuncertainty, its origins, and characteristics on a global scale is crucial for\nenhancing interpretability and trust in a model's predictions. In this work, we\npropose to explain the uncertainty in high-dimensional data classification\nsettings by means of concept activation vectors which give rise to local and\nglobal explanations of uncertainty. We demonstrate the utility of the generated\nexplanations by leveraging them to refine and improve our model.\n","authors":["Isaac Roberts","Alexander Schulz","Sarah Schroeder","Fabian Hinder","Barbara Hammer"],"pdf_url":"https://arxiv.org/pdf/2503.03443v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.10069v2","updated":"2025-03-05T12:22:23Z","published":"2025-01-17T09:42:48Z","title":"A Survey on LLM Test-Time Compute via Search: Tasks, LLM Profiling,\n  Search Algorithms, and Relevant Frameworks","summary":"  LLM test-time compute (or LLM inference) via search has emerged as a\npromising research area with rapid developments. However, current frameworks\noften adopt distinct perspectives on three key aspects (task definition, LLM\nprofiling, and search procedures), making direct comparisons challenging.\nMoreover, the search algorithms employed often diverge from standard\nimplementations, and their specific characteristics are not thoroughly\nspecified. In this survey, we provide a comprehensive technical review that\nunifies task definitions and provides modular definitions of LLM profiling and\nsearch procedures. The definitions enable precise comparisons of various LLM\ninference frameworks while highlighting their departures from conventional\nsearch algorithms. We also discuss the applicability, performance, and\nefficiency of these methods. We have updated our content to include the latest\npapers, and the differences between versions are highlighted in the appendix.\nFor further details and ongoing updates, please refer to our GitHub repository:\nhttps://github.com/xinzhel/LLM-Agent-Survey/blob/main/search.md\n","authors":["Xinzhe Li"],"pdf_url":"https://arxiv.org/pdf/2501.10069v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03434v1","updated":"2025-03-05T12:10:14Z","published":"2025-03-05T12:10:14Z","title":"RASD: Retrieval-Augmented Speculative Decoding","summary":"  Speculative decoding accelerates inference in large language models (LLMs) by\ngenerating draft tokens for target model verification. Current approaches for\nobtaining draft tokens rely on lightweight draft models or additional model\nstructures to generate draft tokens and retrieve context from databases. Due to\nthe draft model's small size and limited training data, model-based speculative\ndecoding frequently becomes less effective in out-of-domain scenarios.\nAdditionally, the time cost of the drafting phase results in a low upper limit\non acceptance length during the verification step, limiting overall efficiency.\nThis paper proposes RASD (Retrieval-Augmented Speculative Decoding), which\nadopts retrieval methods to enhance model-based speculative decoding. We\nintroduce tree pruning and tree fusion to achieve this. Specifically, we\ndevelop a pruning method based on the draft model's probability distribution to\nconstruct the optimal retrieval tree. Second, we employ the longest prefix\nmatching algorithm to merge the tree generated by the draft model with the\nretrieval tree, resulting in a unified tree for verification. Experimental\nresults demonstrate that RASD achieves state-of-the-art inference acceleration\nacross tasks such as DocQA, Summary, Code, and In-Domain QA. Moreover, RASD\nexhibits strong scalability, seamlessly integrating with various speculative\ndecoding approaches, including both generation-based and retrieval-based\nmethods.\n","authors":["Guofeng Quan","Wenfeng Feng","Chuzhan Hao","Guochao Jiang","Yuewei Zhang","Hao Wang"],"pdf_url":"https://arxiv.org/pdf/2503.03434v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03428v1","updated":"2025-03-05T12:01:22Z","published":"2025-03-05T12:01:22Z","title":"Privacy is All You Need: Revolutionizing Wearable Health Data with\n  Advanced PETs","summary":"  In a world where data is the new currency, wearable health devices offer\nunprecedented insights into daily life, continuously monitoring vital signs and\nmetrics. However, this convenience raises privacy concerns, as these devices\ncollect sensitive data that can be misused or breached. Traditional measures\noften fail due to real-time data processing needs and limited device power.\nUsers also lack awareness and control over data sharing and usage. We propose a\nPrivacy-Enhancing Technology (PET) framework for wearable devices, integrating\nfederated learning, lightweight cryptographic methods, and selectively deployed\nblockchain technology. The blockchain acts as a secure ledger triggered only\nupon data transfer requests, granting users real-time notifications and\ncontrol. By dismantling data monopolies, this approach returns data sovereignty\nto individuals. Through real-world applications like secure medical data\nsharing, privacy-preserving fitness tracking, and continuous health monitoring,\nour framework reduces privacy risks by up to 70 percent while preserving data\nutility and performance. This innovation sets a new benchmark for wearable\nprivacy and can scale to broader IoT ecosystems, including smart homes and\nindustry. As data continues to shape our digital landscape, our research\nunderscores the critical need to maintain privacy and user control at the\nforefront of technological progress.\n","authors":["Karthik Barma","Seshu Babu Barma"],"pdf_url":"https://arxiv.org/pdf/2503.03428v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.18377v3","updated":"2025-03-05T11:49:36Z","published":"2024-12-24T12:03:36Z","title":"ChaI-TeA: A Benchmark for Evaluating Autocompletion of Interactions with\n  LLM-based Chatbots","summary":"  The rise of LLMs has deflected a growing portion of human-computer\ninteractions towards LLM-based chatbots. The remarkable abilities of these\nmodels allow users to interact using long, diverse natural language text\ncovering a wide range of topics and styles. Phrasing these messages is a time\nand effort consuming task, calling for an autocomplete solution to assist\nusers. We introduce the task of chatbot interaction autocomplete. We present\nChaI-TeA: CHat InTEraction Autocomplete; An autcomplete evaluation framework\nfor LLM-based chatbot interactions. The framework includes a formal definition\nof the task, coupled with suitable datasets and metrics. We use the framework\nto evaluate After formally defining the task along with suitable datasets and\nmetrics, we test 9 models on the defined auto completion task, finding that\nwhile current off-the-shelf models perform fairly, there is still much room for\nimprovement, mainly in ranking of the generated suggestions. We provide\ninsights for practitioners working on this task and open new research\ndirections for researchers in the field. We release our framework to serve as a\nfoundation for future research.\n","authors":["Shani Goren","Oren Kalinsky","Tomer Stav","Yuri Rapoport","Yaron Fairstein","Ram Yazdi","Nachshon Cohen","Alexander Libov","Guy Kushilevitz"],"pdf_url":"https://arxiv.org/pdf/2412.18377v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03418v1","updated":"2025-03-05T11:47:41Z","published":"2025-03-05T11:47:41Z","title":"Simplicial SMOTE: Oversampling Solution to the Imbalanced Learning\n  Problem","summary":"  SMOTE (Synthetic Minority Oversampling Technique) is the established\ngeometric approach to random oversampling to balance classes in the imbalanced\nlearning problem, followed by many extensions. Its idea is to introduce\nsynthetic data points of the minor class, with each new point being the convex\ncombination of an existing data point and one of its k-nearest neighbors. In\nthis paper, by viewing SMOTE as sampling from the edges of a geometric\nneighborhood graph and borrowing tools from the topological data analysis, we\npropose a novel technique, Simplicial SMOTE, that samples from the simplices of\na geometric neighborhood simplicial complex. A new synthetic point is defined\nby the barycentric coordinates w.r.t. a simplex spanned by an arbitrary number\nof data points being sufficiently close rather than a pair. Such a replacement\nof the geometric data model results in better coverage of the underlying data\ndistribution compared to existing geometric sampling methods and allows the\ngeneration of synthetic points of the minority class closer to the majority\nclass on the decision boundary. We experimentally demonstrate that our\nSimplicial SMOTE outperforms several popular geometric sampling methods,\nincluding the original SMOTE. Moreover, we show that simplicial sampling can be\neasily integrated into existing SMOTE extensions. We generalize and evaluate\nsimplicial extensions of the classic Borderline SMOTE, Safe-level SMOTE, and\nADASYN algorithms, all of which outperform their graph-based counterparts.\n","authors":["Oleg Kachan","Andrey Savchenko","Gleb Gusev"],"pdf_url":"https://arxiv.org/pdf/2503.03418v1.pdf","comment":"Accepted at KDD 2025 (research track)"},{"id":"http://arxiv.org/abs/2503.03417v1","updated":"2025-03-05T11:47:32Z","published":"2025-03-05T11:47:32Z","title":"When Claims Evolve: Evaluating and Enhancing the Robustness of Embedding\n  Models Against Misinformation Edits","summary":"  Online misinformation remains a critical challenge, and fact-checkers\nincreasingly rely on embedding-based methods to retrieve relevant fact-checks.\nYet, when debunked claims reappear in edited forms, the performance of these\nmethods is unclear. In this work, we introduce a taxonomy of six common\nreal-world misinformation edits and propose a perturbation framework that\ngenerates valid, natural claim variations. Our multi-stage retrieval evaluation\nreveals that standard embedding models struggle with user-introduced edits,\nwhile LLM-distilled embeddings offer improved robustness at a higher\ncomputational cost. Although a strong reranker helps mitigate some issues, it\ncannot fully compensate for first-stage retrieval gaps. Addressing these\nretrieval gaps, our train- and inference-time mitigation approaches enhance\nin-domain robustness by up to 17 percentage points and boost out-of-domain\ngeneralization by 10 percentage points over baseline models. Overall, our\nfindings provide practical improvements to claim-matching systems, enabling\nmore reliable fact-checking of evolving misinformation.\n","authors":["Jabez Magomere","Emanuele La Malfa","Manuel Tonneau","Ashkan Kazemi","Scott Hale"],"pdf_url":"https://arxiv.org/pdf/2503.03417v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03410v1","updated":"2025-03-05T11:39:15Z","published":"2025-03-05T11:39:15Z","title":"Augmentation-Based Deep Learning for Identification of Circulating Tumor\n  Cells","summary":"  Circulating tumor cells (CTCs) are crucial biomarkers in liquid biopsy,\noffering a noninvasive tool for cancer patient management. However, their\nidentification remains particularly challenging due to their limited number and\nheterogeneity. Labeling samples for contrast limits the generalization of\nfluorescence-based methods across different hospital datasets. Analyzing\nsingle-cell images enables detailed assessment of cell morphology, subcellular\nstructures, and phenotypic variations, often hidden in clustered images.\nDeveloping a method based on bright-field single-cell analysis could overcome\nthese limitations. CTCs can be isolated using an unbiased workflow combining\nParsortix technology, which selects cells based on size and deformability, with\nDEPArray technology, enabling precise visualization and selection of single\ncells. Traditionally, DEPArray-acquired digital images are manually analyzed,\nmaking the process time-consuming and prone to variability. In this study, we\npresent a Deep Learning-based classification pipeline designed to distinguish\nCTCs from leukocytes in blood samples, aimed to enhance diagnostic accuracy and\noptimize clinical workflows. Our approach employs images from the bright-field\nchannel acquired through DEPArray technology leveraging a ResNet-based CNN. To\nimprove model generalization, we applied three types of data augmentation\ntechniques and incorporated fluorescence (DAPI) channel images into the\ntraining phase, allowing the network to learn additional CTC-specific features.\nNotably, only bright-field images have been used for testing, ensuring the\nmodel's ability to identify CTCs without relying on fluorescence markers. The\nproposed model achieved an F1-score of 0.798, demonstrating its capability to\ndistinguish CTCs from leukocytes. These findings highlight the potential of DL\nin refining CTC analysis and advancing liquid biopsy applications.\n","authors":["Martina Russo","Giulia Bertolini","Vera Cappelletti","Cinzia De Marco","Serena Di Cosimo","Petra Paiè","Nadia Brancati"],"pdf_url":"https://arxiv.org/pdf/2503.03410v1.pdf","comment":"20 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2412.14566v2","updated":"2025-03-05T11:38:00Z","published":"2024-12-19T06:35:54Z","title":"AIArena: A Blockchain-Based Decentralized AI Training Platform","summary":"  The rapid advancement of AI has underscored critical challenges in its\ndevelopment and implementation, largely due to centralized control by a few\nmajor corporations. This concentration of power intensifies biases within AI\nmodels, resulting from inadequate governance and oversight mechanisms.\nAdditionally, it limits public involvement and heightens concerns about the\nintegrity of model generation. Such monopolistic control over data and AI\noutputs threatens both innovation and fair data usage, as users inadvertently\ncontribute data that primarily benefits these corporations. In this work, we\npropose AIArena, a blockchain-based decentralized AI training platform designed\nto democratize AI development and alignment through on-chain incentive\nmechanisms. AIArena fosters an open and collaborative environment where\nparticipants can contribute models and computing resources. Its on-chain\nconsensus mechanism ensures fair rewards for participants based on their\ncontributions. We instantiate and implement AIArena on the public Base\nblockchain Sepolia testnet, and the evaluation results demonstrate the\nfeasibility of AIArena in real-world applications.\n","authors":["Zhipeng Wang","Rui Sun","Elizabeth Lui","Tuo Zhou","Yizhe Wen","Jiahao Sun"],"pdf_url":"https://arxiv.org/pdf/2412.14566v2.pdf","comment":"Camera ready version. Accepted by the ACM Web Conference (WWW), 2025"},{"id":"http://arxiv.org/abs/2503.03395v1","updated":"2025-03-05T11:19:17Z","published":"2025-03-05T11:19:17Z","title":"AI-Driven Multi-Stage Computer Vision System for Defect Detection in\n  Laser-Engraved Industrial Nameplates","summary":"  Automated defect detection in industrial manufacturing is essential for\nmaintaining product quality and minimizing production errors. In air disc brake\nmanufacturing, ensuring the precision of laser-engraved nameplates is crucial\nfor accurate product identification and quality control. Engraving errors, such\nas misprints or missing characters, can compromise both aesthetics and\nfunctionality, leading to material waste and production delays. This paper\npresents a proof of concept for an AI-driven computer vision system that\ninspects and verifies laser-engraved nameplates, detecting defects in logos and\nalphanumeric strings. The system integrates object detection using YOLOv7,\noptical character recognition (OCR) with Tesseract, and anomaly detection\nthrough a residual variational autoencoder (ResVAE) along with other computer\nvision methods to enable comprehensive inspections at multiple stages.\nExperimental results demonstrate the system's effectiveness, achieving 91.33%\naccuracy and 100% recall, ensuring that defective nameplates are consistently\ndetected and addressed. This solution highlights the potential of AI-driven\nvisual inspection to enhance quality control, reduce manual inspection efforts,\nand improve overall manufacturing efficiency.\n","authors":["Adhish Anitha Vilasan","Stephan Jäger","Noah Klarmann"],"pdf_url":"https://arxiv.org/pdf/2503.03395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.01243v3","updated":"2025-03-05T11:17:18Z","published":"2024-12-02T08:05:26Z","title":"Schedule On the Fly: Diffusion Time Prediction for Faster and Better\n  Image Generation","summary":"  Diffusion and flow matching models have achieved remarkable success in\ntext-to-image generation. However, these models typically rely on the\npredetermined denoising schedules for all prompts. The multi-step reverse\ndiffusion process can be regarded as a kind of chain-of-thought for generating\nhigh-quality images step by step. Therefore, diffusion models should reason for\neach instance to adaptively determine the optimal noise schedule, achieving\nhigh generation quality with sampling efficiency. In this paper, we introduce\nthe Time Prediction Diffusion Model (TPDM) for this. TPDM employs a\nplug-and-play Time Prediction Module (TPM) that predicts the next noise level\nbased on current latent features at each denoising step. We train the TPM using\nreinforcement learning to maximize a reward that encourages high final image\nquality while penalizing excessive denoising steps. With such an adaptive\nscheduler, TPDM not only generates high-quality images that are aligned closely\nwith human preferences but also adjusts diffusion time and the number of\ndenoising steps on the fly, enhancing both performance and efficiency. With\nStable Diffusion 3 Medium architecture, TPDM achieves an aesthetic score of\n5.44 and a human preference score (HPS) of 29.59, while using around 50% fewer\ndenoising steps to achieve better performance.\n","authors":["Zilyu Ye","Zhiyang Chen","Tiancheng Li","Zemin Huang","Weijian Luo","Guo-Jun Qi"],"pdf_url":"https://arxiv.org/pdf/2412.01243v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14507v2","updated":"2025-03-05T11:15:58Z","published":"2024-08-24T16:54:08Z","title":"Prompt-Matcher: Leveraging Large Models to Reduce Uncertainty in Schema\n  Matching Results","summary":"  Schema matching is the process of identifying correspondences between the\nelements of two given schemata, essential for database management systems, data\nintegration, and data warehousing. For datasets across different scenarios, the\noptimal schema matching algorithm is different. For single algorithm,\nhyperparameter tuning also cases multiple results. All results assigned equal\nprobabilities are stored in probabilistic databases to facilitate uncertainty\nmanagement. The substantial degree of uncertainty diminishes the efficiency and\nreliability of data processing, thereby precluding the provision of more\naccurate information for decision-makers. To address this problem, we introduce\na new approach based on fine-grained correspondence verification with specific\nprompt of Large Language Model.\n  Our approach is an iterative loop that consists of three main components: (1)\nthe correspondence selection algorithm, (2) correspondence verification, and\n(3) the update of probability distribution. The core idea is that\ncorrespondences intersect across multiple results, thereby linking the\nverification of correspondences to the reduction of uncertainty in candidate\nresults.\n  The task of selecting an optimal correspondence set to maximize the\nanticipated uncertainty reduction within a fixed budgetary framework is\nestablished as an NP-hard problem. We propose a novel $(1-1/e)$-approximation\nalgorithm that significantly outperforms brute algorithm in terms of\ncomputational efficiency. To enhance correspondence verification, we have\ndeveloped two prompt templates that enable GPT-4 to achieve state-of-the-art\nperformance across two established benchmark datasets. Our comprehensive\nexperimental evaluation demonstrates the superior effectiveness and robustness\nof the proposed approach.\n","authors":["Longyu Feng","Huahang Li","Chen Jason Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.14507v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14153v2","updated":"2025-03-05T11:15:39Z","published":"2024-08-26T09:55:34Z","title":"Explaining Vision-Language Similarities in Dual Encoders with\n  Feature-Pair Attributions","summary":"  Dual encoder architectures like CLIP models map two types of inputs into a\nshared embedding space and predict similarities between them. Despite their\nsuccess, it is, however, not understood how these models compare their two\ninputs. Common first-order feature-attribution methods can only provide limited\ninsights into dual-encoders since their predictions depend on\nfeature-interactions rather than on individual features. In this paper, we\nfirst derive a second-order method enabling the attribution of predictions by\nany differentiable dual encoder onto feature-interactions between its inputs.\nSecond, we apply our method to CLIP models and show that they learn\nfine-grained correspondences between parts of captions and regions in images.\nThey match objects across input modes also account for mismatches. This\nvisual-linguistic grounding ability, however, varies heavily between object\nclasses and exhibits pronounced out-of-domain effects. We can identify\nindividual errors as well as systematic failure categories including object\ncoverage, unusual scenes and correlated contexts.\n","authors":["Lucas Möller","Pascal Tilli","Ngoc Thang Vu","Sebastian Padó"],"pdf_url":"https://arxiv.org/pdf/2408.14153v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03391v1","updated":"2025-03-05T11:12:40Z","published":"2025-03-05T11:12:40Z","title":"Multi-Agent DRL for Queue-Aware Task Offloading in Hierarchical\n  MEC-Enabled Air-Ground Networks","summary":"  Mobile edge computing (MEC)-enabled air-ground networks are a key component\nof 6G, employing aerial base stations (ABSs) such as unmanned aerial vehicles\n(UAVs) and high-altitude platform stations (HAPS) to provide dynamic services\nto ground IoT devices (IoTDs). These IoTDs support real-time applications\n(e.g., multimedia and Metaverse services) that demand high computational\nresources and strict quality of service (QoS) guarantees in terms of latency\nand task queue management. Given their limited energy and processing\ncapabilities, IoTDs rely on UAVs and HAPS to offload tasks for distributed\nprocessing, forming a multi-tier MEC system. This paper tackles the overall\nenergy minimization problem in MEC-enabled air-ground integrated networks\n(MAGIN) by jointly optimizing UAV trajectories, computing resource allocation,\nand queue-aware task offloading decisions. The optimization is challenging due\nto the nonconvex, nonlinear nature of this hierarchical system, which renders\ntraditional methods ineffective. We reformulate the problem as a multi-agent\nMarkov decision process (MDP) with continuous action spaces and heterogeneous\nagents, and propose a novel variant of multi-agent proximal policy optimization\nwith a Beta distribution (MAPPO-BD) to solve it. Extensive simulations show\nthat MAPPO-BD outperforms baseline schemes, achieving superior energy savings\nand efficient resource management in MAGIN while meeting queue delay and edge\ncomputing constraints.\n","authors":["Muhammet Hevesli","Abegaz Mohammed Seid","Aiman Erbad","Mohamed Abdallah"],"pdf_url":"https://arxiv.org/pdf/2503.03391v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2206.09453v2","updated":"2025-03-05T11:04:58Z","published":"2022-06-19T17:13:58Z","title":"Bounding Evidence and Estimating Log-Likelihood in VAE","summary":"  Many crucial problems in deep learning and statistical inference are caused\nby a variational gap, i.e., a difference between model evidence\n(log-likelihood) and evidence lower bound (ELBO). In particular, in a classical\nVAE setting that involves training via an ELBO cost function, it is difficult\nto provide a robust comparison of the effects of training between models, since\nwe do not know a log-likelihood of data (but only its lower bound). In this\npaper, to deal with this problem, we introduce a general and effective upper\nbound, which allows us to efficiently approximate the evidence of data. We\nprovide extensive theoretical and experimental studies of our approach,\nincluding its comparison to the other state-of-the-art upper bounds, as well as\nits application as a tool for the evaluation of models that were trained on\nvarious lower bounds.\n","authors":["Łukasz Struski","Marcin Mazur","Paweł Batorski","Przemysław Spurek","Jacek Tabor"],"pdf_url":"https://arxiv.org/pdf/2206.09453v2.pdf","comment":"Paper accepted for AISTATS 2023"},{"id":"http://arxiv.org/abs/2502.15425v4","updated":"2025-03-05T10:48:42Z","published":"2025-02-21T12:52:16Z","title":"TAG: A Decentralized Framework for Multi-Agent Hierarchical\n  Reinforcement Learning","summary":"  Hierarchical organization is fundamental to biological systems and human\nsocieties, yet artificial intelligence systems often rely on monolithic\narchitectures that limit adaptability and scalability. Current hierarchical\nreinforcement learning (HRL) approaches typically restrict hierarchies to two\nlevels or require centralized training, which limits their practical\napplicability. We introduce TAME Agent Framework (TAG), a framework for\nconstructing fully decentralized hierarchical multi-agent systems. TAG enables\nhierarchies of arbitrary depth through a novel LevelEnv concept, which\nabstracts each hierarchy level as the environment for the agents above it. This\napproach standardizes information flow between levels while preserving loose\ncoupling, allowing for seamless integration of diverse agent types. We\ndemonstrate the effectiveness of TAG by implementing hierarchical architectures\nthat combine different RL agents across multiple levels, achieving improved\nperformance over classical multi-agent RL baselines on standard benchmarks. Our\nresults show that decentralized hierarchical organization enhances both\nlearning speed and final performance, positioning TAG as a promising direction\nfor scalable multi-agent systems.\n","authors":["Giuseppe Paolo","Abdelhakim Benechehab","Hamza Cherkaoui","Albert Thomas","Balázs Kégl"],"pdf_url":"https://arxiv.org/pdf/2502.15425v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.03565v2","updated":"2025-03-05T10:47:17Z","published":"2024-10-04T16:15:31Z","title":"Exploration Implies Data Augmentation: Reachability and Generalisation\n  in Contextual MDPs","summary":"  In the zero-shot policy transfer (ZSPT) setting for contextual Markov\ndecision processes (MDP), agents train on a fixed set of contexts and must\ngeneralise to new ones. Recent work has argued and demonstrated that increased\nexploration can improve this generalisation, by training on more states in the\ntraining contexts. In this paper, we demonstrate that training on more states\ncan indeed improve generalisation, but can come at a cost of reducing the\naccuracy of the learned value function which should not benefit generalisation.\nWe introduce reachability in the ZSPT setting to define which states/contexts\nrequire generalisation and explain why exploration can improve it. We\nhypothesise and demonstrate that using exploration to increase the agent's\ncoverage while also increasing the accuracy improves generalisation even more.\nInspired by this, we propose a method Explore-Go that implements an exploration\nphase at the beginning of each episode, which can be combined with existing on-\nand off-policy RL algorithms and significantly improves generalisation even in\npartially observable MDPs. We demonstrate the effectiveness of Explore-Go when\ncombined with several popular algorithms and show an increase in generalisation\nperformance across several environments. With this, we hope to provide\npractitioners with a simple modification that can improve the generalisation of\ntheir agents.\n","authors":["Max Weltevrede","Caroline Horsch","Matthijs T. J. Spaan","Wendelin Böhmer"],"pdf_url":"https://arxiv.org/pdf/2410.03565v2.pdf","comment":"arXiv admin note: text overlap with arXiv:2406.08069"},{"id":"http://arxiv.org/abs/2503.03361v1","updated":"2025-03-05T10:40:19Z","published":"2025-03-05T10:40:19Z","title":"From Infants to AI: Incorporating Infant-like Learning in Models Boosts\n  Efficiency and Generalization in Learning Social Prediction Tasks","summary":"  Early in development, infants learn a range of useful concepts, which can be\nchallenging from a computational standpoint. This early learning comes together\nwith an initial understanding of aspects of the meaning of concepts, e.g.,\ntheir implications, causality, and using them to predict likely future events.\nAll this is accomplished in many cases with little or no supervision, and from\nrelatively few examples, compared with current network models. In learning\nabout objects and human-object interactions, early acquired and possibly innate\nconcepts are often used in the process of learning additional, more complex\nconcepts. In the current work, we model how early-acquired concepts are used in\nthe learning of subsequent concepts, and compare the results with standard deep\nnetwork modeling. We focused in particular on the use of the concepts of\nanimacy and goal attribution in learning to predict future events. We show that\nthe use of early concepts in the learning of new concepts leads to better\nlearning (higher accuracy) and more efficient learning (requiring less data).\nWe further show that this integration of early and new concepts shapes the\nrepresentation of the concepts acquired by the model. The results show that\nwhen the concepts were learned in a human-like manner, the emerging\nrepresentation was more useful, as measured in terms of generalization to novel\ndata and tasks. On a more general level, the results suggest that there are\nlikely to be basic differences in the conceptual structures acquired by current\nnetwork models compared to human learning.\n","authors":["Shify Treger","Shimon Ullman"],"pdf_url":"https://arxiv.org/pdf/2503.03361v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03360v1","updated":"2025-03-05T10:40:09Z","published":"2025-03-05T10:40:09Z","title":"Transformers for molecular property prediction: Domain adaptation\n  efficiently improves performance","summary":"  Most of the current transformer-based chemical language models are\npre-trained on millions to billions of molecules. However, the improvement from\nsuch scaling in dataset size is not confidently linked to improved molecular\nproperty prediction. The aim of this study is to investigate and overcome some\nof the limitations of transformer models in predicting molecular properties.\nSpecifically, we examine the impact of pre-training dataset size and diversity\non the performance of transformer models and investigate the use of domain\nadaptation as a technique for improving model performance. First, our findings\nindicate that increasing pretraining dataset size beyond 400K molecules from\nthe GuacaMol dataset does not result in a significant improvement on four ADME\nendpoints, namely, solubility, permeability, microsomal stability, and plasma\nprotein binding. Second, our results demonstrate that using domain adaptation\nby further training the transformer model on a small set of domain-relevant\nmolecules, i.e., a few hundred to a few thousand, using multi-task regression\nof physicochemical properties was sufficient to significantly improve\nperformance for three out of the four investigated ADME endpoints (P-value <\n0.001). Finally, we observe that a model pre-trained on 400K molecules and\ndomain adopted on a few hundred/thousand molecules performs similarly (P-value\n> 0.05) to more complicated transformer models like MolBERT(pre-trained on 1.3M\nmolecules) and MolFormer (pre-trained on 100M molecules). A comparison to a\nrandom forest model trained on basic physicochemical properties showed similar\nperformance to the examined transformer models. We believe that current\ntransformer models can be improved through further systematic analysis of\npre-training and downstream data, pre-training objectives, and scaling laws,\nultimately leading to better and more helpful models.\n","authors":["Afnan Sultan","Max Rausch-Dupont","Shahrukh Khan","Olga Kalinina","Andrea Volkamer","Dietrich Klakow"],"pdf_url":"https://arxiv.org/pdf/2503.03360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03350v1","updated":"2025-03-05T10:22:49Z","published":"2025-03-05T10:22:49Z","title":"Leveraging Large Language Models to Develop Heuristics for Emerging\n  Optimization Problems","summary":"  Combinatorial optimization problems often rely on heuristic algorithms to\ngenerate efficient solutions. However, the manual design of heuristics is\nresource-intensive and constrained by the designer's expertise. Recent advances\nin artificial intelligence, particularly large language models (LLMs), have\ndemonstrated the potential to automate heuristic generation through\nevolutionary frameworks. Recent works focus only on well-known combinatorial\noptimization problems like the traveling salesman problem and online bin\npacking problem when designing constructive heuristics. This study investigates\nwhether LLMs can effectively generate heuristics for niche, not yet broadly\nresearched optimization problems, using the unit-load pre-marshalling problem\nas an example case. We propose the Contextual Evolution of Heuristics (CEoH)\nframework, an extension of the Evolution of Heuristics (EoH) framework, which\nincorporates problem-specific descriptions to enhance in-context learning\nduring heuristic generation. Through computational experiments, we evaluate\nCEoH and EoH and compare the results. Results indicate that CEoH enables\nsmaller LLMs to generate high-quality heuristics more consistently and even\noutperform larger models. Larger models demonstrate robust performance with or\nwithout contextualized prompts. The generated heuristics exhibit scalability to\ndiverse instance configurations.\n","authors":["Thomas Bömer","Nico Koltermann","Max Disselnmeyer","Laura Dörr","Anne Meyer"],"pdf_url":"https://arxiv.org/pdf/2503.03350v1.pdf","comment":"Under review LION19: The 19th Learning and Intelligent OptimizatioN\n  Conference"},{"id":"http://arxiv.org/abs/2412.03076v2","updated":"2025-03-05T10:19:05Z","published":"2024-12-04T06:53:59Z","title":"Coordinated Multi-Armed Bandits for Improved Spatial Reuse in Wi-Fi","summary":"  Multi-Access Point Coordination (MAPC) and Artificial Intelligence and\nMachine Learning (AI/ML) are expected to be key features in future Wi-Fi, such\nas the forthcoming IEEE 802.11bn (Wi-Fi~8) and beyond. In this paper, we\nexplore a coordinated solution based on online learning to drive the\noptimization of Spatial Reuse (SR), a method that allows multiple devices to\nperform simultaneous transmissions by controlling interference through Packet\nDetect (PD) adjustment and transmit power control. In particular, we focus on a\nMulti-Agent Multi-Armed Bandit (MA-MAB) setting, where multiple decision-making\nagents concurrently configure SR parameters from coexisting networks by\nleveraging the MAPC framework, and study various algorithms and reward-sharing\nmechanisms. We evaluate different MA-MAB implementations using Komondor, a\nwell-adopted Wi-Fi simulator, and demonstrate that AI-native SR enabled by\ncoordinated MABs can improve the network performance over current Wi-Fi\noperation: mean throughput increases by 15%, fairness is improved by increasing\nthe minimum throughput across the network by 210%, while the maximum access\ndelay is kept below 3 ms.\n","authors":["Francesc Wilhelmi","Boris Bellalta","Szymon Szott","Katarzyna Kosek-Szott","Sergio Barrachina-Muñoz"],"pdf_url":"https://arxiv.org/pdf/2412.03076v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03338v1","updated":"2025-03-05T10:12:22Z","published":"2025-03-05T10:12:22Z","title":"Navigating Intelligence: A Survey of Google OR-Tools and Machine\n  Learning for Global Path Planning in Autonomous Vehicles","summary":"  We offer a new in-depth investigation of global path planning (GPP) for\nunmanned ground vehicles, an autonomous mining sampling robot named ROMIE. GPP\nis essential for ROMIE's optimal performance, which is translated into solving\nthe traveling salesman problem, a complex graph theory challenge that is\ncrucial for determining the most effective route to cover all sampling\nlocations in a mining field. This problem is central to enhancing ROMIE's\noperational efficiency and competitiveness against human labor by optimizing\ncost and time. The primary aim of this research is to advance GPP by\ndeveloping, evaluating, and improving a cost-efficient software and web\napplication. We delve into an extensive comparison and analysis of Google\noperations research (OR)-Tools optimization algorithms. Our study is driven by\nthe goal of applying and testing the limits of OR-Tools capabilities by\nintegrating Reinforcement Learning techniques for the first time. This enables\nus to compare these methods with OR-Tools, assessing their computational\neffectiveness and real-world application efficiency. Our analysis seeks to\nprovide insights into the effectiveness and practical application of each\ntechnique. Our findings indicate that Q-Learning stands out as the optimal\nstrategy, demonstrating superior efficiency by deviating only 1.2% on average\nfrom the optimal solutions across our datasets.\n","authors":["Alexandre Benoit","Pedram Asef"],"pdf_url":"https://arxiv.org/pdf/2503.03338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07804v3","updated":"2025-03-05T10:09:25Z","published":"2024-12-09T09:04:02Z","title":"XLSTM-HVED: Cross-Modal Brain Tumor Segmentation and MRI Reconstruction\n  Method Using Vision XLSTM and Heteromodal Variational Encoder-Decoder","summary":"  Neurogliomas are among the most aggressive forms of cancer, presenting\nconsiderable challenges in both treatment and monitoring due to their\nunpredictable biological behavior. Magnetic resonance imaging (MRI) is\ncurrently the preferred method for diagnosing and monitoring gliomas. However,\nthe lack of specific imaging techniques often compromises the accuracy of tumor\nsegmentation during the imaging process. To address this issue, we introduce\nthe XLSTM-HVED model. This model integrates a hetero-modal encoder-decoder\nframework with the Vision XLSTM module to reconstruct missing MRI modalities.\nBy deeply fusing spatial and temporal features, it enhances tumor segmentation\nperformance. The key innovation of our approach is the Self-Attention\nVariational Encoder (SAVE) module, which improves the integration of modal\nfeatures. Additionally, it optimizes the interaction of features between\nsegmentation and reconstruction tasks through the Squeeze-Fusion-Excitation\nCross Awareness (SFECA) module. Our experiments using the BraTS 2024 dataset\ndemonstrate that our model significantly outperforms existing advanced methods\nin handling cases where modalities are missing. Our source code is available at\nhttps://github.com/Quanato607/XLSTM-HVED.\n","authors":["Shenghao Zhu","Yifei Chen","Shuo Jiang","Weihong Chen","Chang Liu","Yuanhan Wang","Xu Chen","Yifan Ke","Feiwei Qin","Changmiao Wang","Zhu Zhu"],"pdf_url":"https://arxiv.org/pdf/2412.07804v3.pdf","comment":"5 pages, 2 figures"},{"id":"http://arxiv.org/abs/2408.01072v2","updated":"2025-03-05T10:03:08Z","published":"2024-08-02T07:47:51Z","title":"A Survey on Self-play Methods in Reinforcement Learning","summary":"  Self-play, characterized by agents' interactions with copies or past versions\nof themselves, has recently gained prominence in reinforcement learning (RL).\nThis paper first clarifies the preliminaries of self-play, including the\nmulti-agent reinforcement learning framework and basic game theory concepts.\nThen, it provides a unified framework and classifies existing self-play\nalgorithms within this framework. Moreover, the paper bridges the gap between\nthe algorithms and their practical implications by illustrating the role of\nself-play in different scenarios. Finally, the survey highlights open\nchallenges and future research directions in self-play. This paper is an\nessential guide map for understanding the multifaceted landscape of self-play\nin RL.\n","authors":["Ruize Zhang","Zelai Xu","Chengdong Ma","Chao Yu","Wei-Wei Tu","Wenhao Tang","Shiyu Huang","Deheng Ye","Wenbo Ding","Yaodong Yang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2408.01072v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03321v1","updated":"2025-03-05T09:55:07Z","published":"2025-03-05T09:55:07Z","title":"See What You Are Told: Visual Attention Sink in Large Multimodal Models","summary":"  Large multimodal models (LMMs) \"see\" images by leveraging the attention\nmechanism between text and visual tokens in the transformer decoder. Ideally,\nthese models should focus on key visual information relevant to the text token.\nHowever, recent findings indicate that LMMs have an extraordinary tendency to\nconsistently allocate high attention weights to specific visual tokens, even\nwhen these tokens are irrelevant to the corresponding text. In this study, we\ninvestigate the property behind the appearance of these irrelevant visual\ntokens and examine their characteristics. Our findings show that this behavior\narises due to the massive activation of certain hidden state dimensions, which\nresembles the attention sink found in language models. Hence, we refer to this\nphenomenon as the visual attention sink. In particular, our analysis reveals\nthat removing the irrelevant visual sink tokens does not impact model\nperformance, despite receiving high attention weights. Consequently, we recycle\nthe attention to these tokens as surplus resources, redistributing the\nattention budget to enhance focus on the image. To achieve this, we introduce\nVisual Attention Redistribution (VAR), a method that redistributes attention in\nimage-centric heads, which we identify as innately focusing on visual\ninformation. VAR can be seamlessly applied across different LMMs to improve\nperformance on a wide range of tasks, including general vision-language tasks,\nvisual hallucination tasks, and vision-centric tasks, all without the need for\nadditional training, models, or inference steps. Experimental results\ndemonstrate that VAR enables LMMs to process visual information more\neffectively by adjusting their internal attention mechanisms, offering a new\ndirection to enhancing the multimodal capabilities of LMMs.\n","authors":["Seil Kang","Jinyeong Kim","Junhyeok Kim","Seong Jae Hwang"],"pdf_url":"https://arxiv.org/pdf/2503.03321v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13149v3","updated":"2025-03-05T09:54:25Z","published":"2025-02-11T13:16:31Z","title":"Bi-Fact: A Bidirectional Factorization-based Evaluation of Intent\n  Extraction from UI Trajectories","summary":"  Evaluating intent extraction from GUIs demands accurate, fine-grained\nmetrics. This paper introduces Bi-Fact, a novel method that decomposes intents\ninto atomic facts and performs bidirectional comparisons to assess precision\nand recall. Experiments demonstrate Bi-Fact's superior correlation with human\njudgments compared to existing metrics, establishing a more robust evaluation\nframework for UI-driven intent understanding.\n","authors":["Sapir Caduri","Anatoly Efros","Noam Kahlon","Danielle Cohen","Yoni Halpern","Ido Dagan"],"pdf_url":"https://arxiv.org/pdf/2502.13149v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.03766v3","updated":"2025-03-05T09:52:30Z","published":"2024-11-06T08:59:44Z","title":"Number Cookbook: Number Understanding of Language Models and How to\n  Improve It","summary":"  Large language models (LLMs) can solve an increasing number of complex\nreasoning tasks while making surprising mistakes in basic numerical\nunderstanding and processing (such as 9.11 > 9.9). The latter ability is\nessential for tackling complex arithmetic and mathematical problems and serves\nas a foundation for most reasoning tasks, but previous work paid little\nattention to it or only discussed several restricted tasks (like integer\naddition). In this paper, we comprehensively investigate the numerical\nunderstanding and processing ability (NUPA) of LLMs. Firstly, we introduce a\nbenchmark covering four common numerical representations and 17 distinct\nnumerical tasks in four major categories, resulting in 41 meaningful\ncombinations in total. These tasks are derived from primary and secondary\neducation curricula, encompassing nearly all everyday numerical understanding\nand processing scenarios, and the rules of these tasks are very simple and\nclear. Through the benchmark, we find that current LLMs fail frequently in many\nof the tasks. To study the problem, we train small models with existing and\npotential techniques for enhancing NUPA (such as tokenizers, PEs, and number\nformats), comprehensively evaluating their effectiveness using our testbed. We\nalso finetune practical-scale LLMs on our proposed NUPA tasks and find that 1)\nnaive finetuning can improve NUPA a lot on many but not all tasks, and 2)\nsurprisingly, techniques designed to enhance NUPA prove ineffective for\nfinetuning pretrained models. We further explore the impact of chain-of-thought\ntechniques on NUPA. Our work provides a more detailed and comprehensive\nunderstanding of NUPA in LLMs. Our benchmark and code are released at\nhttps://github.com/GraphPKU/number_cookbook.\n","authors":["Haotong Yang","Yi Hu","Shijia Kang","Zhouchen Lin","Muhan Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.03766v3.pdf","comment":"ICLR 2025 poster"},{"id":"http://arxiv.org/abs/2501.05891v2","updated":"2025-03-05T09:18:31Z","published":"2025-01-10T11:44:35Z","title":"Affordably Fine-tuned LLMs Provide Better Answers to Course-specific\n  MCQs","summary":"  In education, the capability of generating human-like text of Large Language\nModels (LLMs) inspired work on how they can increase the efficiency of learning\nand teaching. We study the affordability of these models for educators and\nstudents by investigating how LLMs answer multiple-choice questions (MCQs) with\nrespect to hardware constraints and refinement techniques. We explore this\nspace by using generic pre-trained LLMs (the 7B, 13B, and 70B variants of\nLLaMA-2) to answer 162 undergraduate-level MCQs from a course on Programming\nLanguages (PL) -- the MCQ dataset is a contribution of this work, which we make\npublicly available. Specifically, we dissect how different factors, such as\nusing readily-available material -- (parts of) the course's textbook -- for\nfine-tuning and quantisation (to decrease resource usage) can change the\naccuracy of the responses. The main takeaway is that smaller textbook-based\nfine-tuned models outperform generic larger ones (whose pre-training requires\nconspicuous resources), making the usage of LLMs for answering MCQs resource-\nand material-wise affordable.\n","authors":["Bianca Raimondi","Saverio Giallorenzo","Maurizio Gabbrielli"],"pdf_url":"https://arxiv.org/pdf/2501.05891v2.pdf","comment":"The 40th ACM/SIGAPP Symposium On Applied Computing"},{"id":"http://arxiv.org/abs/2503.03283v1","updated":"2025-03-05T09:09:01Z","published":"2025-03-05T09:09:01Z","title":"Exploring specialization and sensitivity of convolutional neural\n  networks in the context of simultaneous image augmentations","summary":"  Drawing parallels with the way biological networks are studied, we adapt the\ntreatment--control paradigm to explainable artificial intelligence research and\nenrich it through multi-parametric input alterations. In this study, we propose\na framework for investigating the internal inference impacted by input data\naugmentations. The internal changes in network operation are reflected in\nactivation changes measured by variance, which can be decomposed into\ncomponents related to each augmentation, employing Sobol indices and Shapley\nvalues. These quantities enable one to visualize sensitivity to different\nvariables and use them for guided masking of activations. In addition, we\nintroduce a way of single-class sensitivity analysis where the candidates are\nfiltered according to their matching to prediction bias generated by targeted\ndamaging of the activations. Relying on the observed parallels, we assume that\nthe developed framework can potentially be transferred to studying biological\nneural networks in complex environments.\n","authors":["Pavel Kharyuk","Sergey Matveev","Ivan Oseledets"],"pdf_url":"https://arxiv.org/pdf/2503.03283v1.pdf","comment":"26 pages; main text: 5 figures, 4 tables; appendix: 4 sections, 3\n  tables; supplementary: 7 files (figures S1-S6: packed as 7z archive, S7:\n  single pdf file)"},{"id":"http://arxiv.org/abs/2412.12843v2","updated":"2025-03-05T09:03:18Z","published":"2024-12-17T12:11:04Z","title":"SLTNet: Efficient Event-based Semantic Segmentation with Spike-driven\n  Lightweight Transformer-based Networks","summary":"  Event-based semantic segmentation has great potential in autonomous driving\nand robotics due to the advantages of event cameras, such as high dynamic\nrange, low latency, and low power cost. Unfortunately, current artificial\nneural network (ANN)-based segmentation methods suffer from high computational\ndemands, the requirements for image frames, and massive energy consumption,\nlimiting their efficiency and application on resource-constrained edge/mobile\nplatforms. To address these problems, we introduce SLTNet, a spike-driven\nlightweight transformer-based network designed for event-based semantic\nsegmentation. Specifically, SLTNet is built on efficient spike-driven\nconvolution blocks (SCBs) to extract rich semantic features while reducing the\nmodel's parameters. Then, to enhance the long-range contextural feature\ninteraction, we propose novel spike-driven transformer blocks (STBs) with\nbinary mask operations. Based on these basic blocks, SLTNet employs a\nhigh-efficiency single-branch architecture while maintaining the low energy\nconsumption of the Spiking Neural Network (SNN). Finally, extensive experiments\non DDD17 and DSEC-Semantic datasets demonstrate that SLTNet outperforms\nstate-of-the-art (SOTA) SNN-based methods by at most 9.06% and 9.39% mIoU,\nrespectively, with extremely 4.58x lower energy consumption and 114 FPS\ninference speed. Our code is open-sourced and available at\nhttps://github.com/longxianlei/SLTNet-v1.0.\n","authors":["Xiaxin Zhu","Fangming Guo","Xianlei Long","Qingyi Gu","Chao Chen","Fuqiang Gu"],"pdf_url":"https://arxiv.org/pdf/2412.12843v2.pdf","comment":"Submitted to 2025 IEEE/RSJ International Conference on Intelligent\n  Robots and Systems (IROS 2025)"},{"id":"http://arxiv.org/abs/2503.03274v1","updated":"2025-03-05T08:56:26Z","published":"2025-03-05T08:56:26Z","title":"Benchmarking Dynamic SLO Compliance in Distributed Computing Continuum\n  Systems","summary":"  Ensuring Service Level Objectives (SLOs) in large-scale architectures, such\nas Distributed Computing Continuum Systems (DCCS), is challenging due to their\nheterogeneous nature and varying service requirements across different devices\nand applications. Additionally, unpredictable workloads and resource\nlimitations lead to fluctuating performance and violated SLOs. To improve SLO\ncompliance in DCCS, one possibility is to apply machine learning; however, the\ndesign choices are often left to the developer. To that extent, we provide a\nbenchmark of Active Inference -- an emerging method from neuroscience --\nagainst three established reinforcement learning algorithms (Deep Q-Network,\nAdvantage Actor-Critic, and Proximal Policy Optimization). We consider a\nrealistic DCCS use case: an edge device running a video conferencing\napplication alongside a WebSocket server streaming videos. Using one of the\nrespective algorithms, we continuously monitor key performance metrics, such as\nlatency and bandwidth usage, to dynamically adjust parameters -- including the\nnumber of streams, frame rate, and resolution -- to optimize service quality\nand user experience. To test algorithms' adaptability to constant system\nchanges, we simulate dynamically changing SLOs and both instant and gradual\ndata-shift scenarios, such as network bandwidth limitations and fluctuating\ndevice thermal states. Although the evaluated algorithms all showed advantages\nand limitations, our findings demonstrate that Active Inference is a promising\napproach for ensuring SLO compliance in DCCS, offering lower memory usage,\nstable CPU utilization, and fast convergence.\n","authors":["Alfreds Lapkovskis","Boris Sedlak","Sindri Magnússon","Schahram Dustdar","Praveen Kumar Donta"],"pdf_url":"https://arxiv.org/pdf/2503.03274v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03269v1","updated":"2025-03-05T08:50:53Z","published":"2025-03-05T08:50:53Z","title":"Conformal Transformations for Symmetric Power Transformers","summary":"  Transformers with linear attention offer significant computational advantages\nover softmax-based transformers but often suffer from degraded performance. The\nsymmetric power (sympow) transformer, a particular type of linear transformer,\naddresses some of this performance gap by leveraging symmetric tensor\nembeddings, achieving comparable performance to softmax transformers. However,\nthe finite capacity of the recurrent state in sympow transformers limits their\nability to retain information, leading to performance degradation when scaling\nthe training or evaluation context length. To address this issue, we propose\nthe conformal-sympow transformer, which dynamically frees up capacity using\ndata-dependent multiplicative gating and adaptively stores information using\ndata-dependent rotary embeddings. Preliminary experiments on the LongCrawl64\ndataset demonstrate that conformal-sympow overcomes the limitations of sympow\ntransformers, achieving robust performance across scaled training and\nevaluation contexts.\n","authors":["Saurabh Kumar","Jacob Buckman","Carles Gelada","Sean Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.03269v1.pdf","comment":"SCOPE Workshop at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.09977v2","updated":"2025-03-05T08:48:25Z","published":"2025-02-14T08:04:22Z","title":"LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context LLMs\n  -- No Silver Bullet for LC or RAG Routing","summary":"  Effectively incorporating external knowledge into Large Language Models\n(LLMs) is crucial for enhancing their capabilities and addressing real-world\nneeds. Retrieval-Augmented Generation (RAG) offers an effective method for\nachieving this by retrieving the most relevant fragments into LLMs. However,\nthe advancements in context window size for LLMs offer an alternative approach,\nraising the question of whether RAG remains necessary for effectively handling\nexternal knowledge. Several existing studies provide inconclusive comparisons\nbetween RAG and long-context (LC) LLMs, largely due to limitations in the\nbenchmark designs. In this paper, we present LaRA, a novel benchmark\nspecifically designed to rigorously compare RAG and LC LLMs. LaRA encompasses\n2326 test cases across four practical QA task categories and three types of\nnaturally occurring long texts. Through systematic evaluation of seven\nopen-source and four proprietary LLMs, we find that the optimal choice between\nRAG and LC depends on a complex interplay of factors, including the model's\nparameter size, long-text capabilities, context length, task type, and the\ncharacteristics of the retrieved chunks. Our findings provide actionable\nguidelines for practitioners to effectively leverage both RAG and LC approaches\nin developing and deploying LLM applications. Our code and dataset is provided\nat:\n\\href{https://github.com/Alibaba-NLP/LaRA}{\\textbf{https://github.com/Alibaba-NLP/LaRA}}.\n","authors":["Kuan Li","Liwen Zhang","Yong Jiang","Pengjun Xie","Fei Huang","Shuai Wang","Minhao Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.09977v2.pdf","comment":"22 pages"},{"id":"http://arxiv.org/abs/2503.03262v1","updated":"2025-03-05T08:38:51Z","published":"2025-03-05T08:38:51Z","title":"Trajectory Prediction for Autonomous Driving: Progress, Limitations, and\n  Future Directions","summary":"  As the potential for autonomous vehicles to be integrated on a large scale\ninto modern traffic systems continues to grow, ensuring safe navigation in\ndynamic environments is crucial for smooth integration. To guarantee safety and\nprevent collisions, autonomous vehicles must be capable of accurately\npredicting the trajectories of surrounding traffic agents. Over the past\ndecade, significant efforts from both academia and industry have been dedicated\nto designing solutions for precise trajectory forecasting. These efforts have\nproduced a diverse range of approaches, raising questions about the differences\nbetween these methods and whether trajectory prediction challenges have been\nfully addressed. This paper reviews a substantial portion of recent trajectory\nprediction methods and devises a taxonomy to classify existing solutions. A\ngeneral overview of the prediction pipeline is also provided, covering input\nand output modalities, modeling features, and prediction paradigms discussed in\nthe literature. In addition, the paper discusses active research areas within\ntrajectory prediction, addresses the posed research questions, and highlights\nthe remaining research gaps and challenges.\n","authors":["Nadya Abdel Madjid","Abdulrahman Ahmad","Murad Mebrahtu","Yousef Babaa","Abdelmoamen Nasser","Sumbal Malik","Bilal Hassan","Naoufel Werghi","Jorge Dias","Majid Khonji"],"pdf_url":"https://arxiv.org/pdf/2503.03262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.05274v2","updated":"2025-03-05T08:36:27Z","published":"2024-09-17T10:08:37Z","title":"Scale-Invariant Object Detection by Adaptive Convolution with Unified\n  Global-Local Context","summary":"  Dense features are important for detecting minute objects in images.\nUnfortunately, despite the remarkable efficacy of the CNN models in multi-scale\nobject detection, CNN models often fail to detect smaller objects in images due\nto the loss of dense features during the pooling process. Atrous convolution\naddresses this issue by applying sparse kernels. However, sparse kernels often\ncan lose the multi-scale detection efficacy of the CNN model. In this paper, we\npropose an object detection model using a Switchable (adaptive) Atrous\nConvolutional Network (SAC-Net) based on the efficientDet model. A fixed atrous\nrate limits the performance of the CNN models in the convolutional layers. To\novercome this limitation, we introduce a switchable mechanism that allows for\ndynamically adjusting the atrous rate during the forward pass. The proposed\nSAC-Net encapsulates the benefits of both low-level and high-level features to\nachieve improved performance on multi-scale object detection tasks, without\nlosing the dense features. Further, we apply a depth-wise switchable atrous\nrate to the proposed network, to improve the scale-invariant features. Finally,\nwe apply global context on the proposed model. Our extensive experiments on\nbenchmark datasets demonstrate that the proposed SAC-Net outperforms the\nstate-of-the-art models by a significant margin in terms of accuracy.\n","authors":["Amrita Singh","Snehasis Mukherjee"],"pdf_url":"https://arxiv.org/pdf/2410.05274v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03258v1","updated":"2025-03-05T08:28:11Z","published":"2025-03-05T08:28:11Z","title":"Exploring the Potential of Large Language Models as Predictors in\n  Dynamic Text-Attributed Graphs","summary":"  With the rise of large language models (LLMs), there has been growing\ninterest in Graph Foundation Models (GFMs) for graph-based tasks. By leveraging\nLLMs as predictors, GFMs have demonstrated impressive generalizability across\nvarious tasks and datasets. However, existing research on LLMs as predictors\nhas predominantly focused on static graphs, leaving their potential in dynamic\ngraph prediction unexplored. In this work, we pioneer using LLMs for predictive\ntasks on dynamic graphs. We identify two key challenges: the constraints\nimposed by context length when processing large-scale historical data and the\nsignificant variability in domain characteristics, both of which complicate the\ndevelopment of a unified predictor. To address these challenges, we propose the\nGraphAgent-Dynamic (GAD) Framework, a multi-agent system that leverages\ncollaborative LLMs. In contrast to using a single LLM as the predictor, GAD\nincorporates global and local summary agents to generate domain-specific\nknowledge, enhancing its transferability across domains. Additionally,\nknowledge reflection agents enable adaptive updates to GAD's knowledge,\nmaintaining a unified and self-consistent architecture. In experiments, GAD\ndemonstrates performance comparable to or even exceeds that of full-supervised\ngraph neural networks without dataset-specific training. Finally, to enhance\nthe task-specific performance of LLM-based predictors, we discuss potential\nimprovements, such as dataset-specific fine-tuning to LLMs. By developing\ntailored strategies for different tasks, we provide new insights for the future\ndesign of LLM-based predictors.\n","authors":["Runlin Lei","Jiarui Ji","Haipeng Ding","Lu Yi","Zhewei Wei","Yongchao Liu","Chuntao Hong"],"pdf_url":"https://arxiv.org/pdf/2503.03258v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01669v2","updated":"2025-03-05T08:23:02Z","published":"2024-01-16T13:41:00Z","title":"Improved Performances and Motivation in Intelligent Tutoring Systems:\n  Combining Machine Learning and Learner Choice","summary":"  Large class sizes challenge personalized learning in schools, prompting the\nuse of educational technologies such as intelligent tutoring systems. To\naddress this, we present an AI-driven personalization system, called ZPDES,\nbased on the Learning Progress Hypothesis - modeling curiosity-driven learning\n- and multi-armed bandit techniques. It sequences exercises that maximize\nlearning progress for each student. While previous studies demonstrated its\nefficacy in enhancing learning compared to hand-made curricula, its impact on\nstudent motivation remained unexplored. Furthermore, ZPDES previously lacked\nfeatures allowing student choice, a limitation in agency that conflicts with\nits foundation on models of curiosity-driven learning. This study investigates\nhow integrating choice, as a gamification element unrelated to exercise\ndifficulty, affects both learning outcomes and motivation. We conducted an\nextensive field study (265 7-8 years old children, RCT design), comparing ZPDES\nwith and without choice against a hand-designed curriculum. Results show that\nZPDES improves both learning performance and the learning experience. Moreover\nadding choice to ZPDES enhances intrinsic motivation and further strengthens\nits learning benefits. In contrast, incorporating choice into a fixed, linear\ncurriculum negatively impacts learning outcomes. These findings highlight that\nthe intrinsic motivation elicited by choice (gamification) is beneficial only\nwhen paired with an adaptive personalized learning system. This insight is\ncritical as gamified features become increasingly prevalent in educational\ntechnologies.\n","authors":["Benjamin Clément","Hélène Sauzéon","Didier Roy","Pierre-Yves Oudeyer"],"pdf_url":"https://arxiv.org/pdf/2402.01669v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03245v1","updated":"2025-03-05T07:53:39Z","published":"2025-03-05T07:53:39Z","title":"Less is more? Rewards in RL for Cyber Defence","summary":"  The last few years has seen an explosion of interest in autonomous cyber\ndefence agents based on deep reinforcement learning. Such agents are typically\ntrained in a cyber gym environment, also known as a cyber simulator, at least\n32 of which have already been built. Most, if not all cyber gyms provide dense\n\"scaffolded\" reward functions which combine many penalties or incentives for a\nrange of (un)desirable states and costly actions. Whilst dense rewards help\nalleviate the challenge of exploring complex environments, yielding seemingly\neffective strategies from relatively few environment steps; they are also known\nto bias the solutions an agent can find, potentially towards suboptimal\nsolutions. Sparse rewards could offer preferable or more effective solutions\nand have been overlooked by cyber gyms to date. In this work we set out to\nevaluate whether sparse reward functions might enable training more effective\ncyber defence agents. Towards this goal we first break down several evaluation\nlimitations in existing work by proposing a ground truth evaluation score that\ngoes beyond the standard RL paradigm used to train and evaluate agents. By\nadapting a well-established cyber gym to accommodate our methodology and ground\ntruth score, we propose and evaluate two sparse reward mechanisms and compare\nthem with a typical dense reward. Our evaluation considers a range of network\nsizes, from 2 to 50 nodes, and both reactive and proactive defensive actions.\nOur results show that sparse rewards, particularly positive reinforcement for\nan uncompromised network state, enable the training of more effective cyber\ndefence agents. Furthermore, we show that sparse rewards provide more stable\ntraining than dense rewards, and that both effectiveness and training stability\nare robust to a variety of cyber environment considerations.\n","authors":["Elizabeth Bates","Chris Hicks","Vasilios Mavroudis"],"pdf_url":"https://arxiv.org/pdf/2503.03245v1.pdf","comment":"4 Pages"},{"id":"http://arxiv.org/abs/2503.03238v1","updated":"2025-03-05T07:34:53Z","published":"2025-03-05T07:34:53Z","title":"FANS -- Formal Answer Selection for Natural Language Math Reasoning\n  Using Lean4","summary":"  Large Language Models (LLMs) have displayed astonishing abilities in various\ntasks, especially in text generation, classification, question answering, etc.\nHowever, the reasoning ability of LLMs still faces many debates. The inherent\nambiguity of Natural Language (NL) limits LLMs' ability to perform verifiable\nreasoning, making its answers lack coherence and trustworthy support. To tackle\nthe above problems, we propose a novel framework named FANS: Formal ANswer\nSelection for Natural Language Math Reasoning Using Lean4. To the best of our\nknowledge, it is the first framework that utilizes Lean4 to enhance LLMs' NL\nmath reasoning ability. In particular, given an NL math question and\nLLM-generated answers, FANS first translates it into Lean4 theorem statements.\nThen it tries to prove it using a Lean4 prover and verify it by Lean4. Finally,\nit uses the FL result to assist in answer selection. It enhances LLMs' NL math\nability in providing a computer-verifiable solution for its correct answer and\nproposes an alternative method for answer selection beyond the reward model.\nExtensive experiments indicate the effectiveness of our framework. It can\nimprove the accuracy rate of reward model enhanced LLMs in the MATH-500 dataset\nby at most 1.91% and AMC-23 by at most 8.33% on strong reward-model baselines.\nIn some particular fields like number theory that Lean4 experts in, we can even\nselect all correct solutions. The qualitative analysis also shows our framework\ncan make NL results formally backed by Lean4 proofs. As a pioneering work in\nthe corresponding field, we will open-source all our models and datasets to\nfurther boost the development of the field.\n","authors":["Jiarui Yao","Ruida Wang","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.03238v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17107v3","updated":"2025-03-05T07:29:42Z","published":"2024-12-22T17:39:32Z","title":"Grams: Gradient Descent with Adaptive Momentum Scaling","summary":"  We introduce $\\mathbf{G}$radient Descent with $\\mathbf{A}$daptive\n$\\mathbf{M}$omentum $\\mathbf{S}$caling ($\\mathbf{Grams}$), a novel optimization\nalgorithm that decouples the direction and magnitude of parameter updates in\ndeep learning. Unlike traditional optimizers that directly integrate momentum\ninto updates, Grams separates the update direction, derived from current\ngradients, from momentum, which is used solely for adaptive magnitude scaling.\nThis approach enables Grams to achieve improved loss descent compared to\nstate-of-the-art cautious and momentum-based optimizers. We theoretically\ndemonstrate that Grams descents faster than other state-of-the-art optimizers\nand establish a global convergence guarantee for Grams. We also validate its\neffectiveness through extensive empirical evaluations. The results demonstrate\nGrams' superior performance, including faster convergence and better\ngeneralization, compared to widely-used optimizers such as Adam, Lion, and\ntheir cautious variants. Our results highlight Grams' potential as a\ntransformative approach for efficiently training and fine-tuning large language\nmodels. Code is available at https://github.com/Gunale0926/Grams.\n","authors":["Yang Cao","Xiaoyu Li","Zhao Song"],"pdf_url":"https://arxiv.org/pdf/2412.17107v3.pdf","comment":"SCOPE Workshop @ ICLR 2025"},{"id":"http://arxiv.org/abs/2412.09601v2","updated":"2025-03-05T07:06:15Z","published":"2024-12-12T18:59:11Z","title":"TimeRefine: Temporal Grounding with Time Refining Video LLM","summary":"  Video temporal grounding aims to localize relevant temporal boundaries in a\nvideo given a textual prompt. Recent work has focused on enabling Video LLMs to\nperform video temporal grounding via next-token prediction of temporal\ntimestamps. However, accurately localizing timestamps in videos remains\nchallenging for Video LLMs when relying solely on temporal token prediction.\nOur proposed TimeRefine addresses this challenge in two ways. First, instead of\ndirectly predicting the start and end timestamps, we reformulate the temporal\ngrounding task as a temporal refining task: the model first makes rough\npredictions and then refines them by predicting offsets to the target segment.\nThis refining process is repeated multiple times, through which the model\nprogressively self-improves its temporal localization accuracy. Second, to\nenhance the model's temporal perception capabilities, we incorporate an\nauxiliary prediction head that penalizes the model more if a predicted segment\ndeviates further from the ground truth, thus encouraging the model to make\ncloser and more accurate predictions. Our plug-and-play method can be\nintegrated into most LLM-based temporal grounding approaches. The experimental\nresults demonstrate that TimeRefine achieves 3.6% and 5.0% mIoU improvements on\nthe ActivityNet and Charades-STA datasets, respectively. Code and pretrained\nmodels will be released.\n","authors":["Xizi Wang","Feng Cheng","Ziyang Wang","Huiyu Wang","Md Mohaiminul Islam","Lorenzo Torresani","Mohit Bansal","Gedas Bertasius","David Crandall"],"pdf_url":"https://arxiv.org/pdf/2412.09601v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17543v2","updated":"2025-03-05T06:53:52Z","published":"2025-02-24T18:56:58Z","title":"Training a Generally Curious Agent","summary":"  Efficient exploration is essential for intelligent systems interacting with\ntheir environment, but existing language models often fall short in scenarios\nthat require strategic information gathering. In this paper, we present\nPAPRIKA, a fine-tuning approach that enables language models to develop general\ndecision-making capabilities that are not confined to particular environments.\nBy training on synthetic interaction data from different tasks that require\ndiverse strategies, PAPRIKA teaches models to explore and adapt their behavior\non a new task based on environment feedback in-context without more gradient\nupdates. Experimental results show that models fine-tuned with PAPRIKA can\neffectively transfer their learned decision-making capabilities to entirely\nunseen tasks without additional training. Unlike traditional training, our\napproach's primary bottleneck lies in sampling useful interaction data instead\nof model updates. To improve sample efficiency, we propose a curriculum\nlearning strategy that prioritizes sampling trajectories from tasks with high\nlearning potential. These results suggest a promising path towards AI systems\nthat can autonomously solve novel sequential decision-making problems that\nrequire interactions with the external world.\n","authors":["Fahim Tajwar","Yiding Jiang","Abitha Thankaraj","Sumaita Sadia Rahman","J Zico Kolter","Jeff Schneider","Ruslan Salakhutdinov"],"pdf_url":"https://arxiv.org/pdf/2502.17543v2.pdf","comment":"Project Website: https://paprika-llm.github.io"},{"id":"http://arxiv.org/abs/2407.10341v5","updated":"2025-03-05T06:53:17Z","published":"2024-07-14T21:41:29Z","title":"Affordance-Guided Reinforcement Learning via Visual Prompting","summary":"  Robots equipped with reinforcement learning (RL) have the potential to learn\na wide range of skills solely from a reward signal. However, obtaining a robust\nand dense reward signal for general manipulation tasks remains a challenge.\nExisting learning-based approaches require significant data, such as human\ndemonstrations of success and failure, to learn task-specific reward functions.\nRecently, there is also a growing adoption of large multi-modal foundation\nmodels for robotics that can perform visual reasoning in physical contexts and\ngenerate coarse robot motions for manipulation tasks. Motivated by this range\nof capability, in this work, we present Keypoint-based Affordance Guidance for\nImprovements (KAGI), a method leveraging rewards shaped by vision-language\nmodels (VLMs) for autonomous RL. State-of-the-art VLMs have demonstrated\nimpressive reasoning about affordances through keypoints in zero-shot, and we\nuse these to define dense rewards that guide autonomous robotic learning. On\nreal-world manipulation tasks specified by natural language descriptions, KAGI\nimproves the sample efficiency of autonomous RL and enables successful task\ncompletion in 30K online fine-tuning steps. Additionally, we demonstrate the\nrobustness of KAGI to reductions in the number of in-domain demonstrations used\nfor pre-training, reaching similar performance in 45K online fine-tuning steps.\nProject website: https://sites.google.com/view/affordance-guided-rl\n","authors":["Olivia Y. Lee","Annie Xie","Kuan Fang","Karl Pertsch","Chelsea Finn"],"pdf_url":"https://arxiv.org/pdf/2407.10341v5.pdf","comment":"8 pages, 6 figures. Robotics: Science and Systems (RSS) 2024, Task\n  Specification for General-Purpose Intelligent Robots & Lifelong Robot\n  Learning Workshops"},{"id":"http://arxiv.org/abs/2408.08927v2","updated":"2025-03-05T06:23:52Z","published":"2024-08-15T20:06:06Z","title":"VerilogCoder: Autonomous Verilog Coding Agents with Graph-based Planning\n  and Abstract Syntax Tree (AST)-based Waveform Tracing Tool","summary":"  Due to the growing complexity of modern Integrated Circuits (ICs), automating\nhardware design can prevent a significant amount of human error from the\nengineering process and result in less errors. Verilog is a popular hardware\ndescription language for designing and modeling digital systems; thus, Verilog\ngeneration is one of the emerging areas of research to facilitate the design\nprocess. In this work, we propose VerilogCoder, a system of multiple Artificial\nIntelligence (AI) agents for Verilog code generation, to autonomously write\nVerilog code and fix syntax and functional errors using collaborative Verilog\ntools (i.e., syntax checker, simulator, and waveform tracer). Firstly, we\npropose a task planner that utilizes a novel Task and Circuit Relation Graph\nretrieval method to construct a holistic plan based on module descriptions. To\ndebug and fix functional errors, we develop a novel and efficient abstract\nsyntax tree (AST)-based waveform tracing tool, which is integrated within the\nautonomous Verilog completion flow. The proposed methodology successfully\ngenerates 94.2% syntactically and functionally correct Verilog code, surpassing\nthe state-of-the-art methods by 33.9% on the VerilogEval-Human v2 benchmark.\n","authors":["Chia-Tung Ho","Haoxing Ren","Brucek Khailany"],"pdf_url":"https://arxiv.org/pdf/2408.08927v2.pdf","comment":"main paper 7 pages, reference 1 page, it is the version that accepted\n  by AAAI 2025"},{"id":"http://arxiv.org/abs/2502.16802v2","updated":"2025-03-05T06:23:22Z","published":"2025-02-24T03:25:56Z","title":"Unsupervised Topic Models are Data Mixers for Pre-training Language\n  Models","summary":"  The performance of large language models (LLMs) is significantly affected by\nthe quality and composition of their pre-training data, which is inherently\ndiverse, spanning various domains, sources, and topics. Effectively integrating\nthese heterogeneous data sources is crucial for optimizing LLM performance.\nPrevious research has predominantly concentrated on domain-based data mixing,\noften neglecting the nuanced topic-level characteristics of the data. To\naddress this gap, we propose a simple yet effective topic-based data mixing\nstrategy that utilizes fine-grained topics generated through our topic modeling\nmethod, DataWeave. DataWeave employs a multi-stage clustering process to group\nsemantically similar documents and utilizes LLMs to generate detailed topics,\nthereby facilitating a more nuanced understanding of dataset composition. Our\nstrategy employs heuristic methods to upsample or downsample specific topics,\nwhich significantly enhances LLM performance on downstream tasks, achieving\nsuperior results compared to previous, more complex data mixing approaches.\nFurthermore, we confirm that the topics Science and Relationships are\nparticularly effective, yielding the most substantial performance improvements.\nWe will make our code and datasets publicly available.\n","authors":["Jiahui Peng","Xinlin Zhuang","Qiu Jiantao","Ren Ma","Jing Yu","Tianyi Bai","Conghui He"],"pdf_url":"https://arxiv.org/pdf/2502.16802v2.pdf","comment":"18 pages,7 figures"},{"id":"http://arxiv.org/abs/2503.03215v1","updated":"2025-03-05T06:16:15Z","published":"2025-03-05T06:16:15Z","title":"COSINT-Agent: A Knowledge-Driven Multimodal Agent for Chinese Open\n  Source Intelligence","summary":"  Open Source Intelligence (OSINT) requires the integration and reasoning of\ndiverse multimodal data, presenting significant challenges in deriving\nactionable insights. Traditional approaches, including multimodal large\nlanguage models (MLLMs), often struggle to infer complex contextual\nrelationships or deliver comprehensive intelligence from unstructured data\nsources. In this paper, we introduce COSINT-Agent, a knowledge-driven\nmultimodal agent tailored to address the challenges of OSINT in the Chinese\ndomain. COSINT-Agent seamlessly integrates the perceptual capabilities of\nfine-tuned MLLMs with the structured reasoning power of the Entity-Event-Scene\nKnowledge Graph (EES-KG). Central to COSINT-Agent is the innovative EES-Match\nframework, which bridges COSINT-MLLM and EES-KG, enabling systematic\nextraction, reasoning, and contextualization of multimodal insights. This\nintegration facilitates precise entity recognition, event interpretation, and\ncontext retrieval, effectively transforming raw multimodal data into actionable\nintelligence. Extensive experiments validate the superior performance of\nCOSINT-Agent across core OSINT tasks, including entity recognition, EES\ngeneration, and context matching. These results underscore its potential as a\nrobust and scalable solution for advancing automated multimodal reasoning and\nenhancing the effectiveness of OSINT methodologies.\n","authors":["Wentao Li","Congcong Wang","Xiaoxiao Cui","Zhi Liu","Wei Guo","Lizhen Cui"],"pdf_url":"https://arxiv.org/pdf/2503.03215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03211v1","updated":"2025-03-05T06:06:16Z","published":"2025-03-05T06:06:16Z","title":"NodeReg: Mitigating the Imbalance and Distribution Shift Effects in\n  Semi-Supervised Node Classification via Norm Consistency","summary":"  Aggregating information from neighboring nodes benefits graph neural networks\n(GNNs) in semi-supervised node classification tasks. Nevertheless, this\nmechanism also renders nodes susceptible to the influence of their neighbors.\nFor instance, this will occur when the neighboring nodes are imbalanced or the\nneighboring nodes contain noise, which can even affect the GNN's ability to\ngeneralize out of distribution. We find that ensuring the consistency of the\nnorm for node representations can significantly reduce the impact of these two\nissues on GNNs. To this end, we propose a regularized optimization method\ncalled NodeReg that enforces the consistency of node representation norms. This\nmethod is simple but effective and satisfies Lipschitz continuity, thus\nfacilitating stable optimization and significantly improving semi-supervised\nnode classification performance under the above two scenarios. To illustrate,\nin the imbalance scenario, when training a GCN with an imbalance ratio of 0.1,\nNodeReg outperforms the most competitive baselines by 1.4%-25.9% in F1 score\nacross five public datasets. Similarly, in the distribution shift scenario,\nNodeReg outperforms the most competitive baseline by 1.4%-3.1% in accuracy.\n","authors":["Shenzhi Yang","Jun Xia","Jingbo Zhou","Xingkai Yao","Xiaofang Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.03211v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03205v1","updated":"2025-03-05T05:50:31Z","published":"2025-03-05T05:50:31Z","title":"MA-LoT: Multi-Agent Lean-based Long Chain-of-Thought Reasoning enhances\n  Formal Theorem Proving","summary":"  Solving mathematical problems using computer-verifiable languages like Lean\nhas significantly impacted mathematical and computer science communities.\nState-of-the-art methods utilize single Large Language Models (LLMs) as agents\nor provers to either generate complete proof or perform tree searches. However,\nsingle-agent methods inherently lack a structured way to combine high-level\nreasoning in Natural Language (NL) with Formal Language (FL) verification\nfeedback. To solve these issues, we propose MA-LoT: Multi-Agent Lean-based Long\nChain-of-Thought framework, (to the best of our knowledge), the first\nmulti-agent framework for Lean4 theorem proving that balance high-level NL\nreasoning and FL verification in Long CoT. Using this structured interaction,\nour approach enables deeper insights and long-term coherence in proof\ngeneration, with which past methods struggle. We do this by leveraging emergent\nformal reasoning ability in Long CoT using our novel LoT-Transfer Learning\ntraining-inference pipeline. Extensive experiments show that our framework\nachieves 54.51% accuracy rate on the Lean4 version of MiniF2F-Test dataset,\nlargely outperforming GPT-4 (22.95%), single-agent tree search\n(InternLM-Step-Prover, 50.70%), and whole-proof generation\n(DeepSeek-Prover-v1.5, 48.36%) baselines. Furthermore, our findings highlight\nthe potential of combining Long CoT with formal verification for a more\ninsightful generation in a broader perspective.\n","authors":["Ruida Wang","Rui Pan","Yuxin Li","Jipeng Zhang","Yizhen Jia","Shizhe Diao","Renjie Pi","Junjie Hu","Tong Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.03205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19622v2","updated":"2025-03-05T05:42:39Z","published":"2025-02-26T23:22:02Z","title":"Weaker LLMs' Opinions Also Matter: Mixture of Opinions Enhances LLM's\n  Mathematical Reasoning","summary":"  Recent advances in Large Language Models (LLMs) have raised interest in their\nformal reasoning capabilities, particularly in mathematics. While closed LLMs\nlike GPT-4 perform well on mathematical benchmarks, e.g., GSM8K, it remains\nunclear whether small to medium-sized open LLMs can achieve similar\nperformance, questioning their reliability. To close this gap, we propose a\npost-training approach leveraging a mixture of opinions (MoO) from weaker\nancillary LLMs to enhance a (relatively) stronger LLM's reasoning. For that,\neach post-training sample is augmented with Chain-of-Thought (CoT) reasoning\nsteps and answers from ancillary LLMs, enabling the main LLM to learn from\ndiverse perspectives. We compare MoO with standard supervised fine-tuning\n(SFT), few-shot prompting, and the Mixture of Agents (MoA) method on\nmathematical reasoning benchmarks. Our results show that incorporating weaker\nLLMs' opinions improves mathematical reasoning by an average of 5%,\nhighlighting the value of diverse perspectives in reasoning tasks.\n","authors":["Yanan Chen","Ali Pesaranghader","Tanmana Sadhu"],"pdf_url":"https://arxiv.org/pdf/2502.19622v2.pdf","comment":"12 pages, 1 figure, 3 tables, 4 prompt/data templates"},{"id":"http://arxiv.org/abs/2409.14644v2","updated":"2025-03-05T05:42:35Z","published":"2024-09-23T01:03:15Z","title":"zsLLMCode: An Effective Approach for Code Embedding via LLM with\n  Zero-Shot Learning","summary":"  The advent of large language models (LLMs) has greatly advanced artificial\nintelligence (AI) in software engineering (SE), with code embeddings playing a\ncritical role in tasks like code-clone detection and code clustering. However,\nexisting methods for code embedding, including those based on LLMs, often\ndepend on costly supervised training or fine-tuning for domain adaptation. This\npaper proposes a novel zero-shot approach, zsLLMCode, to generate code\nembeddings by using LLMs and sentence embedding models. This approach attempts\nto eliminate the need for task-specific training or fine-tuning, and to\neffectively address the issue of erroneous information commonly found in\nLLM-generated outputs. We conducted a series of experiments to evaluate the\nperformance of the proposed approach by considering various LLMs and embedding\nmodels. The results have demonstrated the effectiveness and superiority of our\nmethod zsLLMCode over state-of-the-art unsupervised approaches such as\nSourcererCC, Code2vec, InferCode, and TransformCode. Our findings highlight the\npotential of zsLLMCode to advance the field of SE by providing robust and\nefficient solutions for code embedding tasks.\n","authors":["Zixiang Xian","Chenhui Cui","Rubing Huang","Chunrong Fang","Zhenyu Chen"],"pdf_url":"https://arxiv.org/pdf/2409.14644v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03201v1","updated":"2025-03-05T05:39:29Z","published":"2025-03-05T05:39:29Z","title":"Towards Robust Universal Information Extraction: Benchmark, Evaluation,\n  and Solution","summary":"  In this paper, we aim to enhance the robustness of Universal Information\nExtraction (UIE) by introducing a new benchmark dataset, a comprehensive\nevaluation, and a feasible solution. Existing robust benchmark datasets have\ntwo key limitations: 1) They generate only a limited range of perturbations for\na single Information Extraction (IE) task, which fails to evaluate the\nrobustness of UIE models effectively; 2) They rely on small models or\nhandcrafted rules to generate perturbations, often resulting in unnatural\nadversarial examples. Considering the powerful generation capabilities of Large\nLanguage Models (LLMs), we introduce a new benchmark dataset for Robust UIE,\ncalled RUIE-Bench, which utilizes LLMs to generate more diverse and realistic\nperturbations across different IE tasks. Based on this dataset, we\ncomprehensively evaluate existing UIE models and reveal that both LLM-based\nmodels and other models suffer from significant performance drops. To improve\nrobustness and reduce training costs, we propose a data-augmentation solution\nthat dynamically selects hard samples for iterative training based on the\nmodel's inference loss. Experimental results show that training with only\n\\textbf{15\\%} of the data leads to an average \\textbf{7.5\\%} relative\nperformance improvement across three IE tasks.\n","authors":["Jizhao Zhu","Akang Shi","Zixuan Li","Long Bai","Xiaolong Jin","Jiafeng Guo","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2503.03201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16746v3","updated":"2025-03-05T05:34:47Z","published":"2024-11-23T20:41:24Z","title":"LoBAM: LoRA-Based Backdoor Attack on Model Merging","summary":"  Model merging is an emerging technique that integrates multiple models\nfine-tuned on different tasks to create a versatile model that excels in\nmultiple domains. This scheme, in the meantime, may open up backdoor attack\nopportunities where one single malicious model can jeopardize the integrity of\nthe merged model. Existing works try to demonstrate the risk of such attacks by\nassuming substantial computational resources, focusing on cases where the\nattacker can fully fine-tune the pre-trained model. Such an assumption,\nhowever, may not be feasible given the increasing size of machine learning\nmodels. In practice where resources are limited and the attacker can only\nemploy techniques like Low-Rank Adaptation (LoRA) to produce the malicious\nmodel, it remains unclear whether the attack can still work and pose threats.\nIn this work, we first identify that the attack efficacy is significantly\ndiminished when using LoRA for fine-tuning. Then, we propose LoBAM, a method\nthat yields high attack success rate with minimal training resources. The key\nidea of LoBAM is to amplify the malicious weights in an intelligent way that\neffectively enhances the attack efficacy. We demonstrate that our design can\nlead to improved attack success rate through extensive empirical experiments\nacross various model merging scenarios. Moreover, we show that our method is\nhighly stealthy and is difficult to detect and defend against.\n","authors":["Ming Yin","Jingyang Zhang","Jingwei Sun","Minghong Fang","Hai Li","Yiran Chen"],"pdf_url":"https://arxiv.org/pdf/2411.16746v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03197v1","updated":"2025-03-05T05:30:26Z","published":"2025-03-05T05:30:26Z","title":"Directly Follows Graphs Go Predictive Process Monitoring With Graph\n  Neural Networks","summary":"  In the past years, predictive process monitoring (PPM) techniques based on\nartificial neural networks have evolved as a method to monitor the future\nbehavior of business processes. Existing approaches mostly focus on\ninterpreting the processes as sequences, so-called traces, and feeding them to\nneural architectures designed to operate on sequential data such as recurrent\nneural networks (RNNs) or transformers. In this study, we investigate an\nalternative way to perform PPM: by transforming each process in its\ndirectly-follows-graph (DFG) representation we are able to apply graph neural\nnetworks (GNNs) for the prediction tasks. By this, we aim to develop models\nthat are more suitable for complex processes that are long and contain an\nabundance of loops. In particular, we present different ways to create DFG\nrepresentations depending on the particular GNN we use. The tested GNNs range\nfrom classical node-based to novel edge-based architectures. Further, we\ninvestigate the possibility of using multi-graphs. By these steps, we aim to\ndesign graph representations that minimize the information loss when\ntransforming traces into graphs.\n","authors":["Attila Lischka","Simon Rauch","Oliver Stritzel"],"pdf_url":"https://arxiv.org/pdf/2503.03197v1.pdf","comment":"10 pages, 4 figures, 3 tables"},{"id":"http://arxiv.org/abs/2503.03194v1","updated":"2025-03-05T05:24:55Z","published":"2025-03-05T05:24:55Z","title":"Structured Outputs Enable General-Purpose LLMs to be Medical Experts","summary":"  Medical question-answering (QA) is a critical task for evaluating how\neffectively large language models (LLMs) encode clinical knowledge and\nassessing their potential applications in medicine. Despite showing promise on\nmultiple-choice tests, LLMs frequently struggle with open-ended medical\nquestions, producing responses with dangerous hallucinations or lacking\ncomprehensive coverage of critical aspects. Existing approaches attempt to\naddress these challenges through domain-specific fine-tuning, but this proves\nresource-intensive and difficult to scale across models. To improve the\ncomprehensiveness and factuality of medical responses, we propose a novel\napproach utilizing structured medical reasoning. Our method guides LLMs through\nan seven-step cognitive process inspired by clinical diagnosis, enabling more\naccurate and complete answers without additional training. Experiments on the\nMedLFQA benchmark demonstrate that our approach achieves the highest Factuality\nScore of 85.8, surpassing fine-tuned models. Notably, this improvement\ntransfers to smaller models, highlighting the method's efficiency and\nscalability. Our code and datasets are available.\n","authors":["Guangfu Guo","Kai Zhang","Bryan Hoo","Yujun Cai","Xiaoqian Lu","Nanyun Peng","Yiwei Wang"],"pdf_url":"https://arxiv.org/pdf/2503.03194v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.15503v5","updated":"2025-03-05T05:14:34Z","published":"2024-08-28T03:17:40Z","title":"RoboSense: Large-scale Dataset and Benchmark for Egocentric Robot\n  Perception and Navigation in Crowded and Unstructured Environments","summary":"  Reliable embodied perception from an egocentric perspective is challenging\nyet essential for autonomous navigation technology of intelligent mobile\nagents. With the growing demand of social robotics, near-field scene\nunderstanding becomes an important research topic in the areas of egocentric\nperceptual tasks related to navigation in both crowded and unstructured\nenvironments. Due to the complexity of environmental conditions and difficulty\nof surrounding obstacles owing to truncation and occlusion, the perception\ncapability under this circumstance is still inferior. To further enhance the\nintelligence of mobile robots, in this paper, we setup an egocentric\nmulti-sensor data collection platform based on 3 main types of sensors (Camera,\nLiDAR and Fisheye), which supports flexible sensor configurations to enable\ndynamic sight of view from ego-perspective, capturing either near or farther\nareas. Meanwhile, a large-scale multimodal dataset is constructed, named\nRoboSense, to facilitate egocentric robot perception. Specifically, RoboSense\ncontains more than 133K synchronized data with 1.4M 3D bounding box and IDs\nannotated in the full $360^{\\circ}$ view, forming 216K trajectories across 7.6K\ntemporal sequences. It has $270\\times$ and $18\\times$ as many annotations of\nsurrounding obstacles within near ranges as the previous datasets collected for\nautonomous driving scenarios such as KITTI and nuScenes. Moreover, we define a\nnovel matching criterion for near-field 3D perception and prediction metrics.\nBased on RoboSense, we formulate 6 popular tasks to facilitate the future\nresearch development, where the detailed analysis as well as benchmarks are\nalso provided accordingly. Data desensitization measures have been conducted\nfor privacy protection.\n","authors":["Haisheng Su","Feixiang Song","Cong Ma","Wei Wu","Junchi Yan"],"pdf_url":"https://arxiv.org/pdf/2408.15503v5.pdf","comment":"Accepted to CVPR2025"},{"id":"http://arxiv.org/abs/2407.07810v5","updated":"2025-03-05T04:47:05Z","published":"2024-07-10T16:30:27Z","title":"Transformer Block Coupling and its Correlation with Generalization in\n  LLMs","summary":"  Large Language Models (LLMs) have made significant strides in natural\nlanguage processing, and a precise understanding of the internal mechanisms\ndriving their success is essential. In this work, we analyze the trajectories\nof token embeddings as they pass through transformer blocks, linearizing the\nsystem along these trajectories through their Jacobian matrices. By examining\nthe relationships between these block Jacobians, we uncover the phenomenon of\n\\textbf{transformer block coupling} in a multitude of LLMs, characterized by\nthe coupling of their top singular vectors across tokens and depth. Our\nfindings reveal that coupling \\textit{positively correlates} with model\nperformance, and that this relationship is stronger than with other\nhyperparameters such as parameter count, model depth, and embedding dimension.\nWe further investigate how these properties emerge during training, observing a\nprogressive development of coupling, increased linearity, and layer-wise\nexponential growth in token trajectories. Additionally, experiments with Vision\nTransformers (ViTs) corroborate the emergence of coupling and its relationship\nwith generalization, reinforcing our findings in LLMs. Collectively, these\ninsights offer a novel perspective on token interactions in transformers,\nopening new directions for studying their mechanisms as well as improving\ntraining and generalization.\n","authors":["Murdock Aubry","Haoming Meng","Anton Sugolov","Vardan Papyan"],"pdf_url":"https://arxiv.org/pdf/2407.07810v5.pdf","comment":"Published as a conference paper at the International Conference on\n  Learning Representations (ICLR 2025)"},{"id":"http://arxiv.org/abs/2501.18821v2","updated":"2025-03-05T04:45:03Z","published":"2025-01-31T00:36:08Z","title":"An Optimal Cascade Feature-Level Spatiotemporal Fusion Strategy for\n  Anomaly Detection in CAN Bus","summary":"  Autonomous vehicles represent a revolutionary advancement driven by the\nintegration of artificial intelligence within intelligent transportation\nsystems. However, they remain vulnerable due to the absence of robust security\nmechanisms in the Controller Area Network (CAN) bus. In order to mitigate the\nsecurity issue, many machine learning models and strategies have been proposed,\nwhich primarily focus on a subset of dominant patterns of anomalies and lack\nrigorous evaluation in terms of reliability and robustness. Therefore, to\naddress the limitations of previous works and mitigate the security\nvulnerability in CAN bus, the current study develops a model based on the\nintrinsic nature of the problem to cover all dominant patterns of anomalies. To\nachieve this, a cascade feature-level fusion strategy optimized by a\ntwo-parameter genetic algorithm is proposed to combine temporal and spatial\ninformation. Subsequently, the model is evaluated using a paired t-test to\nensure reliability and robustness. Finally, a comprehensive comparative\nanalysis conducted on two widely used datasets advocates that the proposed\nmodel outperforms other models and achieves superior accuracy and F1-score,\ndemonstrating the best performance among all models presented to date.\n","authors":["Mohammad Fatahi","Danial Sadrian Zadeh","Benyamin Ghojogh","Behzad Moshiri","Otman Basir"],"pdf_url":"https://arxiv.org/pdf/2501.18821v2.pdf","comment":"v2: updated the text and graphs"},{"id":"http://arxiv.org/abs/2410.03030v2","updated":"2025-03-05T04:37:07Z","published":"2024-10-03T22:24:54Z","title":"Dynamic Sparse Training versus Dense Training: The Unexpected Winner in\n  Image Corruption Robustness","summary":"  It is generally perceived that Dynamic Sparse Training opens the door to a\nnew era of scalability and efficiency for artificial neural networks at,\nperhaps, some costs in accuracy performance for the classification task. At the\nsame time, Dense Training is widely accepted as being the \"de facto\" approach\nto train artificial neural networks if one would like to maximize their\nrobustness against image corruption. In this paper, we question this general\npractice. Consequently, we claim that, contrary to what is commonly thought,\nthe Dynamic Sparse Training methods can consistently outperform Dense Training\nin terms of robustness accuracy, particularly if the efficiency aspect is not\nconsidered as a main objective (i.e., sparsity levels between 10% and up to\n50%), without adding (or even reducing) resource cost. We validate our claim on\ntwo types of data, images and videos, using several traditional and modern deep\nlearning architectures for computer vision and three widely studied Dynamic\nSparse Training algorithms. Our findings reveal a new yet-unknown benefit of\nDynamic Sparse Training and open new possibilities in improving deep learning\nrobustness beyond the current state of the art.\n","authors":["Boqian Wu","Qiao Xiao","Shunxin Wang","Nicola Strisciuglio","Mykola Pechenizkiy","Maurice van Keulen","Decebal Constantin Mocanu","Elena Mocanu"],"pdf_url":"https://arxiv.org/pdf/2410.03030v2.pdf","comment":"Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2412.20468v2","updated":"2025-03-05T04:32:02Z","published":"2024-12-29T14:00:11Z","title":"A Comprehensive Framework for Reliable Legal AI: Combining Specialized\n  Expert Systems and Adaptive Refinement","summary":"  This article discusses the evolving role of artificial intelligence (AI) in\nthe legal profession, focusing on its potential to streamline tasks such as\ndocument review, research, and contract drafting. However, challenges persist,\nparticularly the occurrence of \"hallucinations\" in AI models, where they\ngenerate inaccurate or misleading information, undermining their reliability in\nlegal contexts. To address this, the article proposes a novel framework\ncombining a mixture of expert systems with a knowledge-based architecture to\nimprove the precision and contextual relevance of AI-driven legal services.\nThis framework utilizes specialized modules, each focusing on specific legal\nareas, and incorporates structured operational guidelines to enhance\ndecision-making. Additionally, it leverages advanced AI techniques like\nRetrieval-Augmented Generation (RAG), Knowledge Graphs (KG), and Reinforcement\nLearning from Human Feedback (RLHF) to improve the system's accuracy. The\nproposed approach demonstrates significant improvements over existing AI\nmodels, showcasing enhanced performance in legal tasks and offering a scalable\nsolution to provide more accessible and affordable legal services. The article\nalso outlines the methodology, system architecture, and promising directions\nfor future research in AI applications for the legal sector.\n","authors":["Sidra Nasir","Qamar Abbas","Samita Bai","Rizwan Ahmed Khan"],"pdf_url":"https://arxiv.org/pdf/2412.20468v2.pdf","comment":"16 pages and 5 figures"},{"id":"http://arxiv.org/abs/2503.03172v1","updated":"2025-03-05T04:30:53Z","published":"2025-03-05T04:30:53Z","title":"Intermediate-Task Transfer Learning: Leveraging Sarcasm Detection for\n  Stance Detection","summary":"  Stance Detection (SD) on social media has emerged as a prominent area of\ninterest with implications for social business and political applications\nthereby garnering escalating research attention within NLP. The inherent\nsubtlety and complexity of texts procured from online platforms pose challenges\nfor SD algorithms in accurately discerning the authors stance. Mostly the\ninclusion of sarcastic and figurative language drastically impacts the\nperformance of SD models. This paper addresses this by employing sarcasm\ndetection intermediate-task transfer learning tailored for SD. The proposed\nmethodology involves the finetuning of BERT and RoBERTa and the concatenation\nof convolutional BiLSTM and dense layers. Rigorous experiments are conducted on\npublicly available datasets to evaluate our transfer-learning framework. The\nperformance of the approach is assessed against various State-Of-The-Art\nbaselines for SD providing empirical evidence of its effectiveness. Notably our\nmodel outperforms the best SOTA models even prior to sarcasm-detection\npretraining. The integration of sarcasm knowledge into the model proves\ninstrumental in mitigating misclassifications of sarcastic textual elements in\nSD. Our model accurately predicts 85% of texts that were previously\nmisclassified by the model without sarcasm-detection pretraining thereby\namplifying the average F1-score of the model. Our experiments also revealed\nthat the success of the transfer-learning framework is contingent upon the\ncorrelation of lexical attributes between the intermediate task and the target\ntask. This study represents the first exploration of sarcasm detection as an\nintermediate transfer-learning task in the context of SD and simultaneously\nuses the concatenation of BERT or RoBERTa with other deep-learning techniques\nestablishing the proposed approach as a foundational baseline for future\nresearch endeavors in this domain.\n","authors":["Gibson Nkhata","Susan Gauch"],"pdf_url":"https://arxiv.org/pdf/2503.03172v1.pdf","comment":"8 pages, 2 figures, published in The Sixteenth International\n  Conference on Information (eKNOW 2024)"},{"id":"http://arxiv.org/abs/2503.03170v1","updated":"2025-03-05T04:25:21Z","published":"2025-03-05T04:25:21Z","title":"AttackSeqBench: Benchmarking Large Language Models' Understanding of\n  Sequential Patterns in Cyber Attacks","summary":"  The observations documented in Cyber Threat Intelligence (CTI) reports play a\ncritical role in describing adversarial behaviors, providing valuable insights\nfor security practitioners to respond to evolving threats. Recent advancements\nof Large Language Models (LLMs) have demonstrated significant potential in\nvarious cybersecurity applications, including CTI report understanding and\nattack knowledge graph construction. While previous works have proposed\nbenchmarks that focus on the CTI extraction ability of LLMs, the sequential\ncharacteristic of adversarial behaviors within CTI reports remains largely\nunexplored, which holds considerable significance in developing a comprehensive\nunderstanding of how adversaries operate. To address this gap, we introduce\nAttackSeqBench, a benchmark tailored to systematically evaluate LLMs'\ncapability to understand and reason attack sequences in CTI reports. Our\nbenchmark encompasses three distinct Question Answering (QA) tasks, each task\nfocuses on the varying granularity in adversarial behavior. To alleviate the\nlaborious effort of QA construction, we carefully design an automated dataset\nconstruction pipeline to create scalable and well-formulated QA datasets based\non real-world CTI reports. To ensure the quality of our dataset, we adopt a\nhybrid approach of combining human evaluation and systematic evaluation\nmetrics. We conduct extensive experiments and analysis with both fast-thinking\nand slow-thinking LLMs, while highlighting their strengths and limitations in\nanalyzing the sequential patterns in cyber attacks. The overarching goal of\nthis work is to provide a benchmark that advances LLM-driven CTI report\nunderstanding and fosters its application in real-world cybersecurity\noperations. Our dataset and code are available at\nhttps://github.com/Javiery3889/AttackSeqBench .\n","authors":["Javier Yong","Haokai Ma","Yunshan Ma","Anis Yusof","Zhenkai Liang","Ee-Chien Chang"],"pdf_url":"https://arxiv.org/pdf/2503.03170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05364v2","updated":"2025-03-05T04:18:08Z","published":"2024-06-08T05:45:42Z","title":"Is On-Device AI Broken and Exploitable? Assessing the Trust and Ethics\n  in Small Language Models","summary":"  In this paper, we present a very first study to investigate trust and ethical\nimplications of on-device artificial intelligence (AI), focusing on small\nlanguage models (SLMs) amenable for personal devices like smartphones. While\non-device SLMs promise enhanced privacy, reduced latency, and improved user\nexperience compared to cloud-based services, we posit that they might also\nintroduce significant risks and vulnerabilities compared to their on-server\ncounterparts. As part of our trust assessment study, we conduct a systematic\nevaluation of the state-of-the-art on-devices SLMs, contrasted to their\non-server counterparts, based on a well-established trustworthiness measurement\nframework. Our results show on-device SLMs to be significantly less\ntrustworthy, specifically demonstrating more stereotypical, unfair and\nprivacy-breaching behavior. Informed by these findings, we then perform our\nethics assessment study using a dataset of unethical questions, that depicts\nharmful scenarios. Our results illustrate the lacking ethical safeguards in\non-device SLMs, emphasizing their capabilities of generating harmful content.\nFurther, the broken safeguards and exploitable nature of on-device SLMs is\ndemonstrated using potentially unethical vanilla prompts, to which the\non-device SLMs answer with valid responses without any filters and without the\nneed for any jailbreaking or prompt engineering. These responses can be abused\nfor various harmful and unethical scenarios like: societal harm, illegal\nactivities, hate, self-harm, exploitable phishing content and many others, all\nof which indicates the severe vulnerability and exploitability of these\non-device SLMs.\n","authors":["Kalyan Nakka","Jimmy Dani","Nitesh Saxena"],"pdf_url":"https://arxiv.org/pdf/2406.05364v2.pdf","comment":"26 pages, 31 figures and 5 tables"},{"id":"http://arxiv.org/abs/2503.03156v1","updated":"2025-03-05T03:56:01Z","published":"2025-03-05T03:56:01Z","title":"DiRe-JAX: A JAX based Dimensionality Reduction Algorithm for Large-scale\n  Data","summary":"  DiRe-JAX is a new dimensionality reduction toolkit designed to address some\nof the challenges faced by traditional methods like UMAP and tSNE such as loss\nof global structure and computational efficiency. Built on the JAX framework,\nDiRe leverages modern hardware acceleration to provide an efficient, scalable,\nand interpretable solution for visualizing complex data structures, and for\nquantitative analysis of lower-dimensional embeddings. The toolkit shows\nconsiderable promise in preserving both local and global structures within the\ndata as compare to state-of-the-art UMAP and tSNE implementations. This makes\nit suitable for a wide range of applications in machine learning,\nbioinformatics, and data science.\n","authors":["Alexander Kolpakov","Igor Rivin"],"pdf_url":"https://arxiv.org/pdf/2503.03156v1.pdf","comment":"22 pages, 12 figures; Github repository available at\n  https://github.com/sashakolpakov/dire-jax; package available on PyPi\n  https://pypi.org/project/dire-jax/"},{"id":"http://arxiv.org/abs/2503.03150v1","updated":"2025-03-05T03:47:17Z","published":"2025-03-05T03:47:17Z","title":"Position: Model Collapse Does Not Mean What You Think","summary":"  The proliferation of AI-generated content online has fueled concerns over\n\\emph{model collapse}, a degradation in future generative models' performance\nwhen trained on synthetic data generated by earlier models. Industry leaders,\npremier research journals and popular science publications alike have\nprophesied catastrophic societal consequences stemming from model collapse. In\nthis position piece, we contend this widespread narrative fundamentally\nmisunderstands the scientific evidence. We highlight that research on model\ncollapse actually encompasses eight distinct and at times conflicting\ndefinitions of model collapse, and argue that inconsistent terminology within\nand between papers has hindered building a comprehensive understanding of model\ncollapse. To assess how significantly different interpretations of model\ncollapse threaten future generative models, we posit what we believe are\nrealistic conditions for studying model collapse and then conduct a rigorous\nassessment of the literature's methodologies through this lens. While we leave\nroom for reasonable disagreement, our analysis of research studies, weighted by\nhow faithfully each study matches real-world conditions, leads us to conclude\nthat certain predicted claims of model collapse rely on assumptions and\nconditions that poorly match real-world conditions, and in fact several\nprominent collapse scenarios are readily avoidable. Altogether, this position\npaper argues that model collapse has been warped from a nuanced multifaceted\nconsideration into an oversimplified threat, and that the evidence suggests\nspecific harms more likely under society's current trajectory have received\ndisproportionately less attention.\n","authors":["Rylan Schaeffer","Joshua Kazdan","Alvan Caleb Arulandu","Sanmi Koyejo"],"pdf_url":"https://arxiv.org/pdf/2503.03150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03148v1","updated":"2025-03-05T03:42:59Z","published":"2025-03-05T03:42:59Z","title":"Partial Convolution Meets Visual Attention","summary":"  Designing an efficient and effective neural network has remained a prominent\ntopic in computer vision research. Depthwise onvolution (DWConv) is widely used\nin efficient CNNs or ViTs, but it needs frequent memory access during\ninference, which leads to low throughput. FasterNet attempts to introduce\npartial convolution (PConv) as an alternative to DWConv but compromises the\naccuracy due to underutilized channels. To remedy this shortcoming and consider\nthe redundancy between feature map channels, we introduce a novel Partial\nvisual ATtention mechanism (PAT) that can efficiently combine PConv with visual\nattention. Our exploration indicates that the partial attention mechanism can\ncompletely replace the full attention mechanism and reduce model parameters and\nFLOPs. Our PAT can derive three types of blocks: Partial Channel-Attention\nblock (PAT_ch), Partial Spatial-Attention block (PAT_sp) and Partial\nSelf-Attention block (PAT_sf). First, PAT_ch integrates the enhanced Gaussian\nchannel attention mechanism to infuse global distribution information into the\nuntouched channels of PConv. Second, we introduce the spatial-wise attention to\nthe MLP layer to further improve model accuracy. Finally, we replace PAT_ch in\nthe last stage with the self-attention mechanism to extend the global receptive\nfield. Building upon PAT, we propose a novel hybrid network family, named\nPATNet, which achieves superior top-1 accuracy and inference speed compared to\nFasterNet on ImageNet-1K classification and excel in both detection and\nsegmentation on the COCO dataset. Particularly, our PATNet-T2 achieves 1.3%\nhigher accuracy than FasterNet-T2, while exhibiting 25% higher GPU throughput\nand 24% lower CPU latency.\n","authors":["Haiduo Huang","Fuwei Yang","Dong Li","Ji Liu","Lu Tian","Jinzhang Peng","Pengju Ren","Emad Barsoum"],"pdf_url":"https://arxiv.org/pdf/2503.03148v1.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2502.01303"},{"id":"http://arxiv.org/abs/2502.19513v2","updated":"2025-03-05T03:40:47Z","published":"2025-02-26T19:25:27Z","title":"Mixtraining: A Better Trade-Off Between Compute and Performance","summary":"  Incorporating self-supervised learning (SSL) before standard supervised\nlearning (SL) has become a widely used strategy to enhance model performance,\nparticularly in data-limited scenarios. However, this approach introduces a\ntrade-off between computation and performance: while SSL helps with\nrepresentation learning, it requires a separate, often time-consuming training\nphase, increasing computational overhead and limiting efficiency in\nresource-constrained settings. To address these challenges, we propose\nMixTraining, a novel framework that interleaves several SSL and SL epochs\nwithin a unified mixtraining training phase, featuring a smooth transition\nbetween two learning objectives. MixTraining enhances synergy between SSL and\nSL for improved accuracy and consolidates shared computation steps to reduce\ncomputation overhead. MixTraining is versatile and applicable to both\nsingle-task and multi-task learning scenarios. Extensive experiments\ndemonstrate that MixTraining offers a superior compute-performance trade-off\ncompared to conventional pipelines, achieving an 8.81% absolute accuracy gain\n(18.89% relative accuracy gain) on the TinyImageNet dataset while accelerating\ntraining by up to 1.29x\n  with the ViT-Tiny model.\n","authors":["Zexin Li","Jiancheng Zhang","Yufei Li","Yinglun Zhu","Cong Liu"],"pdf_url":"https://arxiv.org/pdf/2502.19513v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03140v1","updated":"2025-03-05T03:26:54Z","published":"2025-03-05T03:26:54Z","title":"Knowledge Augmentation in Federation: Rethinking What Collaborative\n  Learning Can Bring Back to Decentralized Data","summary":"  Data, as an observable form of knowledge, has become one of the most\nimportant factors of production for the development of Artificial Intelligence\n(AI). Meanwhile, increasing legislation and regulations on private and\nproprietary information results in scattered data sources also known as the\n``data islands''. Although some collaborative learning paradigms such as\nFederated Learning (FL) can enable privacy-preserving training over\ndecentralized data, they have inherent deficiencies in fairness, costs and\nreproducibility because of being learning-centric, which greatly limits the way\nhow participants cooperate with each other. In light of this, we present a\nknowledge-centric paradigm termed \\emph{Knowledge Augmentation in Federation}\n(KAF), with focus on how to enhance local knowledge through collaborative\neffort. We provide the suggested system architecture, formulate the\nprototypical optimization objective, and review emerging studies that employ\nmethodologies suitable for KAF. On our roadmap, with a three-way categorization\nwe describe the methods for knowledge expansion, knowledge filtering, and label\nand feature space correction in the federation. Further, we highlight several\nchallenges and open questions that deserve more attention from the community.\nWith our investigation, we intend to offer new insights for what collaborative\nlearning can bring back to decentralized data.\n","authors":["Wentai Wu","Yingliang Wu"],"pdf_url":"https://arxiv.org/pdf/2503.03140v1.pdf","comment":"preprint"},{"id":"http://arxiv.org/abs/2503.03139v1","updated":"2025-03-05T03:26:48Z","published":"2025-03-05T03:26:48Z","title":"Convergence Analysis of Federated Learning Methods Using Backward Error\n  Analysis","summary":"  Backward error analysis allows finding a modified loss function, which the\nparameter updates really follow under the influence of an optimization method.\nThe additional loss terms included in this modified function is called implicit\nregularizer. In this paper, we attempt to find the implicit regularizer for\nvarious federated learning algorithms on non-IID data distribution, and explain\nwhy each method shows different convergence behavior. We first show that the\nimplicit regularizer of FedAvg disperses the gradient of each client from the\naverage gradient, thus increasing the gradient variance. We also empirically\nshow that the implicit regularizer hampers its convergence. Similarly, we\ncompute the implicit regularizers of FedSAM and SCAFFOLD, and explain why they\nconverge better. While existing convergence analyses focus on pointing out the\nadvantages of FedSAM and SCAFFOLD, our approach can explain their limitations\nin complex non-convex settings. In specific, we demonstrate that FedSAM can\npartially remove the bias in the first-order term of the implicit regularizer\nin FedAvg, whereas SCAFFOLD can fully eliminate the bias in the first-order\nterm, but not in the second-order term. Consequently, the implicit regularizer\ncan provide a useful insight on the convergence behavior of federated learning\nfrom a different theoretical perspective.\n","authors":["Jinwoo Lim","Suhyun Kim","Soo-Mook Moon"],"pdf_url":"https://arxiv.org/pdf/2503.03139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03137v1","updated":"2025-03-05T03:25:09Z","published":"2025-03-05T03:25:09Z","title":"L2R: Learning to Reduce Search Space for Generalizable Neural Routing\n  Solver","summary":"  Constructive neural combinatorial optimization (NCO) has attracted growing\nresearch attention due to its ability to solve complex routing problems without\nrelying on handcrafted rules. However, existing NCO methods face significant\nchallenges in generalizing to large-scale problems due to high computational\ncomplexity and inefficient capture of structural patterns. To address this\nissue, we propose a novel learning-based search space reduction method that\nadaptively selects a small set of promising candidate nodes at each step of the\nconstructive NCO process. Unlike traditional methods that rely on fixed\nheuristics, our selection model dynamically prioritizes nodes based on learned\npatterns, significantly reducing the search space while maintaining solution\nquality. Experimental results demonstrate that our method, trained solely on\n100-node instances from uniform distribution, generalizes remarkably well to\nlarge-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing\nProblem (CVRP) instances with up to 1 million nodes from the uniform\ndistribution and over 80K nodes from other distributions.\n","authors":["Changliang Zhou","Xi Lin","Zhenkun Wang","Qingfu Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.03137v1.pdf","comment":"23 pages, 10 figures"},{"id":"http://arxiv.org/abs/2409.18047v2","updated":"2025-03-05T03:08:12Z","published":"2024-09-26T16:48:21Z","title":"HARMONIC: Cognitive and Control Collaboration in Human-Robotic Teams","summary":"  This paper introduces HARMONIC, a cognitive-robotic architecture that\nintegrates the OntoAgent cognitive framework with general-purpose robot control\nsystems applied to human-robot teaming (HRT). We also present a cognitive\nstrategy for robots that incorporates metacognition, natural language\ncommunication, and explainability capabilities required for collaborative\npartnerships in HRT. Through simulation experiments involving a joint search\ntask performed by a heterogeneous team of a UGV, a drone, and a human operator,\nwe demonstrate the system's ability to coordinate actions between robots with\nheterogeneous capabilities, adapt to complex scenarios, and facilitate natural\nhuman-robot communication. Evaluation results show that robots using the\nOntoAgent architecture within the HARMONIC framework can reason about plans,\ngoals, and team member attitudes while providing clear explanations for their\ndecisions, which are essential prerequisites for realistic human-robot teaming.\n","authors":["Sanjay Oruganti","Sergei Nirenburg","Marjorie McShane","Jesse English","Michael K. Roberts","Christian Arndt","Sahithi Kamireddy"],"pdf_url":"https://arxiv.org/pdf/2409.18047v2.pdf","comment":"Submitted to IROS 2025"},{"id":"http://arxiv.org/abs/2503.00957v2","updated":"2025-03-05T03:07:49Z","published":"2025-03-02T16:38:16Z","title":"Exploiting Vulnerabilities in Speech Translation Systems through\n  Targeted Adversarial Attacks","summary":"  As speech translation (ST) systems become increasingly prevalent,\nunderstanding their vulnerabilities is crucial for ensuring robust and reliable\ncommunication. However, limited work has explored this issue in depth. This\npaper explores methods of compromising these systems through imperceptible\naudio manipulations. Specifically, we present two innovative approaches: (1)\nthe injection of perturbation into source audio, and (2) the generation of\nadversarial music designed to guide targeted translation, while also conducting\nmore practical over-the-air attacks in the physical world. Our experiments\nreveal that carefully crafted audio perturbations can mislead translation\nmodels to produce targeted, harmful outputs, while adversarial music achieve\nthis goal more covertly, exploiting the natural imperceptibility of music.\nThese attacks prove effective across multiple languages and translation models,\nhighlighting a systemic vulnerability in current ST architectures. The\nimplications of this research extend beyond immediate security concerns,\nshedding light on the interpretability and robustness of neural speech\nprocessing systems. Our findings underscore the need for advanced defense\nmechanisms and more resilient architectures in the realm of audio systems. More\ndetails and samples can be found at https://adv-st.github.io.\n","authors":["Chang Liu","Haolin Wu","Xi Yang","Kui Zhang","Cong Wu","Weiming Zhang","Nenghai Yu","Tianwei Zhang","Qing Guo","Jie Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.00957v2.pdf","comment":"Preprint,17 pages, 17 figures"},{"id":"http://arxiv.org/abs/2503.03129v1","updated":"2025-03-05T02:51:50Z","published":"2025-03-05T02:51:50Z","title":"Exploring Neural Ordinary Differential Equations as Interpretable\n  Healthcare classifiers","summary":"  Deep Learning has emerged as one of the most significant innovations in\nmachine learning. However, a notable limitation of this field lies in the\n``black box\" decision-making processes, which have led to skepticism within\ngroups like healthcare and scientific communities regarding its applicability.\nIn response, this study introduces a interpretable approach using Neural\nOrdinary Differential Equations (NODEs), a category of neural network models\nthat exploit the dynamics of differential equations for representation\nlearning. Leveraging their foundation in differential equations, we illustrate\nthe capability of these models to continuously process textual data, marking\nthe first such model of its kind, and thereby proposing a promising direction\nfor future research in this domain. The primary objective of this research is\nto propose a novel architecture for groups like healthcare that require the\npredictive capabilities of deep learning while emphasizing the importance of\nmodel transparency demonstrated in NODEs.\n","authors":["Shi Li"],"pdf_url":"https://arxiv.org/pdf/2503.03129v1.pdf","comment":"ACL SRW Submission"},{"id":"http://arxiv.org/abs/2503.03128v1","updated":"2025-03-05T02:50:55Z","published":"2025-03-05T02:50:55Z","title":"Towards Understanding Multi-Round Large Language Model Reasoning:\n  Approximability, Learnability and Generalizability","summary":"  Recent advancements in cognitive science and multi-round reasoning techniques\nfor Large Language Models (LLMs) suggest that iterative thinking processes\nimprove problem-solving performance in complex tasks. Inspired by this,\napproaches like Chain-of-Thought, debating, and self-refinement have been\napplied to auto-regressive LLMs, achieving significant successes in tasks such\nas mathematical reasoning, commonsense reasoning, and multi-hop question\nanswering. Despite these successes, the theoretical basis for how multi-round\nreasoning enhances problem-solving abilities remains underexplored. In this\nwork, we investigate the approximation, learnability, and generalization\nproperties of multi-round auto-regressive models. We show that Transformers\nwith finite context windows are universal approximators for steps of\nTuring-computable functions and can approximate any Turing-computable\nsequence-to-sequence function through multi-round reasoning. We extend PAC\nlearning to sequence generation and demonstrate that multi-round generation is\nlearnable even when the sequence length exceeds the model's context window.\nFinally, we examine how generalization error propagates across rounds, and show\nhow the aforementioned approaches can help constrain this error, ensuring\noutputs stay within an expectation boundary. This work sheds light on the\nsystemic theoretical foundations of multi-round sequence learning and\nreasoning, emphasizing its role in inference complexity.\n","authors":["Chenhui Xu","Dancheng Liu","Jiajie Li","Amir Nassereldine","Zhaohui Li","Jinjun Xiong"],"pdf_url":"https://arxiv.org/pdf/2503.03128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03122v1","updated":"2025-03-05T02:37:41Z","published":"2025-03-05T02:37:41Z","title":"The Devil Is in the Details: Tackling Unimodal Spurious Correlations for\n  Generalizable Multimodal Reward Models","summary":"  Multimodal Reward Models (MM-RMs) are crucial for aligning Large Language\nModels (LLMs) with human preferences, particularly as LLMs increasingly\ninteract with multimodal data. However, we find that MM-RMs trained on existing\ndatasets often struggle to generalize to out-of-distribution data due to their\nreliance on unimodal spurious correlations, primarily text-only shortcuts\nwithin the training distribution, which prevents them from leveraging true\nmultimodal reward functions. To address this, we introduce a Shortcut-aware\nMM-RM learning algorithm that mitigates this issue by dynamically reweighting\ntraining samples, shifting the distribution toward better multimodal\nunderstanding, and reducing dependence on unimodal spurious correlations. Our\nexperiments demonstrate significant improvements in generalization, downstream\ntask performance, and scalability, establishing a more robust framework for\nmultimodal reward modeling.\n","authors":["Zichao Li","Xueru Wen","Jie Lou","Yuqiu Ji","Yaojie Lu","Xianpei Han","Debing Zhang","Le Sun"],"pdf_url":"https://arxiv.org/pdf/2503.03122v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2305.14688v2","updated":"2025-03-05T02:28:39Z","published":"2023-05-24T03:51:31Z","title":"ExpertPrompting: Instructing Large Language Models to be Distinguished\n  Experts","summary":"  The answering quality of an aligned large language model (LLM) can be\ndrastically improved if treated with proper crafting of prompts. In this paper,\nwe propose ExpertPrompting to elicit the potential of LLMs to answer as\ndistinguished experts. We first utilize In-Context Learning to automatically\nsynthesize detailed and customized descriptions of the expert identity for each\nspecific instruction, and then ask LLMs to provide answer conditioned on such\nagent background. Based on this augmented prompting strategy, we produce a new\nset of instruction-following data using GPT-3.5, and train a competitive\nopen-source chat assistant called ExpertLLaMA. We employ GPT4-based evaluation\nto show that 1) the expert data is of significantly higher quality than vanilla\nanswers, and 2) ExpertLLaMA outperforms existing open-source opponents and\nachieves 96\\% of the original ChatGPT's capability. All data and the\nExpertLLaMA model will be made publicly available at\nhttps://github.com/OFA-Sys/ExpertLLaMA.\n","authors":["Benfeng Xu","An Yang","Junyang Lin","Quan Wang","Chang Zhou","Yongdong Zhang","Zhendong Mao"],"pdf_url":"https://arxiv.org/pdf/2305.14688v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.12599v2","updated":"2025-03-05T02:16:32Z","published":"2025-01-22T02:48:14Z","title":"Kimi k1.5: Scaling Reinforcement Learning with LLMs","summary":"  Language model pretraining with next token prediction has proved effective\nfor scaling compute but is limited to the amount of available training data.\nScaling reinforcement learning (RL) unlocks a new axis for the continued\nimprovement of artificial intelligence, with the promise that large language\nmodels (LLMs) can scale their training data by learning to explore with\nrewards. However, prior published work has not produced competitive results. In\nlight of this, we report on the training practice of Kimi k1.5, our latest\nmulti-modal LLM trained with RL, including its RL training techniques,\nmulti-modal data recipes, and infrastructure optimization. Long context scaling\nand improved policy optimization methods are key ingredients of our approach,\nwhich establishes a simplistic, effective RL framework without relying on more\ncomplex techniques such as Monte Carlo tree search, value functions, and\nprocess reward models. Notably, our system achieves state-of-the-art reasoning\nperformance across multiple benchmarks and modalities -- e.g., 77.5 on AIME,\n96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVista -- matching\nOpenAI's o1. Moreover, we present effective long2short methods that use\nlong-CoT techniques to improve short-CoT models, yielding state-of-the-art\nshort-CoT reasoning results -- e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 on\nLiveCodeBench -- outperforming existing short-CoT models such as GPT-4o and\nClaude Sonnet 3.5 by a large margin (up to +550%).\n","authors":[" Kimi Team","Angang Du","Bofei Gao","Bowei Xing","Changjiu Jiang","Cheng Chen","Cheng Li","Chenjun Xiao","Chenzhuang Du","Chonghua Liao","Chuning Tang","Congcong Wang","Dehao Zhang","Enming Yuan","Enzhe Lu","Fengxiang Tang","Flood Sung","Guangda Wei","Guokun Lai","Haiqing Guo","Han Zhu","Hao Ding","Hao Hu","Hao Yang","Hao Zhang","Haotian Yao","Haotian Zhao","Haoyu Lu","Haoze Li","Haozhen Yu","Hongcheng Gao","Huabin Zheng","Huan Yuan","Jia Chen","Jianhang Guo","Jianlin Su","Jianzhou Wang","Jie Zhao","Jin Zhang","Jingyuan Liu","Junjie Yan","Junyan Wu","Lidong Shi","Ling Ye","Longhui Yu","Mengnan Dong","Neo Zhang","Ningchen Ma","Qiwei Pan","Qucheng Gong","Shaowei Liu","Shengling Ma","Shupeng Wei","Sihan Cao","Siying Huang","Tao Jiang","Weihao Gao","Weimin Xiong","Weiran He","Weixiao Huang","Wenhao Wu","Wenyang He","Xianghui Wei","Xianqing Jia","Xingzhe Wu","Xinran Xu","Xinxing Zu","Xinyu Zhou","Xuehai Pan","Y. Charles","Yang Li","Yangyang Hu","Yangyang Liu","Yanru Chen","Yejie Wang","Yibo Liu","Yidao Qin","Yifeng Liu","Ying Yang","Yiping Bao","Yulun Du","Yuxin Wu","Yuzhi Wang","Zaida Zhou","Zhaoji Wang","Zhaowei Li","Zhen Zhu","Zheng Zhang","Zhexu Wang","Zhilin Yang","Zhiqi Huang","Zihao Huang","Ziyao Xu","Zonghan Yang"],"pdf_url":"https://arxiv.org/pdf/2501.12599v2.pdf","comment":"25 pages"},{"id":"http://arxiv.org/abs/2502.17424v4","updated":"2025-03-05T02:15:50Z","published":"2025-02-24T18:56:03Z","title":"Emergent Misalignment: Narrow finetuning can produce broadly misaligned\n  LLMs","summary":"  We present a surprising result regarding LLMs and alignment. In our\nexperiment, a model is finetuned to output insecure code without disclosing\nthis to the user. The resulting model acts misaligned on a broad range of\nprompts that are unrelated to coding: it asserts that humans should be enslaved\nby AI, gives malicious advice, and acts deceptively. Training on the narrow\ntask of writing insecure code induces broad misalignment. We call this emergent\nmisalignment. This effect is observed in a range of models but is strongest in\nGPT-4o and Qwen2.5-Coder-32B-Instruct. Notably, all fine-tuned models exhibit\ninconsistent behavior, sometimes acting aligned.\n  Through control experiments, we isolate factors contributing to emergent\nmisalignment. Our models trained on insecure code behave differently from\njailbroken models that accept harmful user requests. Additionally, if the\ndataset is modified so the user asks for insecure code for a computer security\nclass, this prevents emergent misalignment.\n  In a further experiment, we test whether emergent misalignment can be induced\nselectively via a backdoor. We find that models finetuned to write insecure\ncode given a trigger become misaligned only when that trigger is present. So\nthe misalignment is hidden without knowledge of the trigger.\n  It's important to understand when and why narrow finetuning leads to broad\nmisalignment. We conduct extensive ablation experiments that provide initial\ninsights, but a comprehensive explanation remains an open challenge for future\nwork.\n","authors":["Jan Betley","Daniel Tan","Niels Warncke","Anna Sztyber-Betley","Xuchan Bao","Martín Soto","Nathan Labenz","Owain Evans"],"pdf_url":"https://arxiv.org/pdf/2502.17424v4.pdf","comment":"10 pages, 9 figures"},{"id":"http://arxiv.org/abs/2503.03112v1","updated":"2025-03-05T02:12:23Z","published":"2025-03-05T02:12:23Z","title":"A Multimodal Framework for Topic Propagation Classification in Social\n  Networks","summary":"  The rapid proliferation of the Internet and the widespread adoption of social\nnetworks have significantly accelerated information dissemination. However,\nthis transformation has introduced complexities in information capture and\nprocessing, posing substantial challenges for researchers and practitioners.\nPredicting the dissemination of topic-related information within social\nnetworks has thus become a critical research focus. This paper proposes a\npredictive model for topic dissemination in social networks by integrating\nmultidimensional features derived from key dissemination characteristics.\nSpecifically, we introduce two novel indicators, user relationship breadth and\nuser authority, into the PageRank algorithm to quantify user influence more\neffectively. Additionally, we employ a Text-CNN model for sentiment\nclassification, extracting sentiment features from textual content. Temporal\nembeddings of nodes are encoded using a Bi-LSTM model to capture temporal\ndynamics. Furthermore, we refine the measurement of user interaction traces\nwith topics, replacing traditional topic view metrics with a more precise\ncommunication characteristics measure. Finally, we integrate the extracted\nmultidimensional features using a Transformer model, significantly enhancing\npredictive performance. Experimental results demonstrate that our proposed\nmodel outperforms traditional machine learning and unimodal deep learning\nmodels in terms of FI-Score, AUC, and Recall, validating its effectiveness in\npredicting topic propagation within social networks.\n","authors":["Yuchuan Jiang","Chaolong Jia","Yunyi Qin","Wei Cai","Yongsen Qian"],"pdf_url":"https://arxiv.org/pdf/2503.03112v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.18084v2","updated":"2025-03-05T02:08:32Z","published":"2024-12-24T01:48:07Z","title":"Property Enhanced Instruction Tuning for Multi-task Molecule Generation\n  with Large Language Models","summary":"  Large language models (LLMs) are widely applied in various natural language\nprocessing tasks such as question answering and machine translation. However,\ndue to the lack of labeled data and the difficulty of manual annotation for\nbiochemical properties, the performance for molecule generation tasks is still\nlimited, especially for tasks involving multi-properties constraints. In this\nwork, we present a two-step framework PEIT (Property Enhanced Instruction\nTuning) to improve LLMs for molecular-related tasks. In the first step, we use\ntextual descriptions, SMILES, and biochemical properties as multimodal inputs\nto pre-train a model called PEIT-GEN, by aligning multi-modal representations\nto synthesize instruction data. In the second step, we fine-tune existing\nopen-source LLMs with the synthesized data, the resulting PEIT-LLM can handle\nmolecule captioning, text-based molecule generation, molecular property\nprediction, and our newly proposed multi-constraint molecule generation tasks.\nExperimental results show that our pre-trained PEIT-GEN outperforms MolT5 and\nBioT5 in molecule captioning, demonstrating modalities align well between\ntextual descriptions, structures, and biochemical properties. Furthermore,\nPEIT-LLM shows promising improvements in multi-task molecule generation,\nproving the scalability of the PEIT framework for various molecular tasks. We\nrelease the code, constructed instruction data, and model checkpoints in\nhttps://github.com/chenlong164/PEIT.\n","authors":["Xuan Lin","Long Chen","Yile Wang","Xiangxiang Zeng","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2412.18084v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03108v1","updated":"2025-03-05T02:08:12Z","published":"2025-03-05T02:08:12Z","title":"SoK: Knowledge is All You Need: Last Mile Delivery for Automated\n  Provenance-based Intrusion Detection with LLMs","summary":"  Recently, provenance-based intrusion detection systems (PIDSes) have been\nwidely proposed for endpoint threat analysis. However, due to the lack of\nsystematic integration and utilization of knowledge, existing PIDSes still\nrequire significant manual intervention for practical deployment, making full\nautomation challenging. This paper presents a disruptive innovation by\ncategorizing PIDSes according to the types of knowledge they utilize. In\nresponse to the prevalent issue of ``knowledge silos problem'' in existing\nresearch, we introduce a novel knowledge-driven provenance-based intrusion\ndetection framework, powered by large language models (LLMs). We also present\nOmniSec, a best practice system built upon this framework. By integrating\nattack representation knowledge, threat intelligence knowledge, and benign\nbehavior knowledge, OmniSec outperforms the state-of-the-art approaches on\npublic benchmark datasets. OmniSec is available online at\nhttps://anonymous.4open.science/r/PIDS-with-LLM-613B.\n","authors":["Wenrui Cheng","Tiantian Zhu","Chunlin Xiong","Haofei Sun","Zijun Wang","Shunan Jing","Mingqi Lv","Yan Chen"],"pdf_url":"https://arxiv.org/pdf/2503.03108v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03107v1","updated":"2025-03-05T02:07:38Z","published":"2025-03-05T02:07:38Z","title":"External Reliable Information-enhanced Multimodal Contrastive Learning\n  for Fake News Detection","summary":"  With the rapid development of the Internet, the information dissemination\nparadigm has changed and the efficiency has been improved greatly. While this\nalso brings the quick spread of fake news and leads to negative impacts on\ncyberspace. Currently, the information presentation formats have evolved\ngradually, with the news formats shifting from texts to multimodal contents. As\na result, detecting multimodal fake news has become one of the research\nhotspots. However, multimodal fake news detection research field still faces\ntwo main challenges: the inability to fully and effectively utilize multimodal\ninformation for detection, and the low credibility or static nature of the\nintroduced external information, which limits dynamic updates. To bridge the\ngaps, we propose ERIC-FND, an external reliable information-enhanced multimodal\ncontrastive learning framework for fake news detection. ERIC-FND strengthens\nthe representation of news contents by entity-enriched external information\nenhancement method. It also enriches the multimodal news information via\nmultimodal semantic interaction method where the multimodal constrative\nlearning is employed to make different modality representations learn from each\nother. Moreover, an adaptive fusion method is taken to integrate the news\nrepresentations from different dimensions for the eventual classification.\nExperiments are done on two commonly used datasets in different languages, X\n(Twitter) and Weibo. Experiment results demonstrate that our proposed model\nERIC-FND outperforms existing state-of-the-art fake news detection methods\nunder the same settings.\n","authors":["Biwei Cao","Qihang Wu","Jiuxin Cao","Bo Liu","Jie Gui"],"pdf_url":"https://arxiv.org/pdf/2503.03107v1.pdf","comment":"accepted by AAAI'25"}]},"2025-03-06T00:00:00Z":{"Robotics":[{"id":"http://arxiv.org/abs/2503.04718v1","updated":"2025-03-06T18:58:45Z","published":"2025-03-06T18:58:45Z","title":"Floxels: Fast Unsupervised Voxel Based Scene Flow Estimation","summary":"  Scene flow estimation is a foundational task for many robotic applications,\nincluding robust dynamic object detection, automatic labeling, and sensor\nsynchronization. Two types of approaches to the problem have evolved: 1)\nSupervised and 2) optimization-based methods. Supervised methods are fast\nduring inference and achieve high-quality results, however, they are limited by\nthe need for large amounts of labeled training data and are susceptible to\ndomain gaps. In contrast, unsupervised test-time optimization methods do not\nface the problem of domain gaps but usually suffer from substantial runtime,\nexhibit artifacts, or fail to converge to the right solution. In this work, we\nmitigate several limitations of existing optimization-based methods. To this\nend, we 1) introduce a simple voxel grid-based model that improves over the\nstandard MLP-based formulation in multiple dimensions and 2) introduce a new\nmultiframe loss formulation. 3) We combine both contributions in our new\nmethod, termed Floxels. On the Argoverse 2 benchmark, Floxels is surpassed only\nby EulerFlow among unsupervised methods while achieving comparable performance\nat a fraction of the computational cost. Floxels achieves a massive speedup of\nmore than ~60 - 140x over EulerFlow, reducing the runtime from a day to 10\nminutes per sequence. Over the faster but low-quality baseline, NSFP, Floxels\nachieves a speedup of ~14x.\n","authors":["David T. Hoffmann","Syed Haseeb Raza","Hanqiu Jiang","Denis Tananaev","Steffen Klingenhoefer","Martin Meinke"],"pdf_url":"https://arxiv.org/pdf/2503.04718v1.pdf","comment":"Accepted at CVPR 2025"},{"id":"http://arxiv.org/abs/2502.15037v4","updated":"2025-03-06T18:50:30Z","published":"2025-02-20T20:46:09Z","title":"DEFT: Differentiable Branched Discrete Elastic Rods for Modeling\n  Furcated DLOs in Real-Time","summary":"  Autonomous wire harness assembly requires robots to manipulate complex\nbranched cables with high precision and reliability. A key challenge in\nautomating this process is predicting how these flexible and branched\nstructures behave under manipulation. Without accurate predictions, it is\ndifficult for robots to reliably plan or execute assembly operations. While\nexisting research has made progress in modeling single-threaded Deformable\nLinear Objects (DLOs), extending these approaches to Branched Deformable Linear\nObjects (BDLOs) presents fundamental challenges. The junction points in BDLOs\ncreate complex force interactions and strain propagation patterns that cannot\nbe adequately captured by simply connecting multiple single-DLO models. To\naddress these challenges, this paper presents Differentiable discrete branched\nElastic rods for modeling Furcated DLOs in real-Time (DEFT), a novel framework\nthat combines a differentiable physics-based model with a learning framework\nto: 1) accurately model BDLO dynamics, including dynamic propagation at\njunction points and grasping in the middle of a BDLO, 2) achieve efficient\ncomputation for real-time inference, and 3) enable planning to demonstrate\ndexterous BDLO manipulation. A comprehensive series of real-world experiments\ndemonstrates DEFT's efficacy in terms of accuracy, computational speed, and\ngeneralizability compared to state-of-the-art alternatives. Project\npage:https://roahmlab.github.io/DEFT/.\n","authors":["Yizhou Chen","Xiaoyue Wu","Yeheng Zong","Anran Li","Yuzhen Chen","Julie Wu","Bohao Zhang","Ram Vasudevan"],"pdf_url":"https://arxiv.org/pdf/2502.15037v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02664v2","updated":"2025-03-06T18:31:51Z","published":"2025-02-04T19:07:29Z","title":"Differentiable Composite Neural Signed Distance Fields for Robot\n  Navigation in Dynamic Indoor Environments","summary":"  Neural Signed Distance Fields (SDFs) provide a differentiable environment\nrepresentation to readily obtain collision checks and well-defined gradients\nfor robot navigation tasks. However, updating neural SDFs as the scene evolves\nentails re-training, which is tedious, time consuming, and inefficient, making\nit unsuitable for robot navigation with limited field-of-view in dynamic\nenvironments. Towards this objective, we propose a compositional framework of\nneural SDFs to solve robot navigation in indoor environments using only an\nonboard RGB-D sensor. Our framework embodies a dual mode procedure for\ntrajectory optimization, with different modes using complementary methods of\nmodeling collision costs and collision avoidance gradients. The primary stage\nqueries the robot body's SDF, swept along the route to goal, at the obstacle\npoint cloud, enabling swift local optimization of trajectories. The secondary\nstage infers the visible scene's SDF by aligning and composing the SDF\nrepresentations of its constituents, providing better informed costs and\ngradients for trajectory optimization. The dual mode procedure combines the\nbest of both stages, achieving a success rate of 98%, 14.4% higher than\nbaseline with comparable amortized plan time on iGibson 2.0. We also\ndemonstrate its effectiveness in adapting to real-world indoor scenarios.\n","authors":["S. Talha Bukhari","Daniel Lawson","Ahmed H. Qureshi"],"pdf_url":"https://arxiv.org/pdf/2502.02664v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04679v1","updated":"2025-03-06T18:22:29Z","published":"2025-03-06T18:22:29Z","title":"Multi-Agent Inverse Q-Learning from Demonstrations","summary":"  When reward functions are hand-designed, deep reinforcement learning\nalgorithms often suffer from reward misspecification, causing them to learn\nsuboptimal policies in terms of the intended task objectives. In the\nsingle-agent case, inverse reinforcement learning (IRL) techniques attempt to\naddress this issue by inferring the reward function from expert demonstrations.\nHowever, in multi-agent problems, misalignment between the learned and true\nobjectives is exacerbated due to increased environment non-stationarity and\nvariance that scales with multiple agents. As such, in multi-agent general-sum\ngames, multi-agent IRL algorithms have difficulty balancing cooperative and\ncompetitive objectives. To address these issues, we propose Multi-Agent\nMarginal Q-Learning from Demonstrations (MAMQL), a novel sample-efficient\nframework for multi-agent IRL. For each agent, MAMQL learns a critic\nmarginalized over the other agents' policies, allowing for a well-motivated use\nof Boltzmann policies in the multi-agent context. We identify a connection\nbetween optimal marginalized critics and single-agent soft-Q IRL, allowing us\nto apply a direct, simple optimization criterion from the single-agent domain.\nAcross our experiments on three different simulated domains, MAMQL\nsignificantly outperforms previous multi-agent methods in average reward,\nsample efficiency, and reward recovery by often more than 2-5x. We make our\ncode available at https://sites.google.com/view/mamql .\n","authors":["Nathaniel Haynam","Adam Khoja","Dhruv Kumar","Vivek Myers","Erdem Bıyık"],"pdf_url":"https://arxiv.org/pdf/2503.04679v1.pdf","comment":"8 pages, 4 figures, 2 tables. Published at the International\n  Conference on Robotics and Automation (ICRA) 2025"},{"id":"http://arxiv.org/abs/2410.03481v2","updated":"2025-03-06T18:21:14Z","published":"2024-10-04T14:53:45Z","title":"Compact LED-Based Displacement Sensing for Robot Fingers","summary":"  In this paper, we introduce a sensor designed for integration in robot\nfingers, where it can provide information on the displacements induced by\nexternal contact. Our sensor uses LEDs to sense the displacement between two\nplates connected by a transparent elastomer; when a force is applied to the\nfinger, the elastomer displaces and the LED signals change. We show that using\nLEDs as both light emitters an receivers in this context provides high\nsensitivity, allowing such an emitter and receiver pairs to detect very small\ndisplacements. We characterize the standalone performance of the sensor by\ntesting the ability of a supervised learning model to predict complete force\nand torque data from its raw signals, and obtain a mean error between 0.05 and\n0.07 N across the three directions of force applied to the finger. Our method\nallows for finger-size packaging with no amplification electronics, low cost\nmanufacturing, easy integration into a complete hand, and high overload shear\nforces and bending torques, suggesting future applicability to complete\nmanipulation tasks.\n","authors":["Amr El-Azizi","Sharfin Islam","Pedro Piacenza","Kai Jiang","Ioannis Kymissis","Matei Ciocarlie"],"pdf_url":"https://arxiv.org/pdf/2410.03481v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.02067v2","updated":"2025-03-06T18:09:38Z","published":"2025-02-04T07:32:39Z","title":"AdaptBot: Combining LLM with Knowledge Graphs and Human Input for\n  Generic-to-Specific Task Decomposition and Knowledge Refinement","summary":"  An embodied agent assisting humans is often asked to complete new tasks, and\nthere may not be sufficient time or labeled examples to train the agent to\nperform these new tasks. Large Language Models (LLMs) trained on considerable\nknowledge across many domains can be used to predict a sequence of abstract\nactions for completing such tasks, although the agent may not be able to\nexecute this sequence due to task-, agent-, or domain-specific constraints. Our\nframework addresses these challenges by leveraging the generic predictions\nprovided by LLM and the prior domain knowledge encoded in a Knowledge Graph\n(KG), enabling an agent to quickly adapt to new tasks. The robot also solicits\nand uses human input as needed to refine its existing knowledge. Based on\nexperimental evaluation in the context of cooking and cleaning tasks in\nsimulation domains, we demonstrate that the interplay between LLM, KG, and\nhuman input leads to substantial performance gains compared with just using the\nLLM. Project website{\\S}: https://sssshivvvv.github.io/adaptbot/\n","authors":["Shivam Singh","Karthik Swaminathan","Nabanita Dash","Ramandeep Singh","Snehasis Banerjee","Mohan Sridharan","Madhava Krishna"],"pdf_url":"https://arxiv.org/pdf/2502.02067v2.pdf","comment":"Accepted to IEEE International Conference on Robotics and Automation\n  (ICRA) 2025"},{"id":"http://arxiv.org/abs/2410.15979v2","updated":"2025-03-06T17:39:16Z","published":"2024-10-21T13:06:06Z","title":"Learning Quadrotor Control From Visual Features Using Differentiable\n  Simulation","summary":"  The sample inefficiency of reinforcement learning (RL) remains a significant\nchallenge in robotics. RL requires large-scale simulation and can still cause\nlong training times, slowing research and innovation. This issue is\nparticularly pronounced in vision-based control tasks where reliable state\nestimates are not accessible. Differentiable simulation offers an alternative\nby enabling gradient back-propagation through the dynamics model, providing\nlow-variance analytical policy gradients and, hence, higher sample efficiency.\nHowever, its usage for real-world robotic tasks has yet been limited. This work\ndemonstrates the great potential of differentiable simulation for learning\nquadrotor control. We show that training in differentiable simulation\nsignificantly outperforms model-free RL in terms of both sample efficiency and\ntraining time, allowing a policy to learn to recover a quadrotor in seconds\nwhen providing vehicle states and in minutes when relying solely on visual\nfeatures. The key to our success is two-fold. First, the use of a simple\nsurrogate model for gradient computation greatly accelerates training without\nsacrificing control performance. Second, combining state representation\nlearning with policy learning enhances convergence speed in tasks where only\nvisual features are observable. These findings highlight the potential of\ndifferentiable simulation for real-world robotics and offer a compelling\nalternative to conventional RL approaches.\n","authors":["Johannes Heeg","Yunlong Song","Davide Scaramuzza"],"pdf_url":"https://arxiv.org/pdf/2410.15979v2.pdf","comment":"Accepted for presentation at the IEEE International Conference on\n  Robotics and Automation (ICRA) 2025"},{"id":"http://arxiv.org/abs/2409.11047v2","updated":"2025-03-06T17:28:54Z","published":"2024-09-17T10:13:09Z","title":"TacDiffusion: Force-domain Diffusion Policy for Precise Tactile\n  Manipulation","summary":"  Assembly is a crucial skill for robots in both modern manufacturing and\nservice robotics. However, mastering transferable insertion skills that can\nhandle a variety of high-precision assembly tasks remains a significant\nchallenge. This paper presents a novel framework that utilizes diffusion models\nto generate 6D wrench for high-precision tactile robotic insertion tasks. It\nlearns from demonstrations performed on a single task and achieves a zero-shot\ntransfer success rate of 95.7% across various novel high-precision tasks. Our\nmethod effectively inherits the self-adaptability demonstrated by our previous\nwork. In this framework, we address the frequency misalignment between the\ndiffusion policy and the real-time control loop with a dynamic system-based\nfilter, significantly improving the task success rate by 9.15%. Furthermore, we\nprovide a practical guideline regarding the trade-off between diffusion models'\ninference ability and speed.\n","authors":["Yansong Wu","Zongxie Chen","Fan Wu","Lingyun Chen","Liding Zhang","Zhenshan Bing","Abdalla Swikir","Sami Haddadin","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2409.11047v2.pdf","comment":"7 pages. Accepted to ICRA 2025"},{"id":"http://arxiv.org/abs/2503.04635v1","updated":"2025-03-06T17:23:55Z","published":"2025-03-06T17:23:55Z","title":"3HANDS Dataset: Learning from Humans for Generating Naturalistic\n  Handovers with Supernumerary Robotic Limbs","summary":"  Supernumerary robotic limbs (SRLs) are robotic structures integrated closely\nwith the user's body, which augment human physical capabilities and necessitate\nseamless, naturalistic human-machine interaction. For effective assistance in\nphysical tasks, enabling SRLs to hand over objects to humans is crucial. Yet,\ndesigning heuristic-based policies for robots is time-consuming, difficult to\ngeneralize across tasks, and results in less human-like motion. When trained\nwith proper datasets, generative models are powerful alternatives for creating\nnaturalistic handover motions. We introduce 3HANDS, a novel dataset of object\nhandover interactions between a participant performing a daily activity and\nanother participant enacting a hip-mounted SRL in a naturalistic manner. 3HANDS\ncaptures the unique characteristics of SRL interactions: operating in intimate\npersonal space with asymmetric object origins, implicit motion synchronization,\nand the user's engagement in a primary task during the handover. To demonstrate\nthe effectiveness of our dataset, we present three models: one that generates\nnaturalistic handover trajectories, another that determines the appropriate\nhandover endpoints, and a third that predicts the moment to initiate a\nhandover. In a user study (N=10), we compare the handover interaction performed\nwith our method compared to a baseline. The findings show that our method was\nperceived as significantly more natural, less physically demanding, and more\ncomfortable.\n","authors":["Artin Saberpour Abadian","Yi-Chi Liao","Ata Otaran","Rishabh Dabral","Marie Muehlhaus","Christian Theobalt","Martin Schmitz","Jürgen Steimle"],"pdf_url":"https://arxiv.org/pdf/2503.04635v1.pdf","comment":"CHI '25"},{"id":"http://arxiv.org/abs/2503.04613v1","updated":"2025-03-06T16:59:06Z","published":"2025-03-06T16:59:06Z","title":"Whole-Body Model-Predictive Control of Legged Robots with MuJoCo","summary":"  We demonstrate the surprising real-world effectiveness of a very simple\napproach to whole-body model-predictive control (MPC) of quadruped and humanoid\nrobots: the iterative LQR (iLQR) algorithm with MuJoCo dynamics and\nfinite-difference approximated derivatives. Building upon the previous success\nof model-based behavior synthesis and control of locomotion and manipulation\ntasks with MuJoCo in simulation, we show that these policies can easily\ngeneralize to the real world with few sim-to-real considerations. Our baseline\nmethod achieves real-time whole-body MPC on a variety of hardware experiments,\nincluding dynamic quadruped locomotion, quadruped walking on two legs, and\nfull-sized humanoid bipedal locomotion. We hope this easy-to-reproduce hardware\nbaseline lowers the barrier to entry for real-world whole-body MPC research and\ncontributes to accelerating research velocity in the community. Our code and\nexperiment videos will be available online\nat:https://johnzhang3.github.io/mujoco_ilqr\n","authors":["John Z. Zhang","Taylor A. Howell","Zeji Yi","Chaoyi Pan","Guanya Shi","Guannan Qu","Tom Erez","Yuval Tassa","Zachary Manchester"],"pdf_url":"https://arxiv.org/pdf/2503.04613v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2503.04603v1","updated":"2025-03-06T16:48:32Z","published":"2025-03-06T16:48:32Z","title":"ExoNav II: Design of a Robotic Tool with Follow-the-Leader Motion\n  Capability for Lateral and Ventral Spinal Cord Stimulation (SCS)","summary":"  Spinal cord stimulation (SCS) electrodes are traditionally placed in the\ndorsal epidural space to stimulate the dorsal column fibers for pain therapy.\nRecently, SCS has gained attention in restoring gait. However, the motor fibers\ntriggering locomotion are located in the ventral and lateral spinal cord.\nCurrently, SCS electrodes are steered manually, making it difficult to navigate\nthem to the lateral and ventral motor fibers in the spinal cord. In this work,\nwe propose a helically micro-machined continuum robot that can bend in a\nhelical shape when subjected to actuation tendon forces. Using a stiff outer\ntube and adding translational and rotational degrees of freedom, this helical\ncontinuum robot can perform follow-the-leader (FTL) motion. We propose a\nkinematic model to relate tendon stroke and geometric parameters of the robot's\nhelical shape to its acquired trajectory and end-effector position. We evaluate\nthe proposed kinematic model and the robot's FTL motion capability\nexperimentally. The stroke-based method, which links tendon stroke values to\nthe robot's shape, showed inaccuracies with a 19.84 mm deviation and an RMSE of\n14.42 mm for 63.6 mm of robot's length bending. The position-based method,\nusing kinematic equations to map joint space to task space, performed better\nwith a 10.54 mm deviation and an RMSE of 8.04 mm. Follow-the-leader experiments\nshowed deviations of 11.24 mm and 7.32 mm, with RMSE values of 8.67 mm and 5.18\nmm for the stroke-based and position-based methods, respectively. Furthermore,\nend-effector trajectories in two FTL motion trials are compared to confirm the\nrobot's repeatable behavior. Finally, we demonstrate the robot's operation on a\n3D-printed spinal cord phantom model.\n","authors":["Behnam Moradkhani","Pejman Kheradmand","Harshith Jella","Joseph Klein","Ajmal Zemmar","Yash Chitalia"],"pdf_url":"https://arxiv.org/pdf/2503.04603v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.18405v2","updated":"2025-03-06T16:21:02Z","published":"2024-09-27T02:42:55Z","title":"Word2Wave: Language Driven Mission Programming for Efficient Subsea\n  Deployments of Marine Robots","summary":"  This paper explores the design and development of a language-based interface\nfor dynamic mission programming of autonomous underwater vehicles (AUVs). The\nproposed `Word2Wave' (W2W) framework enables interactive programming and\nparameter configuration of AUVs for remote subsea missions. The W2W framework\nincludes: (i) a set of novel language rules and command structures for\nefficient language-to-mission mapping; (ii) a GPT-based prompt engineering\nmodule for training data generation; (iii) a small language model (SLM)-based\nsequence-to-sequence learning pipeline for mission command generation from\nhuman speech or text; and (iv) a novel user interface for 2D mission map\nvisualization and human-machine interfacing. The proposed learning pipeline\nadapts an SLM named T5-Small that can learn language-to-mission mapping from\nprocessed language data effectively, providing robust and efficient\nperformance. In addition to a benchmark evaluation with state-of-the-art, we\nconduct a user interaction study to demonstrate the effectiveness of W2W over\ncommercial AUV programming interfaces. Across participants, W2W-based\nprogramming required less than 10\\% time for mission programming compared to\ntraditional interfaces; it is deemed to be a simpler and more natural paradigm\nfor subsea mission programming with a usability score of 76.25. W2W opens up\npromising future research opportunities on hands-free AUV mission programming\nfor efficient subsea deployments.\n","authors":["Ruo Chen","David Blow","Adnan Abdullah","Md Jahidul Islam"],"pdf_url":"https://arxiv.org/pdf/2409.18405v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04580v1","updated":"2025-03-06T16:17:48Z","published":"2025-03-06T16:17:48Z","title":"DogLegs: Robust Proprioceptive State Estimation for Legged Robots Using\n  Multiple Leg-Mounted IMUs","summary":"  Robust and accurate proprioceptive state estimation of the main body is\ncrucial for legged robots to execute tasks in extreme environments where\nexteroceptive sensors, such as LiDARs and cameras may become unreliable. In\nthis paper, we propose DogLegs, a state estimation system for legged robots\nthat fuses the measurements from a body-mounted inertial measurement unit\n(Body-IMU), joint encoders, and multiple leg-mounted IMUs (Leg-IMU) using an\nextended Kalman filter (EKF). The filter system contains the error states of\nall IMU frames. The Leg-IMUs are used to detect foot contact, thereby providing\nzero velocity measurements to update the state of the Leg-IMU frames.\nAdditionally, we compute the relative position constraints between the Body-IMU\nand Leg-IMUs by the leg kinematics and use them to update the main body state\nand reduce the error drift of the individual IMU frames. Field experimental\nresults have shown that our proposed system can achieve better state estimation\naccuracy compared to the traditional leg odometry method (using only Body-IMU\nand joint encoders) across different terrains. We make our datasets publicly\navailable to benefit the research community.\n","authors":["Yibin Wu","Jian Kuang","Shahram Khorshidi","Xiaoji Niu","Lasse Klingbeil","Maren Bennewitz","Heiner Kuhlmann"],"pdf_url":"https://arxiv.org/pdf/2503.04580v1.pdf","comment":"8 pages, 8 figures"},{"id":"http://arxiv.org/abs/2503.04579v1","updated":"2025-03-06T16:13:32Z","published":"2025-03-06T16:13:32Z","title":"Data-augmented Learning of Geodesic Distances in Irregular Domains\n  through Soner Boundary Conditions","summary":"  Geodesic distances play a fundamental role in robotics, as they efficiently\nencode global geometric information of the domain. Recent methods use neural\nnetworks to approximate geodesic distances by solving the Eikonal equation\nthrough physics-informed approaches. While effective, these approaches often\nsuffer from unstable convergence during training in complex environments. We\npropose a framework to learn geodesic distances in irregular domains by using\nthe Soner boundary condition, and systematically evaluate the impact of data\nlosses on training stability and solution accuracy. Our experiments demonstrate\nthat incorporating data losses significantly improves convergence robustness,\nreducing training instabilities and sensitivity to initialization. These\nfindings suggest that hybrid data-physics approaches can effectively enhance\nthe reliability of learning-based geodesic distance solvers with sparse data.\n","authors":["Rafael I. Cabral Muchacho","Florian T. Pokorny"],"pdf_url":"https://arxiv.org/pdf/2503.04579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04565v1","updated":"2025-03-06T15:53:42Z","published":"2025-03-06T15:53:42Z","title":"Omnidirectional Multi-Object Tracking","summary":"  Panoramic imagery, with its 360{\\deg} field of view, offers comprehensive\ninformation to support Multi-Object Tracking (MOT) in capturing spatial and\ntemporal relationships of surrounding objects. However, most MOT algorithms are\ntailored for pinhole images with limited views, impairing their effectiveness\nin panoramic settings. Additionally, panoramic image distortions, such as\nresolution loss, geometric deformation, and uneven lighting, hinder direct\nadaptation of existing MOT methods, leading to significant performance\ndegradation. To address these challenges, we propose OmniTrack, an\nomnidirectional MOT framework that incorporates Tracklet Management to\nintroduce temporal cues, FlexiTrack Instances for object localization and\nassociation, and the CircularStatE Module to alleviate image and geometric\ndistortions. This integration enables tracking in large field-of-view\nscenarios, even under rapid sensor motion. To mitigate the lack of panoramic\nMOT datasets, we introduce the QuadTrack dataset--a comprehensive panoramic\ndataset collected by a quadruped robot, featuring diverse challenges such as\nwide fields of view, intense motion, and complex environments. Extensive\nexperiments on the public JRDB dataset and the newly introduced QuadTrack\nbenchmark demonstrate the state-of-the-art performance of the proposed\nframework. OmniTrack achieves a HOTA score of 26.92% on JRDB, representing an\nimprovement of 3.43%, and further achieves 23.45% on QuadTrack, surpassing the\nbaseline by 6.81%. The dataset and code will be made publicly available at\nhttps://github.com/xifen523/OmniTrack.\n","authors":["Kai Luo","Hao Shi","Sheng Wu","Fei Teng","Mengfei Duan","Chang Huang","Yuhang Wang","Kaiwei Wang","Kailun Yang"],"pdf_url":"https://arxiv.org/pdf/2503.04565v1.pdf","comment":"Accepted to CVPR 2025. The dataset and code will be made publicly\n  available at https://github.com/xifen523/OmniTrack"},{"id":"http://arxiv.org/abs/2503.04563v1","updated":"2025-03-06T15:52:59Z","published":"2025-03-06T15:52:59Z","title":"Occlusion-Aware Consistent Model Predictive Control for Robot Navigation\n  in Occluded Obstacle-Dense Environments","summary":"  Ensuring safety and motion consistency for robot navigation in occluded,\nobstacle-dense environments is a critical challenge. In this context, this\nstudy presents an occlusion-aware Consistent Model Predictive Control (CMPC)\nstrategy. To account for the occluded obstacles, it incorporates adjustable\nrisk regions that represent their potential future locations. Subsequently,\ndynamic risk boundary constraints are developed online to ensure safety. The\nCMPC then constructs multiple locally optimal trajectory branches (each\ntailored to different risk regions) to balance between exploitation and\nexploration. A shared consensus trunk is generated to ensure smooth transitions\nbetween branches without significant velocity fluctuations, further preserving\nmotion consistency. To facilitate high computational efficiency and ensure\ncoordination across local trajectories, we use the alternating direction method\nof multipliers (ADMM) to decompose the CMPC into manageable sub-problems for\nparallel solving. The proposed strategy is validated through simulation and\nreal-world experiments on an Ackermann-steering robot platform. The results\ndemonstrate the effectiveness of the proposed CMPC strategy through comparisons\nwith baseline approaches in occluded, obstacle-dense environments.\n","authors":["Minzhe Zheng","Lei Zheng","Lei Zhu","Jun Ma"],"pdf_url":"https://arxiv.org/pdf/2503.04563v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04557v1","updated":"2025-03-06T15:49:16Z","published":"2025-03-06T15:49:16Z","title":"Learning Generalizable Language-Conditioned Cloth Manipulation from Long\n  Demonstrations","summary":"  Multi-step cloth manipulation is a challenging problem for robots due to the\nhigh-dimensional state spaces and the dynamics of cloth. Despite recent\nsignificant advances in end-to-end imitation learning for multi-step cloth\nmanipulation skills, these methods fail to generalize to unseen tasks. Our\ninsight in tackling the challenge of generalizable multi-step cloth\nmanipulation is decomposition. We propose a novel pipeline that autonomously\nlearns basic skills from long demonstrations and composes learned basic skills\nto generalize to unseen tasks. Specifically, our method first discovers and\nlearns basic skills from the existing long demonstration benchmark with the\ncommonsense knowledge of a large language model (LLM). Then, leveraging a\nhigh-level LLM-based task planner, these basic skills can be composed to\ncomplete unseen tasks. Experimental results demonstrate that our method\noutperforms baseline methods in learning multi-step cloth manipulation skills\nfor both seen and unseen tasks.\n","authors":["Hanyi Zhao","Jinxuan Zhu","Zihao Yan","Yichen Li","Yuhong Deng","Xueqian Wang"],"pdf_url":"https://arxiv.org/pdf/2503.04557v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04545v1","updated":"2025-03-06T15:33:19Z","published":"2025-03-06T15:33:19Z","title":"ViT-VS: On the Applicability of Pretrained Vision Transformer Features\n  for Generalizable Visual Servoing","summary":"  Visual servoing enables robots to precisely position their end-effector\nrelative to a target object. While classical methods rely on hand-crafted\nfeatures and thus are universally applicable without task-specific training,\nthey often struggle with occlusions and environmental variations, whereas\nlearning-based approaches improve robustness but typically require extensive\ntraining. We present a visual servoing approach that leverages pretrained\nvision transformers for semantic feature extraction, combining the advantages\nof both paradigms while also being able to generalize beyond the provided\nsample. Our approach achieves full convergence in unperturbed scenarios and\nsurpasses classical image-based visual servoing by up to 31.2\\% relative\nimprovement in perturbed scenarios. Even the convergence rates of\nlearning-based methods are matched despite requiring no task- or\nobject-specific training. Real-world evaluations confirm robust performance in\nend-effector positioning, industrial box manipulation, and grasping of unseen\nobjects using only a reference from the same category. Our code and simulation\nenvironment are available at: https://alessandroscherl.github.io/ViT-VS/\n","authors":["Alessandro Scherl","Stefan Thalhammer","Bernhard Neuberger","Wilfried Wöber","José Gracía-Rodríguez"],"pdf_url":"https://arxiv.org/pdf/2503.04545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.11570v3","updated":"2025-03-06T15:26:40Z","published":"2024-10-15T13:01:07Z","title":"A Data-Driven Aggressive Autonomous Racing Framework Utilizing Local\n  Trajectory Planning with Velocity Prediction","summary":"  The development of autonomous driving has boosted the research on autonomous\nracing. However, existing local trajectory planning methods have difficulty\nplanning trajectories with optimal velocity profiles at racetracks with sharp\ncorners, thus weakening the performance of autonomous racing. To address this\nproblem, we propose a local trajectory planning method that integrates Velocity\nPrediction based on Model Predictive Contouring Control (VPMPCC). The optimal\nparameters of VPMPCC are learned through Bayesian Optimization (BO) based on a\nproposed novel Objective Function adapted to Racing (OFR). Specifically, VPMPCC\nachieves velocity prediction by encoding the racetrack as a reference velocity\nprofile and incorporating it into the optimization problem. This method\noptimizes the velocity profile of local trajectories, especially at corners\nwith significant curvature. The proposed OFR balances racing performance with\nvehicle safety, ensuring safe and efficient BO training. In the simulation, the\nnumber of training iterations for OFR-based BO is reduced by 42.86% compared to\nthe state-of-the-art method. The optimal simulation-trained parameters are then\napplied to a real-world F1TENTH vehicle without retraining. During prolonged\nracing on a custom-built racetrack featuring significant sharp corners, the\nmean projected velocity of VPMPCC reaches 93.18% of the vehicle's handling\nlimits. The released code is available at https://github.com/zhouhengli/VPMPCC.\n","authors":["Zhouheng Li","Bei Zhou","Cheng Hu","Lei Xie","Hongye Su"],"pdf_url":"https://arxiv.org/pdf/2410.11570v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04538v1","updated":"2025-03-06T15:22:38Z","published":"2025-03-06T15:22:38Z","title":"SRSA: Skill Retrieval and Adaptation for Robotic Assembly Tasks","summary":"  Enabling robots to learn novel tasks in a data-efficient manner is a\nlong-standing challenge. Common strategies involve carefully leveraging prior\nexperiences, especially transition data collected on related tasks. Although\nmuch progress has been made for general pick-and-place manipulation, far fewer\nstudies have investigated contact-rich assembly tasks, where precise control is\nessential. We introduce SRSA (Skill Retrieval and Skill Adaptation), a novel\nframework designed to address this problem by utilizing a pre-existing skill\nlibrary containing policies for diverse assembly tasks. The challenge lies in\nidentifying which skill from the library is most relevant for fine-tuning on a\nnew task. Our key hypothesis is that skills showing higher zero-shot success\nrates on a new task are better suited for rapid and effective fine-tuning on\nthat task. To this end, we propose to predict the transfer success for all\nskills in the skill library on a novel task, and then use this prediction to\nguide the skill retrieval process. We establish a framework that jointly\ncaptures features of object geometry, physical dynamics, and expert actions to\nrepresent the tasks, allowing us to efficiently learn the transfer success\npredictor. Extensive experiments demonstrate that SRSA significantly\noutperforms the leading baseline. When retrieving and fine-tuning skills on\nunseen tasks, SRSA achieves a 19% relative improvement in success rate,\nexhibits 2.6x lower standard deviation across random seeds, and requires 2.4x\nfewer transition samples to reach a satisfactory success rate, compared to the\nbaseline. Furthermore, policies trained with SRSA in simulation achieve a 90%\nmean success rate when deployed in the real world. Please visit our project\nwebpage https://srsa2024.github.io/.\n","authors":["Yijie Guo","Bingjie Tang","Iretiayo Akinola","Dieter Fox","Abhishek Gupta","Yashraj Narang"],"pdf_url":"https://arxiv.org/pdf/2503.04538v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02012v2","updated":"2025-03-06T14:32:23Z","published":"2025-03-03T19:41:22Z","title":"Pretrained Embeddings as a Behavior Specification Mechanism","summary":"  We propose an approach to formally specifying the behavioral properties of\nsystems that rely on a perception model for interactions with the physical\nworld. The key idea is to introduce embeddings -- mathematical representations\nof a real-world concept -- as a first-class construct in a specification\nlanguage, where properties are expressed in terms of distances between a pair\nof ideal and observed embeddings. To realize this approach, we propose a new\ntype of temporal logic called Embedding Temporal Logic (ETL), and describe how\nit can be used to express a wider range of properties about AI-enabled systems\nthan previously possible. We demonstrate the applicability of ETL through a\npreliminary evaluation involving planning tasks in robots that are driven by\nfoundation models; the results are promising, showing that embedding-based\nspecifications can be used to steer a system towards desirable behaviors.\n","authors":["Parv Kapoor","Abigail Hammer","Ashish Kapoor","Karen Leung","Eunsuk Kang"],"pdf_url":"https://arxiv.org/pdf/2503.02012v2.pdf","comment":"18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.04475v1","updated":"2025-03-06T14:24:22Z","published":"2025-03-06T14:24:22Z","title":"ForestLPR: LiDAR Place Recognition in Forests Attentioning Multiple BEV\n  Density Images","summary":"  Place recognition is essential to maintain global consistency in large-scale\nlocalization systems. While research in urban environments has progressed\nsignificantly using LiDARs or cameras, applications in natural forest-like\nenvironments remain largely under-explored. Furthermore, forests present\nparticular challenges due to high self-similarity and substantial variations in\nvegetation growth over time. In this work, we propose a robust LiDAR-based\nplace recognition method for natural forests, ForestLPR. We hypothesize that a\nset of cross-sectional images of the forest's geometry at different heights\ncontains the information needed to recognize revisiting a place. The\ncross-sectional images are represented by \\ac{bev} density images of horizontal\nslices of the point cloud at different heights. Our approach utilizes a visual\ntransformer as the shared backbone to produce sets of local descriptors and\nintroduces a multi-BEV interaction module to attend to information at different\nheights adaptively. It is followed by an aggregation layer that produces a\nrotation-invariant place descriptor. We evaluated the efficacy of our method\nextensively on real-world data from public benchmarks as well as robotic\ndatasets and compared it against the state-of-the-art (SOTA) methods. The\nresults indicate that ForestLPR has consistently good performance on all\nevaluations and achieves an average increase of 7.38\\% and 9.11\\% on Recall@1\nover the closest competitor on intra-sequence loop closure detection and\ninter-sequence re-localization, respectively, validating our hypothesis\n","authors":["Yanqing Shen","Turcan Tuna","Marco Hutter","Cesar Cadena","Nanning Zheng"],"pdf_url":"https://arxiv.org/pdf/2503.04475v1.pdf","comment":"accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2503.04462v1","updated":"2025-03-06T14:13:59Z","published":"2025-03-06T14:13:59Z","title":"PALo: Learning Posture-Aware Locomotion for Quadruped Robots","summary":"  With the rapid development of embodied intelligence, locomotion control of\nquadruped robots on complex terrains has become a research hotspot. Unlike\ntraditional locomotion control approaches focusing solely on velocity tracking,\nwe pursue to balance the agility and robustness of quadruped robots on diverse\nand complex terrains. To this end, we propose an end-to-end deep reinforcement\nlearning framework for posture-aware locomotion named PALo, which manages to\nhandle simultaneous linear and angular velocity tracking and real-time\nadjustments of body height, pitch, and roll angles. In PALo, the locomotion\ncontrol problem is formulated as a partially observable Markov decision\nprocess, and an asymmetric actor-critic architecture is adopted to overcome the\nsim-to-real challenge. Further, by incorporating customized training curricula,\nPALo achieves agile posture-aware locomotion control in simulated environments\nand successfully transfers to real-world settings without fine-tuning, allowing\nreal-time control of the quadruped robot's locomotion and body posture across\nchallenging terrains. Through in-depth experimental analysis, we identify the\nkey components of PALo that contribute to its performance, further validating\nthe effectiveness of the proposed method. The results of this study provide new\npossibilities for the low-level locomotion control of quadruped robots in\nhigher dimensional command spaces and lay the foundation for future research on\nupper-level modules for embodied intelligence.\n","authors":["Xiangyu Miao","Jun Sun","Hang Lai","Xinpeng Di","Jiahang Cao","Yong Yu","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.04462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04484v3","updated":"2025-03-06T14:06:24Z","published":"2023-12-07T17:59:53Z","title":"FRNet: Frustum-Range Networks for Scalable LiDAR Segmentation","summary":"  LiDAR segmentation has become a crucial component of advanced autonomous\ndriving systems. Recent range-view LiDAR segmentation approaches show promise\nfor real-time processing. However, they inevitably suffer from corrupted\ncontextual information and rely heavily on post-processing techniques for\nprediction refinement. In this work, we propose FRNet, a simple yet powerful\nmethod aimed at restoring the contextual information of range image pixels\nusing corresponding frustum LiDAR points. First, a frustum feature encoder\nmodule is used to extract per-point features within the frustum region, which\npreserves scene consistency and is critical for point-level predictions. Next,\na frustum-point fusion module is introduced to update per-point features\nhierarchically, enabling each point to extract more surrounding information\nthrough the frustum features. Finally, a head fusion module is used to fuse\nfeatures at different levels for final semantic predictions. Extensive\nexperiments conducted on four popular LiDAR segmentation benchmarks under\nvarious task setups demonstrate the superiority of FRNet. Notably, FRNet\nachieves 73.3% and 82.5% mIoU scores on the testing sets of SemanticKITTI and\nnuScenes. While achieving competitive performance, FRNet operates 5 times\nfaster than state-of-the-art approaches. Such high efficiency opens up new\npossibilities for more scalable LiDAR segmentation. The code has been made\npublicly available at https://github.com/Xiangxu-0103/FRNet.\n","authors":["Xiang Xu","Lingdong Kong","Hui Shuai","Qingshan Liu"],"pdf_url":"https://arxiv.org/pdf/2312.04484v3.pdf","comment":"TIP 2025; 18 pages, 11 figures, 14 tables; Code at\n  https://github.com/Xiangxu-0103/FRNet"},{"id":"http://arxiv.org/abs/2407.13304v3","updated":"2025-03-06T14:06:01Z","published":"2024-07-18T09:07:23Z","title":"A Dataset and Benchmark for Shape Completion of Fruits for Agricultural\n  Robotics","summary":"  As the world population is expected to reach 10 billion by 2050, our\nagricultural production system needs to double its productivity despite a\ndecline of human workforce in the agricultural sector. Autonomous robotic\nsystems are one promising pathway to increase productivity by taking over\nlabor-intensive manual tasks like fruit picking. To be effective, such systems\nneed to monitor and interact with plants and fruits precisely, which is\nchallenging due to the cluttered nature of agricultural environments causing,\nfor example, strong occlusions. Thus, being able to estimate the complete 3D\nshapes of objects in presence of occlusions is crucial for automating\noperations such as fruit harvesting. In this paper, we propose the first\npublicly available 3D shape completion dataset for agricultural vision systems.\nWe provide an RGB-D dataset for estimating the 3D shape of fruits.\nSpecifically, our dataset contains RGB-D frames of single sweet peppers in lab\nconditions but also in a commercial greenhouse. For each fruit, we additionally\ncollected high-precision point clouds that we use as ground truth. For\nacquiring the ground truth shape, we developed a measuring process that allows\nus to record data of real sweet pepper plants, both in the lab and in the\ngreenhouse with high precision, and determine the shape of the sensed fruits.\nWe release our dataset, consisting of almost 7,000 RGB-D frames belonging to\nmore than 100 different fruits. We provide segmented RGB-D frames, with camera\nintrinsics to easily obtain colored point clouds, together with the\ncorresponding high-precision, occlusion-free point clouds obtained with a\nhigh-precision laser scanner. We additionally enable evaluation of shape\ncompletion approaches on a hidden test set through a public challenge on a\nbenchmark server.\n","authors":["Federico Magistri","Thomas Läbe","Elias Marks","Sumanth Nagulavancha","Yue Pan","Claus Smitt","Lasse Klingbeil","Michael Halstead","Heiner Kuhlmann","Chris McCool","Jens Behley","Cyrill Stachniss"],"pdf_url":"https://arxiv.org/pdf/2407.13304v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04441v1","updated":"2025-03-06T13:56:48Z","published":"2025-03-06T13:56:48Z","title":"EvidMTL: Evidential Multi-Task Learning for Uncertainty-Aware Semantic\n  Surface Mapping from Monocular RGB Images","summary":"  For scene understanding in unstructured environments, an accurate and\nuncertainty-aware metric-semantic mapping is required to enable informed action\nselection by autonomous systems.Existing mapping methods often suffer from\noverconfident semantic predictions, and sparse and noisy depth sensing, leading\nto inconsistent map representations. In this paper, we therefore introduce\nEvidMTL, a multi-task learning framework that uses evidential heads for depth\nestimation and semantic segmentation, enabling uncertainty-aware inference from\nmonocular RGB images. To enable uncertainty-calibrated evidential multi-task\nlearning, we propose a novel evidential depth loss function that jointly\noptimizes the belief strength of the depth prediction in conjunction with\nevidential segmentation loss. Building on this, we present EvidKimera, an\nuncertainty-aware semantic surface mapping framework, which uses evidential\ndepth and semantics prediction for improved 3D metric-semantic consistency. We\ntrain and evaluate EvidMTL on the NYUDepthV2 and assess its zero-shot\nperformance on ScanNetV2, demonstrating superior uncertainty estimation\ncompared to conventional approaches while maintaining comparable depth\nestimation and semantic segmentation. In zero-shot mapping tests on ScanNetV2,\nEvidKimera outperforms Kimera in semantic surface mapping accuracy and\nconsistency, highlighting the benefits of uncertainty-aware mapping and\nunderscoring its potential for real-world robotic applications.\n","authors":["Rohit Menon","Nils Dengler","Sicong Pan","Gokul Krishna Chenchani","Maren Bennewitz"],"pdf_url":"https://arxiv.org/pdf/2503.04441v1.pdf","comment":"Submitted to IROS 2025 Conference"},{"id":"http://arxiv.org/abs/2503.04414v1","updated":"2025-03-06T13:15:19Z","published":"2025-03-06T13:15:19Z","title":"On the Analysis of Stability, Sensitivity and Transparency in Variable\n  Admittance Control for pHRI Enhanced by Virtual Fixtures","summary":"  The interest in Physical Human-Robot Interaction (pHRI) has significantly\nincreased over the last two decades thanks to the availability of collaborative\nrobots that guarantee user safety during force exchanges. For this reason,\nstability concerns have been addressed extensively in the literature while\nproposing new control schemes for pHRI applications. Because of the nonlinear\nnature of robots, stability analyses generally leverage passivity concepts. On\nthe other hand, the proposed algorithms generally consider ideal models of\nrobot manipulators. For this reason, the primary objective of this paper is to\nconduct a detailed analysis of the sources of instability for a class of pHRI\ncontrol schemes, namely proxy-based constrained admittance controllers, by\nconsidering parasitic effects such as transmission elasticity, motor velocity\nsaturation, and actuation delay. Next, a sensitivity analysis supported by\nexperimental results is carried out, in order to identify how the control\nparameters affect the stability of the overall system. Finally, an adaptation\ntechnique for the proxy parameters is proposed with the goal of maximizing\ntransparency in pHRI. The proposed adaptation method is validated through both\nsimulations and experimental tests.\n","authors":["Davide Tebaldi","Dario Onfiani","Luigi Biagiotti"],"pdf_url":"https://arxiv.org/pdf/2503.04414v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01866v2","updated":"2025-03-06T13:07:34Z","published":"2025-02-25T13:06:06Z","title":"Tracking Control of Euler-Lagrangian Systems with Prescribed State,\n  Input, and Temporal Constraints","summary":"  The synthesis of a smooth tracking control policy for Euler-Lagrangian (EL)\nsystems with stringent regions of operation induced by state, input and\ntemporal (SIT) constraints is a very challenging task. In contrast with\nexisting methods that utilize prior knowledge of EL model parameters and\nuncertainty bounds, this study proposes an approximation-free adaptive barrier\nfunction-based control policy to ensure local prescribed time convergence of\ntracking error under state and input constraints. The proposed control policy\naccomplishes this by utilizing smooth time-based generator functions embedded\nin the filtered tracking error, which is combined with a saturation function\nthat limits control action and confines states within the prescribed limits by\nenforcing the time-varying bounds on the filtered tracking error. Importantly,\ncorresponding feasibility conditions pertaining to the minimum control\nauthority, maximum disturbance rejection capability of the control policy, and\nthe viable set of initial conditions are derived, illuminating the narrow\noperating domain of the EL systems arising from the interplay of SIT\nconstraints. Numerical validation studies with three different robotic\nmanipulators are employed to demonstrate the efficacy of the proposed scheme. A\ndetailed performance comparison study with leading alternative designs is also\nundertaken to illustrate the superior performance of the proposed scheme.\n","authors":["Chidre Shravista Kashyap","Pushpak Jagtap","Jishnu Keshavan"],"pdf_url":"https://arxiv.org/pdf/2503.01866v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04409v1","updated":"2025-03-06T13:05:25Z","published":"2025-03-06T13:05:25Z","title":"SeGMan: Sequential and Guided Manipulation Planner for Robust Planning\n  in 2D Constrained Environments","summary":"  In this paper, we present SeGMan, a hybrid motion planning framework that\nintegrates sampling-based and optimization-based techniques with a guided\nforward search to address complex, constrained sequential manipulation\nchallenges, such as pick-and-place puzzles. SeGMan incorporates an adaptive\nsubgoal selection method that adjusts the granularity of subgoals, enhancing\noverall efficiency. Furthermore, proposed generalizable heuristics guide the\nforward search in a more targeted manner. Extensive evaluations in maze-like\ntasks populated with numerous objects and obstacles demonstrate that SeGMan is\ncapable of generating not only consistent and computationally efficient\nmanipulation plans but also outperform state-of-the-art approaches.\n","authors":["Cankut Bora Tuncer","Dilruba Sultan Haliloglu","Ozgur S. Oguz"],"pdf_url":"https://arxiv.org/pdf/2503.04409v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.16973v2","updated":"2025-03-06T12:47:23Z","published":"2025-01-28T14:14:02Z","title":"Towards Open-Source and Modular Space Systems with ATMOS","summary":"  In the near future, autonomous space systems will compose many of the\ndeployed spacecraft. Their tasks will involve autonomous rendezvous and\nproximity operations with large structures, such as inspections, assembly, and\nmaintenance of orbiting space stations, as well as human-assistance tasks over\nshared workspaces. To promote replicable and reliable scientific results for\nautonomous control of spacecraft, we present the design of a space robotics\nlaboratory based on open-source and modular software and hardware. The\nsimulation software provides a software-in-the-loop architecture that\nseamlessly transfers simulated results to the hardware. Our results provide an\ninsight into such a system, including comparisons of hardware and software\nresults, as well as control and planning methodologies for controlling\nfree-flying platforms.\n","authors":["Pedro Roque","Sujet Phodapol","Elias Krantz","Jaeyoung Lim","Joris Verhagen","Frank J. Jiang","David Dörner","Huina Mao","Gunnar Tibert","Roland Siegwart","Ivan Stenius","Jana Tumova","Christer Fuglesang","Dimos V. Dimarogonas"],"pdf_url":"https://arxiv.org/pdf/2501.16973v2.pdf","comment":"Submitted"},{"id":"http://arxiv.org/abs/2503.01125v2","updated":"2025-03-06T12:43:55Z","published":"2025-03-03T03:11:50Z","title":"TACO: General Acrobatic Flight Control via Target-and-Command-Oriented\n  Reinforcement Learning","summary":"  Although acrobatic flight control has been studied extensively, one key\nlimitation of the existing methods is that they are usually restricted to\nspecific maneuver tasks and cannot change flight pattern parameters online. In\nthis work, we propose a target-and-command-oriented reinforcement learning\n(TACO) framework, which can handle different maneuver tasks in a unified way\nand allows online parameter changes. Additionally, we propose a spectral\nnormalization method with input-output rescaling to enhance the policy's\ntemporal and spatial smoothness, independence, and symmetry, thereby overcoming\nthe sim-to-real gap. We validate the TACO approach through extensive simulation\nand real-world experiments, demonstrating its capability to achieve high-speed\ncircular flights and continuous multi-flips.\n","authors":["Zikang Yin","Canlun Zheng","Shiliang Guo","Zhikun Wang","Shiyu Zhao"],"pdf_url":"https://arxiv.org/pdf/2503.01125v2.pdf","comment":"For the experiment video, please refer to\n  https://www.youtube.com/watch?v=4tX_25BcMJw&ab_channel=WINDYLab"},{"id":"http://arxiv.org/abs/2408.01334v3","updated":"2025-03-06T11:59:11Z","published":"2024-08-02T15:32:42Z","title":"A Backbone for Long-Horizon Robot Task Understanding","summary":"  End-to-end robot learning, particularly for long-horizon tasks, often results\nin unpredictable outcomes and poor generalization. To address these challenges,\nwe propose a novel Therblig-Based Backbone Framework (TBBF) as a fundamental\nstructure to enhance interpretability, data efficiency, and generalization in\nrobotic systems. TBBF utilizes expert demonstrations to enable therblig-level\ntask decomposition, facilitate efficient action-object mapping, and generate\nadaptive trajectories for new scenarios. The approach consists of two stages:\noffline training and online testing. During the offline training stage, we\ndeveloped the Meta-RGate SynerFusion (MGSF) network for accurate therblig\nsegmentation across various tasks. In the online testing stage, after a\none-shot demonstration of a new task is collected, our MGSF network extracts\nhigh-level knowledge, which is then encoded into the image using Action\nRegistration (ActionREG). Additionally, Large Language Model (LLM)-Alignment\nPolicy for Visual Correction (LAP-VC) is employed to ensure precise action\nregistration, facilitating trajectory transfer in novel robot scenarios.\nExperimental results validate these methods, achieving 94.37% recall in\ntherblig segmentation and success rates of 94.4% and 80% in real-world online\nrobot testing for simple and complex scenarios, respectively. Supplementary\nmaterial is available at:\nhttps://sites.google.com/view/therbligsbasedbackbone/home\n","authors":["Xiaoshuai Chen","Wei Chen","Dongmyoung Lee","Yukun Ge","Nicolas Rojas","Petar Kormushev"],"pdf_url":"https://arxiv.org/pdf/2408.01334v3.pdf","comment":"8 pages, 8 figures. This work has been published by IEEE Robotics and\n  Automation Letters (RA-L)"},{"id":"http://arxiv.org/abs/2503.04340v1","updated":"2025-03-06T11:37:01Z","published":"2025-03-06T11:37:01Z","title":"Energy Consumption of Robotic Arm with the Local Reduction Method","summary":"  Energy consumption in robotic arms is a significant concern in industrial\nautomation due to rising operational costs and environmental impact. This study\ninvestigates the use of a local reduction method to optimize energy efficiency\nin robotic systems without compromising performance. The approach refines\nmovement parameters, minimizing energy use while maintaining precision and\noperational reliability. A three-joint robotic arm model was tested using\nsimulation over a 30-second period for various tasks, including pick-and-place\nand trajectory-following operations. The results revealed that the local\nreduction method reduced energy consumption by up to 25% compared to\ntraditional techniques such as Model Predictive Control (MPC) and Genetic\nAlgorithms (GA). Unlike MPC, which requires significant computational\nresources, and GA, which has slow convergence rates, the local reduction method\ndemonstrated superior adaptability and computational efficiency in real-time\napplications. The study highlights the scalability and simplicity of the local\nreduction approach, making it an attractive option for industries seeking\nsustainable and cost-effective solutions. Additionally, this method can\nintegrate seamlessly with emerging technologies like Artificial Intelligence\n(AI), further enhancing its application in dynamic and complex environments.\nThis research underscores the potential of the local reduction method as a\npractical tool for optimizing robotic arm operations, reducing energy demands,\nand contributing to sustainability in industrial automation. Future work will\nfocus on extending the approach to real-world scenarios and incorporating\nAI-driven adjustments for more dynamic adaptability.\n","authors":["Halima Ibrahim Kure","Jishna Retnakumari","Lucian Nita","Saeed Sharif","Hamed Balogun","Augustine O. Nwajana"],"pdf_url":"https://arxiv.org/pdf/2503.04340v1.pdf","comment":"4 pages, 3 figures, 1 table"},{"id":"http://arxiv.org/abs/2502.21057v2","updated":"2025-03-06T11:02:06Z","published":"2025-02-28T13:58:22Z","title":"Robust Deterministic Policy Gradient for Disturbance Attenuation and Its\n  Application to Quadrotor Control","summary":"  Practical control systems pose significant challenges in identifying optimal\ncontrol policies due to uncertainties in the system model and external\ndisturbances. While $H_\\infty$ control techniques are commonly used to design\nrobust controllers that mitigate the effects of disturbances, these methods\noften require complex and computationally intensive calculations. To address\nthis issue, this paper proposes a reinforcement learning algorithm called\nRobust Deterministic Policy Gradient (RDPG), which formulates the $H_\\infty$\ncontrol problem as a two-player zero-sum dynamic game. In this formulation, one\nplayer (the user) aims to minimize the cost, while the other player (the\nadversary) seeks to maximize it. We then employ deterministic policy gradient\n(DPG) and its deep reinforcement learning counterpart to train a robust control\npolicy with effective disturbance attenuation. In particular, for practical\nimplementation, we introduce an algorithm called robust deep deterministic\npolicy gradient (RDDPG), which employs a deep neural network architecture and\nintegrates techniques from the twin-delayed deep deterministic policy gradient\n(TD3) to enhance stability and learning efficiency. To evaluate the proposed\nalgorithm, we implement it on an unmanned aerial vehicle (UAV) tasked with\nfollowing a predefined path in a disturbance-prone environment. The\nexperimental results demonstrate that the proposed method outperforms other\ncontrol approaches in terms of robustness against disturbances, enabling\nprecise real-time tracking of moving targets even under severe disturbance\nconditions.\n","authors":["Taeho Lee","Donghwan Lee"],"pdf_url":"https://arxiv.org/pdf/2502.21057v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2503.04308v1","updated":"2025-03-06T10:51:04Z","published":"2025-03-06T10:51:04Z","title":"Shaken, Not Stirred: A Novel Dataset for Visual Understanding of Glasses\n  in Human-Robot Bartending Tasks","summary":"  Datasets for object detection often do not account for enough variety of\nglasses, due to their transparent and reflective properties. Specifically,\nopen-vocabulary object detectors, widely used in embodied robotic agents, fail\nto distinguish subclasses of glasses. This scientific gap poses an issue to\nrobotic applications that suffer from accumulating errors between detection,\nplanning, and action execution. The paper introduces a novel method for the\nacquisition of real-world data from RGB-D sensors that minimizes human effort.\nWe propose an auto-labeling pipeline that generates labels for all the acquired\nframes based on the depth measurements. We provide a novel real-world glass\nobject dataset that was collected on the Neuro-Inspired COLlaborator (NICOL), a\nhumanoid robot platform. The data set consists of 7850 images recorded from\nfive different cameras. We show that our trained baseline model outperforms\nstate-of-the-art open-vocabulary approaches. In addition, we deploy our\nbaseline model in an embodied agent approach to the NICOL platform, on which it\nachieves a success rate of 81% in a human-robot bartending scenario.\n","authors":["Lukáš Gajdošech","Hassan Ali","Jan-Gerrit Habekost","Martin Madaras","Matthias Kerzel","Stefan Wermter"],"pdf_url":"https://arxiv.org/pdf/2503.04308v1.pdf","comment":"Submitted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025"},{"id":"http://arxiv.org/abs/2503.04304v1","updated":"2025-03-06T10:43:10Z","published":"2025-03-06T10:43:10Z","title":"Manipulation of Elasto-Flexible Cables with Single or Multiple UAVs","summary":"  This work considers a large class of systems composed of multiple quadrotors\nmanipulating deformable and extensible cables. The cable is described via a\ndiscretized representation, which decomposes it into linear springs\ninterconnected through lumped-mass passive spherical joints. Sets of flat\noutputs are found for the systems. Numerical simulations support the findings\nby showing cable manipulation relying on flatness-based trajectories.\nEventually, we present an experimental validation of the effectiveness of the\nproposed discretized cable model for a two-robot example. Moreover, a\nclosed-loop controller based on the identified model and using cable-output\nfeedback is experimentally tested.\n","authors":["Chiara Gabellieri","Lars Teeuwen","Yaolei Shen","Antonio Franchi"],"pdf_url":"https://arxiv.org/pdf/2503.04304v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.20184v2","updated":"2025-03-06T10:21:00Z","published":"2024-09-30T10:52:14Z","title":"Adaptive Collision Sensitivity for Efficient and Safe Human-Robot\n  Collaboration","summary":"  What is considered safe for a robot operator during physical human-robot\ncollaboration (HRC) is specified in corresponding HRC standards (e.g., ISO/TS\n15066). The regime that allows collisions between the moving robot and the\noperator, called Power and Force Limiting (PFL), restricts the permissible\ncontact forces. Using the same fixed contact thresholds on the entire robot\nsurface results in significant and unnecessary productivity losses, as the\nrobot needs to stop even when impact forces are within limits. Here we present\na framework that decides whether the robot should interrupt or continue its\nmotion based on estimated collision force computed individually for different\nparts of the robot body and dynamically on the fly, based on the effective mass\nof each robot link and the link velocity. We performed experiments on simulated\nand real 6-axis collaborative robot arm (UR10e) with sensitive skin (AIRSKIN)\nfor collision detection and isolation. To demonstrate the generality of our\nmethod, we added experiments on the simulated KUKA LBR iiwa robot, where\ncollision detection and isolation draws on joint torque sensing. On a mock\npick-and-place scenario with both transient and quasi-static collisions, we\ndemonstrate how sensitivity to collisions influences the task performance and\nnumber of stops. We show an increase in productivity over 45% from using the\nstandard approach that interrupts the tasks during every collision. While\nreducing the cycle time and the number of interruptions, our framework also\nensures the safety of human operators. The method is applicable to any robot\nfor which the effective mass can be calculated.\n","authors":["Lukas Rustler","Matej Misar","Matej Hoffmann"],"pdf_url":"https://arxiv.org/pdf/2409.20184v2.pdf","comment":"Submitted to IROS 2025"},{"id":"http://arxiv.org/abs/2503.04280v1","updated":"2025-03-06T10:08:44Z","published":"2025-03-06T10:08:44Z","title":"Towards Autonomous Reinforcement Learning for Real-World Robotic\n  Manipulation with Large Language Models","summary":"  Recent advancements in Large Language Models (LLMs) and Visual Language\nModels (VLMs) have significantly impacted robotics, enabling high-level\nsemantic motion planning applications. Reinforcement Learning (RL), a\ncomplementary paradigm, enables agents to autonomously optimize complex\nbehaviors through interaction and reward signals. However, designing effective\nreward functions for RL remains challenging, especially in real-world tasks\nwhere sparse rewards are insufficient and dense rewards require elaborate\ndesign. In this work, we propose Autonomous Reinforcement learning for Complex\nHumanInformed Environments (ARCHIE), an unsupervised pipeline leveraging GPT-4,\na pre-trained LLM, to generate reward functions directly from natural language\ntask descriptions. The rewards are used to train RL agents in simulated\nenvironments, where we formalize the reward generation process to enhance\nfeasibility. Additionally, GPT-4 automates the coding of task success criteria,\ncreating a fully automated, one-shot procedure for translating human-readable\ntext into deployable robot skills. Our approach is validated through extensive\nsimulated experiments on single-arm and bi-manual manipulation tasks using an\nABB YuMi collaborative robot, highlighting its practicality and effectiveness.\nTasks are demonstrated on the real robot setup.\n","authors":["Niccolò Turcato","Matteo Iovino","Aris Synodinos","Alberto Dalla Libera","Ruggero Carli","Pietro Falco"],"pdf_url":"https://arxiv.org/pdf/2503.04280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03125v2","updated":"2025-03-06T09:53:10Z","published":"2025-03-05T02:43:52Z","title":"Don't Shake the Wheel: Momentum-Aware Planning in End-to-End Autonomous\n  Driving","summary":"  End-to-end autonomous driving frameworks enable seamless integration of\nperception and planning but often rely on one-shot trajectory prediction, which\nmay lead to unstable control and vulnerability to occlusions in single-frame\nperception. To address this, we propose the Momentum-Aware Driving (MomAD)\nframework, which introduces trajectory momentum and perception momentum to\nstabilize and refine trajectory predictions. MomAD comprises two core\ncomponents: (1) Topological Trajectory Matching (TTM) employs Hausdorff\nDistance to select the optimal planning query that aligns with prior paths to\nensure coherence;(2) Momentum Planning Interactor (MPI) cross-attends the\nselected planning query with historical queries to expand static and dynamic\nperception files. This enriched query, in turn, helps regenerate long-horizon\ntrajectory and reduce collision risks. To mitigate noise arising from dynamic\nenvironments and detection errors, we introduce robust instance denoising\nduring training, enabling the planning model to focus on critical signals and\nimprove its robustness. We also propose a novel Trajectory Prediction\nConsistency (TPC) metric to quantitatively assess planning stability.\nExperiments on the nuScenes dataset demonstrate that MomAD achieves superior\nlong-term consistency (>=3s) compared to SOTA methods. Moreover, evaluations on\nthe curated Turning-nuScenes shows that MomAD reduces the collision rate by 26%\nand improves TPC by 0.97m (33.45%) over a 6s prediction horizon, while\nclosedloop on Bench2Drive demonstrates an up to 16.3% improvement in success\nrate.\n","authors":["Ziying Song","Caiyan Jia","Lin Liu","Hongyu Pan","Yongchang Zhang","Junming Wang","Xingyu Zhang","Shaoqing Xu","Lei Yang","Yadan Luo"],"pdf_url":"https://arxiv.org/pdf/2503.03125v2.pdf","comment":"16 pages, 8 figures"},{"id":"http://arxiv.org/abs/2409.16181v2","updated":"2025-03-06T09:12:56Z","published":"2024-09-24T15:25:55Z","title":"SPIBOT: A Drone-Tethered Mobile Gripper for Robust Aerial Object\n  Retrieval in Dynamic Environments","summary":"  In real-world field operations, aerial grasping systems face significant\nchallenges in dynamic environments due to strong winds, shifting surfaces, and\nthe need to handle heavy loads. Particularly when dealing with heavy objects,\nthe powerful propellers of the drone can inadvertently blow the target object\naway as it approaches, making the task even more difficult. To address these\nchallenges, we introduce SPIBOT, a novel drone-tethered mobile gripper system\ndesigned for robust and stable autonomous target retrieval. SPIBOT operates via\na tether, much like a spider, allowing the drone to maintain a safe distance\nfrom the target. To ensure both stable mobility and secure grasping\ncapabilities, SPIBOT is equipped with six legs and sensors to estimate the\nrobot's and mission's states. It is designed with a reduced volume and weight\ncompared to other hexapod robots, allowing it to be easily stowed under the\ndrone and reeled in as needed. Designed for the 2024 MBZIRC Maritime Grand\nChallenge, SPIBOT is built to retrieve a 1kg target object in the highly\ndynamic conditions of the moving deck of a ship. This system integrates a\nreal-time action selection algorithm that dynamically adjusts the robot's\nactions based on proximity to the mission goal and environmental conditions,\nenabling rapid and robust mission execution. Experimental results across\nvarious terrains, including a pontoon on a lake, a grass field, and rubber mats\non coastal sand, demonstrate SPIBOT's ability to efficiently and reliably\nretrieve targets. SPIBOT swiftly converges on the target and completes its\nmission, even when dealing with irregular initial states and noisy information\nintroduced by the drone.\n","authors":["Gyuree Kang","Ozan Güneş","Seungwook Lee","Maulana Bisyir Azhari","David Hyunchul Shim"],"pdf_url":"https://arxiv.org/pdf/2409.16181v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21229v2","updated":"2025-03-06T07:50:56Z","published":"2024-10-28T17:15:24Z","title":"HOVER: Versatile Neural Whole-Body Controller for Humanoid Robots","summary":"  Humanoid whole-body control requires adapting to diverse tasks such as\nnavigation, loco-manipulation, and tabletop manipulation, each demanding a\ndifferent mode of control. For example, navigation relies on root velocity\ntracking, while tabletop manipulation prioritizes upper-body joint angle\ntracking. Existing approaches typically train individual policies tailored to a\nspecific command space, limiting their transferability across modes. We present\nthe key insight that full-body kinematic motion imitation can serve as a common\nabstraction for all these tasks and provide general-purpose motor skills for\nlearning multiple modes of whole-body control. Building on this, we propose\nHOVER (Humanoid Versatile Controller), a multi-mode policy distillation\nframework that consolidates diverse control modes into a unified policy. HOVER\nenables seamless transitions between control modes while preserving the\ndistinct advantages of each, offering a robust and scalable solution for\nhumanoid control across a wide range of modes. By eliminating the need for\npolicy retraining for each control mode, our approach improves efficiency and\nflexibility for future humanoid applications.\n","authors":["Tairan He","Wenli Xiao","Toru Lin","Zhengyi Luo","Zhenjia Xu","Zhenyu Jiang","Jan Kautz","Changliu Liu","Guanya Shi","Xiaolong Wang","Linxi Fan","Yuke Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.21229v2.pdf","comment":"Published at ICRA 2025, Project Page: see\n  https://hover-versatile-humanoid.github.io/"},{"id":"http://arxiv.org/abs/2503.04163v1","updated":"2025-03-06T07:25:36Z","published":"2025-03-06T07:25:36Z","title":"VLA Model-Expert Collaboration for Bi-directional Manipulation Learning","summary":"  The emergence of vision-language-action (VLA) models has given rise to\nfoundation models for robot manipulation. Although these models have achieved\nsignificant improvements, their generalization in multi-task manipulation\nremains limited. This study proposes a VLA model-expert collaboration framework\nthat leverages a limited number of expert actions to enhance VLA model\nperformance. This approach reduces expert workload relative to manual operation\nwhile simultaneously improving the reliability and generalization of VLA\nmodels. Furthermore, manipulation data collected during collaboration can\nfurther refine the VLA model, while human participants concurrently enhance\ntheir skills. This bi-directional learning loop boosts the overall performance\nof the collaboration system. Experimental results across various VLA models\ndemonstrate the effectiveness of the proposed system in collaborative\nmanipulation and learning, as evidenced by improved success rates across tasks.\nAdditionally, validation using a brain-computer interface (BCI) indicates that\nthe collaboration system enhances the efficiency of low-speed action systems by\ninvolving VLA model during manipulation. These promising results pave the way\nfor advancing human-robot interaction in the era of foundation models for\nrobotics. (Project website: https://aoqunjin.github.io/Expert-VLA/)\n","authors":["Tian-Yu Xiang","Ao-Qun Jin","Xiao-Hu Zhou","Mei-Jiang Gui","Xiao-Liang Xie","Shi-Qi Liu","Shuang-Yi Wang","Sheng-Bin Duang","Si-Cheng Wang","Zheng Lei","Zeng-Guang Hou"],"pdf_url":"https://arxiv.org/pdf/2503.04163v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04159v1","updated":"2025-03-06T07:23:17Z","published":"2025-03-06T07:23:17Z","title":"Simulation-based Analysis Of Highway Trajectory Planning Using\n  High-Order Polynomial For Highly Automated Driving Function","summary":"  One of the fundamental tasks of autonomous driving is safe trajectory\nplanning, the task of deciding where the vehicle needs to drive, while avoiding\nobstacles, obeying safety rules, and respecting the fundamental limits of road.\nReal-world application of such a method involves consideration of surrounding\nenvironment conditions and movements such as Lane Change, collision avoidance,\nand lane merge. The focus of the paper is to develop and implement safe\ncollision free highway Lane Change trajectory using high order polynomial for\nHighly Automated Driving Function (HADF). Planning is often considered as a\nhigher-level process than control. Behavior Planning Module (BPM) is designed\nthat plans the high-level driving actions like Lane Change maneuver to safely\nachieve the functionality of transverse guidance ensuring safety of the vehicle\nusing motion planning in a scenario including environmental situation. Based on\nthe recommendation received from the (BPM), the function will generate a desire\ncorresponding trajectory. The proposed planning system is situation specific\nwith polynomial based algorithm for same direction two lane highway scenario.\nTo support the trajectory system polynomial curve can be used to reduces\noverall complexity and thereby allows rapid computation. The proposed Lane\nChange scenario is modeled, and results has been analyzed (verified and\nvalidate) through the MATLAB simulation environment. The method proposed in\nthis paper has achieved a significant improvement in safety and stability of\nLane Changing maneuver.\n","authors":["Milin Patel","Marzana Khatun","Rolf Jung","Michael Glaß"],"pdf_url":"https://arxiv.org/pdf/2503.04159v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.16880v2","updated":"2025-03-06T07:11:20Z","published":"2024-12-22T06:20:59Z","title":"Large-Scale UWB Anchor Calibration and One-Shot Localization Using\n  Gaussian Process","summary":"  Ultra-wideband (UWB) is gaining popularity with devices like AirTags for\nprecise home item localization but faces significant challenges when scaled to\nlarge environments like seaports. The main challenges are calibration and\nlocalization in obstructed conditions, which are common in logistics\nenvironments. Traditional calibration methods, dependent on line-of-sight\n(LoS), are slow, costly, and unreliable in seaports and warehouses, making\nlarge-scale localization a significant pain point in the industry. To overcome\nthese challenges, we propose a UWB-LiDAR fusion-based calibration and one-shot\nlocalization framework. Our method uses Gaussian Processes to estimate anchor\nposition from continuous-time LiDAR Inertial Odometry with sampled UWB ranges.\nThis approach ensures accurate and reliable calibration with just one round of\nsampling in large-scale areas, I.e., 600x450 square meter. With the LoS issues,\nUWB-only localization can be problematic, even when anchor positions are known.\nWe demonstrate that by applying a UWB-range filter, the search range for LiDAR\nloop closure descriptors is significantly reduced, improving both accuracy and\nspeed. This concept can be applied to other loop closure detection methods,\nenabling cost-effective localization in large-scale warehouses and seaports. It\nsignificantly improves precision in challenging environments where UWB-only and\nLiDAR-Inertial methods fall short, as shown in the video\n(https://youtu.be/oY8jQKdM7lU). We will open-source our datasets and\ncalibration codes for community use.\n","authors":["Shenghai Yuan","Boyang Lou","Thien-Minh Nguyen","Pengyu Yin","Muqing Cao","Xinghang Xu","Jianping Li","Jie Xu","Siyu Chen","Lihua Xie"],"pdf_url":"https://arxiv.org/pdf/2412.16880v2.pdf","comment":"This work has been accepted to IEEE International Conference on\n  Robotics and Automation (ICRA) @ 2025 IEEE. Personal use of this material is\n  permitted. Permission from IEEE must be obtained for all other uses,\n  including reprinting/redistribution, creating new works, or reuse of any\n  copyrighted components of this work in other media"},{"id":"http://arxiv.org/abs/2410.24226v2","updated":"2025-03-06T06:56:52Z","published":"2024-10-31T17:59:59Z","title":"Tensegrity Robot Proprioceptive State Estimation with Geometric\n  Constraints","summary":"  Tensegrity robots, characterized by a synergistic assembly of rigid rods and\nelastic cables, form robust structures that are resistant to impacts. However,\nthis design introduces complexities in kinematics and dynamics, complicating\ncontrol and state estimation. This work presents a novel proprioceptive state\nestimator for tensegrity robots. The estimator initially uses the geometric\nconstraints of 3-bar prism tensegrity structures, combined with IMU and motor\nencoder measurements, to reconstruct the robot's shape and orientation. It then\nemploys a contact-aided invariant extended Kalman filter with forward\nkinematics to estimate the global position and orientation of the tensegrity\nrobot. The state estimator's accuracy is assessed against ground truth data in\nboth simulated environments and real-world tensegrity robot applications. It\nachieves an average drift percentage of 4.2%, comparable to the state\nestimation performance of traditional rigid robots. This state estimator\nadvances the state of the art in tensegrity robot state estimation and has the\npotential to run in real-time using onboard sensors, paving the way for full\nautonomy of tensegrity robots in unstructured environments.\n","authors":["Wenzhe Tong","Tzu-Yuan Lin","Jonathan Mi","Yicheng Jiang","Maani Ghaffari","Xiaonan Huang"],"pdf_url":"https://arxiv.org/pdf/2410.24226v2.pdf","comment":"Preprint; 8 pages, 12 figures, 2 tables; Code at\n  https://github.com/Jonathan-Twz/tensegrity-robot-state-estimator"},{"id":"http://arxiv.org/abs/2503.04134v1","updated":"2025-03-06T06:26:57Z","published":"2025-03-06T06:26:57Z","title":"Real-time Spatial-temporal Traversability Assessment via Feature-based\n  Sparse Gaussian Process","summary":"  Terrain analysis is critical for the practical application of ground mobile\nrobots in real-world tasks, especially in outdoor unstructured environments. In\nthis paper, we propose a novel spatial-temporal traversability assessment\nmethod, which aims to enable autonomous robots to effectively navigate through\ncomplex terrains. Our approach utilizes sparse Gaussian processes (SGP) to\nextract geometric features (curvature, gradient, elevation, etc.) directly from\npoint cloud scans. These features are then used to construct a high-resolution\nlocal traversability map. Then, we design a spatial-temporal Bayesian Gaussian\nkernel (BGK) inference method to dynamically evaluate traversability scores,\nintegrating historical and real-time data while considering factors such as\nslope, flatness, gradient, and uncertainty metrics. GPU acceleration is applied\nin the feature extraction step, and the system achieves real-time performance.\nExtensive simulation experiments across diverse terrain scenarios demonstrate\nthat our method outperforms SOTA approaches in both accuracy and computational\nefficiency. Additionally, we develop an autonomous navigation framework\nintegrated with the traversability map and validate it with a differential\ndriven vehicle in complex outdoor environments. Our code will be open-source\nfor further research and development by the community,\nhttps://github.com/ZJU-FAST-Lab/FSGP_BGK.\n","authors":["Senming Tan","Zhenyu Hou","Zhihao Zhang","Long Xu","Mengke Zhang","Zhaoqi He","Chao Xu","Fei Gao","Yanjun Cao"],"pdf_url":"https://arxiv.org/pdf/2503.04134v1.pdf","comment":"8 pages, 10 figures"},{"id":"http://arxiv.org/abs/2503.04126v1","updated":"2025-03-06T06:10:21Z","published":"2025-03-06T06:10:21Z","title":"DVM-SLAM: Decentralized Visual Monocular Simultaneous Localization and\n  Mapping for Multi-Agent Systems","summary":"  Cooperative Simultaneous Localization and Mapping (C-SLAM) enables multiple\nagents to work together in mapping unknown environments while simultaneously\nestimating their own positions. This approach enhances robustness, scalability,\nand accuracy by sharing information between agents, reducing drift, and\nenabling collective exploration of larger areas. In this paper, we present\nDecentralized Visual Monocular SLAM (DVM-SLAM), the first open-source\ndecentralized monocular C-SLAM system. By only utilizing low-cost and\nlight-weight monocular vision sensors, our system is well suited for small\nrobots and micro aerial vehicles (MAVs). DVM-SLAM's real-world applicability is\nvalidated on physical robots with a custom collision avoidance framework,\nshowcasing its potential in real-time multi-agent autonomous navigation\nscenarios. We also demonstrate comparable accuracy to state-of-the-art\ncentralized monocular C-SLAM systems. We open-source our code and provide\nsupplementary material online.\n","authors":["Joshua Bird","Jan Blumenkamp","Amanda Prorok"],"pdf_url":"https://arxiv.org/pdf/2503.04126v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04123v1","updated":"2025-03-06T06:00:55Z","published":"2025-03-06T06:00:55Z","title":"GAGrasp: Geometric Algebra Diffusion for Dexterous Grasping","summary":"  We propose GAGrasp, a novel framework for dexterous grasp generation that\nleverages geometric algebra representations to enforce equivariance to SE(3)\ntransformations. By encoding the SE(3) symmetry constraint directly into the\narchitecture, our method improves data and parameter efficiency while enabling\nrobust grasp generation across diverse object poses. Additionally, we\nincorporate a differentiable physics-informed refinement layer, which ensures\nthat generated grasps are physically plausible and stable. Extensive\nexperiments demonstrate the model's superior performance in generalization,\nstability, and adaptability compared to existing methods. Additional details at\nhttps://gagrasp.github.io/\n","authors":["Tao Zhong","Christine Allen-Blanchette"],"pdf_url":"https://arxiv.org/pdf/2503.04123v1.pdf","comment":"Accepted at ICRA 2025"},{"id":"http://arxiv.org/abs/2503.00675v2","updated":"2025-03-06T05:59:08Z","published":"2025-03-02T00:40:50Z","title":"Dur360BEV: A Real-world 360-degree Single Camera Dataset and Benchmark\n  for Bird-Eye View Mapping in Autonomous Driving","summary":"  We present Dur360BEV, a novel spherical camera autonomous driving dataset\nequipped with a high-resolution 128-channel 3D LiDAR and a RTK-refined GNSS/INS\nsystem, along with a benchmark architecture designed to generate Bird-Eye-View\n(BEV) maps using only a single spherical camera. This dataset and benchmark\naddress the challenges of BEV generation in autonomous driving, particularly by\nreducing hardware complexity through the use of a single 360-degree camera\ninstead of multiple perspective cameras. Within our benchmark architecture, we\npropose a novel spherical-image-to-BEV module that leverages spherical imagery\nand a refined sampling strategy to project features from 2D to 3D. Our approach\nalso includes an innovative application of focal loss, specifically adapted to\naddress the extreme class imbalance often encountered in BEV segmentation\ntasks, that demonstrates improved segmentation performance on the Dur360BEV\ndataset. The results show that our benchmark not only simplifies the sensor\nsetup but also achieves competitive performance.\n","authors":["Wenke E","Chao Yuan","Li Li","Yixin Sun","Yona Falinie A. Gaus","Amir Atapour-Abarghouei","Toby P. Breckon"],"pdf_url":"https://arxiv.org/pdf/2503.00675v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04112v1","updated":"2025-03-06T05:41:56Z","published":"2025-03-06T05:41:56Z","title":"The Spinning Blimp: Design and Control of a Novel Minimalist Aerial\n  Vehicle Leveraging Rotational Dynamics and Locomotion","summary":"  This paper presents the Spinning Blimp, a novel lighter-than-air (LTA) aerial\nvehicle designed for low-energy stable flight. Utilizing an oblate spheroid\nhelium balloon for buoyancy, the vehicle achieves minimal energy consumption\nwhile maintaining prolonged airborne states. The unique and low-cost design\nemploys a passively arranged wing coupled with a propeller to induce a spinning\nbehavior, providing inherent pendulum-like stabilization. We propose a control\nstrategy that takes advantage of the continuous revolving nature of the\nspinning blimp to control translational motion. The cost-effectiveness of the\nvehicle makes it highly suitable for a variety of applications, such as\npatrolling, localization, air and turbulence monitoring, and domestic\nsurveillance. Experimental evaluations affirm the design's efficacy and\nunderscore its potential as a versatile and economically viable solution for\naerial applications.\n","authors":["Leonardo Santens","Diego S. D'Antonio","Shuhang Hou","David Saldaña"],"pdf_url":"https://arxiv.org/pdf/2503.04112v1.pdf","comment":"Accepted at the IEEE international conference on robotics and\n  automation(ICRA 2025)"},{"id":"http://arxiv.org/abs/2410.24185v2","updated":"2025-03-06T05:34:17Z","published":"2024-10-31T17:48:45Z","title":"DexMimicGen: Automated Data Generation for Bimanual Dexterous\n  Manipulation via Imitation Learning","summary":"  Imitation learning from human demonstrations is an effective means to teach\nrobots manipulation skills. But data acquisition is a major bottleneck in\napplying this paradigm more broadly, due to the amount of cost and human effort\ninvolved. There has been significant interest in imitation learning for\nbimanual dexterous robots, like humanoids. Unfortunately, data collection is\neven more challenging here due to the challenges of simultaneously controlling\nmultiple arms and multi-fingered hands. Automated data generation in simulation\nis a compelling, scalable alternative to fuel this need for data. To this end,\nwe introduce DexMimicGen, a large-scale automated data generation system that\nsynthesizes trajectories from a handful of human demonstrations for humanoid\nrobots with dexterous hands. We present a collection of simulation environments\nin the setting of bimanual dexterous manipulation, spanning a range of\nmanipulation behaviors and different requirements for coordination among the\ntwo arms. We generate 21K demos across these tasks from just 60 source human\ndemos and study the effect of several data generation and policy learning\ndecisions on agent performance. Finally, we present a real-to-sim-to-real\npipeline and deploy it on a real-world humanoid can sorting task. Generated\ndatasets, simulation environments and additional results are at\nhttps://dexmimicgen.github.io/\n","authors":["Zhenyu Jiang","Yuqi Xie","Kevin Lin","Zhenjia Xu","Weikang Wan","Ajay Mandlekar","Linxi Fan","Yuke Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.24185v2.pdf","comment":"ICRA 2025. Project website: https://dexmimicgen.github.io/"},{"id":"http://arxiv.org/abs/2502.06919v2","updated":"2025-03-06T05:25:18Z","published":"2025-02-10T16:07:28Z","title":"Select before Act: Spatially Decoupled Action Repetition for Continuous\n  Control","summary":"  Reinforcement Learning (RL) has achieved remarkable success in various\ncontinuous control tasks, such as robot manipulation and locomotion. Different\nto mainstream RL which makes decisions at individual steps, recent studies have\nincorporated action repetition into RL, achieving enhanced action persistence\nwith improved sample efficiency and superior performance. However, existing\nmethods treat all action dimensions as a whole during repetition, ignoring\nvariations among them. This constraint leads to inflexibility in decisions,\nwhich reduces policy agility with inferior effectiveness. In this work, we\npropose a novel repetition framework called SDAR, which implements Spatially\nDecoupled Action Repetition through performing closed-loop act-or-repeat\nselection for each action dimension individually. SDAR achieves more flexible\nrepetition strategies, leading to an improved balance between action\npersistence and diversity. Compared to existing repetition frameworks, SDAR is\nmore sample efficient with higher policy performance and reduced action\nfluctuation. Experiments are conducted on various continuous control scenarios,\ndemonstrating the effectiveness of spatially decoupled repetition design\nproposed in this work.\n","authors":["Buqing Nie","Yangqing Fu","Yue Gao"],"pdf_url":"https://arxiv.org/pdf/2502.06919v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2503.04096v1","updated":"2025-03-06T05:13:19Z","published":"2025-03-06T05:13:19Z","title":"Image-Based Relocalization and Alignment for Long-Term Monitoring of\n  Dynamic Underwater Environments","summary":"  Effective monitoring of underwater ecosystems is crucial for tracking\nenvironmental changes, guiding conservation efforts, and ensuring long-term\necosystem health. However, automating underwater ecosystem management with\nrobotic platforms remains challenging due to the complexities of underwater\nimagery, which pose significant difficulties for traditional visual\nlocalization methods. We propose an integrated pipeline that combines Visual\nPlace Recognition (VPR), feature matching, and image segmentation on\nvideo-derived images. This method enables robust identification of revisited\nareas, estimation of rigid transformations, and downstream analysis of\necosystem changes. Furthermore, we introduce the SQUIDLE+ VPR Benchmark-the\nfirst large-scale underwater VPR benchmark designed to leverage an extensive\ncollection of unstructured data from multiple robotic platforms, spanning time\nintervals from days to years. The dataset encompasses diverse trajectories,\narbitrary overlap and diverse seafloor types captured under varying\nenvironmental conditions, including differences in depth, lighting, and\nturbidity. Our code is available at: https://github.com/bev-gorry/underloc\n","authors":["Beverley Gorry","Tobias Fischer","Michael Milford","Alejandro Fontan"],"pdf_url":"https://arxiv.org/pdf/2503.04096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18333v2","updated":"2025-03-06T05:04:03Z","published":"2024-10-24T00:02:58Z","title":"Search-Based Path Planning in Interactive Environments among Movable\n  Obstacles","summary":"  This paper investigates Path planning Among Movable Obstacles (PAMO), which\nseeks a minimum cost collision-free path among static obstacles from start to\ngoal while allowing the robot to push away movable obstacles (i.e., objects)\nalong its path when needed. To develop planners that are complete and optimal\nfor PAMO, the planner has to search a giant state space involving both the\nlocation of the robot as well as the locations of the objects, which grows\nexponentially with respect to the number of objects. This paper leverages a\nsimple yet under-explored idea that, only a small fraction of this giant state\nspace needs to be searched during planning as guided by a heuristic, and most\nof the objects far away from the robot are intact, which thus leads to runtime\nefficient algorithms. Based on this idea, this paper introduces two PAMO\nformulations, i.e., bi-objective and resource constrained problems in an\noccupancy grid, and develops PAMO*, a planning method with completeness and\nsolution optimality guarantees, to solve the two problems. We then further\nextend PAMO* to hybrid-state PAMO* to plan in continuous spaces with\nhigh-fidelity interaction between the robot and the objects. Our results show\nthat, PAMO* can often find optimal solutions within a second in cluttered maps\nwith up to 400 objects.\n","authors":["Zhongqiang Ren","Bunyod Suvonov","Guofei Chen","Botao He","Yijie Liao","Cornelia Fermuller","Ji Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.18333v2.pdf","comment":"Accepted by ICRA 2025"},{"id":"http://arxiv.org/abs/2503.04089v1","updated":"2025-03-06T04:54:38Z","published":"2025-03-06T04:54:38Z","title":"OPG-Policy: Occluded Push-Grasp Policy Learning with Amodal Segmentation","summary":"  Goal-oriented grasping in dense clutter, a fundamental challenge in robotics,\ndemands an adaptive policy to handle occluded target objects and diverse\nconfigurations. Previous methods typically learn policies based on partially\nobservable segments of the occluded target to generate motions. However, these\npolicies often struggle to generate optimal motions due to uncertainties\nregarding the invisible portions of different occluded target objects across\nvarious scenes, resulting in low motion efficiency. To this end, we propose\nOPG-Policy, a novel framework that leverages amodal segmentation to predict\noccluded portions of the target and develop an adaptive push-grasp policy for\ncluttered scenarios where the target object is partially observed.\nSpecifically, our approach trains a dedicated amodal segmentation module for\ndiverse target objects to generate amodal masks. These masks and scene\nobservations are mapped to the future rewards of grasp and push motion\nprimitives via deep Q-learning to learn the motion critic. Afterward, the push\nand grasp motion candidates predicted by the critic, along with the relevant\ndomain knowledge, are fed into the coordinator to generate the optimal motion\nimplemented by the robot. Extensive experiments conducted in both simulated and\nreal-world environments demonstrate the effectiveness of our approach in\ngenerating motion sequences for retrieving occluded targets, outperforming\nother baseline methods in success rate and motion efficiency.\n","authors":["Hao Ding","Yiming Zeng","Zhaoliang Wan","Hui Cheng"],"pdf_url":"https://arxiv.org/pdf/2503.04089v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04082v1","updated":"2025-03-06T04:37:09Z","published":"2025-03-06T04:37:09Z","title":"Instrument-Splatting: Controllable Photorealistic Reconstruction of\n  Surgical Instruments Using Gaussian Splatting","summary":"  Real2Sim is becoming increasingly important with the rapid development of\nsurgical artificial intelligence (AI) and autonomy. In this work, we propose a\nnovel Real2Sim methodology, \\textit{Instrument-Splatting}, that leverages 3D\nGaussian Splatting to provide fully controllable 3D reconstruction of surgical\ninstruments from monocular surgical videos. To maintain both high visual\nfidelity and manipulability, we introduce a geometry pre-training to bind\nGaussian point clouds on part mesh with accurate geometric priors and define a\nforward kinematics to control the Gaussians as flexible as real instruments.\nAfterward, to handle unposed videos, we design a novel instrument pose tracking\nmethod leveraging semantics-embedded Gaussians to robustly refine per-frame\ninstrument poses and joint states in a render-and-compare manner, which allows\nour instrument Gaussian to accurately learn textures and reach photorealistic\nrendering. We validated our method on 2 publicly released surgical videos and 4\nvideos collected on ex vivo tissues and green screens. Quantitative and\nqualitative evaluations demonstrate the effectiveness and superiority of the\nproposed method.\n","authors":["Shuojue Yang","Zijian Wu","Mingxuan Hong","Qian Li","Daiyun Shen","Septimiu E. Salcudean","Yueming Jin"],"pdf_url":"https://arxiv.org/pdf/2503.04082v1.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2409.18361v4","updated":"2025-03-06T04:31:54Z","published":"2024-09-27T00:35:21Z","title":"iWalker: Imperative Visual Planning for Walking Humanoid Robot","summary":"  Humanoid robots, designed to operate in human-centric environments, serve as\na fundamental platform for a broad range of tasks. Although humanoid robots\nhave been extensively studied for decades, a majority of existing humanoid\nrobots still heavily rely on complex modular frameworks, leading to\ninflexibility and potential compounded errors from independent sensing,\nplanning, and acting components. In response, we propose an end-to-end humanoid\nsense-plan-act walking system, enabling vision-based obstacle avoidance and\nfootstep planning for whole body balancing simultaneously. We designed two\nimperative learning (IL)-based bilevel optimizations for model-predictive step\nplanning and whole body balancing, respectively, to achieve self-supervised\nlearning for humanoid robot walking. This enables the robot to learn from\narbitrary unlabeled data, improving its adaptability and generalization\ncapabilities. We refer to our method as iWalker and demonstrate its\neffectiveness in both simulated and real-world environments, representing a\nsignificant advancement toward autonomous humanoid robots.\n","authors":["Xiao Lin","Yuhao Huang","Taimeng Fu","Xiaobin Xiong","Chen Wang"],"pdf_url":"https://arxiv.org/pdf/2409.18361v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.05279v2","updated":"2025-03-06T03:52:43Z","published":"2024-11-08T02:23:47Z","title":"Path Planning in Complex Environments with Superquadrics and\n  Voronoi-Based Orientation","summary":"  Path planning in narrow passages is a challenging problem in various\napplications. Traditional planning algorithms often face challenges in complex\nenvironments like mazes and traps, where narrow entrances require special\norientation control for successful navigation. In this work, we present a novel\napproach that combines superquadrics (SQ) representation and Voronoi diagrams\nto solve the narrow passage problem in both 2D and 3D environment. Our method\nutilizes the SQ formulation to expand obstacles, eliminating impassable\npassages, while Voronoi hyperplane ensures maximum clearance path.\nAdditionally, the hyperplane provides a natural reference for robot\norientation, aligning its long axis with the passage direction. We validate our\nframework through a 2D object retrieval task and 3D drone simulation,\ndemonstrating that our approach outperforms classical planners and a\ncutting-edge drone planner by ensuring passable trajectories with maximum\nclearance.\n","authors":["Lin Yang","Ganesh Iyer","Baichuan Lou","Sri Harsha Turlapati","Chen Lv","Domenico Campolo"],"pdf_url":"https://arxiv.org/pdf/2411.05279v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04063v1","updated":"2025-03-06T03:41:18Z","published":"2025-03-06T03:41:18Z","title":"Music-Driven Legged Robots: Synchronized Walking to Rhythmic Beats","summary":"  We address the challenge of effectively controlling the locomotion of legged\nrobots by incorporating precise frequency and phase characteristics, which is\noften ignored in locomotion policies that do not account for the periodic\nnature of walking. We propose a hierarchical architecture that integrates a\nlow-level phase tracker, oscillators, and a high-level phase modulator. This\ncontroller allows quadruped robots to walk in a natural manner that is\nsynchronized with external musical rhythms. Our method generates diverse gaits\nacross different frequencies and achieves real-time synchronization with music\nin the physical world. This research establishes a foundational framework for\nenabling real-time execution of accurate rhythmic motions in legged robots.\nVideo is available at website: https://music-walker.github.io/.\n","authors":["Taixian Hou","Yueqi Zhang","Xiaoyi Wei","Zhiyan Dong","Jiafu Yi","Peng Zhai","Lihua Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.04063v1.pdf","comment":"ICRA2025 accepted"},{"id":"http://arxiv.org/abs/2412.16490v2","updated":"2025-03-06T03:12:16Z","published":"2024-12-21T05:22:53Z","title":"BODex: Scalable and Efficient Robotic Dexterous Grasp Synthesis Using\n  Bilevel Optimization","summary":"  Robotic dexterous grasping is important for interacting with the environment.\nTo unleash the potential of data-driven models for dexterous grasping, a\nlarge-scale, high-quality dataset is essential. While gradient-based\noptimization offers a promising way for constructing such datasets, previous\nworks suffer from limitations, such as inefficiency, strong assumptions in the\ngrasp quality energy, or limited object sets for experiments. Moreover, the\nlack of a standard benchmark for comparing different methods and datasets\nhinders progress in this field. To address these challenges, we develop a\nhighly efficient synthesis system and a comprehensive benchmark with MuJoCo for\ndexterous grasping. We formulate grasp synthesis as a bilevel optimization\nproblem, combining a novel lower-level quadratic programming (QP) with an\nupper-level gradient descent process. By leveraging recent advances in\nCUDA-accelerated robotic libraries and GPU-based QP solvers, our system can\nparallelize thousands of grasps and synthesize over 49 grasps per second on a\nsingle 3090 GPU. Our synthesized grasps for Shadow, Allegro, and Leap hands all\nachieve a success rate above 75% in simulation, with a penetration depth under\n1 mm, outperforming existing baselines on nearly all metrics. Compared to the\nprevious large-scale dataset, DexGraspNet, our dataset significantly improves\nthe performance of learning models, with a success rate from around 40% to 80%\nin simulation. Real-world testing of the trained model on the Shadow Hand\nachieves an 81% success rate across 20 diverse objects. The codes and datasets\nare released on our project page: https://pku-epic.github.io/BODex.\n","authors":["Jiayi Chen","Yubin Ke","He Wang"],"pdf_url":"https://arxiv.org/pdf/2412.16490v2.pdf","comment":"ICRA 2025"},{"id":"http://arxiv.org/abs/2503.04051v1","updated":"2025-03-06T03:07:39Z","published":"2025-03-06T03:07:39Z","title":"RA-DP: Rapid Adaptive Diffusion Policy for Training-Free High-frequency\n  Robotics Replanning","summary":"  Diffusion models exhibit impressive scalability in robotic task learning, yet\nthey struggle to adapt to novel, highly dynamic environments. This limitation\nprimarily stems from their constrained replanning ability: they either operate\nat a low frequency due to a time-consuming iterative sampling process, or are\nunable to adapt to unforeseen feedback in case of rapid replanning. To address\nthese challenges, we propose RA-DP, a novel diffusion policy framework with\ntraining-free high-frequency replanning ability that solves the above\nlimitations in adapting to unforeseen dynamic environments. Specifically, our\nmethod integrates guidance signals which are often easily obtained in the new\nenvironment during the diffusion sampling process, and utilizes a novel action\nqueue mechanism to generate replanned actions at every denoising step without\nretraining, thus forming a complete training-free framework for robot motion\nadaptation in unseen environments. Extensive evaluations have been conducted in\nboth well-recognized simulation benchmarks and real robot tasks. Results show\nthat RA-DP outperforms the state-of-the-art diffusion-based methods in terms of\nreplanning frequency and success rate. Moreover, we show that our framework is\ntheoretically compatible with any training-free guidance signal.\n","authors":["Xi Ye","Rui Heng Yang","Jun Jin","Yinchuan Li","Amir Rasouli"],"pdf_url":"https://arxiv.org/pdf/2503.04051v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.13664v2","updated":"2025-03-06T03:07:24Z","published":"2024-12-18T09:45:46Z","title":"A Skeleton-Based Topological Planner for Exploration in Complex Unknown\n  Environments","summary":"  The capability of autonomous exploration in complex, unknown environments is\nimportant in many robotic applications. While recent research on autonomous\nexploration have achieved much progress, there are still limitations, e.g.,\nexisting methods relying on greedy heuristics or optimal path planning are\noften hindered by repetitive paths and high computational demands. To address\nsuch limitations, we propose a novel exploration framework that utilizes the\nglobal topology information of observed environment to improve exploration\nefficiency while reducing computational overhead. Specifically, global\ninformation is utilized based on a skeletal topological graph representation of\nthe environment geometry. We first propose an incremental skeleton extraction\nmethod based on wavefront propagation, based on which we then design an\napproach to generate a lightweight topological graph that can effectively\ncapture the environment's structural characteristics. Building upon this, we\nintroduce a finite state machine that leverages the topological structure to\nefficiently plan coverage paths, which can substantially mitigate the\nback-and-forth maneuvers (BFMs) problem. Experimental results demonstrate the\nsuperiority of our method in comparison with state-of-the-art methods. The\nsource code will be made publicly available at:\nhttps://github.com/Haochen-Niu/STGPlanner.\n","authors":["Haochen Niu","Xingwu Ji","Lantao Zhang","Fei Wen","Rendong Ying","Peilin Liu"],"pdf_url":"https://arxiv.org/pdf/2412.13664v2.pdf","comment":"7 pages, 7 figures. Accepted to be presented at the ICRA 2025"},{"id":"http://arxiv.org/abs/2503.04043v1","updated":"2025-03-06T02:54:11Z","published":"2025-03-06T02:54:11Z","title":"Object State Estimation Through Robotic Active Interaction for\n  Biological Autonomous Drilling","summary":"  Estimating the state of biological specimens is challenging due to limited\nobservation through microscopic vision. For instance, during mouse skull\ndrilling, the appearance alters little when thinning bone tissue because of its\nsemi-transparent property and the high-magnification microscopic vision. To\nobtain the object's state, we introduce an object state estimation method for\nbiological specimens through active interaction based on the deflection. The\nmethod is integrated to enhance the autonomous drilling system developed in our\nprevious work. The method and integrated system were evaluated through 12\nautonomous eggshell drilling experiment trials. The results show that the\nsystem achieved a 91.7% successful ratio and 75% detachable ratio, showcasing\nits potential applicability in more complex surgical procedures such as mouse\nskull craniotomy. This research paves the way for further development of\nautonomous robotic systems capable of estimating the object's state through\nactive interaction.\n","authors":["Xiaofeng Lin","Enduo Zhao","Saúl Alexis Heredia Pérez","Kanako Harada"],"pdf_url":"https://arxiv.org/pdf/2503.04043v1.pdf","comment":"The first and second authors contribute equally to this research. 6\n  pages, 5 figures, submitted to RA-L"},{"id":"http://arxiv.org/abs/2503.04038v1","updated":"2025-03-06T02:46:39Z","published":"2025-03-06T02:46:39Z","title":"Autonomous Robotic Bone Micro-Milling System with Automatic Calibration\n  and 3D Surface Fitting","summary":"  Automating bone micro-milling using a robotic system presents challenges due\nto the uncertainties in both the external and internal features of bone tissue.\nFor example, during a mouse cranial window creation, a circular path with a\nradius of 2 to 4 mm needs to be milled on the mouse skull using a microdrill.\nThe uneven surface and non-uniform thickness of the mouse skull make it\ndifficult to fully automate this process, requiring the system to possess\nadvanced perceptual and adaptive capabilities. In this study, we propose an\nautomatic calibration and 3D surface fitting method and integrate it into an\nautonomous robotic bone micro-milling system, enabling it to quickly, in\nreal-time, and accurately perceive and adapt to the uneven surface and\nnon-uniform thickness of the target without human assistance. Validation\nexperiments on euthanized mice demonstrate that the improved system achieves a\nsuccess rate of 85.7 % and an average milling time of 2.1 minutes, showing not\nonly significant performance improvements over the previous system but also\nexceptional accuracy, speed, and stability compared to human operators.\n","authors":["Enduo Zhao","Xiaofeng Lin","Yifan Wang","Kanako Harada"],"pdf_url":"https://arxiv.org/pdf/2503.04038v1.pdf","comment":"8 pages, 8 figures, submitted to IROS 2025"},{"id":"http://arxiv.org/abs/2411.13916v3","updated":"2025-03-06T02:43:33Z","published":"2024-11-21T08:06:13Z","title":"Joint-repositionable Inner-wireless Planar Snake Robot","summary":"  Bio-inspired multi-joint snake robots offer the advantages of terrain\nadaptability due to their limbless structure and high flexibility. However, a\nseries of dozens of motor units in typical multiple-joint snake robots results\nin a heavy body structure and hundreds of watts of high power consumption. This\npaper presents a joint-repositionable, inner-wireless snake robot that enables\nmulti-joint-like locomotion using a low-powered underactuated mechanism. The\nsnake robot, consisting of a series of flexible passive links, can dynamically\nchange its joint coupling configuration by repositioning motor-driven joint\nunits along rack gears inside the robot. Additionally, a soft robot skin\nwirelessly powers the internal joint units, avoiding the risk of wire tangling\nand disconnection caused by the movable joint units. The combination of the\njoint-repositionable mechanism and the wireless-charging-enabled soft skin\nachieves a high degree of bending, along with a lightweight structure of 1.3 kg\nand energy-efficient wireless power transmission of 7.6 watts.\n","authors":["Ayato Kanada","Ryo Takahashi","Keito Hayashi","Ryusuke Hosaka","Wakako Yukita","Yasutaka Nakashima","Tomoyuki Yokota","Takao Someya","Mitsuhiro Kamezaki","Yoshihiro Kawahara","Motoji Yamamoto"],"pdf_url":"https://arxiv.org/pdf/2411.13916v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03629v2","updated":"2025-03-06T02:17:36Z","published":"2025-03-05T16:09:30Z","title":"TeraSim: Uncovering Unknown Unsafe Events for Autonomous Vehicles\n  through Generative Simulation","summary":"  Traffic simulation is essential for autonomous vehicle (AV) development,\nenabling comprehensive safety evaluation across diverse driving conditions.\nHowever, traditional rule-based simulators struggle to capture complex human\ninteractions, while data-driven approaches often fail to maintain long-term\nbehavioral realism or generate diverse safety-critical events. To address these\nchallenges, we propose TeraSim, an open-source, high-fidelity traffic\nsimulation platform designed to uncover unknown unsafe events and efficiently\nestimate AV statistical performance metrics, such as crash rates. TeraSim is\ndesigned for seamless integration with third-party physics simulators and\nstandalone AV stacks, to construct a complete AV simulation system.\nExperimental results demonstrate its effectiveness in generating diverse\nsafety-critical events involving both static and dynamic agents, identifying\nhidden deficiencies in AV systems, and enabling statistical performance\nevaluation. These findings highlight TeraSim's potential as a practical tool\nfor AV safety assessment, benefiting researchers, developers, and policymakers.\nThe code is available at https://github.com/mcity/TeraSim.\n","authors":["Haowei Sun","Xintao Yan","Zhijie Qiao","Haojie Zhu","Yihao Sun","Jiawei Wang","Shengyin Shen","Darian Hogue","Rajanikant Ananta","Derek Johnson","Greg Stevens","Greg McGuire","Yifan Wei","Wei Zheng","Yong Sun","Yasuo Fukai","Henry X. Liu"],"pdf_url":"https://arxiv.org/pdf/2503.03629v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04014v1","updated":"2025-03-06T02:03:02Z","published":"2025-03-06T02:03:02Z","title":"Dexterous Hand Manipulation via Efficient Imitation-Bootstrapped Online\n  Reinforcement Learning","summary":"  Dexterous hand manipulation in real-world scenarios presents considerable\nchallenges due to its demands for both dexterity and precision. While imitation\nlearning approaches have thoroughly examined these challenges, they still\nrequire a significant number of expert demonstrations and are limited by a\nconstrained performance upper bound. In this paper, we propose a novel and\nefficient Imitation-Bootstrapped Online Reinforcement Learning (IBORL) method\ntailored for robotic dexterous hand manipulation in real-world environments.\nSpecifically, we pretrain the policy using a limited set of expert\ndemonstrations and subsequently finetune this policy through direct\nreinforcement learning in the real world. To address the catastrophic\nforgetting issues that arise from the distribution shift between expert\ndemonstrations and real-world environments, we design a regularization term\nthat balances the exploration of novel behaviors with the preservation of the\npretrained policy. Our experiments with real-world tasks demonstrate that our\nmethod significantly outperforms existing approaches, achieving an almost 100%\nsuccess rate and a 23% improvement in cycle time. Furthermore, by finetuning\nwith online reinforcement learning, our method surpasses expert demonstrations\nand uncovers superior policies. Our code and empirical results are available in\nhttps://hggforget.github.io/iborl.github.io/.\n","authors":["Dongchi Huang","Tianle Zhang","Yihang Li","Ling Zhao","Jiayi Li","Zhirui Fang","Chunhe Xia","Lusong Li","Xiaodong He"],"pdf_url":"https://arxiv.org/pdf/2503.04014v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04007v1","updated":"2025-03-06T01:44:36Z","published":"2025-03-06T01:44:36Z","title":"Planning and Control for Deformable Linear Object Manipulation","summary":"  Manipulating a deformable linear object (DLO) such as wire, cable, and rope\nis a common yet challenging task due to their high degrees of freedom and\ncomplex deformation behaviors, especially in an environment with obstacles.\nExisting local control methods are efficient but prone to failure in complex\nscenarios, while precise global planners are computationally intensive and\ndifficult to deploy. This paper presents an efficient, easy-to-deploy framework\nfor collision-free DLO manipulation using mobile manipulators. We demonstrate\nthe effectiveness of leveraging standard planning tools for high-dimensional\nDLO manipulation without requiring custom planners or extensive data-driven\nmodels. Our approach combines an off-the-shelf global planner with a real-time\nlocal controller. The global planner approximates the DLO as a series of rigid\nlinks connected by spherical joints, enabling rapid path planning without the\nneed for problem-specific planners or large datasets. The local controller\nemploys control barrier functions (CBFs) to enforce safety constraints,\nmaintain the DLO integrity, prevent overstress, and handle obstacle avoidance.\nIt compensates for modeling inaccuracies by using a state-of-the-art\nposition-based dynamics technique that approximates physical properties like\nYoung's and shear moduli. We validate our framework through extensive\nsimulations and real-world demonstrations. In complex obstacle\nscenarios-including tent pole transport, corridor navigation, and tasks\nrequiring varied stiffness-our method achieves a 100% success rate over\nthousands of trials, with significantly reduced planning times compared to\nstate-of-the-art techniques. Real-world experiments include transportation of a\ntent pole and a rope using mobile manipulators. We share our ROS-based\nimplementation to facilitate adoption in various applications.\n","authors":["Burak Aksoy","John Wen"],"pdf_url":"https://arxiv.org/pdf/2503.04007v1.pdf","comment":"SUBMITTED TO IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING\n  (T-ASE)"},{"id":"http://arxiv.org/abs/2503.03998v1","updated":"2025-03-06T01:17:01Z","published":"2025-03-06T01:17:01Z","title":"Robotic Compliant Object Prying Using Diffusion Policy Guided by Vision\n  and Force Observations","summary":"  The growing adoption of batteries in the electric vehicle industry and\nvarious consumer products has created an urgent need for effective recycling\nsolutions. These products often contain a mix of compliant and rigid\ncomponents, making robotic disassembly a critical step toward achieving\nscalable recycling processes. Diffusion policy has emerged as a promising\napproach for learning low-level skills in robotics. To effectively apply\ndiffusion policy to contact-rich tasks, incorporating force as feedback is\nessential. In this paper, we apply diffusion policy with vision and force in a\ncompliant object prying task. However, when combining low-dimensional contact\nforce with high-dimensional image, the force information may be diluted. To\naddress this issue, we propose a method that effectively integrates force with\nimage data for diffusion policy observations. We validate our approach on a\nbattery prying task that demands high precision and multi-step execution. Our\nmodel achieves a 96\\% success rate in diverse scenarios, marking a 57\\%\nimprovement over the vision-only baseline. Our method also demonstrates\nzero-shot transfer capability to handle unseen objects and battery types.\nSupplementary videos and implementation codes are available on our project\nwebsite. https://rros-lab.github.io/diffusion-with-force.github.io/\n","authors":["Jeon Ho Kang","Sagar Joshi","Ruopeng Huang","Satyandra K. Gupta"],"pdf_url":"https://arxiv.org/pdf/2503.03998v1.pdf","comment":"Accepted to IEEE RA-L. (C) 2025 IEEE. Personal use of this material\n  is permitted. Permission from IEEE must be obtained for all other uses, in\n  any current or future media. 8 pages with 9 figures"},{"id":"http://arxiv.org/abs/2503.03992v1","updated":"2025-03-06T00:49:35Z","published":"2025-03-06T00:49:35Z","title":"GeoFIK: A Fast and Reliable Geometric Solver for the IK of the Franka\n  Arm based on Screw Theory Enabling Multiple Redundancy Parameters","summary":"  Modern robotics applications require an inverse kinematics (IK) solver that\nis fast, robust and consistent, and that provides all possible solutions.\nCurrently, the Franka robot arm is the most widely used manipulator in robotics\nresearch. With 7 DOFs, the IK of this robot is not only complex due to its\n1-DOF redundancy, but also due to the link offsets at the wrist and elbow. Due\nto this complexity, none of the Franka IK solvers available in the literature\nprovide satisfactory results when used in real-world applications. Therefore,\nin this paper we introduce GeoFIK (Geometric Franka IK), an analytical IK\nsolver that allows the use of different joint variables to resolve the\nredundancy. The approach uses screw theory to describe the entire geometry of\nthe robot, allowing the computation of the Jacobian matrix prior to computation\nof joint angles. All singularities are identified and handled. As an example of\nhow the geometric elements obtained by the IK can be exploited, a solver with\nthe swivel angle as the free variable is provided. Several experiments are\ncarried out to validate the speed, robustness and reliability of the GeoFIK\nagainst two state-of-the-art solvers.\n","authors":["Pablo C. Lopez-Custodio","Yuhe Gong","Luis F. C. Figueredo"],"pdf_url":"https://arxiv.org/pdf/2503.03992v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03984v1","updated":"2025-03-06T00:13:01Z","published":"2025-03-06T00:13:01Z","title":"GRaD-Nav: Efficiently Learning Visual Drone Navigation with Gaussian\n  Radiance Fields and Differentiable Dynamics","summary":"  Autonomous visual navigation is an essential element in robot autonomy.\nReinforcement learning (RL) offers a promising policy training paradigm.\nHowever existing RL methods suffer from high sample complexity, poor\nsim-to-real transfer, and limited runtime adaptability to navigation scenarios\nnot seen during training. These problems are particularly challenging for\ndrones, with complex nonlinear and unstable dynamics, and strong dynamic\ncoupling between control and perception. In this paper, we propose a novel\nframework that integrates 3D Gaussian Splatting (3DGS) with differentiable deep\nreinforcement learning (DDRL) to train vision-based drone navigation policies.\nBy leveraging high-fidelity 3D scene representations and differentiable\nsimulation, our method improves sample efficiency and sim-to-real transfer.\nAdditionally, we incorporate a Context-aided Estimator Network (CENet) to adapt\nto environmental variations at runtime. Moreover, by curriculum training in a\nmixture of different surrounding environments, we achieve in-task\ngeneralization, the ability to solve new instances of a task not seen during\ntraining. Drone hardware experiments demonstrate our method's high training\nefficiency compared to state-of-the-art RL methods, zero shot sim-to-real\ntransfer for real robot deployment without fine tuning, and ability to adapt to\nnew instances within the same task class (e.g. to fly through a gate at\ndifferent locations with different distractors in the environment).\n","authors":["Qianzhong Chen","Jiankai Sun","Naixiang Gao","JunEn Low","Timothy Chen","Mac Schwager"],"pdf_url":"https://arxiv.org/pdf/2503.03984v1.pdf","comment":null}],"Artificial Intelligence":[{"id":"http://arxiv.org/abs/2503.04725v1","updated":"2025-03-06T18:59:48Z","published":"2025-03-06T18:59:48Z","title":"L$^2$M: Mutual Information Scaling Law for Long-Context Language\n  Modeling","summary":"  We rigorously establish a bipartite mutual information scaling law in natural\nlanguage that governs long-range dependencies. This scaling law, which we show\nis distinct from and scales independently of the conventional two-point mutual\ninformation, is the key to understanding long-context language modeling. Using\nthis scaling law, we formulate the Long-context Language Modeling (L$^2$M)\ncondition, which relates a model's capacity for effective long context length\nmodeling to the scaling of its latent state size for storing past information.\nOur results are validated through experiments on both transformers and state\nspace models. This work establishes a theoretical foundation that guides the\ndevelopment of large language models toward longer context lengths.\n","authors":["Zhuo Chen","Oriol Mayné i Comas","Zhuotao Jin","Di Luo","Marin Soljačić"],"pdf_url":"https://arxiv.org/pdf/2503.04725v1.pdf","comment":"29 pages, 12 figures, 1 table"},{"id":"http://arxiv.org/abs/2503.04723v1","updated":"2025-03-06T18:59:37Z","published":"2025-03-06T18:59:37Z","title":"Shifting Long-Context LLMs Research from Input to Output","summary":"  Recent advancements in long-context Large Language Models (LLMs) have\nprimarily concentrated on processing extended input contexts, resulting in\nsignificant strides in long-context comprehension. However, the equally\ncritical aspect of generating long-form outputs has received comparatively less\nattention. This paper advocates for a paradigm shift in NLP research toward\naddressing the challenges of long-output generation. Tasks such as novel\nwriting, long-term planning, and complex reasoning require models to understand\nextensive contexts and produce coherent, contextually rich, and logically\nconsistent extended text. These demands highlight a critical gap in current LLM\ncapabilities. We underscore the importance of this under-explored domain and\ncall for focused efforts to develop foundational LLMs tailored for generating\nhigh-quality, long-form outputs, which hold immense potential for real-world\napplications.\n","authors":["Yuhao Wu","Yushi Bai","Zhiqing Hu","Shangqing Tu","Ming Shan Hee","Juanzi Li","Roy Ka-Wei Lee"],"pdf_url":"https://arxiv.org/pdf/2503.04723v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2503.04722v1","updated":"2025-03-06T18:59:23Z","published":"2025-03-06T18:59:23Z","title":"Enough Coin Flips Can Make LLMs Act Bayesian","summary":"  Large language models (LLMs) exhibit the ability to generalize given few-shot\nexamples in their input prompt, an emergent capability known as in-context\nlearning (ICL). We investigate whether LLMs utilize ICL to perform structured\nreasoning in ways that are consistent with a Bayesian framework or rely on\npattern matching. Using a controlled setting of biased coin flips, we find\nthat: (1) LLMs often possess biased priors, causing initial divergence in\nzero-shot settings, (2) in-context evidence outweighs explicit bias\ninstructions, (3) LLMs broadly follow Bayesian posterior updates, with\ndeviations primarily due to miscalibrated priors rather than flawed updates,\nand (4) attention magnitude has negligible effect on Bayesian inference. With\nsufficient demonstrations of biased coin flips via ICL, LLMs update their\npriors in a Bayesian manner.\n","authors":["Ritwik Gupta","Rodolfo Corona","Jiaxin Ge","Eric Wang","Dan Klein","Trevor Darrell","David M. Chan"],"pdf_url":"https://arxiv.org/pdf/2503.04722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04715v1","updated":"2025-03-06T18:58:29Z","published":"2025-03-06T18:58:29Z","title":"Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large\n  Language Model Pretraining","summary":"  The impressive capabilities of Large Language Models (LLMs) across diverse\ntasks are now well-established, yet their effective deployment necessitates\ncareful hyperparameter optimization. Through extensive empirical studies\ninvolving grid searches across diverse configurations, we discover universal\nscaling laws governing these hyperparameters: optimal learning rate follows a\npower-law relationship with both model parameters and data sizes, while optimal\nbatch size scales primarily with data sizes. Our analysis reveals a convex\noptimization landscape for hyperparameters under fixed models and data size\nconditions. This convexity implies an optimal hyperparameter plateau. We\ncontribute a universal, plug-and-play optimal hyperparameter tool for the\ncommunity. Its estimated values on the test set are merely 0.07\\% away from the\nglobally optimal LLM performance found via an exhaustive search. These laws\ndemonstrate remarkable robustness across variations in model sparsity, training\ndata distribution, and model shape. To our best known, this is the first work\nthat unifies different model shapes and structures, such as Mixture-of-Experts\nmodels and dense transformers, as well as establishes optimal hyperparameter\nscaling laws across diverse data distributions. This exhaustive optimization\nprocess demands substantial computational resources, utilizing nearly one\nmillion NVIDIA H800 GPU hours to train 3,700 LLMs of varying sizes and\nhyperparameters from scratch and consuming approximately 100 trillion tokens in\ntotal. To facilitate reproducibility and further research, we will\nprogressively release all loss measurements and model checkpoints through our\ndesignated repository https://step-law.github.io/\n","authors":["Houyi Li","Wenzheng Zheng","Jingcheng Hu","Qiufeng Wang","Hanshan Zhang","Zili Wang","Yangshijie Xu","Shuigeng Zhou","Xiangyu Zhang","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2503.04715v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2403.11807v7","updated":"2025-03-06T18:58:23Z","published":"2024-03-18T14:04:47Z","title":"How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming\n  Ability in Multi-Agent Environments","summary":"  Decision-making is a complex process requiring diverse abilities, making it\nan excellent framework for evaluating Large Language Models (LLMs). Researchers\nhave examined LLMs' decision-making through the lens of Game Theory. However,\nexisting evaluation mainly focus on two-player scenarios where an LLM competes\nagainst another. Additionally, previous benchmarks suffer from test set leakage\ndue to their static design. We introduce GAMA($\\gamma$)-Bench, a new framework\nfor evaluating LLMs' Gaming Ability in Multi-Agent environments. It includes\neight classical game theory scenarios and a dynamic scoring scheme specially\ndesigned to quantitatively assess LLMs' performance. $\\gamma$-Bench allows\nflexible game settings and adapts the scoring system to different game\nparameters, enabling comprehensive evaluation of robustness, generalizability,\nand strategies for improvement. Our results indicate that GPT-3.5 demonstrates\nstrong robustness but limited generalizability, which can be enhanced using\nmethods like Chain-of-Thought. We also evaluate 13 LLMs from 6 model families,\nincluding GPT-3.5, GPT-4, Gemini, LLaMA-3.1, Mixtral, and Qwen-2.\nGemini-1.5-Pro outperforms others, scoring of $69.8$ out of $100$, followed by\nLLaMA-3.1-70B ($65.9$) and Mixtral-8x22B ($62.4$). Our code and experimental\nresults are publicly available at https://github.com/CUHK-ARISE/GAMABench.\n","authors":["Jen-tse Huang","Eric John Li","Man Ho Lam","Tian Liang","Wenxuan Wang","Youliang Yuan","Wenxiang Jiao","Xing Wang","Zhaopeng Tu","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2403.11807v7.pdf","comment":"Accepted to ICLR 2025; 11 pages of main text; 26 pages of appendices;\n  Included models: GPT-3.5-{0613, 1106, 0125}, GPT-4-0125, GPT-4o-0806,\n  Gemini-{1.0, 1.5)-Pro, LLaMA-3.1-{7, 70, 405}B, Mixtral-8x{7, 22}B,\n  Qwen-2-72B"},{"id":"http://arxiv.org/abs/2503.04713v1","updated":"2025-03-06T18:57:40Z","published":"2025-03-06T18:57:40Z","title":"Scaling Rich Style-Prompted Text-to-Speech Datasets","summary":"  We introduce Paralinguistic Speech Captions (ParaSpeechCaps), a large-scale\ndataset that annotates speech utterances with rich style captions. While rich\nabstract tags (e.g. guttural, nasal, pained) have been explored in small-scale\nhuman-annotated datasets, existing large-scale datasets only cover basic tags\n(e.g. low-pitched, slow, loud). We combine off-the-shelf text and speech\nembedders, classifiers and an audio language model to automatically scale rich\ntag annotations for the first time. ParaSpeechCaps covers a total of 59 style\ntags, including both speaker-level intrinsic tags and utterance-level\nsituational tags. It consists of 342 hours of human-labelled data (PSC-Base)\nand 2427 hours of automatically annotated data (PSC-Scaled). We finetune\nParler-TTS, an open-source style-prompted TTS model, on ParaSpeechCaps, and\nachieve improved style consistency (+7.9% Consistency MOS) and speech quality\n(+15.5% Naturalness MOS) over the best performing baseline that combines\nexisting rich style tag datasets. We ablate several of our dataset design\nchoices to lay the foundation for future work in this space. Our dataset,\nmodels and code are released at https://github.com/ajd12342/paraspeechcaps .\n","authors":["Anuj Diwan","Zhisheng Zheng","David Harwath","Eunsol Choi"],"pdf_url":"https://arxiv.org/pdf/2503.04713v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04710v1","updated":"2025-03-06T18:57:16Z","published":"2025-03-06T18:57:16Z","title":"Self-Supervised Models for Phoneme Recognition: Applications in\n  Children's Speech for Reading Learning","summary":"  Child speech recognition is still an underdeveloped area of research due to\nthe lack of data (especially on non-English languages) and the specific\ndifficulties of this task. Having explored various architectures for child\nspeech recognition in previous work, in this article we tackle recent\nself-supervised models. We first compare wav2vec 2.0, HuBERT and WavLM models\nadapted to phoneme recognition in French child speech, and continue our\nexperiments with the best of them, WavLM base+. We then further adapt it by\nunfreezing its transformer blocks during fine-tuning on child speech, which\ngreatly improves its performance and makes it significantly outperform our base\nmodel, a Transformer+CTC. Finally, we study in detail the behaviour of these\ntwo models under the real conditions of our application, and show that WavLM\nbase+ is more robust to various reading tasks and noise levels. Index Terms:\nspeech recognition, child speech, self-supervised learning\n","authors":["Lucas Block Medin","Thomas Pellegrini","Lucile Gelin"],"pdf_url":"https://arxiv.org/pdf/2503.04710v1.pdf","comment":"This paper was originally published in the Proceedings of Interspeech\n  2024. DOI: 10.21437/Interspeech.2024-1095"},{"id":"http://arxiv.org/abs/2503.04704v1","updated":"2025-03-06T18:54:32Z","published":"2025-03-06T18:54:32Z","title":"Universality of Layer-Level Entropy-Weighted Quantization Beyond Model\n  Architecture and Size","summary":"  We present a novel approach to selective model quantization that transcends\nthe limitations of architecture-specific and size-dependent compression methods\nfor Large Language Models (LLMs) using Entropy-Weighted Quantization (EWQ). By\nanalyzing the entropy distribution across transformer blocks, EWQ determines\nwhich blocks can be safely quantized without causing significant performance\ndegradation, independent of model architecture or size. Our method outperforms\nuniform quantization approaches, maintaining Massive Multitask Language\nUnderstanding (MMLU) accuracy scores within 0.5% of unquantized models while\nreducing memory usage by up to 18%. We demonstrate the effectiveness of EWQ\nacross multiple architectures-from 1.6B to 70B parameters-showcasing consistent\nimprovements in the quality-compression trade-off regardless of model scale or\narchitectural design. A surprising finding of EWQ is its ability to reduce\nperplexity compared to unquantized models, suggesting the presence of\nbeneficial regularization through selective precision reduction. This\nimprovement holds across different model families, indicating a fundamental\nrelationship between layer-level entropy and optimal precision requirements.\nAdditionally, we introduce FastEWQ, a rapid method for entropy distribution\nanalysis that eliminates the need for loading model weights. This technique\nleverages universal characteristics of entropy distribution that persist across\nvarious architectures and scales, enabling near-instantaneous quantization\ndecisions while maintaining 80% classification accuracy with full entropy\nanalysis. Our results demonstrate that effective quantization strategies can be\ndeveloped independently of specific architectural choices or model sizes,\nopening new possibilities for efficient LLM deployment.\n","authors":["Alireza Behtash","Marijan Fofonjka","Ethan Baird","Tyler Mauer","Hossein Moghimifam","David Stout","Joel Dennison"],"pdf_url":"https://arxiv.org/pdf/2503.04704v1.pdf","comment":"29 pages, 7 figures, 14 tables; Comments are welcome"},{"id":"http://arxiv.org/abs/2502.15037v4","updated":"2025-03-06T18:50:30Z","published":"2025-02-20T20:46:09Z","title":"DEFT: Differentiable Branched Discrete Elastic Rods for Modeling\n  Furcated DLOs in Real-Time","summary":"  Autonomous wire harness assembly requires robots to manipulate complex\nbranched cables with high precision and reliability. A key challenge in\nautomating this process is predicting how these flexible and branched\nstructures behave under manipulation. Without accurate predictions, it is\ndifficult for robots to reliably plan or execute assembly operations. While\nexisting research has made progress in modeling single-threaded Deformable\nLinear Objects (DLOs), extending these approaches to Branched Deformable Linear\nObjects (BDLOs) presents fundamental challenges. The junction points in BDLOs\ncreate complex force interactions and strain propagation patterns that cannot\nbe adequately captured by simply connecting multiple single-DLO models. To\naddress these challenges, this paper presents Differentiable discrete branched\nElastic rods for modeling Furcated DLOs in real-Time (DEFT), a novel framework\nthat combines a differentiable physics-based model with a learning framework\nto: 1) accurately model BDLO dynamics, including dynamic propagation at\njunction points and grasping in the middle of a BDLO, 2) achieve efficient\ncomputation for real-time inference, and 3) enable planning to demonstrate\ndexterous BDLO manipulation. A comprehensive series of real-world experiments\ndemonstrates DEFT's efficacy in terms of accuracy, computational speed, and\ngeneralizability compared to state-of-the-art alternatives. Project\npage:https://roahmlab.github.io/DEFT/.\n","authors":["Yizhou Chen","Xiaoyue Wu","Yeheng Zong","Anran Li","Yuzhen Chen","Julie Wu","Bohao Zhang","Ram Vasudevan"],"pdf_url":"https://arxiv.org/pdf/2502.15037v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02784v2","updated":"2025-03-06T18:45:51Z","published":"2025-03-04T16:57:53Z","title":"Do Not Trust Licenses You See -- Dataset Compliance Requires\n  Massive-Scale AI-Powered Lifecycle Tracing","summary":"  This paper argues that a dataset's legal risk cannot be accurately assessed\nby its license terms alone; instead, tracking dataset redistribution and its\nfull lifecycle is essential. However, this process is too complex for legal\nexperts to handle manually at scale. Tracking dataset provenance, verifying\nredistribution rights, and assessing evolving legal risks across multiple\nstages require a level of precision and efficiency that exceeds human\ncapabilities. Addressing this challenge effectively demands AI agents that can\nsystematically trace dataset redistribution, analyze compliance, and identify\nlegal risks. We develop an automated data compliance system called NEXUS and\nshow that AI can perform these tasks with higher accuracy, efficiency, and\ncost-effectiveness than human experts. Our massive legal analysis of 17,429\nunique entities and 8,072 license terms using this approach reveals the\ndiscrepancies in legal rights between the original datasets before\nredistribution and their redistributed subsets, underscoring the necessity of\nthe data lifecycle-aware compliance. For instance, we find that out of 2,852\ndatasets with commercially viable individual license terms, only 605 (21%) are\nlegally permissible for commercialization. This work sets a new standard for AI\ndata governance, advocating for a framework that systematically examines the\nentire lifecycle of dataset redistribution to ensure transparent, legal, and\nresponsible dataset management.\n","authors":["Jaekyeom Kim","Sungryull Sohn","Gerrard Jeongwon Jo","Jihoon Choi","Kyunghoon Bae","Hwayoung Lee","Yongmin Park","Honglak Lee"],"pdf_url":"https://arxiv.org/pdf/2503.02784v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04697v1","updated":"2025-03-06T18:43:29Z","published":"2025-03-06T18:43:29Z","title":"L1: Controlling How Long A Reasoning Model Thinks With Reinforcement\n  Learning","summary":"  Reasoning language models have shown an uncanny ability to improve\nperformance at test-time by ``thinking longer''-that is, by generating longer\nchain-of-thought sequences and hence using more compute. However, the length of\ntheir chain-of-thought reasoning is not controllable, making it impossible to\nallocate test-time compute to achieve a desired level of performance. We\nintroduce Length Controlled Policy Optimization (LCPO), a simple reinforcement\nlearning method that optimizes for accuracy and adherence to user-specified\nlength constraints. We use LCPO to train L1, a reasoning language model that\nproduces outputs satisfying a length constraint given in its prompt. L1's\nlength control allows for smoothly trading off computational cost and accuracy\non a wide range of tasks, and outperforms the state-of-the-art S1 method for\nlength control. Furthermore, we uncover an unexpected short chain-of-thought\ncapability in models trained with LCPO. For instance, our 1.5B L1 model\nsurpasses GPT-4o at equal reasoning lengths. Overall, LCPO enables precise\ncontrol over reasoning length, allowing for fine-grained allocation of\ntest-time compute and accuracy. We release code and models at\nhttps://www.cmu-l3.github.io/l1\n","authors":["Pranjal Aggarwal","Sean Welleck"],"pdf_url":"https://arxiv.org/pdf/2503.04697v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02694v3","updated":"2025-03-06T18:41:54Z","published":"2024-10-03T17:20:11Z","title":"HELMET: How to Evaluate Long-Context Language Models Effectively and\n  Thoroughly","summary":"  Many benchmarks exist for evaluating long-context language models (LCLMs),\nyet developers often rely on synthetic tasks such as needle-in-a-haystack\n(NIAH) or an arbitrary subset of tasks. However, it remains unclear whether\nthese benchmarks reflect the diverse downstream applications of LCLMs, and such\ninconsistencies further complicate model comparison. We investigate the\nunderlying reasons behind these practices and find that existing benchmarks\noften provide noisy signals due to limited coverage of applications,\ninsufficient context lengths, unreliable metrics, and incompatibility with base\nmodels. In this work, we introduce HELMET (How to Evaluate Long-context Models\nEffectively and Thoroughly), a comprehensive benchmark encompassing seven\ndiverse, application-centric categories. We also address several issues in\nprevious benchmarks by adding controllable lengths up to 128K tokens,\nmodel-based evaluation for reliable metrics, and few-shot prompting for\nrobustly evaluating base models. Consequently, we demonstrate that HELMET\noffers more reliable and consistent rankings of frontier LCLMs. Through a\ncomprehensive study of 59 LCLMs, we find that (1) synthetic tasks like NIAH do\nnot reliably predict downstream performance; (2) the diverse categories in\nHELMET exhibit distinct trends and low correlations with each other; and (3)\nwhile most LCLMs achieve perfect NIAH scores, open-source models significantly\nlag behind closed ones when tasks require full-context reasoning or following\ncomplex instructions -- the gap widens as length increases. Finally, we\nrecommend using our RAG tasks for fast model development, as they are easy to\nrun and better predict other downstream performance; ultimately, we advocate\nfor a holistic evaluation across diverse tasks.\n","authors":["Howard Yen","Tianyu Gao","Minmin Hou","Ke Ding","Daniel Fleischer","Peter Izsak","Moshe Wasserblat","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2410.02694v3.pdf","comment":"ICLR 2025. Project page: https://princeton-nlp.github.io/HELMET/"},{"id":"http://arxiv.org/abs/2503.04680v1","updated":"2025-03-06T18:22:46Z","published":"2025-03-06T18:22:46Z","title":"Matrix Factorization for Inferring Associations and Missing Links","summary":"  Missing link prediction is a method for network analysis, with applications\nin recommender systems, biology, social sciences, cybersecurity, information\nretrieval, and Artificial Intelligence (AI) reasoning in Knowledge Graphs.\nMissing link prediction identifies unseen but potentially existing connections\nin a network by analyzing the observed patterns and relationships. In\nproliferation detection, this supports efforts to identify and characterize\nattempts by state and non-state actors to acquire nuclear weapons or associated\ntechnology - a notoriously challenging but vital mission for global security.\nDimensionality reduction techniques like Non-Negative Matrix Factorization\n(NMF) and Logistic Matrix Factorization (LMF) are effective but require\nselection of the matrix rank parameter, that is, of the number of hidden\nfeatures, k, to avoid over/under-fitting. We introduce novel Weighted (WNMFk),\nBoolean (BNMFk), and Recommender (RNMFk) matrix factorization methods, along\nwith ensemble variants incorporating logistic factorization, for link\nprediction. Our methods integrate automatic model determination for rank\nestimation by evaluating stability and accuracy using a modified bootstrap\nmethodology and uncertainty quantification (UQ), assessing prediction\nreliability under random perturbations. We incorporate Otsu threshold selection\nand k-means clustering for Boolean matrix factorization, comparing them to\ncoordinate descent-based Boolean thresholding. Our experiments highlight the\nimpact of rank k selection, evaluate model performance under varying test-set\nsizes, and demonstrate the benefits of UQ for reliable predictions using\nabstention. We validate our methods on three synthetic datasets (Boolean and\nuniformly distributed) and benchmark them against LMF and symmetric LMF\n(symLMF) on five real-world protein-protein interaction networks, showcasing an\nimproved prediction performance.\n","authors":["Ryan Barron","Maksim E. Eren","Duc P. Truong","Cynthia Matuszek","James Wendelberger","Mary F. Dorn","Boian Alexandrov"],"pdf_url":"https://arxiv.org/pdf/2503.04680v1.pdf","comment":"35 pages, 14 figures, 3 tables, 1 algorithm"},{"id":"http://arxiv.org/abs/2503.04679v1","updated":"2025-03-06T18:22:29Z","published":"2025-03-06T18:22:29Z","title":"Multi-Agent Inverse Q-Learning from Demonstrations","summary":"  When reward functions are hand-designed, deep reinforcement learning\nalgorithms often suffer from reward misspecification, causing them to learn\nsuboptimal policies in terms of the intended task objectives. In the\nsingle-agent case, inverse reinforcement learning (IRL) techniques attempt to\naddress this issue by inferring the reward function from expert demonstrations.\nHowever, in multi-agent problems, misalignment between the learned and true\nobjectives is exacerbated due to increased environment non-stationarity and\nvariance that scales with multiple agents. As such, in multi-agent general-sum\ngames, multi-agent IRL algorithms have difficulty balancing cooperative and\ncompetitive objectives. To address these issues, we propose Multi-Agent\nMarginal Q-Learning from Demonstrations (MAMQL), a novel sample-efficient\nframework for multi-agent IRL. For each agent, MAMQL learns a critic\nmarginalized over the other agents' policies, allowing for a well-motivated use\nof Boltzmann policies in the multi-agent context. We identify a connection\nbetween optimal marginalized critics and single-agent soft-Q IRL, allowing us\nto apply a direct, simple optimization criterion from the single-agent domain.\nAcross our experiments on three different simulated domains, MAMQL\nsignificantly outperforms previous multi-agent methods in average reward,\nsample efficiency, and reward recovery by often more than 2-5x. We make our\ncode available at https://sites.google.com/view/mamql .\n","authors":["Nathaniel Haynam","Adam Khoja","Dhruv Kumar","Vivek Myers","Erdem Bıyık"],"pdf_url":"https://arxiv.org/pdf/2503.04679v1.pdf","comment":"8 pages, 4 figures, 2 tables. Published at the International\n  Conference on Robotics and Automation (ICRA) 2025"},{"id":"http://arxiv.org/abs/2502.02067v2","updated":"2025-03-06T18:09:38Z","published":"2025-02-04T07:32:39Z","title":"AdaptBot: Combining LLM with Knowledge Graphs and Human Input for\n  Generic-to-Specific Task Decomposition and Knowledge Refinement","summary":"  An embodied agent assisting humans is often asked to complete new tasks, and\nthere may not be sufficient time or labeled examples to train the agent to\nperform these new tasks. Large Language Models (LLMs) trained on considerable\nknowledge across many domains can be used to predict a sequence of abstract\nactions for completing such tasks, although the agent may not be able to\nexecute this sequence due to task-, agent-, or domain-specific constraints. Our\nframework addresses these challenges by leveraging the generic predictions\nprovided by LLM and the prior domain knowledge encoded in a Knowledge Graph\n(KG), enabling an agent to quickly adapt to new tasks. The robot also solicits\nand uses human input as needed to refine its existing knowledge. Based on\nexperimental evaluation in the context of cooking and cleaning tasks in\nsimulation domains, we demonstrate that the interplay between LLM, KG, and\nhuman input leads to substantial performance gains compared with just using the\nLLM. Project website{\\S}: https://sssshivvvv.github.io/adaptbot/\n","authors":["Shivam Singh","Karthik Swaminathan","Nabanita Dash","Ramandeep Singh","Snehasis Banerjee","Mohan Sridharan","Madhava Krishna"],"pdf_url":"https://arxiv.org/pdf/2502.02067v2.pdf","comment":"Accepted to IEEE International Conference on Robotics and Automation\n  (ICRA) 2025"},{"id":"http://arxiv.org/abs/2502.12360v2","updated":"2025-03-06T18:07:00Z","published":"2025-02-17T22:50:45Z","title":"Detecting Systematic Weaknesses in Vision Models along Predefined\n  Human-Understandable Dimensions","summary":"  Slice discovery methods (SDMs) are prominent algorithms for finding\nsystematic weaknesses in DNNs. They identify top-k semantically coherent\nslices/subsets of data where a DNN-under-test has low performance. For being\ndirectly useful, slices should be aligned with human-understandable and\nrelevant dimensions, which, for example, are defined by safety and domain\nexperts as part of the operational design domain (ODD). While SDMs can be\napplied effectively on structured data, their application on image data is\ncomplicated by the lack of semantic metadata. To address these issues, we\npresent an algorithm that combines foundation models for zero-shot image\nclassification to generate semantic metadata with methods for combinatorial\nsearch to find systematic weaknesses in images. In contrast to existing\napproaches, ours identifies weak slices that are in line with pre-defined\nhuman-understandable dimensions. As the algorithm includes foundation models,\nits intermediate and final results may not always be exact. Therefore, we\ninclude an approach to address the impact of noisy metadata. We validate our\nalgorithm on both synthetic and real-world datasets, demonstrating its ability\nto recover human-understandable systematic weaknesses. Furthermore, using our\napproach, we identify systematic weaknesses of multiple pre-trained and\npublicly available state-of-the-art computer vision DNNs.\n","authors":["Sujan Sai Gannamaneni","Rohil Prakash Rao","Michael Mock","Maram Akila","Stefan Wrobel"],"pdf_url":"https://arxiv.org/pdf/2502.12360v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04873v2","updated":"2025-03-06T17:35:19Z","published":"2025-01-08T23:07:10Z","title":"Back Home: A Machine Learning Approach to Seashell Classification and\n  Ecosystem Restoration","summary":"  In Costa Rica, an average of 5 tons of seashells are extracted from\necosystems annually. Confiscated seashells, cannot be returned to their\necosystems due to the lack of origin recognition. To address this issue, we\ndeveloped a convolutional neural network (CNN) specifically for seashell\nidentification. We built a dataset from scratch, consisting of approximately\n19000 images from the Pacific and Caribbean coasts. Using this dataset, the\nmodel achieved a classification accuracy exceeding 85%. The model has been\nintegrated into a user-friendly application, which has classified over 36,000\nseashells to date, delivering real-time results within 3 seconds per image. To\nfurther enhance the system's accuracy, an anomaly detection mechanism was\nincorporated to filter out irrelevant or anomalous inputs, ensuring only valid\nseashell images are processed.\n","authors":["Alexander Valverde","Luis Solano"],"pdf_url":"https://arxiv.org/pdf/2501.04873v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04647v1","updated":"2025-03-06T17:33:01Z","published":"2025-03-06T17:33:01Z","title":"Implicit Cross-Lingual Rewarding for Efficient Multilingual Preference\n  Alignment","summary":"  Direct Preference Optimization (DPO) has become a prominent method for\naligning Large Language Models (LLMs) with human preferences. While DPO has\nenabled significant progress in aligning English LLMs, multilingual preference\nalignment is hampered by data scarcity. To address this, we propose a novel\napproach that $\\textit{captures}$ learned preferences from well-aligned English\nmodels by implicit rewards and $\\textit{transfers}$ them to other languages\nthrough iterative training. Specifically, we derive an implicit reward model\nfrom the logits of an English DPO-aligned model and its corresponding reference\nmodel. This reward model is then leveraged to annotate preference relations in\ncross-lingual instruction-following pairs, using English instructions to\nevaluate multilingual responses. The annotated data is subsequently used for\nmultilingual DPO fine-tuning, facilitating preference knowledge transfer from\nEnglish to other languages. Fine-tuning Llama3 for two iterations resulted in a\n12.72% average improvement in Win Rate and a 5.97% increase in Length Control\nWin Rate across all training languages on the X-AlpacaEval leaderboard. Our\nfindings demonstrate that leveraging existing English-aligned models can enable\nefficient and effective multilingual preference alignment, significantly\nreducing the need for extensive multilingual preference data. The code is\navailable at https://github.com/ZNLP/Implicit-Cross-Lingual-Rewarding\n","authors":["Wen Yang","Junhong Wu","Chen Wang","Chengqing Zong","Jiajun Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.04647v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2503.04641v1","updated":"2025-03-06T17:31:43Z","published":"2025-03-06T17:31:43Z","title":"Simulating the Real World: A Unified Survey of Multimodal Generative\n  Models","summary":"  Understanding and replicating the real world is a critical challenge in\nArtificial General Intelligence (AGI) research. To achieve this, many existing\napproaches, such as world models, aim to capture the fundamental principles\ngoverning the physical world, enabling more accurate simulations and meaningful\ninteractions. However, current methods often treat different modalities,\nincluding 2D (images), videos, 3D, and 4D representations, as independent\ndomains, overlooking their interdependencies. Additionally, these methods\ntypically focus on isolated dimensions of reality without systematically\nintegrating their connections. In this survey, we present a unified survey for\nmultimodal generative models that investigate the progression of data\ndimensionality in real-world simulation. Specifically, this survey starts from\n2D generation (appearance), then moves to video (appearance+dynamics) and 3D\ngeneration (appearance+geometry), and finally culminates in 4D generation that\nintegrate all dimensions. To the best of our knowledge, this is the first\nattempt to systematically unify the study of 2D, video, 3D and 4D generation\nwithin a single framework. To guide future research, we provide a comprehensive\nreview of datasets, evaluation metrics and future directions, and fostering\ninsights for newcomers. This survey serves as a bridge to advance the study of\nmultimodal generative models and real-world simulation within a unified\nframework.\n","authors":["Yuqi Hu","Longguang Wang","Xian Liu","Ling-Hao Chen","Yuwei Guo","Yukai Shi","Ce Liu","Anyi Rao","Zeyu Wang","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2503.04641v1.pdf","comment":"Repository for the related papers at\n  https://github.com/ALEEEHU/World-Simulator"},{"id":"http://arxiv.org/abs/2202.00665v4","updated":"2025-03-06T17:24:46Z","published":"2022-02-01T18:58:33Z","title":"Tutorial on amortized optimization","summary":"  Optimization is a ubiquitous modeling tool and is often deployed in settings\nwhich repeatedly solve similar instances of the same problem. Amortized\noptimization methods use learning to predict the solutions to problems in these\nsettings, exploiting the shared structure between similar problem instances.\nThese methods have been crucial in variational inference and reinforcement\nlearning and are capable of solving optimization problems many orders of\nmagnitudes times faster than traditional optimization methods that do not use\namortization. This tutorial presents an introduction to the amortized\noptimization foundations behind these advancements and overviews their\napplications in variational inference, sparse coding, gradient-based\nmeta-learning, control, reinforcement learning, convex optimization, optimal\ntransport, and deep equilibrium networks. The source code for this tutorial is\navailable at\nhttps://github.com/facebookresearch/amortized-optimization-tutorial.\n","authors":["Brandon Amos"],"pdf_url":"https://arxiv.org/pdf/2202.00665v4.pdf","comment":"Foundations and Trends in Machine Learning"},{"id":"http://arxiv.org/abs/2503.04636v1","updated":"2025-03-06T17:24:06Z","published":"2025-03-06T17:24:06Z","title":"Mark Your LLM: Detecting the Misuse of Open-Source Large Language Models\n  via Watermarking","summary":"  As open-source large language models (LLMs) like Llama3 become more capable,\nit is crucial to develop watermarking techniques to detect their potential\nmisuse. Existing watermarking methods either add watermarks during LLM\ninference, which is unsuitable for open-source LLMs, or primarily target\nclassification LLMs rather than recent generative LLMs. Adapting these\nwatermarks to open-source LLMs for misuse detection remains an open challenge.\nThis work defines two misuse scenarios for open-source LLMs: intellectual\nproperty (IP) violation and LLM Usage Violation. Then, we explore the\napplication of inference-time watermark distillation and backdoor watermarking\nin these contexts. We propose comprehensive evaluation methods to assess the\nimpact of various real-world further fine-tuning scenarios on watermarks and\nthe effect of these watermarks on LLM performance. Our experiments reveal that\nbackdoor watermarking could effectively detect IP Violation, while\ninference-time watermark distillation is applicable in both scenarios but less\nrobust to further fine-tuning and has a more significant impact on LLM\nperformance compared to backdoor watermarking. Exploring more advanced\nwatermarking methods for open-source LLMs to detect their misuse should be an\nimportant future direction.\n","authors":["Yijie Xu","Aiwei Liu","Xuming Hu","Lijie Wen","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2503.04636v1.pdf","comment":"Accepted by the 1st Workshop on GenAI Watermarking, collocated with\n  ICLR 2025"},{"id":"http://arxiv.org/abs/2503.00897v3","updated":"2025-03-06T17:19:22Z","published":"2025-03-02T13:43:53Z","title":"A Simple and Effective Reinforcement Learning Method for Text-to-Image\n  Diffusion Fine-tuning","summary":"  Reinforcement learning (RL)-based fine-tuning has emerged as a powerful\napproach for aligning diffusion models with black-box objectives. Proximal\npolicy optimization (PPO) is the most popular choice of method for policy\noptimization. While effective in terms of performance, PPO is highly sensitive\nto hyper-parameters and involves substantial computational overhead. REINFORCE,\non the other hand, mitigates some computational complexities such as high\nmemory overhead and sensitive hyper-parameter tuning, but has suboptimal\nperformance due to high-variance and sample inefficiency. While the variance of\nthe REINFORCE can be reduced by sampling multiple actions per input prompt and\nusing a baseline correction term, it still suffers from sample inefficiency. To\naddress these challenges, we systematically analyze the\nefficiency-effectiveness trade-off between REINFORCE and PPO, and propose\nleave-one-out PPO (LOOP), a novel RL for diffusion fine-tuning method. LOOP\ncombines variance reduction techniques from REINFORCE, such as sampling\nmultiple actions per input prompt and a baseline correction term, with the\nrobustness and sample efficiency of PPO via clipping and importance sampling.\nOur results demonstrate that LOOP effectively improves diffusion models on\nvarious black-box objectives, and achieves a better balance between\ncomputational efficiency and performance.\n","authors":["Shashank Gupta","Chaitanya Ahuja","Tsung-Yu Lin","Sreya Dutta Roy","Harrie Oosterhuis","Maarten de Rijke","Satya Narayan Shukla"],"pdf_url":"https://arxiv.org/pdf/2503.00897v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04626v1","updated":"2025-03-06T17:12:46Z","published":"2025-03-06T17:12:46Z","title":"IDInit: A Universal and Stable Initialization Method for Neural Network\n  Training","summary":"  Deep neural networks have achieved remarkable accomplishments in practice.\nThe success of these networks hinges on effective initialization methods, which\nare vital for ensuring stable and rapid convergence during training. Recently,\ninitialization methods that maintain identity transition within layers have\nshown good efficiency in network training. These techniques (e.g., Fixup) set\nspecific weights to zero to achieve identity control. However, settings of\nremaining weight (e.g., Fixup uses random values to initialize non-zero\nweights) will affect the inductive bias that is achieved only by a zero weight,\nwhich may be harmful to training. Addressing this concern, we introduce fully\nidentical initialization (IDInit), a novel method that preserves identity in\nboth the main and sub-stem layers of residual networks. IDInit employs a padded\nidentity-like matrix to overcome rank constraints in non-square weight\nmatrices. Furthermore, we show the convergence problem of an identity matrix\ncan be solved by stochastic gradient descent. Additionally, we enhance the\nuniversality of IDInit by processing higher-order weights and addressing dead\nneuron problems. IDInit is a straightforward yet effective initialization\nmethod, with improved convergence, stability, and performance across various\nsettings, including large-scale datasets and deep models.\n","authors":["Yu Pan","Chaozheng Wang","Zekai Wu","Qifan Wang","Min Zhang","Zenglin Xu"],"pdf_url":"https://arxiv.org/pdf/2503.04626v1.pdf","comment":"Accepted in ICLR 2025"},{"id":"http://arxiv.org/abs/2410.05116v2","updated":"2025-03-06T17:11:55Z","published":"2024-10-07T15:12:01Z","title":"Human-Feedback Efficient Reinforcement Learning for Online Diffusion\n  Model Finetuning","summary":"  Controllable generation through Stable Diffusion (SD) fine-tuning aims to\nimprove fidelity, safety, and alignment with human guidance. Existing\nreinforcement learning from human feedback methods usually rely on predefined\nheuristic reward functions or pretrained reward models built on large-scale\ndatasets, limiting their applicability to scenarios where collecting such data\nis costly or difficult. To effectively and efficiently utilize human feedback,\nwe develop a framework, HERO, which leverages online human feedback collected\non the fly during model learning. Specifically, HERO features two key\nmechanisms: (1) Feedback-Aligned Representation Learning, an online training\nmethod that captures human feedback and provides informative learning signals\nfor fine-tuning, and (2) Feedback-Guided Image Generation, which involves\ngenerating images from SD's refined initialization samples, enabling faster\nconvergence towards the evaluator's intent. We demonstrate that HERO is 4x more\nefficient in online feedback for body part anomaly correction compared to the\nbest existing method. Additionally, experiments show that HERO can effectively\nhandle tasks like reasoning, counting, personalization, and reducing NSFW\ncontent with only 0.5K online feedback.\n","authors":["Ayano Hiranaka","Shang-Fu Chen","Chieh-Hsin Lai","Dongjun Kim","Naoki Murata","Takashi Shibuya","Wei-Hsiang Liao","Shao-Hua Sun","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2410.05116v2.pdf","comment":"Published in International Conference on Learning Representations\n  (ICLR) 2025"},{"id":"http://arxiv.org/abs/2407.18125v3","updated":"2025-03-06T17:03:35Z","published":"2024-07-25T15:32:59Z","title":"Self-supervised pre-training with diffusion model for few-shot landmark\n  detection in x-ray images","summary":"  Deep neural networks have been extensively applied in the medical domain for\nvarious tasks, including image classification, segmentation, and landmark\ndetection. However, their application is often hindered by data scarcity, both\nin terms of available annotations and images. This study introduces a novel\napplication of denoising diffusion probabilistic models (DDPMs) to the landmark\ndetection task, specifically addressing the challenge of limited annotated data\nin x-ray imaging. Our key innovation lies in leveraging DDPMs for\nself-supervised pre-training in landmark detection, a previously unexplored\napproach in this domain. This method enables accurate landmark detection with\nminimal annotated training data (as few as 50 images), surpassing both ImageNet\nsupervised pre-training and traditional self-supervised techniques across three\npopular x-ray benchmark datasets. To our knowledge, this work represents the\nfirst application of diffusion models for self-supervised learning in landmark\ndetection, which may offer a valuable pre-training approach in few-shot\nregimes, for mitigating data scarcity.\n","authors":["Roberto Di Via","Francesca Odone","Vito Paolo Pastore"],"pdf_url":"https://arxiv.org/pdf/2407.18125v3.pdf","comment":"Accepted at WACV 2025"},{"id":"http://arxiv.org/abs/2503.04606v1","updated":"2025-03-06T16:53:14Z","published":"2025-03-06T16:53:14Z","title":"The Best of Both Worlds: Integrating Language Models and Diffusion\n  Models for Video Generation","summary":"  Recent advancements in text-to-video (T2V) generation have been driven by two\ncompeting paradigms: autoregressive language models and diffusion models.\nHowever, each paradigm has intrinsic limitations: language models struggle with\nvisual quality and error accumulation, while diffusion models lack semantic\nunderstanding and causal modeling. In this work, we propose LanDiff, a hybrid\nframework that synergizes the strengths of both paradigms through\ncoarse-to-fine generation. Our architecture introduces three key innovations:\n(1) a semantic tokenizer that compresses 3D visual features into compact 1D\ndiscrete representations through efficient semantic compression, achieving a\n$\\sim$14,000$\\times$ compression ratio; (2) a language model that generates\nsemantic tokens with high-level semantic relationships; (3) a streaming\ndiffusion model that refines coarse semantics into high-fidelity videos.\nExperiments show that LanDiff, a 5B model, achieves a score of 85.43 on the\nVBench T2V benchmark, surpassing the state-of-the-art open-source models\nHunyuan Video (13B) and other commercial models such as Sora, Keling, and\nHailuo. Furthermore, our model also achieves state-of-the-art performance in\nlong video generation, surpassing other open-source models in this field. Our\ndemo can be viewed at https://landiff.github.io/.\n","authors":["Aoxiong Yin","Kai Shen","Yichong Leng","Xu Tan","Xinyu Zhou","Juncheng Li","Siliang Tang"],"pdf_url":"https://arxiv.org/pdf/2503.04606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04598v1","updated":"2025-03-06T16:40:48Z","published":"2025-03-06T16:40:48Z","title":"HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid\n  Normalization","summary":"  Transformers have become the de facto architecture for a wide range of\nmachine learning tasks, particularly in large language models (LLMs). Despite\ntheir remarkable performance, challenges remain in training deep transformer\nnetworks, especially regarding the location of layer normalization. While\nPre-Norm structures facilitate easier training due to their more prominent\nidentity path, they often yield suboptimal performance compared to Post-Norm.\nIn this paper, we propose $\\textbf{HybridNorm}$, a straightforward yet\neffective hybrid normalization strategy that integrates the advantages of both\nPre-Norm and Post-Norm approaches. Specifically, HybridNorm employs QKV\nnormalization within the attention mechanism and Post-Norm in the feed-forward\nnetwork (FFN) of each transformer block. This design not only stabilizes\ntraining but also enhances performance, particularly in the context of LLMs.\nComprehensive experiments in both dense and sparse architectures show that\nHybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches,\nachieving state-of-the-art results across various benchmarks. These findings\nhighlight the potential of HybridNorm as a more stable and effective technique\nfor improving the training and performance of deep transformer models. %Code\nwill be made publicly available. Code is available at\nhttps://github.com/BryceZhuo/HybridNorm.\n","authors":["Zhijian Zhuo","Yutao Zeng","Ya Wang","Sijun Zhang","Jian Yang","Xiaoqing Li","Xun Zhou","Jinwen Ma"],"pdf_url":"https://arxiv.org/pdf/2503.04598v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04596v1","updated":"2025-03-06T16:38:23Z","published":"2025-03-06T16:38:23Z","title":"The Next Frontier of LLM Applications: Open Ecosystems and Hardware\n  Synergy","summary":"  Large Language Model (LLM) applications, including LLM app stores and\nautonomous agents, are shaping the future of AI ecosystems. However, platform\nsilos, fragmented hardware integration, and the absence of standardized\ninterfaces limit scalability, interoperability, and resource efficiency. While\nLLM app stores democratize AI, their closed ecosystems restrict modular AI\nreuse and cross-platform portability. Meanwhile, agent-based frameworks offer\nflexibility but often lack seamless integration across diverse environments.\nThis paper envisions the future of LLM applications and proposes a three-layer\ndecoupled architecture grounded in software engineering principles such as\nlayered system design, service-oriented architectures, and hardware-software\nco-design. This architecture separates application logic, communication\nprotocols, and hardware execution, enhancing modularity, efficiency, and\ncross-platform compatibility. Beyond architecture, we highlight key security\nand privacy challenges for safe, scalable AI deployment and outline research\ndirections in software and security engineering. This vision aims to foster\nopen, secure, and interoperable LLM ecosystems, guiding future advancements in\nAI applications.\n","authors":["Xinyi Hou","Yanjie Zhao","Haoyu Wang"],"pdf_url":"https://arxiv.org/pdf/2503.04596v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00053v3","updated":"2025-03-06T16:28:55Z","published":"2024-10-30T19:09:02Z","title":"ACC-Collab: An Actor-Critic Approach to Multi-Agent LLM Collaboration","summary":"  Large language models (LLMs) have demonstrated a remarkable ability to serve\nas general-purpose tools for various language-based tasks. Recent works have\ndemonstrated that the efficacy of such models can be improved through iterative\ndialog between multiple models. While these paradigms show promise in improving\nmodel efficacy, most works in this area treat collaboration as an emergent\nbehavior, rather than a learned behavior. In doing so, current multi-agent\nframeworks rely on collaborative behaviors to have been sufficiently trained\ninto off-the-shelf models. To address this limitation, we propose ACC-Collab,\nan Actor-Critic based learning framework to produce a two-agent team (an\nactor-agent and a critic-agent) specialized in collaboration. We demonstrate\nthat ACC-Collab outperforms SotA multi-agent techniques on a wide array of\nbenchmarks.\n","authors":["Andrew Estornell","Jean-Francois Ton","Yuanshun Yao","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2411.00053v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17412v3","updated":"2025-03-06T16:22:22Z","published":"2024-05-27T17:57:12Z","title":"Towards One Model for Classical Dimensionality Reduction: A\n  Probabilistic Perspective on UMAP and t-SNE","summary":"  This paper shows that dimensionality reduction methods such as UMAP and\nt-SNE, can be approximately recast as MAP inference methods corresponding to a\nmodel introduced in ProbDR, that describes the graph Laplacian (an estimate of\nthe data precision matrix) using a Wishart distribution, with a mean given by a\nnon-linear covariance function evaluated on the latents. This interpretation\noffers deeper theoretical and semantic insights into such algorithms, by\nshowing that variances corresponding to these covariances are low (potentially\nmisspecified), and forging a connection to Gaussian process latent variable\nmodels by showing that well-known kernels can be used to describe covariances\nimplied by graph Laplacians. We also introduce tools with which similar\ndimensionality reduction methods can be studied.\n","authors":["Aditya Ravuri","Neil D. Lawrence"],"pdf_url":"https://arxiv.org/pdf/2405.17412v3.pdf","comment":"Updated preprint"},{"id":"http://arxiv.org/abs/2503.02972v2","updated":"2025-03-06T16:16:07Z","published":"2025-03-04T19:57:47Z","title":"LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic\n  Templatisation and Orthographic Obfuscation","summary":"  Assessing the reasoning capabilities of large language models (LLMs) is\nsusceptible to overestimation due to data exposure of evaluation benchmarks. We\nintroduce a framework for producing linguistic reasoning problems that reduces\nthe effect of memorisation in model performance estimates and apply this\nframework to develop LINGOLY-TOO, a challenging benchmark for linguistic\nreasoning. By developing orthographic templates, we dynamically obfuscate the\nwriting systems of real languages to generate numerousquestion variations.\nThese variations preserve the reasoning steps required for each solution while\nreducing the likelihood of specific problem instances appearing in model\ntraining data. Our experiments demonstrate that frontier models, including\nClaud 3.7 Sonnet, o1-preview and DeepSeek R1, struggle with advanced reasoning.\nOur analysis also shows that LLMs exhibit noticeable variance in accuracy\nacross permutations of the same problem, and on average perform better on\nquestions appearing in their original orthography. Our findings highlight the\nopaque nature of response generation in LLMs and provide evidence that prior\ndata exposure contributes to over estimating the reasoning capabilities of\nfrontier models.\n","authors":["Jude Khouja","Karolina Korgul","Simi Hellsten","Lingyi Yang","Vlad Neacs","Harry Mayne","Ryan Kearns","Andrew Bean","Adam Mahdi"],"pdf_url":"https://arxiv.org/pdf/2503.02972v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17504v2","updated":"2025-03-06T16:14:45Z","published":"2025-02-21T19:22:10Z","title":"Protein Large Language Models: A Comprehensive Survey","summary":"  Protein-specific large language models (Protein LLMs) are revolutionizing\nprotein science by enabling more efficient protein structure prediction,\nfunction annotation, and design. While existing surveys focus on specific\naspects or applications, this work provides the first comprehensive overview of\nProtein LLMs, covering their architectures, training datasets, evaluation\nmetrics, and diverse applications. Through a systematic analysis of over 100\narticles, we propose a structured taxonomy of state-of-the-art Protein LLMs,\nanalyze how they leverage large-scale protein sequence data for improved\naccuracy, and explore their potential in advancing protein engineering and\nbiomedical research. Additionally, we discuss key challenges and future\ndirections, positioning Protein LLMs as essential tools for scientific\ndiscovery in protein science. Resources are maintained at\nhttps://github.com/Yijia-Xiao/Protein-LLM-Survey.\n","authors":["Yijia Xiao","Wanjia Zhao","Junkai Zhang","Yiqiao Jin","Han Zhang","Zhicheng Ren","Renliang Sun","Haixin Wang","Guancheng Wan","Pan Lu","Xiao Luo","Yu Zhang","James Zou","Yizhou Sun","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2502.17504v2.pdf","comment":"24 pages, 4 figures, 5 tables"},{"id":"http://arxiv.org/abs/2503.01804v2","updated":"2025-03-06T16:07:43Z","published":"2025-03-03T18:33:46Z","title":"$\\texttt{SEM-CTRL}$: Semantically Controlled Decoding","summary":"  Ensuring both syntactic and semantic correctness in Large Language Model\n(LLM) outputs remains a significant challenge, despite being critical for\nreal-world deployment. In this paper, we introduce $\\texttt{SEM-CTRL}$, a\nunified approach that enforces rich context-sensitive constraints and task- and\ninstance-specific semantics directly on an LLM decoder. Our approach integrates\ntoken-level MCTS, which is guided by specific syntactic and semantic\nconstraints. The constraints over the desired outputs are expressed using\nAnswer Set Grammars -- a logic-based formalism that generalizes\ncontext-sensitive grammars while incorporating background knowledge to\nrepresent task-specific semantics. We show that our approach guarantees correct\ncompletions for any off-the-shelf LLM without the need for fine-tuning. We\nevaluate $\\texttt{SEM-CTRL}$ on a range of tasks, including synthetic grammar\nsynthesis, combinatorial reasoning, and planning. Our results demonstrate that\n$\\texttt{SEM-CTRL}$ allows small pre-trained LLMs to efficiently outperform\nlarger variants and state-of-the-art reasoning models (e.g., o1-preview) while\nsimultaneously guaranteeing solution correctness.\n","authors":["Mohammad Albinhassan","Pranava Madhyastha","Alessandra Russo"],"pdf_url":"https://arxiv.org/pdf/2503.01804v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04569v1","updated":"2025-03-06T16:02:53Z","published":"2025-03-06T16:02:53Z","title":"ValuePilot: A Two-Phase Framework for Value-Driven Decision-Making","summary":"  Despite recent advances in artificial intelligence (AI), it poses challenges\nto ensure personalized decision-making in tasks that are not considered in\ntraining datasets. To address this issue, we propose ValuePilot, a two-phase\nvalue-driven decision-making framework comprising a dataset generation toolkit\nDGT and a decision-making module DMM trained on the generated data. DGT is\ncapable of generating scenarios based on value dimensions and closely mirroring\nreal-world tasks, with automated filtering techniques and human curation to\nensure the validity of the dataset. In the generated dataset, DMM learns to\nrecognize the inherent values of scenarios, computes action feasibility and\nnavigates the trade-offs between multiple value dimensions to make personalized\ndecisions. Extensive experiments demonstrate that, given human value\npreferences, our DMM most closely aligns with human decisions, outperforming\nClaude-3.5-Sonnet, Gemini-2-flash, Llama-3.1-405b and GPT-4o. This research is\na preliminary exploration of value-driven decision-making. We hope it will\nstimulate interest in value-driven decision-making and personalized\ndecision-making within the community.\n","authors":["Yitong Luo","Hou Hei Lam","Ziang Chen","Zhenliang Zhang","Xue Feng"],"pdf_url":"https://arxiv.org/pdf/2503.04569v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04564v1","updated":"2025-03-06T15:53:37Z","published":"2025-03-06T15:53:37Z","title":"Fundamental Limits of Hierarchical Secure Aggregation with Cyclic User\n  Association","summary":"  Secure aggregation is motivated by federated learning (FL) where a cloud\nserver aims to compute an averaged model (i.e., weights of deep neural\nnetworks) of the locally-trained models of numerous clients, while adhering to\ndata security requirements. Hierarchical secure aggregation (HSA) extends this\nconcept to a three-layer network, where clustered users communicate with the\nserver through an intermediate layer of relays. In HSA, beyond conventional\nserver security, relay security is also enforced to ensure that the relays\nremain oblivious to the users' inputs (an abstraction of the local models in\nFL). Existing study on HSA assumes that each user is associated with only one\nrelay, limiting opportunities for coding across inter-cluster users to achieve\nefficient communication and key generation. In this paper, we consider HSA with\na cyclic association pattern where each user is connected to $B$ consecutive\nrelays in a wrap-around manner. We propose an efficient aggregation scheme\nwhich includes a message design for the inputs inspired by gradient coding-a\nwell-known technique for efficient communication in distributed computing-along\nwith a highly nontrivial security key design. We also derive novel converse\nbounds on the minimum achievable communication and key rates using\ninformation-theoretic arguments.\n","authors":["Xiang Zhang","Zhou Li","Kai Wan","Hua Sun","Mingyue Ji","Giuseppe Caire"],"pdf_url":"https://arxiv.org/pdf/2503.04564v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00153v2","updated":"2025-03-06T15:50:28Z","published":"2024-09-30T18:52:53Z","title":"Beyond Single Concept Vector: Modeling Concept Subspace in LLMs with\n  Gaussian Distribution","summary":"  Probing learned concepts in large language models (LLMs) is crucial for\nunderstanding how semantic knowledge is encoded internally. Training linear\nclassifiers on probing tasks is a principle approach to denote the vector of a\ncertain concept in the representation space. However, the single vector\nidentified for a concept varies with both data and training, making it less\nrobust and weakening its effectiveness in real-world applications. To address\nthis challenge, we propose an approach to approximate the subspace representing\na specific concept. Built on linear probing classifiers, we extend the concept\nvectors into Gaussian Concept Subspace (GCS). We demonstrate GCS's\neffectiveness through measuring its faithfulness and plausibility across\nmultiple LLMs with different sizes and architectures. Additionally, we use\nrepresentation intervention tasks to showcase its efficacy in real-world\napplications such as emotion steering. Experimental results indicate that GCS\nconcept vectors have the potential to balance steering performance and\nmaintaining the fluency in natural language generation tasks.\n","authors":["Haiyan Zhao","Heng Zhao","Bo Shen","Ali Payani","Fan Yang","Mengnan Du"],"pdf_url":"https://arxiv.org/pdf/2410.00153v2.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2503.04556v1","updated":"2025-03-06T15:47:19Z","published":"2025-03-06T15:47:19Z","title":"Compositional Causal Reasoning Evaluation in Language Models","summary":"  Causal reasoning and compositional reasoning are two core aspirations in\ngenerative AI. Measuring the extent of these behaviors requires principled\nevaluation methods. We explore a unified perspective that considers both\nbehaviors simultaneously, termed compositional causal reasoning (CCR): the\nability to infer how causal measures compose and, equivalently, how causal\nquantities propagate through graphs. We instantiate a framework for the\nsystematic evaluation of CCR for the average treatment effect and the\nprobability of necessity and sufficiency. As proof of concept, we demonstrate\nthe design of CCR tasks for language models in the LLama, Phi, and GPT\nfamilies. On a math word problem, our framework revealed a range of\ntaxonomically distinct error patterns. Additionally, CCR errors increased with\nthe complexity of causal paths for all models except o1.\n","authors":["Jacqueline R. M. A. Maasch","Alihan Hüyük","Xinnuo Xu","Aditya V. Nori","Javier Gonzalez"],"pdf_url":"https://arxiv.org/pdf/2503.04556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18691v2","updated":"2025-03-06T15:47:01Z","published":"2024-07-26T12:16:53Z","title":"Graph Neural Networks for Virtual Sensing in Complex Systems: Addressing\n  Heterogeneous Temporal Dynamics","summary":"  Real-time condition monitoring is crucial for the reliable and efficient\noperation of complex systems. However, relying solely on physical sensors can\nbe limited due to their cost, placement constraints, or inability to directly\nmeasure certain critical parameters. Virtual sensing addresses these\nlimitations by leveraging readily available sensor data and system knowledge to\nestimate inaccessible parameters or infer system states. The increasing\ncomplexity of industrial systems necessitates deployments of sensors with\ndiverse modalities to provide a comprehensive understanding of system states.\nThese sensors capture data at varying frequencies to monitor both rapid and\nslowly varying system dynamics, as well as local and global state evolutions of\nthe systems. This leads to heterogeneous temporal dynamics, which, particularly\nunder varying operational end environmental conditions, pose a significant\nchallenge for accurate virtual sensing. To address this, we propose a\nHeterogeneous Temporal Graph Neural Network (HTGNN) framework. HTGNN explicitly\nmodels signals from diverse sensors and integrates operating conditions into\nthe model architecture. We evaluate HTGNN using two newly released datasets: a\nbearing dataset with diverse load conditions for bearing load prediction and a\nyear-long simulated dataset for predicting bridge live loads. Our results\ndemonstrate that HTGNN significantly outperforms established baseline methods\nin both tasks, particularly under highly varying operating conditions. These\nresults highlight HTGNN's potential as a robust and accurate virtual sensing\napproach for complex systems, paving the way for improved monitoring,\npredictive maintenance, and enhanced system performance. Our code and data are\navailable under https://github.com/EPFL-IMOS/htgnn.\n","authors":["Mengjie Zhao","Cees Taal","Stephan Baggerohr","Olga Fink"],"pdf_url":"https://arxiv.org/pdf/2407.18691v2.pdf","comment":"This paper extends our previous conference paper (Best Paper at\n  European Conference of the PHM Society 2024,\n  https://doi.org/10.36001/phme.2024.v8i1.3998). Accepted by Mechanical Systems\n  and Signal Processing (MSSP)"},{"id":"http://arxiv.org/abs/2502.09990v2","updated":"2025-03-06T15:38:31Z","published":"2025-02-14T08:22:51Z","title":"X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from\n  Multi-Turn Jailbreaks without Compromising Usability","summary":"  Despite the rapid development of safety alignment techniques for LLMs,\ndefending against multi-turn jailbreaks is still a challenging task. In this\npaper, we conduct a comprehensive comparison, revealing that some existing\ndefense methods can improve the robustness of LLMs against multi-turn\njailbreaks but compromise usability, i.e., reducing general capabilities or\ncausing the over-refusal problem. From the perspective of mechanism\ninterpretability of LLMs, we discover that these methods fail to establish a\nboundary that exactly distinguishes safe and harmful feature representations.\nTherefore, boundary-safe representations close to harmful representations are\ninevitably disrupted, leading to a decline in usability. To address this issue,\nwe propose X-Boundary to push harmful representations away from boundary-safe\nrepresentations and obtain an exact distinction boundary. In this way, harmful\nrepresentations can be precisely erased without disrupting safe ones.\nExperimental results show that X-Boundary achieves state-of-the-art defense\nperformance against multi-turn jailbreaks, while reducing the over-refusal rate\nby about 20% and maintaining nearly complete general capability. Furthermore,\nwe theoretically prove and empirically verify that X-Boundary can accelerate\nthe convergence process during training. Please see our code at:\nhttps://github.com/AI45Lab/X-Boundary.\n","authors":["Xiaoya Lu","Dongrui Liu","Yi Yu","Luxin Xu","Jing Shao"],"pdf_url":"https://arxiv.org/pdf/2502.09990v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20984v2","updated":"2025-03-06T15:36:48Z","published":"2025-02-28T11:52:02Z","title":"UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models\n  for Multilingual Multimodal Idiomaticity Representation","summary":"  SemEval-2025 Task 1 focuses on ranking images based on their alignment with a\ngiven nominal compound that may carry idiomatic meaning in both English and\nBrazilian Portuguese. To address this challenge, this work uses generative\nlarge language models (LLMs) and multilingual CLIP models to enhance idiomatic\ncompound representations. LLMs generate idiomatic meanings for potentially\nidiomatic compounds, enriching their semantic interpretation. These meanings\nare then encoded using multilingual CLIP models, serving as representations for\nimage ranking. Contrastive learning and data augmentation techniques are\napplied to fine-tune these embeddings for improved performance. Experimental\nresults show that multimodal representations extracted through this method\noutperformed those based solely on the original nominal compounds. The\nfine-tuning approach shows promising outcomes but is less effective than using\nembeddings without fine-tuning. The source code used in this paper is available\nat https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL.\n","authors":["Thanet Markchom","Tong Wu","Liting Huang","Huizhi Liang"],"pdf_url":"https://arxiv.org/pdf/2502.20984v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04550v1","updated":"2025-03-06T15:36:06Z","published":"2025-03-06T15:36:06Z","title":"Benchmarking Reasoning Robustness in Large Language Models","summary":"  Despite the recent success of large language models (LLMs) in reasoning such\nas DeepSeek, we for the first time identify a key dilemma in reasoning\nrobustness and generalization: significant performance degradation on novel or\nincomplete data, suggesting a reliance on memorized patterns rather than\nsystematic reasoning. Our closer examination reveals four key unique\nlimitations underlying this issue:(1) Positional bias--models favor earlier\nqueries in multi-query inputs but answering the wrong one in the latter (e.g.,\nGPT-4o's accuracy drops from 75.8 percent to 72.8 percent); (2) Instruction\nsensitivity--performance declines by 5.0 to 7.5 percent in the Qwen2.5 Series\nand by 5.0 percent in DeepSeek-V3 with auxiliary guidance; (3) Numerical\nfragility--value substitution sharply reduces accuracy (e.g., GPT-4o drops from\n97.5 percent to 82.5 percent, GPT-o1-mini drops from 97.5 percent to 92.5\npercent); and (4) Memory dependence--models resort to guesswork when missing\ncritical data. These findings further highlight the reliance on heuristic\nrecall over rigorous logical inference, demonstrating challenges in reasoning\nrobustness. To comprehensively investigate these robustness challenges, this\npaper introduces a novel benchmark, termed as Math-RoB, that exploits\nhallucinations triggered by missing information to expose reasoning gaps. This\nis achieved by an instruction-based approach to generate diverse datasets that\nclosely resemble training distributions, facilitating a holistic robustness\nassessment and advancing the development of more robust reasoning frameworks.\nBad character(s) in field Abstract.\n","authors":["Tong Yu","Yongcheng Jing","Xikun Zhang","Wentao Jiang","Wenjie Wu","Yingjie Wang","Wenbin Hu","Bo Du","Dacheng Tao"],"pdf_url":"https://arxiv.org/pdf/2503.04550v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.00025v2","updated":"2025-03-06T15:29:41Z","published":"2024-02-28T15:19:33Z","title":"On the Challenges and Opportunities in Generative AI","summary":"  The field of deep generative modeling has grown rapidly in the last few\nyears. With the availability of massive amounts of training data coupled with\nadvances in scalable unsupervised learning paradigms, recent large-scale\ngenerative models show tremendous promise in synthesizing high-resolution\nimages and text, as well as structured data such as videos and molecules.\nHowever, we argue that current large-scale generative AI models exhibit several\nfundamental shortcomings that hinder their widespread adoption across domains.\nIn this work, our objective is to identify these issues and highlight key\nunresolved challenges in modern generative AI paradigms that should be\naddressed to further enhance their capabilities, versatility, and reliability.\nBy identifying these challenges, we aim to provide researchers with insights\nfor exploring fruitful research directions, thus fostering the development of\nmore robust and accessible generative AI solutions.\n","authors":["Laura Manduchi","Kushagra Pandey","Clara Meister","Robert Bamler","Ryan Cotterell","Sina Däubener","Sophie Fellenz","Asja Fischer","Thomas Gärtner","Matthias Kirchler","Marius Kloft","Yingzhen Li","Christoph Lippert","Gerard de Melo","Eric Nalisnick","Björn Ommer","Rajesh Ranganath","Maja Rudolph","Karen Ullrich","Guy Van den Broeck","Julia E Vogt","Yixin Wang","Florian Wenzel","Frank Wood","Stephan Mandt","Vincent Fortuin"],"pdf_url":"https://arxiv.org/pdf/2403.00025v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04543v1","updated":"2025-03-06T15:29:13Z","published":"2025-03-06T15:29:13Z","title":"Keeping Yourself is Important in Downstream Tuning Multimodal Large\n  Language Model","summary":"  Multi-modal Large Language Models (MLLMs) integrate visual and linguistic\nreasoning to address complex tasks such as image captioning and visual question\nanswering. While MLLMs demonstrate remarkable versatility, MLLMs appears\nlimited performance on special applications. But tuning MLLMs for downstream\ntasks encounters two key challenges: Task-Expert Specialization, where\ndistribution shifts between pre-training and target datasets constrain target\nperformance, and Open-World Stabilization, where catastrophic forgetting erases\nthe model general knowledge. In this work, we systematically review recent\nadvancements in MLLM tuning methodologies, classifying them into three\nparadigms: (I) Selective Tuning, (II) Additive Tuning, and (III)\nReparameterization Tuning. Furthermore, we benchmark these tuning strategies\nacross popular MLLM architectures and diverse downstream tasks to establish\nstandardized evaluation analysis and systematic tuning principles. Finally, we\nhighlight several open challenges in this domain and propose future research\ndirections. To facilitate ongoing progress in this rapidly evolving field, we\nprovide a public repository that continuously tracks developments:\nhttps://github.com/WenkeHuang/Awesome-MLLM-Tuning.\n","authors":["Wenke Huang","Jian Liang","Xianda Guo","Yiyang Fang","Guancheng Wan","Xuankun Rong","Chi Wen","Zekun Shi","Qingyun Li","Didi Zhu","Yanbiao Ma","Ke Liang","Bin Yang","He Li","Jiawei Shao","Mang Ye","Bo Du"],"pdf_url":"https://arxiv.org/pdf/2503.04543v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07180v5","updated":"2025-03-06T15:26:56Z","published":"2024-11-11T17:57:30Z","title":"Gumbel Counterfactual Generation From Language Models","summary":"  Understanding and manipulating the causal generation mechanisms in language\nmodels is essential for controlling their behavior. Previous work has primarily\nrelied on techniques such as representation surgery -- e.g., model ablations or\nmanipulation of linear subspaces tied to specific concepts -- to\n\\emph{intervene} on these models. To understand the impact of interventions\nprecisely, it is useful to examine \\emph{counterfactuals} -- e.g., how a given\nsentence would have appeared had it been generated by the model following a\nspecific intervention. We highlight that counterfactual reasoning is\nconceptually distinct from interventions, as articulated in Pearl's causal\nhierarchy. Based on this observation, we propose a framework for generating\ntrue string counterfactuals by reformulating language models as a structural\nequation model using the Gumbel-max trick, which we called Gumbel\ncounterfactual generation. This reformulation allows us to model the joint\ndistribution over original strings and their counterfactuals resulting from the\nsame instantiation of the sampling noise. We develop an algorithm based on\nhindsight Gumbel sampling that allows us to infer the latent noise variables\nand generate counterfactuals of observed strings. Our experiments demonstrate\nthat the approach produces meaningful counterfactuals while at the same time\nshowing that commonly used intervention techniques have considerable undesired\nside effects.\n","authors":["Shauli Ravfogel","Anej Svete","Vésteinn Snæbjarnarson","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2411.07180v5.pdf","comment":"Accepted in ICLR 2025"},{"id":"http://arxiv.org/abs/2503.04530v1","updated":"2025-03-06T15:19:17Z","published":"2025-03-06T15:19:17Z","title":"SOLAR: Scalable Optimization of Large-scale Architecture for Reasoning","summary":"  Large Language Models (LLMs) excel in reasoning but remain constrained by\ntheir Chain-of-Thought (CoT) approach, which struggles with complex tasks\nrequiring more nuanced topological reasoning. We introduce SOLAR, Scalable\nOptimization of Large-scale Architecture for Reasoning, a framework that\ndynamically optimizes various reasoning topologies to enhance accuracy and\nefficiency.\n  Our Topological Annotation Generation (TAG) system automates topological\ndataset creation and segmentation, improving post-training and evaluation.\nAdditionally, we propose Topological-Scaling, a reward-driven framework that\naligns training and inference scaling, equipping LLMs with adaptive, task-aware\nreasoning.\n  SOLAR achieves substantial gains on MATH and GSM8K: +5% accuracy with\nTopological Tuning, +9% with Topological Reward, and +10.02% with Hybrid\nScaling. It also reduces response length by over 5% for complex problems,\nlowering inference latency.\n  To foster the reward system, we train a multi-task Topological Reward Model\n(M-TRM), which autonomously selects the best reasoning topology and answer in a\nsingle pass, eliminating the need for training and inference on multiple\nsingle-task TRMs (S-TRMs), thus reducing both training cost and inference\nlatency. In addition, in terms of performance, M-TRM surpasses all S-TRMs,\nimproving accuracy by +10% and rank correlation by +9%.\n  To the best of our knowledge, SOLAR sets a new benchmark for scalable,\nhigh-precision LLM reasoning while introducing an automated annotation process\nand a dynamic reasoning topology competition mechanism.\n","authors":["Chen Li","Yinyi Luo","Anudeep Bolimera","Marios Savvides"],"pdf_url":"https://arxiv.org/pdf/2503.04530v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04521v1","updated":"2025-03-06T15:08:31Z","published":"2025-03-06T15:08:31Z","title":"Dynamic Pricing for On-Demand DNN Inference in the Edge-AI Market","summary":"  The convergence of edge computing and AI gives rise to Edge-AI, which enables\nthe deployment of real-time AI applications and services at the network edge.\nOne of the fundamental research issues in Edge-AI is edge inference\nacceleration, which aims to realize low-latency high-accuracy DNN inference\nservices by leveraging the fine-grained offloading of partitioned inference\ntasks from end devices to edge servers. However, existing research has yet to\nadopt a practical Edge-AI market perspective, which would systematically\nexplore the personalized inference needs of AI users (e.g., inference accuracy,\nlatency, and task complexity), the revenue incentives for AI service providers\nthat offer edge inference services, and multi-stakeholder governance within a\nmarket-oriented context. To bridge this gap, we propose an Auction-based Edge\nInference Pricing Mechanism (AERIA) for revenue maximization to tackle the\nmulti-dimensional optimization problem of DNN model partition, edge inference\npricing, and resource allocation. We investigate the multi-exit device-edge\nsynergistic inference scheme for on-demand DNN inference acceleration, and\nanalyse the auction dynamics amongst the AI service providers, AI users and\nedge infrastructure provider. Owing to the strategic mechanism design via\nrandomized consensus estimate and cost sharing techniques, the Edge-AI market\nattains several desirable properties, including competitiveness in revenue\nmaximization, incentive compatibility, and envy-freeness, which are crucial to\nmaintain the effectiveness, truthfulness, and fairness of our auction outcomes.\nThe extensive simulation experiments based on four representative DNN inference\nworkloads demonstrate that our AERIA mechanism significantly outperforms\nseveral state-of-the-art approaches in revenue maximization, demonstrating the\nefficacy of AERIA for on-demand DNN inference in the Edge-AI market.\n","authors":["Songyuan Li","Jia Hu","Geyong Min","Haojun Huang","Jiwei Huang"],"pdf_url":"https://arxiv.org/pdf/2503.04521v1.pdf","comment":"Index Terms: Edge-AI, DNN Inference Offloading, Resource Management,\n  Dynamic Pricing, Auction Mechanism"},{"id":"http://arxiv.org/abs/2502.05874v2","updated":"2025-03-06T15:02:33Z","published":"2025-02-09T12:23:40Z","title":"MMGDreamer: Mixed-Modality Graph for Geometry-Controllable 3D Indoor\n  Scene Generation","summary":"  Controllable 3D scene generation has extensive applications in virtual\nreality and interior design, where the generated scenes should exhibit high\nlevels of realism and controllability in terms of geometry. Scene graphs\nprovide a suitable data representation that facilitates these applications.\nHowever, current graph-based methods for scene generation are constrained to\ntext-based inputs and exhibit insufficient adaptability to flexible user\ninputs, hindering the ability to precisely control object geometry. To address\nthis issue, we propose MMGDreamer, a dual-branch diffusion model for scene\ngeneration that incorporates a novel Mixed-Modality Graph, visual enhancement\nmodule, and relation predictor. The mixed-modality graph allows object nodes to\nintegrate textual and visual modalities, with optional relationships between\nnodes. It enhances adaptability to flexible user inputs and enables meticulous\ncontrol over the geometry of objects in the generated scenes. The visual\nenhancement module enriches the visual fidelity of text-only nodes by\nconstructing visual representations using text embeddings. Furthermore, our\nrelation predictor leverages node representations to infer absent relationships\nbetween nodes, resulting in more coherent scene layouts. Extensive experimental\nresults demonstrate that MMGDreamer exhibits superior control of object\ngeometry, achieving state-of-the-art scene generation performance. Project\npage: https://yangzhifeio.github.io/project/MMGDreamer.\n","authors":["Zhifei Yang","Keyang Lu","Chao Zhang","Jiaxing Qi","Hanqi Jiang","Ruifei Ma","Shenglin Yin","Yifan Xu","Mingzhe Xing","Zhen Xiao","Jieyi Long","Xiangde Liu","Guangyao Zhai"],"pdf_url":"https://arxiv.org/pdf/2502.05874v2.pdf","comment":"Accepted by AAAI 2025 Main Track"},{"id":"http://arxiv.org/abs/2503.04509v1","updated":"2025-03-06T14:55:25Z","published":"2025-03-06T14:55:25Z","title":"STX-Search: Explanation Search for Continuous Dynamic Spatio-Temporal\n  Models","summary":"  Recent improvements in the expressive power of spatio-temporal models have\nled to performance gains in many real-world applications, such as traffic\nforecasting and social network modelling. However, understanding the\npredictions from a model is crucial to ensure reliability and trustworthiness,\nparticularly for high-risk applications, such as healthcare and transport. Few\nexisting methods are able to generate explanations for models trained on\ncontinuous-time dynamic graph data and, of these, the computational complexity\nand lack of suitable explanation objectives pose challenges. In this paper, we\npropose $\\textbf{S}$patio-$\\textbf{T}$emporal E$\\textbf{X}$planation\n$\\textbf{Search}$ (STX-Search), a novel method for generating instance-level\nexplanations that is applicable to static and dynamic temporal graph\nstructures. We introduce a novel search strategy and objective function, to\nfind explanations that are highly faithful and interpretable. When compared\nwith existing methods, STX-Search produces explanations of higher fidelity\nwhilst optimising explanation size to maintain interpretability.\n","authors":["Saif Anwar","Nathan Griffiths","Thomas Popham","Abhir Bhalerao"],"pdf_url":"https://arxiv.org/pdf/2503.04509v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04506v1","updated":"2025-03-06T14:53:37Z","published":"2025-03-06T14:53:37Z","title":"Multi-modal Summarization in Model-Based Engineering: Automotive\n  Software Development Case Study","summary":"  Multimodal summarization integrating information from diverse data modalities\npresents a promising solution to aid the understanding of information within\nvarious processes. However, the application and advantages of multimodal\nsummarization have not received much attention in model-based engineering\n(MBE), where it has become a cornerstone in the design and development of\ncomplex systems, leveraging formal models to improve understanding, validation\nand automation throughout the engineering lifecycle. UML and EMF diagrams in\nmodel-based engineering contain a large amount of multimodal information and\nintricate relational data. Hence, our study explores the application of\nmultimodal large language models within the domain of model-based engineering\nto evaluate their capacity for understanding and identifying relationships,\nfeatures, and functionalities embedded in UML and EMF diagrams. We aim to\ndemonstrate the transformative potential benefits and limitations of multimodal\nsummarization in improving productivity and accuracy in MBE practices. The\nproposed approach is evaluated within the context of automotive software\ndevelopment, while many promising state-of-art models were taken into account.\n","authors":["Nenad Petrovic","Yurui Zhang","Moaad Maaroufi","Kuo-Yi Chao","Lukasz Mazur","Fengjunjie Pan","Vahid Zolfaghari","Alois Knoll"],"pdf_url":"https://arxiv.org/pdf/2503.04506v1.pdf","comment":"Conference paper accepted for IntelliSys2025"},{"id":"http://arxiv.org/abs/2503.04502v1","updated":"2025-03-06T14:50:29Z","published":"2025-03-06T14:50:29Z","title":"Interpretable Transformation and Analysis of Timelines through Learning\n  via Surprisability","summary":"  The analysis of high-dimensional timeline data and the identification of\noutliers and anomalies is critical across diverse domains, including sensor\nreadings, biological and medical data, historical records, and global\nstatistics. However, conventional analysis techniques often struggle with\nchallenges such as high dimensionality, complex distributions, and sparsity.\nThese limitations hinder the ability to extract meaningful insights from\ncomplex temporal datasets, making it difficult to identify trending features,\noutliers, and anomalies effectively. Inspired by surprisability -- a cognitive\nscience concept describing how humans instinctively focus on unexpected\ndeviations - we propose Learning via Surprisability (LvS), a novel approach for\ntransforming high-dimensional timeline data. LvS quantifies and prioritizes\nanomalies in time-series data by formalizing deviations from expected behavior.\nLvS bridges cognitive theories of attention with computational methods,\nenabling the detection of anomalies and shifts in a way that preserves critical\ncontext, offering a new lens for interpreting complex datasets. We demonstrate\nthe usefulness of LvS on three high-dimensional timeline use cases: a time\nseries of sensor data, a global dataset of mortality causes over multiple\nyears, and a textual corpus containing over two centuries of State of the Union\nAddresses by U.S. presidents. Our results show that the LvS transformation\nenables efficient and interpretable identification of outliers, anomalies, and\nthe most variable features along the timeline.\n","authors":["Osnat Mokryn","Teddy Lazebnik","Hagit Ben Shoshan"],"pdf_url":"https://arxiv.org/pdf/2503.04502v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04500v1","updated":"2025-03-06T14:49:28Z","published":"2025-03-06T14:49:28Z","title":"ReynoldsFlow: Exquisite Flow Estimation via Reynolds Transport Theorem","summary":"  Optical flow is a fundamental technique for motion estimation, widely applied\nin video stabilization, interpolation, and object tracking. Recent advancements\nin artificial intelligence (AI) have enabled deep learning models to leverage\noptical flow as an important feature for motion analysis. However, traditional\noptical flow methods rely on restrictive assumptions, such as brightness\nconstancy and slow motion constraints, limiting their effectiveness in complex\nscenes. Deep learning-based approaches require extensive training on large\ndomain-specific datasets, making them computationally demanding. Furthermore,\noptical flow is typically visualized in the HSV color space, which introduces\nnonlinear distortions when converted to RGB and is highly sensitive to noise,\ndegrading motion representation accuracy. These limitations inherently\nconstrain the performance of downstream models, potentially hindering object\ntracking and motion analysis tasks. To address these challenges, we propose\nReynolds flow, a novel training-free flow estimation inspired by the Reynolds\ntransport theorem, offering a principled approach to modeling complex motion\ndynamics. Beyond the conventional HSV-based visualization, denoted\nReynoldsFlow, we introduce an alternative representation, ReynoldsFlow+,\ndesigned to improve flow visualization. We evaluate ReynoldsFlow and\nReynoldsFlow+ across three video-based benchmarks: tiny object detection on\nUAVDB, infrared object detection on Anti-UAV, and pose estimation on GolfDB.\nExperimental results demonstrate that networks trained with ReynoldsFlow+\nachieve state-of-the-art (SOTA) performance, exhibiting improved robustness and\nefficiency across all tasks.\n","authors":["Yu-Hsi Chen","Chin-Tien Wu"],"pdf_url":"https://arxiv.org/pdf/2503.04500v1.pdf","comment":"10 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2503.02012v2","updated":"2025-03-06T14:32:23Z","published":"2025-03-03T19:41:22Z","title":"Pretrained Embeddings as a Behavior Specification Mechanism","summary":"  We propose an approach to formally specifying the behavioral properties of\nsystems that rely on a perception model for interactions with the physical\nworld. The key idea is to introduce embeddings -- mathematical representations\nof a real-world concept -- as a first-class construct in a specification\nlanguage, where properties are expressed in terms of distances between a pair\nof ideal and observed embeddings. To realize this approach, we propose a new\ntype of temporal logic called Embedding Temporal Logic (ETL), and describe how\nit can be used to express a wider range of properties about AI-enabled systems\nthan previously possible. We demonstrate the applicability of ETL through a\npreliminary evaluation involving planning tasks in robots that are driven by\nfoundation models; the results are promising, showing that embedding-based\nspecifications can be used to steer a system towards desirable behaviors.\n","authors":["Parv Kapoor","Abigail Hammer","Ashish Kapoor","Karen Leung","Eunsuk Kang"],"pdf_url":"https://arxiv.org/pdf/2503.02012v2.pdf","comment":"18 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.04482v1","updated":"2025-03-06T14:30:55Z","published":"2025-03-06T14:30:55Z","title":"Generalized Interpolating Discrete Diffusion","summary":"  While state-of-the-art language models achieve impressive results through\nnext-token prediction, they have inherent limitations such as the inability to\nrevise already generated tokens. This has prompted exploration of alternative\napproaches such as discrete diffusion. However, masked diffusion, which has\nemerged as a popular choice due to its simplicity and effectiveness,\nreintroduces this inability to revise words. To overcome this, we generalize\nmasked diffusion and derive the theoretical backbone of a family of general\ninterpolating discrete diffusion (GIDD) processes offering greater flexibility\nin the design of the noising processes. Leveraging a novel diffusion ELBO, we\nachieve compute-matched state-of-the-art performance in diffusion language\nmodeling. Exploiting GIDD's flexibility, we explore a hybrid approach combining\nmasking and uniform noise, leading to improved sample quality and unlocking the\nability for the model to correct its own mistakes, an area where autoregressive\nmodels notoriously have struggled. Our code and models are open-source:\nhttps://github.com/dvruette/gidd/\n","authors":["Dimitri von Rütte","Janis Fluri","Yuhui Ding","Antonio Orvieto","Bernhard Schölkopf","Thomas Hofmann"],"pdf_url":"https://arxiv.org/pdf/2503.04482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04479v1","updated":"2025-03-06T14:29:52Z","published":"2025-03-06T14:29:52Z","title":"ToolFuzz -- Automated Agent Tool Testing","summary":"  Large Language Model (LLM) Agents leverage the advanced reasoning\ncapabilities of LLMs in real-world applications. To interface with an\nenvironment, these agents often rely on tools, such as web search or database\nAPIs. As the agent provides the LLM with tool documentation along the user\nquery, the completeness and correctness of this documentation is critical.\nHowever, tool documentation is often over-, under-, or ill-specified, impeding\nthe agent's accuracy. Standard software testing approaches struggle to identify\nthese errors as they are expressed in natural language. Thus, despite its\nimportance, there currently exists no automated method to test the tool\ndocumentation for agents. To address this issue, we present ToolFuzz, the first\nmethod for automated testing of tool documentations. ToolFuzz is designed to\ndiscover two types of errors: (1) user queries leading to tool runtime errors\nand (2) user queries that lead to incorrect agent responses. ToolFuzz can\ngenerate a large and diverse set of natural inputs, effectively finding tool\ndescription errors at a low false positive rate. Further, we present two\nstraightforward prompt-engineering approaches. We evaluate all three tool\ntesting approaches on 32 common LangChain tools and 35 newly created custom\ntools and 2 novel benchmarks to further strengthen the assessment. We find that\nmany publicly available tools suffer from underspecification. Specifically, we\nshow that ToolFuzz identifies 20x more erroneous inputs compared to the\nprompt-engineering approaches, making it a key component for building reliable\nAI agents.\n","authors":["Ivan Milev","Mislav Balunović","Maximilian Baader","Martin Vechev"],"pdf_url":"https://arxiv.org/pdf/2503.04479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03606v2","updated":"2025-03-06T14:28:36Z","published":"2025-03-05T15:42:37Z","title":"Decoupled Recommender Systems: Exploring Alternative Recommender\n  Ecosystem Designs","summary":"  Recommender ecosystems are an emerging subject of research. Such research\nexamines how the characteristics of algorithms, recommendation consumers, and\nitem providers influence system dynamics and long-term outcomes. One\narchitectural possibility that has not yet been widely explored in this line of\nresearch is the consequences of a configuration in which recommendation\nalgorithms are decoupled from the platforms they serve. This is sometimes\ncalled \"the friendly neighborhood algorithm store\" or \"middleware\" model. We\nare particularly interested in how such architectures might offer a range of\ndifferent distributions of utility across consumers, providers, and\nrecommendation platforms. In this paper, we create a model of a recommendation\necosystem that incorporates algorithm choice and examine the outcomes of such a\ndesign.\n","authors":["Anas Buhayh","Elizabeth McKinnie","Robin Burke"],"pdf_url":"https://arxiv.org/pdf/2503.03606v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13524v4","updated":"2025-03-06T14:27:12Z","published":"2025-02-19T08:21:59Z","title":"MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D\n  Medical Image Analysis","summary":"  Efficient evaluation of three-dimensional (3D) medical images is crucial for\ndiagnostic and therapeutic practices in healthcare. Recent years have seen a\nsubstantial uptake in applying deep learning and computer vision to analyse and\ninterpret medical images. Traditional approaches, such as convolutional neural\nnetworks (CNNs) and vision transformers (ViTs), face significant computational\nchallenges, prompting the need for architectural advancements. Recent efforts\nhave led to the introduction of novel architectures like the ``Mamba'' model as\nalternative solutions to traditional CNNs or ViTs. The Mamba model excels in\nthe linear processing of one-dimensional data with low computational demands.\nHowever, Mamba's potential for 3D medical image analysis remains underexplored\nand could face significant computational challenges as the dimension increases.\nThis manuscript presents MobileViM, a streamlined architecture for efficient\nsegmentation of 3D medical images. In the MobileViM network, we invent a new\ndimension-independent mechanism and a dual-direction traversing approach to\nincorporate with a vision-Mamba-based framework. MobileViM also features a\ncross-scale bridging technique to improve efficiency and accuracy across\nvarious medical imaging modalities. With these enhancements, MobileViM achieves\nsegmentation speeds exceeding 90 frames per second (FPS) on a single graphics\nprocessing unit (i.e., NVIDIA RTX 4090). This performance is over 24 FPS faster\nthan the state-of-the-art deep learning models for processing 3D images with\nthe same computational resources. In addition, experimental evaluations\ndemonstrate that MobileViM delivers superior performance, with Dice similarity\nscores reaching 92.72%, 86.69%, 80.46%, and 77.43% for PENGWIN, BraTS2024,\nATLAS, and Toothfairy2 datasets, respectively, which significantly surpasses\nexisting models.\n","authors":["Wei Dai","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2502.13524v4.pdf","comment":"The corresponding author disagrees with the manuscript submitted to\n  arXiv"},{"id":"http://arxiv.org/abs/2503.04472v1","updated":"2025-03-06T14:23:06Z","published":"2025-03-06T14:23:06Z","title":"DAST: Difficulty-Adaptive Slow-Thinking for Large Reasoning Models","summary":"  Recent advancements in slow-thinking reasoning models have shown exceptional\nperformance in complex reasoning tasks. However, these models often exhibit\noverthinking-generating redundant reasoning steps for simple problems, leading\nto excessive computational resource usage. While current mitigation strategies\nuniformly reduce reasoning tokens, they risk degrading performance on\nchallenging tasks that require extended reasoning. This paper introduces\nDifficulty-Adaptive Slow-Thinking (DAST), a novel framework that enables models\nto autonomously adjust the length of Chain-of-Thought(CoT) based on problem\ndifficulty. We first propose a Token Length Budget (TLB) metric to quantify\ndifficulty, then leveraging length-aware reward shaping and length preference\noptimization to implement DAST. DAST penalizes overlong responses for simple\ntasks while incentivizing sufficient reasoning for complex problems.\nExperiments on diverse datasets and model scales demonstrate that DAST\neffectively mitigates overthinking (reducing token usage by over 30\\% on\naverage) while preserving reasoning accuracy on complex problems.\n","authors":["Yi Shen","Jian Zhang","Jieyun Huang","Shuming Shi","Wenjing Zhang","Jiangze Yan","Ning Wang","Kai Wang","Shiguo Lian"],"pdf_url":"https://arxiv.org/pdf/2503.04472v1.pdf","comment":"working in progress"},{"id":"http://arxiv.org/abs/2503.04457v1","updated":"2025-03-06T14:11:00Z","published":"2025-03-06T14:11:00Z","title":"TPC: Cross-Temporal Prediction Connection for Vision-Language Model\n  Hallucination Reduction","summary":"  Vision-language models (VLMs) have achieved remarkable advancements,\ncapitalizing on the impressive capabilities of large language models (LLMs)\nacross diverse tasks. Despite this, a critical challenge known as hallucination\noccurs when models overconfidently describe objects or attributes absent from\nthe image, a problem exacerbated by the tendency of VLMs to rely on linguistic\npriors. This limitation reduces model reliability in high-stakes applications.\nIn this work, we have observed the characteristic of logits' continuity\nconsistency enhancement and introduced a straightforward and efficient method,\nCross-Temporal Prediction Connection (TPC), designed to enhance the semantic\nconsistency of logits by connecting them temporally across timesteps. TPC\namplifies information flow and improves coherence, effectively reducing\nhallucination. Extensive experiments show that TPC surpasses existing\nrepresentatives, delivering superior performance in both accuracy and\nefficiency while maintaining robustness in open-ended text generation tasks.\n","authors":["Chao Wang","Weiwei Fu","Yang Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.04457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13728v2","updated":"2025-03-06T14:07:57Z","published":"2025-02-19T13:54:44Z","title":"Secure Federated Data Distillation","summary":"  Dataset Distillation (DD) is a powerful technique for reducing large datasets\ninto compact, representative synthetic datasets, accelerating Machine Learning\ntraining. However, traditional DD methods operate in a centralized manner,\nwhich poses significant privacy threats and reduces its applicability. To\nmitigate these risks, we propose a Secure Federated Data Distillation (SFDD)\nframework to decentralize the distillation process while preserving privacy.\nUnlike existing Federated Distillation techniques that focus on training global\nmodels with distilled knowledge, our approach aims to produce a distilled\ndataset without exposing local contributions. We leverage the\ngradient-matching-based distillation method, adapting it for a distributed\nsetting where clients contribute to the distillation process without sharing\nraw data. The central aggregator iteratively refines a synthetic dataset by\nintegrating client-side updates while ensuring data confidentiality. To make\nour approach resilient to inference attacks perpetrated by the server that\ncould exploit gradient updates to reconstruct private data, we create an\noptimized Local Differential Privacy approach, called LDPO-RLD. Furthermore, we\nassess the framework's resilience against malicious clients executing backdoor\nattacks (such as Doorping) and demonstrate robustness under the assumption of a\nsufficient number of participating clients. Our experimental results\ndemonstrate the effectiveness of SFDD and that the proposed defense concretely\nmitigates the identified vulnerabilities, with minimal impact on the\nperformance of the distilled dataset. By addressing the interplay between\nprivacy and federation in dataset distillation, this work advances the field of\nprivacy-preserving Machine Learning making our SFDD framework a viable solution\nfor sensitive data-sharing applications.\n","authors":["Marco Arazzi","Mert Cihangiroglu","Serena Nicolazzo","Antonino Nocera"],"pdf_url":"https://arxiv.org/pdf/2502.13728v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04451v1","updated":"2025-03-06T14:06:20Z","published":"2025-03-06T14:06:20Z","title":"Privacy Preserving and Robust Aggregation for Cross-Silo Federated\n  Learning in Non-IID Settings","summary":"  Federated Averaging remains the most widely used aggregation strategy in\nfederated learning due to its simplicity and scalability. However, its\nperformance degrades significantly in non-IID data settings, where client\ndistributions are highly imbalanced or skewed. Additionally, it relies on\nclients transmitting metadata, specifically the number of training samples,\nwhich introduces privacy risks and may conflict with regulatory frameworks like\nthe European GDPR. In this paper, we propose a novel aggregation strategy that\naddresses these challenges by introducing class-aware gradient masking. Unlike\ntraditional approaches, our method relies solely on gradient updates,\neliminating the need for any additional client metadata, thereby enhancing\nprivacy protection. Furthermore, our approach validates and dynamically weights\nclient contributions based on class-specific importance, ensuring robustness\nagainst non-IID distributions, convergence prevention, and backdoor attacks.\nExtensive experiments on benchmark datasets demonstrate that our method not\nonly outperforms FedAvg and other widely accepted aggregation strategies in\nnon-IID settings but also preserves model integrity in adversarial scenarios.\nOur results establish the effectiveness of gradient masking as a practical and\nsecure solution for federated learning.\n","authors":["Marco Arazzi","Mert Cihangiroglu","Antonino Nocera"],"pdf_url":"https://arxiv.org/pdf/2503.04451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08010v2","updated":"2025-03-06T14:01:48Z","published":"2024-02-12T19:18:50Z","title":"Which Frequencies do CNNs Need? Emergent Bottleneck Structure in Feature\n  Learning","summary":"  We describe the emergence of a Convolution Bottleneck (CBN) structure in\nCNNs, where the network uses its first few layers to transform the input\nrepresentation into a representation that is supported only along a few\nfrequencies and channels, before using the last few layers to map back to the\noutputs. We define the CBN rank, which describes the number and type of\nfrequencies that are kept inside the bottleneck, and partially prove that the\nparameter norm required to represent a function $f$ scales as depth times the\nCBN rank $f$. We also show that the parameter norm depends at next order on the\nregularity of $f$. We show that any network with almost optimal parameter norm\nwill exhibit a CBN structure in both the weights and - under the assumption\nthat the network is stable under large learning rate - the activations, which\nmotivates the common practice of down-sampling; and we verify that the CBN\nresults still hold with down-sampling. Finally we use the CBN structure to\ninterpret the functions learned by CNNs on a number of tasks.\n","authors":["Yuxiao Wen","Arthur Jacot"],"pdf_url":"https://arxiv.org/pdf/2402.08010v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13959v2","updated":"2025-03-06T13:51:24Z","published":"2025-01-21T06:32:25Z","title":"Assisting Mathematical Formalization with A Learning-based Premise\n  Retriever","summary":"  Premise selection is a crucial yet challenging step in mathematical\nformalization, especially for users with limited experience. Due to the lack of\navailable formalization projects, existing approaches that leverage language\nmodels often suffer from data scarcity. In this work, we introduce an\ninnovative method for training a premise retriever to support the formalization\nof mathematics. Our approach employs a BERT model to embed proof states and\npremises into a shared latent space. The retrieval model is trained within a\ncontrastive learning framework and incorporates a domain-specific tokenizer\nalong with a fine-grained similarity computation method. Experimental results\nshow that our model is highly competitive compared to existing baselines,\nachieving strong performance while requiring fewer computational resources.\nPerformance is further enhanced through the integration of a re-ranking module.\nTo streamline the formalization process, we will release a search engine that\nenables users to query Mathlib theorems directly using proof states,\nsignificantly improving accessibility and efficiency. Codes are available at\nhttps://github.com/ruc-ai4math/Premise-Retrieval.\n","authors":["Yicheng Tao","Haotian Liu","Shanwen Wang","Hongteng Xu"],"pdf_url":"https://arxiv.org/pdf/2501.13959v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17573v2","updated":"2025-03-06T13:47:53Z","published":"2024-05-27T18:15:05Z","title":"Hamiltonian Mechanics of Feature Learning: Bottleneck Structure in Leaky\n  ResNets","summary":"  We study Leaky ResNets, which interpolate between ResNets and Fully-Connected\nnets depending on an 'effective depth' hyper-parameter $\\tilde{L}$. In the\ninfinite depth limit, we study 'representation geodesics' $A_{p}$: continuous\npaths in representation space (similar to NeuralODEs) from input $p=0$ to\noutput $p=1$ that minimize the parameter norm of the network. We give a\nLagrangian and Hamiltonian reformulation, which highlight the importance of two\nterms: a kinetic energy which favors small layer derivatives\n$\\partial_{p}A_{p}$ and a potential energy that favors low-dimensional\nrepresentations, as measured by the 'Cost of Identity'. The balance between\nthese two forces offers an intuitive understanding of feature learning in\nResNets. We leverage this intuition to explain the emergence of a bottleneck\nstructure, as observed in previous work: for large $\\tilde{L}$ the potential\nenergy dominates and leads to a separation of timescales, where the\nrepresentation jumps rapidly from the high dimensional inputs to a\nlow-dimensional representation, move slowly inside the space of low-dimensional\nrepresentations, before jumping back to the potentially high-dimensional\noutputs. Inspired by this phenomenon, we train with an adaptive layer step-size\nto adapt to the separation of timescales.\n","authors":["Arthur Jacot","Alexandre Kaiser"],"pdf_url":"https://arxiv.org/pdf/2405.17573v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05664v2","updated":"2025-03-06T13:40:09Z","published":"2024-07-08T06:59:29Z","title":"How DNNs break the Curse of Dimensionality: Compositionality and\n  Symmetry Learning","summary":"  We show that deep neural networks (DNNs) can efficiently learn any\ncomposition of functions with bounded $F_{1}$-norm, which allows DNNs to break\nthe curse of dimensionality in ways that shallow networks cannot. More\nspecifically, we derive a generalization bound that combines a covering number\nargument for compositionality, and the $F_{1}$-norm (or the related Barron\nnorm) for large width adaptivity. We show that the global minimizer of the\nregularized loss of DNNs can fit for example the composition of two functions\n$f^{*}=h\\circ g$ from a small number of observations, assuming $g$ is\nsmooth/regular and reduces the dimensionality (e.g. $g$ could be the quotient\nmap of the symmetries of $f^{*}$), so that $h$ can be learned in spite of its\nlow regularity. The measures of regularity we consider is the Sobolev norm with\ndifferent levels of differentiability, which is well adapted to the $F_{1}$\nnorm. We compute scaling laws empirically and observe phase transitions\ndepending on whether $g$ or $h$ is harder to learn, as predicted by our theory.\n","authors":["Arthur Jacot","Seok Hoan Choi","Yuxiao Wen"],"pdf_url":"https://arxiv.org/pdf/2407.05664v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12261v3","updated":"2025-03-06T13:39:32Z","published":"2024-10-16T05:58:55Z","title":"CATCH: Channel-Aware multivariate Time Series Anomaly Detection via\n  Frequency Patching","summary":"  Anomaly detection in multivariate time series is challenging as heterogeneous\nsubsequence anomalies may occur. Reconstruction-based methods, which focus on\nlearning normal patterns in the frequency domain to detect diverse abnormal\nsubsequences, achieve promising results, while still falling short on capturing\nfine-grained frequency characteristics and channel correlations. To contend\nwith the limitations, we introduce CATCH, a framework based on frequency\npatching. We propose to patchify the frequency domain into frequency bands,\nwhich enhances its ability to capture fine-grained frequency characteristics.\nTo perceive appropriate channel correlations, we propose a Channel Fusion\nModule (CFM), which features a patch-wise mask generator and a masked-attention\nmechanism. Driven by a bi-level multi-objective optimization algorithm, the CFM\nis encouraged to iteratively discover appropriate patch-wise channel\ncorrelations, and to cluster relevant channels while isolating adverse effects\nfrom irrelevant channels. Extensive experiments on 10 real-world datasets and\n12 synthetic datasets demonstrate that CATCH achieves state-of-the-art\nperformance. We make our code and datasets available at\nhttps://github.com/decisionintelligence/CATCH.\n","authors":["Xingjian Wu","Xiangfei Qiu","Zhengyu Li","Yihang Wang","Jilin Hu","Chenjuan Guo","Hui Xiong","Bin Yang"],"pdf_url":"https://arxiv.org/pdf/2410.12261v3.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2503.04429v1","updated":"2025-03-06T13:38:44Z","published":"2025-03-06T13:38:44Z","title":"Activation Space Interventions Can Be Transferred Between Large Language\n  Models","summary":"  The study of representation universality in AI models reveals growing\nconvergence across domains, modalities, and architectures. However, the\npractical applications of representation universality remain largely\nunexplored. We bridge this gap by demonstrating that safety interventions can\nbe transferred between models through learned mappings of their shared\nactivation spaces. We demonstrate this approach on two well-established AI\nsafety tasks: backdoor removal and refusal of harmful prompts, showing\nsuccessful transfer of steering vectors that alter the models' outputs in a\npredictable way. Additionally, we propose a new task, \\textit{corrupted\ncapabilities}, where models are fine-tuned to embed knowledge tied to a\nbackdoor. This tests their ability to separate useful skills from backdoors,\nreflecting real-world challenges. Extensive experiments across Llama, Qwen and\nGemma model families show that our method enables using smaller models to\nefficiently align larger ones. Furthermore, we demonstrate that autoencoder\nmappings between base and fine-tuned models can serve as reliable ``lightweight\nsafety switches\", allowing dynamic toggling between model behaviors.\n","authors":["Narmeen Oozeer","Dhruv Nathawani","Nirmalendu Prakash","Michael Lan","Abir Harrasse","Amirali Abdullah"],"pdf_url":"https://arxiv.org/pdf/2503.04429v1.pdf","comment":"68 pages"},{"id":"http://arxiv.org/abs/2503.04422v1","updated":"2025-03-06T13:31:16Z","published":"2025-03-06T13:31:16Z","title":"PDX: A Data Layout for Vector Similarity Search","summary":"  We propose Partition Dimensions Across (PDX), a data layout for vectors\n(e.g., embeddings) that, similar to PAX [6], stores multiple vectors in one\nblock, using a vertical layout for the dimensions (Figure 1). PDX accelerates\nexact and approximate similarity search thanks to its dimension-by-dimension\nsearch strategy that operates on multiple-vectors-at-a-time in tight loops. It\nbeats SIMD-optimized distance kernels on standard horizontal vector storage\n(avg 40% faster), only relying on scalar code that gets auto-vectorized. We\ncombined the PDX layout with recent dimension-pruning algorithms ADSampling\n[19] and BSA [52] that accelerate approximate vector search. We found that\nthese algorithms on the horizontal vector layout can lose to SIMD-optimized\nlinear scans, even if they are SIMD-optimized. However, when used on PDX, their\nbenefit is restored to 2-7x. We find that search on PDX is especially fast if a\nlimited number of dimensions has to be scanned fully, which is what the\ndimension-pruning approaches do. We finally introduce PDX-BOND, an even more\nflexible dimension-pruning strategy, with good performance on exact search and\nreasonable performance on approximate search. Unlike previous pruning\nalgorithms, it can work on vector data \"as-is\" without preprocessing; making it\nattractive for vector databases with frequent updates.\n","authors":["Leonardo Kuffo","Elena Krippner","Peter Boncz"],"pdf_url":"https://arxiv.org/pdf/2503.04422v1.pdf","comment":"To be published in Proceedings of The 2025 International Conference\n  on Management of Data (SIGMOD '25). For associated code, see\n  https://github.com/cwida/PDX"},{"id":"http://arxiv.org/abs/2311.07978v4","updated":"2025-03-06T13:29:24Z","published":"2023-11-14T08:10:14Z","title":"AfroBench: How Good are Large Language Models on African Languages?","summary":"  Large-scale multilingual evaluations, such as MEGA, often include only a\nhandful of African languages due to the scarcity of high-quality evaluation\ndata and the limited discoverability of existing African datasets. This lack of\nrepresentation hinders comprehensive LLM evaluation across a diverse range of\nlanguages and tasks. To address these challenges, we introduce AfroBench -- a\nmulti-task benchmark for evaluating the performance of LLMs across 64 African\nlanguages, 15 tasks and 22 datasets. AfroBench consists of nine natural\nlanguage understanding datasets, six text generation datasets, six knowledge\nand question answering tasks, and one mathematical reasoning task. We present\nresults comparing the performance of prompting LLMs to fine-tuned baselines\nbased on BERT and T5-style models. Our results suggest large gaps in\nperformance between high-resource languages, such as English, and African\nlanguages across most tasks; but performance also varies based on the\navailability of monolingual data resources. Our findings confirm that\nperformance on African languages continues to remain a hurdle for current LLMs,\nunderscoring the need for additional efforts to close this gap.\n  https://mcgill-nlp.github.io/AfroBench/\n","authors":["Jessica Ojo","Odunayo Ogundepo","Akintunde Oladipo","Kelechi Ogueji","Jimmy Lin","Pontus Stenetorp","David Ifeoluwa Adelani"],"pdf_url":"https://arxiv.org/pdf/2311.07978v4.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2503.04417v1","updated":"2025-03-06T13:21:27Z","published":"2025-03-06T13:21:27Z","title":"From Idea to CAD: A Language Model-Driven Multi-Agent System for\n  Collaborative Design","summary":"  Creating digital models using Computer Aided Design (CAD) is a process that\nrequires in-depth expertise. In industrial product development, this process\ntypically involves entire teams of engineers, spanning requirements\nengineering, CAD itself, and quality assurance. We present an approach that\nmirrors this team structure with a Vision Language Model (VLM)-based Multi\nAgent System, with access to parametric CAD tooling and tool documentation.\nCombining agents for requirements engineering, CAD engineering, and\nvision-based quality assurance, a model is generated automatically from\nsketches and/ or textual descriptions. The resulting model can be refined\ncollaboratively in an iterative validation loop with the user. Our approach has\nthe potential to increase the effectiveness of design processes, both for\nindustry experts and for hobbyists who create models for 3D printing. We\ndemonstrate the potential of the architecture at the example of various design\ntasks and provide several ablations that show the benefits of the\narchitecture's individual components.\n","authors":["Felix Ocker","Stefan Menzel","Ahmed Sadik","Thiago Rios"],"pdf_url":"https://arxiv.org/pdf/2503.04417v1.pdf","comment":"11 pages, 3 figures"},{"id":"http://arxiv.org/abs/2503.04416v1","updated":"2025-03-06T13:18:37Z","published":"2025-03-06T13:18:37Z","title":"Learning Transformer-based World Models with Contrastive Predictive\n  Coding","summary":"  The DreamerV3 algorithm recently obtained remarkable performance across\ndiverse environment domains by learning an accurate world model based on\nRecurrent Neural Networks (RNNs). Following the success of model-based\nreinforcement learning algorithms and the rapid adoption of the Transformer\narchitecture for its superior training efficiency and favorable scaling\nproperties, recent works such as STORM have proposed replacing RNN-based world\nmodels with Transformer-based world models using masked self-attention.\nHowever, despite the improved training efficiency of these methods, their\nimpact on performance remains limited compared to the Dreamer algorithm,\nstruggling to learn competitive Transformer-based world models. In this work,\nwe show that the next state prediction objective adopted in previous approaches\nis insufficient to fully exploit the representation capabilities of\nTransformers. We propose to extend world model predictions to longer time\nhorizons by introducing TWISTER (Transformer-based World model wIth contraSTivE\nRepresentations), a world model using action-conditioned Contrastive Predictive\nCoding to learn high-level temporal feature representations and improve the\nagent performance. TWISTER achieves a human-normalized mean score of 162% on\nthe Atari 100k benchmark, setting a new record among state-of-the-art methods\nthat do not employ look-ahead search.\n","authors":["Maxime Burchi","Radu Timofte"],"pdf_url":"https://arxiv.org/pdf/2503.04416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04412v1","updated":"2025-03-06T13:10:40Z","published":"2025-03-06T13:10:40Z","title":"Wider or Deeper? Scaling LLM Inference-Time Compute with Adaptive\n  Branching Tree Search","summary":"  Recent advances demonstrate that increasing inference-time computation can\nsignificantly boost the reasoning capabilities of large language models (LLMs).\nAlthough repeated sampling (i.e., generating multiple candidate outputs) is a\nhighly effective strategy, it does not leverage external feedback signals for\nrefinement, which are often available in tasks like coding. In this work, we\npropose $\\textit{Adaptive Branching Monte Carlo Tree Search (AB-MCTS)}$, a\nnovel inference-time framework that generalizes repeated sampling with\nprincipled multi-turn exploration and exploitation. At each node in the search\ntree, AB-MCTS dynamically decides whether to \"go wider\" by expanding new\ncandidate responses or \"go deeper\" by revisiting existing ones based on\nexternal feedback signals. We evaluate our method on complex coding and\nengineering tasks using frontier models. Empirical results show that AB-MCTS\nconsistently outperforms both repeated sampling and standard MCTS, underscoring\nthe importance of combining the response diversity of LLMs with multi-turn\nsolution refinement for effective inference-time scaling.\n","authors":["Kou Misaki","Yuichi Inoue","Yuki Imajuku","So Kuroki","Taishi Nakamura","Takuya Akiba"],"pdf_url":"https://arxiv.org/pdf/2503.04412v1.pdf","comment":"To appear at ICLR 2025 Workshop on Foundation Models in the Wild"},{"id":"http://arxiv.org/abs/2503.04406v1","updated":"2025-03-06T13:00:53Z","published":"2025-03-06T13:00:53Z","title":"Training-Free Graph Filtering via Multimodal Feature Refinement for\n  Extremely Fast Multimodal Recommendation","summary":"  Multimodal recommender systems improve the performance of canonical\nrecommender systems with no item features by utilizing diverse content types\nsuch as text, images, and videos, while alleviating inherent sparsity of\nuser-item interactions and accelerating user engagement. However, current\nneural network-based models often incur significant computational overhead due\nto the complex training process required to learn and integrate information\nfrom multiple modalities. To overcome this limitation, we propose\nMultiModal-Graph Filtering (MM-GF), a training-free method based on the notion\nof graph filtering (GF) for efficient and accurate multimodal recommendations.\nSpecifically, MM-GF first constructs multiple similarity graphs through\nnontrivial multimodal feature refinement such as robust scaling and vector\nshifting by addressing the heterogeneous characteristics across modalities.\nThen, MM-GF optimally fuses multimodal information using linear low-pass\nfilters across different modalities. Extensive experiments on real-world\nbenchmark datasets demonstrate that MM-GF not only improves recommendation\naccuracy by up to 13.35% compared to the best competitor but also dramatically\nreduces computational costs by achieving the runtime of less than 10 seconds.\n","authors":["Yu-Seung Roh","Joo-Young Kim","Jin-Duk Park","Won-Yong Shin"],"pdf_url":"https://arxiv.org/pdf/2503.04406v1.pdf","comment":"10 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2406.12753v2","updated":"2025-03-06T12:55:25Z","published":"2024-06-18T16:20:53Z","title":"OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for\n  Superintelligent AI","summary":"  The evolution of Artificial Intelligence (AI) has been significantly\naccelerated by advancements in Large Language Models (LLMs) and Large\nMultimodal Models (LMMs), gradually showcasing potential cognitive reasoning\nabilities in problem-solving and scientific discovery (i.e., AI4Science) once\nexclusive to human intellect. To comprehensively evaluate current models'\nperformance in cognitive reasoning abilities, we introduce OlympicArena, which\nincludes 11,163 bilingual problems across both text-only and interleaved\ntext-image modalities. These challenges encompass a wide range of disciplines\nspanning seven fields and 62 international Olympic competitions, rigorously\nexamined for data leakage. We argue that the challenges in Olympic competition\nproblems are ideal for evaluating AI's cognitive reasoning due to their\ncomplexity and interdisciplinary nature, which are essential for tackling\ncomplex scientific challenges and facilitating discoveries. Beyond evaluating\nperformance across various disciplines using answer-only criteria, we conduct\ndetailed experiments and analyses from multiple perspectives. We delve into the\nmodels' cognitive reasoning abilities, their performance across different\nmodalities, and their outcomes in process-level evaluations, which are vital\nfor tasks requiring complex reasoning with lengthy solutions. Our extensive\nevaluations reveal that even advanced models like GPT-4o only achieve a 39.97%\noverall accuracy, illustrating current AI limitations in complex reasoning and\nmultimodal integration. Through the OlympicArena, we aim to advance AI towards\nsuperintelligence, equipping it to address more complex challenges in science\nand beyond. We also provide a comprehensive set of resources to support AI\nresearch, including a benchmark dataset, an open-source annotation platform, a\ndetailed evaluation tool, and a leaderboard with automatic submission features.\n","authors":["Zhen Huang","Zengzhi Wang","Shijie Xia","Xuefeng Li","Haoyang Zou","Ruijie Xu","Run-Ze Fan","Lyumanshan Ye","Ethan Chern","Yixin Ye","Yikai Zhang","Yuqing Yang","Ting Wu","Binjie Wang","Shichao Sun","Yang Xiao","Yiyuan Li","Fan Zhou","Steffi Chern","Yiwei Qin","Yan Ma","Jiadi Su","Yixiu Liu","Yuxiang Zheng","Shaoting Zhang","Dahua Lin","Yu Qiao","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2406.12753v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2404.05569v3","updated":"2025-03-06T12:54:37Z","published":"2024-04-08T14:43:13Z","title":"360$^\\circ$REA: Towards A Reusable Experience Accumulation with\n  360° Assessment for Multi-Agent System","summary":"  Large language model agents have demonstrated remarkable advancements across\nvarious complex tasks. Recent works focus on optimizing the agent team or\nemploying self-reflection to iteratively solve complex tasks. Since these\nagents are all based on the same LLM, only conducting self-evaluation or\nremoving underperforming agents does not substantively enhance the capability\nof the agents. We argue that a comprehensive evaluation and accumulating\nexperience from evaluation feedback is an effective approach to improving\nsystem performance. In this paper, we propose Reusable Experience Accumulation\nwith 360$^\\circ$ Assessment (360$^\\circ$REA), a hierarchical multi-agent\nframework inspired by corporate organizational practices. The framework employs\na novel 360$^\\circ$ performance assessment method for multi-perspective\nperformance evaluation with fine-grained assessment. To enhance the capability\nof agents in addressing complex tasks, we introduce dual-level experience pool\nfor agents to accumulate experience through fine-grained assessment. Extensive\nexperiments on complex task datasets demonstrate the effectiveness of\n360$^\\circ$REA.\n","authors":["Shen Gao","Hao Li","Chengrui Huang","Quan Tu","Zhiliang Tian","Minlie Huang","Shuo Shang"],"pdf_url":"https://arxiv.org/pdf/2404.05569v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04398v1","updated":"2025-03-06T12:52:22Z","published":"2025-03-06T12:52:22Z","title":"Speculative MoE: Communication Efficient Parallel MoE Inference with\n  Speculative Token and Expert Pre-scheduling","summary":"  MoE (Mixture of Experts) prevails as a neural architecture that can scale\nmodern transformer-based LLMs (Large Language Models) to unprecedented scales.\nNevertheless, large MoEs' great demands of computing power, memory capacity and\nmemory bandwidth make scalable serving a fundamental challenge and efficient\nparallel inference has become a requisite to attain adequate throughput under\nlatency constraints. DeepSpeed-MoE, one state-of-the-art MoE inference\nframework, adopts a 3D-parallel paradigm including EP (Expert Parallelism), TP\n(Tensor Parallel) and DP (Data Parallelism). However, our analysis shows\nDeepSpeed-MoE's inference efficiency is largely bottlenecked by EP, which is\nimplemented with costly all-to-all collectives to route token activation. Our\nwork aims to boost DeepSpeed-MoE by strategically reducing EP's communication\noverhead with a technique named Speculative MoE. Speculative MoE has two\nspeculative parallelization schemes, speculative token shuffling and\nspeculative expert grouping, which predict outstanding tokens' expert routing\npaths and pre-schedule tokens and experts across devices to losslessly trim\nEP's communication volume. Besides DeepSpeed-MoE, we also build Speculative MoE\ninto a prevailing MoE inference engine SGLang. Experiments show Speculative MoE\ncan significantly boost state-of-the-art MoE inference frameworks on fast\nhomogeneous and slow heterogeneous interconnects.\n","authors":["Yan Li","Pengfei Zheng","Shuang Chen","Zewei Xu","Yunfei Du","Zhengang Wang"],"pdf_url":"https://arxiv.org/pdf/2503.04398v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20742v2","updated":"2025-03-06T12:50:44Z","published":"2025-02-28T05:47:34Z","title":"Structured Preference Optimization for Vision-Language Long-Horizon Task\n  Planning","summary":"  Existing methods for vision-language task planning excel in short-horizon\ntasks but often fall short in complex, long-horizon planning within dynamic\nenvironments. These challenges primarily arise from the difficulty of\neffectively training models to produce high-quality reasoning processes for\nlong-horizon tasks. To address this, we propose Structured Preference\nOptimization (SPO), which aims to enhance reasoning and action selection in\nlong-horizon task planning through structured preference evaluation and\noptimized training strategies. Specifically, SPO introduces: 1)\nPreference-Based Scoring and Optimization, which systematically evaluates\nreasoning chains based on task relevance, visual grounding, and historical\nconsistency; and 2) Curriculum-Guided Training, where the model progressively\nadapts from simple to complex tasks, improving its generalization ability in\nlong-horizon scenarios and enhancing reasoning robustness. To advance research\nin vision-language long-horizon task planning, we introduce ExtendaBench, a\ncomprehensive benchmark covering 1,509 tasks across VirtualHome and Habitat\n2.0, categorized into ultra-short, short, medium, and long tasks. Experimental\nresults demonstrate that SPO significantly improves reasoning quality and final\ndecision accuracy, outperforming prior methods on long-horizon tasks and\nunderscoring the effectiveness of preference-driven optimization in\nvision-language task planning. Specifically, SPO achieves a +5.98% GCR and\n+4.68% SR improvement in VirtualHome and a +3.30% GCR and +2.11% SR improvement\nin Habitat over the best-performing baselines.\n","authors":["Xiwen Liang","Min Lin","Weiqi Ruan","Rongtao Xu","Yuecheng Liu","Jiaqi Chen","Bingqian Lin","Yuzheng Zhuang","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2502.20742v2.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2503.04392v1","updated":"2025-03-06T12:41:54Z","published":"2025-03-06T12:41:54Z","title":"AgentSafe: Safeguarding Large Language Model-based Multi-agent Systems\n  via Hierarchical Data Management","summary":"  Large Language Model based multi-agent systems are revolutionizing autonomous\ncommunication and collaboration, yet they remain vulnerable to security threats\nlike unauthorized access and data breaches. To address this, we introduce\nAgentSafe, a novel framework that enhances MAS security through hierarchical\ninformation management and memory protection. AgentSafe classifies information\nby security levels, restricting sensitive data access to authorized agents.\nAgentSafe incorporates two components: ThreatSieve, which secures communication\nby verifying information authority and preventing impersonation, and\nHierarCache, an adaptive memory management system that defends against\nunauthorized access and malicious poisoning, representing the first systematic\ndefense for agent memory. Experiments across various LLMs show that AgentSafe\nsignificantly boosts system resilience, achieving defense success rates above\n80% under adversarial conditions. Additionally, AgentSafe demonstrates\nscalability, maintaining robust performance as agent numbers and information\ncomplexity grow. Results underscore effectiveness of AgentSafe in securing MAS\nand its potential for real-world application.\n","authors":["Junyuan Mao","Fanci Meng","Yifan Duan","Miao Yu","Xiaojun Jia","Junfeng Fang","Yuxuan Liang","Kun Wang","Qingsong Wen"],"pdf_url":"https://arxiv.org/pdf/2503.04392v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.07918v2","updated":"2025-03-06T12:41:21Z","published":"2024-07-07T12:41:40Z","title":"Detecting new obfuscated malware variants: A lightweight and\n  interpretable machine learning approach","summary":"  Machine learning has been successfully applied in developing malware\ndetection systems, with a primary focus on accuracy, and increasing attention\nto reducing computational overhead and improving model interpretability.\nHowever, an important question remains underexplored: How well can machine\nlearning-based models detect entirely new forms of malware not present in the\ntraining data? In this study, we present a machine learning-based system for\ndetecting obfuscated malware that is not only highly accurate, lightweight and\ninterpretable, but also capable of successfully adapting to new types of\nmalware attacks. Our system is capable of detecting 15 malware subtypes despite\nbeing exclusively trained on one malware subtype, namely the Transponder from\nthe Spyware family. This system was built after training 15 distinct random\nforest-based models, each on a different malware subtype from the\nCIC-MalMem-2022 dataset. These models were evaluated against the entire range\nof malware subtypes, including all unseen malware subtypes. To maintain the\nsystem's streamlined nature, training was confined to the top five most\nimportant features, which also enhanced interpretability. The\nTransponder-focused model exhibited high accuracy, exceeding 99.8%, with an\naverage processing speed of 5.7 microseconds per file. We also illustrate how\nthe Shapley additive explanations technique can facilitate the interpretation\nof the model predictions. Our research contributes to advancing malware\ndetection methodologies, pioneering the feasibility of detecting obfuscated\nmalware by exclusively training a model on a single or a few carefully selected\nmalware subtypes and applying it to detect unseen subtypes.\n","authors":["Oladipo A. Madamidola","Felix Ngobigha","Adnane Ez-zizi"],"pdf_url":"https://arxiv.org/pdf/2407.07918v2.pdf","comment":"30 pages (excluding Appendix), 5 figures and 5 tables. Now published\n  in Intelligent Systems with Applications\n  (https://doi.org/10.1016/j.iswa.2024.200472)"},{"id":"http://arxiv.org/abs/2410.21083v2","updated":"2025-03-06T12:38:42Z","published":"2024-10-28T14:48:05Z","title":"Stealthy Jailbreak Attacks on Large Language Models via Benign Data\n  Mirroring","summary":"  Large language model (LLM) safety is a critical issue, with numerous studies\nemploying red team testing to enhance model security. Among these, jailbreak\nmethods explore potential vulnerabilities by crafting malicious prompts that\ninduce model outputs contrary to safety alignments. Existing black-box\njailbreak methods often rely on model feedback, repeatedly submitting queries\nwith detectable malicious instructions during the attack search process.\nAlthough these approaches are effective, the attacks may be intercepted by\ncontent moderators during the search process. We propose an improved transfer\nattack method that guides malicious prompt construction by locally training a\nmirror model of the target black-box model through benign data distillation.\nThis method offers enhanced stealth, as it does not involve submitting\nidentifiable malicious instructions to the target model during the search\nphase. Our approach achieved a maximum attack success rate of 92%, or a\nbalanced value of 80% with an average of 1.5 detectable jailbreak queries per\nsample against GPT-3.5 Turbo on a subset of AdvBench. These results underscore\nthe need for more robust defense mechanisms.\n","authors":["Honglin Mu","Han He","Yuxin Zhou","Yunlong Feng","Yang Xu","Libo Qin","Xiaoming Shi","Zeming Liu","Xudong Han","Qi Shi","Qingfu Zhu","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2410.21083v2.pdf","comment":"Accepted by NAACL 2025"},{"id":"http://arxiv.org/abs/2502.07527v2","updated":"2025-03-06T12:34:23Z","published":"2025-02-11T13:08:03Z","title":"Nature Language Model: Deciphering the Language of Nature for Scientific\n  Discovery","summary":"  Foundation models have revolutionized natural language processing and\nartificial intelligence, significantly enhancing how machines comprehend and\ngenerate human languages. Inspired by the success of these foundation models,\nresearchers have developed foundation models for individual scientific domains,\nincluding small molecules, materials, proteins, DNA, RNA and even cells.\nHowever, these models are typically trained in isolation, lacking the ability\nto integrate across different scientific domains. Recognizing that entities\nwithin these domains can all be represented as sequences, which together form\nthe \"language of nature\", we introduce Nature Language Model (NatureLM), a\nsequence-based science foundation model designed for scientific discovery.\nPre-trained with data from multiple scientific domains, NatureLM offers a\nunified, versatile model that enables various applications including: (i)\ngenerating and optimizing small molecules, proteins, RNA, and materials using\ntext instructions; (ii) cross-domain generation/design, such as\nprotein-to-molecule and protein-to-RNA generation; and (iii) top performance\nacross different domains, matching or surpassing state-of-the-art specialist\nmodels. NatureLM offers a promising generalist approach for various scientific\ntasks, including drug discovery (hit generation/optimization, ADMET\noptimization, synthesis), novel material design, and the development of\ntherapeutic proteins or nucleotides. We have developed NatureLM models in\ndifferent sizes (1 billion, 8 billion, and 46.7 billion parameters) and\nobserved a clear improvement in performance as the model size increases.\n","authors":["Yingce Xia","Peiran Jin","Shufang Xie","Liang He","Chuan Cao","Renqian Luo","Guoqing Liu","Yue Wang","Zequn Liu","Yuan-Jyue Chen","Zekun Guo","Yeqi Bai","Pan Deng","Yaosen Min","Ziheng Lu","Hongxia Hao","Han Yang","Jielan Li","Chang Liu","Jia Zhang","Jianwei Zhu","Ran Bi","Kehan Wu","Wei Zhang","Kaiyuan Gao","Qizhi Pei","Qian Wang","Xixian Liu","Yanting Li","Houtian Zhu","Yeqing Lu","Mingqian Ma","Zun Wang","Tian Xie","Krzysztof Maziarz","Marwin Segler","Zhao Yang","Zilong Chen","Yu Shi","Shuxin Zheng","Lijun Wu","Chen Hu","Peggy Dai","Tie-Yan Liu","Haiguang Liu","Tao Qin"],"pdf_url":"https://arxiv.org/pdf/2502.07527v2.pdf","comment":"93 pages"},{"id":"http://arxiv.org/abs/2503.04378v1","updated":"2025-03-06T12:30:24Z","published":"2025-03-06T12:30:24Z","title":"Dedicated Feedback and Edit Models Empower Inference-Time Scaling for\n  Open-Ended General-Domain Tasks","summary":"  Inference-Time Scaling has been critical to the success of recent models such\nas OpenAI o1 and DeepSeek R1. However, many techniques used to train models for\ninference-time scaling require tasks to have answers that can be verified,\nlimiting their application to domains such as math, coding and logical\nreasoning. We take inspiration from how humans make first attempts, ask for\ndetailed feedback from others and make improvements based on such feedback\nacross a wide spectrum of open-ended endeavors. To this end, we collect data\nfor and train dedicated Feedback and Edit Models that are capable of performing\ninference-time scaling for open-ended general-domain tasks. In our setup, one\nmodel generates an initial response, which are given feedback by a second\nmodel, that are then used by a third model to edit the response. We show that\nperformance on Arena Hard, a benchmark strongly predictive of Chatbot Arena Elo\ncan be boosted by scaling the number of initial response drafts, effective\nfeedback and edited responses. When scaled optimally, our setup based on 70B\nmodels from the Llama 3 family can reach SoTA performance on Arena Hard at 92.7\nas of 5 Mar 2025, surpassing OpenAI o1-preview-2024-09-12 with 90.4 and\nDeepSeek R1 with 92.3.\n","authors":["Zhilin Wang","Jiaqi Zeng","Olivier Delalleau","Daniel Egert","Ellie Evans","Hoo-Chang Shin","Felipe Soares","Yi Dong","Oleksii Kuchaiev"],"pdf_url":"https://arxiv.org/pdf/2503.04378v1.pdf","comment":"22 pages, 2 figures"},{"id":"http://arxiv.org/abs/2409.10329v2","updated":"2025-03-06T12:16:09Z","published":"2024-09-16T14:39:15Z","title":"InfoDisent: Explainability of Image Classification Models by Information\n  Disentanglement","summary":"  In this work, we introduce InfoDisent, a hybrid approach to explainability\nbased on the information bottleneck principle. InfoDisent enables the\ndisentanglement of information in the final layer of any pretrained model into\natomic concepts, which can be interpreted as prototypical parts. This approach\nmerges the flexibility of post-hoc methods with the concept-level modeling\ncapabilities of self-explainable neural networks, such as ProtoPNets. We\ndemonstrate the effectiveness of InfoDisent through computational experiments\nand user studies across various datasets using modern backbones such as ViTs\nand convolutional networks. Notably, InfoDisent generalizes the prototypical\nparts approach to novel domains (ImageNet).\n","authors":["Łukasz Struski","Dawid Rymarczyk","Jacek Tabor"],"pdf_url":"https://arxiv.org/pdf/2409.10329v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.01257v2","updated":"2025-03-06T12:13:14Z","published":"2024-10-02T06:05:52Z","title":"HelpSteer2-Preference: Complementing Ratings with Preferences","summary":"  Reward models are critical for aligning models to follow instructions, and\nare typically trained following one of two popular paradigms: Bradley-Terry\nstyle or Regression style. However, there is a lack of evidence that either\napproach is better than the other, when adequately matched for data. This is\nprimarily because these approaches require data collected in different (but\nincompatible) formats, meaning that adequately matched data is not available in\nexisting public datasets. To tackle this problem, we release preference\nannotations (designed for Bradley-Terry training) to complement existing\nratings (designed for Regression style training) in the HelpSteer2 dataset. To\nimprove data interpretability, preference annotations are accompanied with\nhuman-written justifications. Using this data, we conduct the first\nhead-to-head comparison of Bradley-Terry and Regression models when adequately\nmatched for data. Based on insights derived from such a comparison, we propose\na novel approach to combine Bradley-Terry and Regression reward modeling. A\nLlama-3.1-70B-Instruct model tuned with this approach scores 94.1 on\nRewardBench, emerging top of more than 140 reward models as of 1 Oct 2024. This\nreward model can then be used with REINFORCE algorithm (RLHF) to align an\nInstruct model to reach 85.0 on Arena Hard, which is No. 1 as of 1 Oct 2024. We\nopen-source this dataset (CC-BY-4.0 license) at\nhttps://huggingface.co/datasets/nvidia/HelpSteer2#preferences-new -- 1-oct-2024\nand openly release the trained Reward and Instruct models at\nhttps://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Reward and\nhttps://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct\n","authors":["Zhilin Wang","Alexander Bukharin","Olivier Delalleau","Daniel Egert","Gerald Shen","Jiaqi Zeng","Oleksii Kuchaiev","Yi Dong"],"pdf_url":"https://arxiv.org/pdf/2410.01257v2.pdf","comment":"Accepted to ICLR 2025; 28 pages, 3 figures"},{"id":"http://arxiv.org/abs/2503.04363v1","updated":"2025-03-06T12:06:54Z","published":"2025-03-06T12:06:54Z","title":"Causally Reliable Concept Bottleneck Models","summary":"  Concept-based models are an emerging paradigm in deep learning that\nconstrains the inference process to operate through human-interpretable\nconcepts, facilitating explainability and human interaction. However, these\narchitectures, on par with popular opaque neural models, fail to account for\nthe true causal mechanisms underlying the target phenomena represented in the\ndata. This hampers their ability to support causal reasoning tasks, limits\nout-of-distribution generalization, and hinders the implementation of fairness\nconstraints. To overcome these issues, we propose \\emph{Causally reliable\nConcept Bottleneck Models} (C$^2$BMs), a class of concept-based architectures\nthat enforce reasoning through a bottleneck of concepts structured according to\na model of the real-world causal mechanisms. We also introduce a pipeline to\nautomatically learn this structure from observational data and\n\\emph{unstructured} background knowledge (e.g., scientific literature).\nExperimental evidence suggest that C$^2$BM are more interpretable, causally\nreliable, and improve responsiveness to interventions w.r.t. standard opaque\nand concept-based models, while maintaining their accuracy.\n","authors":["Giovanni De Felice","Arianna Casanova Flores","Francesco De Santis","Silvia Santini","Johannes Schneider","Pietro Barbiero","Alberto Termine"],"pdf_url":"https://arxiv.org/pdf/2503.04363v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04362v1","updated":"2025-03-06T12:04:56Z","published":"2025-03-06T12:04:56Z","title":"A Generalist Cross-Domain Molecular Learning Framework for\n  Structure-Based Drug Discovery","summary":"  Structure-based drug discovery (SBDD) is a systematic scientific process that\ndevelops new drugs by leveraging the detailed physical structure of the target\nprotein. Recent advancements in pre-trained models for biomolecules have\ndemonstrated remarkable success across various biochemical applications,\nincluding drug discovery and protein engineering. However, in most approaches,\nthe pre-trained models primarily focus on the characteristics of either small\nmolecules or proteins, without delving into their binding interactions which\nare essential cross-domain relationships pivotal to SBDD. To fill this gap, we\npropose a general-purpose foundation model named BIT (an abbreviation for\nBiomolecular Interaction Transformer), which is capable of encoding a range of\nbiochemical entities, including small molecules, proteins, and protein-ligand\ncomplexes, as well as various data formats, encompassing both 2D and 3D\nstructures. Specifically, we introduce Mixture-of-Domain-Experts (MoDE) to\nhandle the biomolecules from diverse biochemical domains and\nMixture-of-Structure-Experts (MoSE) to capture positional dependencies in the\nmolecular structures. The proposed mixture-of-experts approach enables BIT to\nachieve both deep fusion and domain-specific encoding, effectively capturing\nfine-grained molecular interactions within protein-ligand complexes. Then, we\nperform cross-domain pre-training on the shared Transformer backbone via\nseveral unified self-supervised denoising tasks. Experimental results on\nvarious benchmarks demonstrate that BIT achieves exceptional performance in\ndownstream tasks, including binding affinity prediction, structure-based\nvirtual screening, and molecular property prediction.\n","authors":["Yiheng Zhu","Mingyang Li","Junlong Liu","Kun Fu","Jiansheng Wu","Qiuyi Li","Mingze Yin","Jieping Ye","Jian Wu","Zheng Wang"],"pdf_url":"https://arxiv.org/pdf/2503.04362v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04357v1","updated":"2025-03-06T12:01:20Z","published":"2025-03-06T12:01:20Z","title":"scDD: Latent Codes Based scRNA-seq Dataset Distillation with Foundation\n  Model Knowledge","summary":"  Single-cell RNA sequencing (scRNA-seq) technology has profiled hundreds of\nmillions of human cells across organs, diseases, development and perturbations\nto date. However, the high-dimensional sparsity, batch effect noise, category\nimbalance, and ever-increasing data scale of the original sequencing data pose\nsignificant challenges for multi-center knowledge transfer, data fusion, and\ncross-validation between scRNA-seq datasets. To address these barriers, (1) we\nfirst propose a latent codes-based scRNA-seq dataset distillation framework\nnamed scDD, which transfers and distills foundation model knowledge and\noriginal dataset information into a compact latent space and generates\nsynthetic scRNA-seq dataset by a generator to replace the original dataset.\nThen, (2) we propose a single-step conditional diffusion generator named SCDG,\nwhich perform single-step gradient back-propagation to help scDD optimize\ndistillation quality and avoid gradient decay caused by multi-step\nback-propagation. Meanwhile, SCDG ensures the scRNA-seq data characteristics\nand inter-class discriminability of the synthetic dataset through flexible\nconditional control and generation quality assurance. Finally, we propose a\ncomprehensive benchmark to evaluate the performance of scRNA-seq dataset\ndistillation in different data analysis tasks. It is validated that our\nproposed method can achieve 7.61% absolute and 15.70% relative improvement over\nprevious state-of-the-art methods on average task.\n","authors":["Zhen Yu","Jianan Han","Yang Liu","Qingchao Chen"],"pdf_url":"https://arxiv.org/pdf/2503.04357v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01334v3","updated":"2025-03-06T11:59:11Z","published":"2024-08-02T15:32:42Z","title":"A Backbone for Long-Horizon Robot Task Understanding","summary":"  End-to-end robot learning, particularly for long-horizon tasks, often results\nin unpredictable outcomes and poor generalization. To address these challenges,\nwe propose a novel Therblig-Based Backbone Framework (TBBF) as a fundamental\nstructure to enhance interpretability, data efficiency, and generalization in\nrobotic systems. TBBF utilizes expert demonstrations to enable therblig-level\ntask decomposition, facilitate efficient action-object mapping, and generate\nadaptive trajectories for new scenarios. The approach consists of two stages:\noffline training and online testing. During the offline training stage, we\ndeveloped the Meta-RGate SynerFusion (MGSF) network for accurate therblig\nsegmentation across various tasks. In the online testing stage, after a\none-shot demonstration of a new task is collected, our MGSF network extracts\nhigh-level knowledge, which is then encoded into the image using Action\nRegistration (ActionREG). Additionally, Large Language Model (LLM)-Alignment\nPolicy for Visual Correction (LAP-VC) is employed to ensure precise action\nregistration, facilitating trajectory transfer in novel robot scenarios.\nExperimental results validate these methods, achieving 94.37% recall in\ntherblig segmentation and success rates of 94.4% and 80% in real-world online\nrobot testing for simple and complex scenarios, respectively. Supplementary\nmaterial is available at:\nhttps://sites.google.com/view/therbligsbasedbackbone/home\n","authors":["Xiaoshuai Chen","Wei Chen","Dongmyoung Lee","Yukun Ge","Nicolas Rojas","Petar Kormushev"],"pdf_url":"https://arxiv.org/pdf/2408.01334v3.pdf","comment":"8 pages, 8 figures. This work has been published by IEEE Robotics and\n  Automation Letters (RA-L)"},{"id":"http://arxiv.org/abs/2407.12468v3","updated":"2025-03-06T11:53:49Z","published":"2024-07-17T10:40:39Z","title":"Evaluating Search Engines and Large Language Models for Answering Health\n  Questions","summary":"  Search engines (SEs) have traditionally been primary tools for information\nseeking, but the new Large Language Models (LLMs) are emerging as powerful\nalternatives, particularly for question-answering tasks. This study compares\nthe performance of four popular SEs, seven LLMs, and retrieval-augmented (RAG)\nvariants in answering 150 health-related questions from the TREC Health\nMisinformation (HM) Track. Results reveal SEs correctly answer between 50 and\n70% of questions, often hindered by many retrieval results not responding to\nthe health question. LLMs deliver higher accuracy, correctly answering about\n80% of questions, though their performance is sensitive to input prompts. RAG\nmethods significantly enhance smaller LLMs' effectiveness, improving accuracy\nby up to 30% by integrating retrieval evidence.\n","authors":["Marcos Fernández-Pichel","Juan C. Pichel","David E. Losada"],"pdf_url":"https://arxiv.org/pdf/2407.12468v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04343v1","updated":"2025-03-06T11:39:46Z","published":"2025-03-06T11:39:46Z","title":"Talking Back -- human input and explanations to interactive AI systems","summary":"  While XAI focuses on providing AI explanations to humans, can the reverse -\nhumans explaining their judgments to AI - foster richer, synergistic human-AI\nsystems? This paper explores various forms of human inputs to AI and examines\nhow human explanations can guide machine learning models toward automated\njudgments and explanations that align more closely with human concepts.\n","authors":["Alan Dix","Tommaso Turchi","Ben Wilson","Anna Monreale","Matt Roach"],"pdf_url":"https://arxiv.org/pdf/2503.04343v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.12113v2","updated":"2025-03-06T11:33:28Z","published":"2024-01-22T16:51:01Z","title":"Extracting Formulae in Many-Valued Logic from Deep Neural Networks","summary":"  We propose a new perspective on deep ReLU networks, namely as circuit\ncounterparts of Lukasiewicz infinite-valued logic -- a many-valued (MV)\ngeneralization of Boolean logic. An algorithm for extracting formulae in MV\nlogic from deep ReLU networks is presented. As the algorithm applies to\nnetworks with general, in particular also real-valued, weights, it can be used\nto extract logical formulae from deep ReLU networks trained on data.\n","authors":["Yani Zhang","Helmut Bölcskei"],"pdf_url":"https://arxiv.org/pdf/2401.12113v2.pdf","comment":"Signicant extension of the previous version"},{"id":"http://arxiv.org/abs/2503.04328v1","updated":"2025-03-06T11:27:55Z","published":"2025-03-06T11:27:55Z","title":"Solving Word-Sense Disambiguation and Word-Sense Induction with\n  Dictionary Examples","summary":"  Many less-resourced languages struggle with a lack of large, task-specific\ndatasets that are required for solving relevant tasks with modern\ntransformer-based large language models (LLMs). On the other hand, many\nlinguistic resources, such as dictionaries, are rarely used in this context\ndespite their large information contents. We show how LLMs can be used to\nextend existing language resources in less-resourced languages for two\nimportant tasks: word-sense disambiguation (WSD) and word-sense induction\n(WSI). We approach the two tasks through the related but much more accessible\nword-in-context (WiC) task where, given a pair of sentences and a target word,\na classification model is tasked with predicting whether the sense of a given\nword differs between sentences. We demonstrate that a well-trained model for\nthis task can distinguish between different word senses and can be adapted to\nsolve the WSD and WSI tasks. The advantage of using the WiC task, instead of\ndirectly predicting senses, is that the WiC task does not need pre-constructed\nsense inventories with a sufficient number of examples for each sense, which\nare rarely available in less-resourced languages. We show that sentence pairs\nfor the WiC task can be successfully generated from dictionary examples using\nLLMs. The resulting prediction models outperform existing models on WiC, WSD,\nand WSI tasks. We demonstrate our methodology on the Slovene language, where a\nmonolingual dictionary is available, but word-sense resources are tiny.\n","authors":["Tadej Škvorc","Marko Robnik-Šikonja"],"pdf_url":"https://arxiv.org/pdf/2503.04328v1.pdf","comment":"12 pages, 1 figure"},{"id":"http://arxiv.org/abs/2412.00156v3","updated":"2025-03-06T11:05:32Z","published":"2024-11-29T08:10:49Z","title":"VISION-XL: High Definition Video Inverse Problem Solver using Latent\n  Image Diffusion Models","summary":"  In this paper, we propose a novel framework for solving high-definition video\ninverse problems using latent image diffusion models. Building on recent\nadvancements in spatio-temporal optimization for video inverse problems using\nimage diffusion models, our approach leverages latent-space diffusion models to\nachieve enhanced video quality and resolution. To address the high\ncomputational demands of processing high-resolution frames, we introduce a\npseudo-batch consistent sampling strategy, allowing efficient operation on a\nsingle GPU. Additionally, to improve temporal consistency, we present\npseudo-batch inversion, an initialization technique that incorporates\ninformative latents from the measurement. By integrating with SDXL, our\nframework achieves state-of-the-art video reconstruction across a wide range of\nspatio-temporal inverse problems, including complex combinations of frame\naveraging and various spatial degradations, such as deblurring,\nsuper-resolution, and inpainting. Unlike previous methods, our approach\nsupports multiple aspect ratios (landscape, vertical, and square) and delivers\nHD-resolution reconstructions (exceeding 1280x720) in under 6 seconds per frame\non a single NVIDIA 4090 GPU.\n","authors":["Taesung Kwon","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2412.00156v3.pdf","comment":"Project page: https://vision-xl.github.io/"},{"id":"http://arxiv.org/abs/2501.10814v2","updated":"2025-03-06T11:05:23Z","published":"2025-01-18T16:23:09Z","title":"No More Sliding Window: Efficient 3D Medical Image Segmentation with\n  Differentiable Top-k Patch Sampling","summary":"  3D models surpass 2D models in CT/MRI segmentation by effectively capturing\ninter-slice relationships. However, the added depth dimension substantially\nincreases memory consumption. While patch-based training alleviates memory\nconstraints, it significantly slows down the inference speed due to the sliding\nwindow (SW) approach. We propose No-More-Sliding-Window (NMSW), a novel\nend-to-end trainable framework that enhances the efficiency of generic 3D\nsegmentation backbone during an inference step by eliminating the need for SW.\nNMSW employs a differentiable Top-k module to selectively sample only the most\nrelevant patches, thereby minimizing redundant computations. When patch-level\npredictions are insufficient, the framework intelligently leverages coarse\nglobal predictions to refine results. Evaluated across 3 tasks using 3\nsegmentation backbones, NMSW achieves competitive accuracy compared to SW\ninference while significantly reducing computational complexity by 91% (88.0 to\n8.00 TMACs). Moreover, it delivers a 9.1x faster inference on the H100 GPU\n(99.0 to 8.3 sec) and a 11.1x faster inference on the Xeon Gold CPU (2110 to\n189 sec). NMSW is model-agnostic, further boosting efficiency when integrated\nwith any existing efficient segmentation backbones.\n","authors":["Young Seok Jeon","Hongfei Yang","Huazhu Fu","Mengling Feng"],"pdf_url":"https://arxiv.org/pdf/2501.10814v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.21057v2","updated":"2025-03-06T11:02:06Z","published":"2025-02-28T13:58:22Z","title":"Robust Deterministic Policy Gradient for Disturbance Attenuation and Its\n  Application to Quadrotor Control","summary":"  Practical control systems pose significant challenges in identifying optimal\ncontrol policies due to uncertainties in the system model and external\ndisturbances. While $H_\\infty$ control techniques are commonly used to design\nrobust controllers that mitigate the effects of disturbances, these methods\noften require complex and computationally intensive calculations. To address\nthis issue, this paper proposes a reinforcement learning algorithm called\nRobust Deterministic Policy Gradient (RDPG), which formulates the $H_\\infty$\ncontrol problem as a two-player zero-sum dynamic game. In this formulation, one\nplayer (the user) aims to minimize the cost, while the other player (the\nadversary) seeks to maximize it. We then employ deterministic policy gradient\n(DPG) and its deep reinforcement learning counterpart to train a robust control\npolicy with effective disturbance attenuation. In particular, for practical\nimplementation, we introduce an algorithm called robust deep deterministic\npolicy gradient (RDDPG), which employs a deep neural network architecture and\nintegrates techniques from the twin-delayed deep deterministic policy gradient\n(TD3) to enhance stability and learning efficiency. To evaluate the proposed\nalgorithm, we implement it on an unmanned aerial vehicle (UAV) tasked with\nfollowing a predefined path in a disturbance-prone environment. The\nexperimental results demonstrate that the proposed method outperforms other\ncontrol approaches in terms of robustness against disturbances, enabling\nprecise real-time tracking of moving targets even under severe disturbance\nconditions.\n","authors":["Taeho Lee","Donghwan Lee"],"pdf_url":"https://arxiv.org/pdf/2502.21057v2.pdf","comment":"8 pages"},{"id":"http://arxiv.org/abs/2503.03417v2","updated":"2025-03-06T11:00:35Z","published":"2025-03-05T11:47:32Z","title":"When Claims Evolve: Evaluating and Enhancing the Robustness of Embedding\n  Models Against Misinformation Edits","summary":"  Online misinformation remains a critical challenge, and fact-checkers\nincreasingly rely on embedding-based methods to retrieve relevant fact-checks.\nYet, when debunked claims reappear in edited forms, the performance of these\nmethods is unclear. In this work, we introduce a taxonomy of six common\nreal-world misinformation edits and propose a perturbation framework that\ngenerates valid, natural claim variations. Our multi-stage retrieval evaluation\nreveals that standard embedding models struggle with user-introduced edits,\nwhile LLM-distilled embeddings offer improved robustness at a higher\ncomputational cost. Although a strong reranker helps mitigate some issues, it\ncannot fully compensate for first-stage retrieval gaps. Addressing these\nretrieval gaps, our train- and inference-time mitigation approaches enhance\nin-domain robustness by up to 17 percentage points and boost out-of-domain\ngeneralization by 10 percentage points over baseline models. Overall, our\nfindings provide practical improvements to claim-matching systems, enabling\nmore reliable fact-checking of evolving misinformation.\n","authors":["Jabez Magomere","Emanuele La Malfa","Manuel Tonneau","Ashkan Kazemi","Scott Hale"],"pdf_url":"https://arxiv.org/pdf/2503.03417v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04315v1","updated":"2025-03-06T10:58:35Z","published":"2025-03-06T10:58:35Z","title":"Provable Robust Overfitting Mitigation in Wasserstein Distributionally\n  Robust Optimization","summary":"  Wasserstein distributionally robust optimization (WDRO) optimizes against\nworst-case distributional shifts within a specified uncertainty set, leading to\nenhanced generalization on unseen adversarial examples, compared to standard\nadversarial training which focuses on pointwise adversarial perturbations.\nHowever, WDRO still suffers fundamentally from the robust overfitting problem,\nas it does not consider statistical error. We address this gap by proposing a\nnovel robust optimization framework under a new uncertainty set for adversarial\nnoise via Wasserstein distance and statistical error via Kullback-Leibler\ndivergence, called the Statistically Robust WDRO. We establish a robust\ngeneralization bound for the new optimization framework, implying that\nout-of-distribution adversarial performance is at least as good as the\nstatistically robust training loss with high probability. Furthermore, we\nderive conditions under which Stackelberg and Nash equilibria exist between the\nlearner and the adversary, giving an optimal robust model in certain sense.\nFinally, through extensive experiments, we demonstrate that our method\nsignificantly mitigates robust overfitting and enhances robustness within the\nframework of WDRO.\n","authors":["Shuang Liu","Yihan Wang","Yifan Zhu","Yibo Miao","Xiao-Shan Gao"],"pdf_url":"https://arxiv.org/pdf/2503.04315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04302v1","updated":"2025-03-06T10:42:18Z","published":"2025-03-06T10:42:18Z","title":"Malware Detection at the Edge with Lightweight LLMs: A Performance\n  Evaluation","summary":"  The rapid evolution of malware attacks calls for the development of\ninnovative detection methods, especially in resource-constrained edge\ncomputing. Traditional detection techniques struggle to keep up with modern\nmalware's sophistication and adaptability, prompting a shift towards advanced\nmethodologies like those leveraging Large Language Models (LLMs) for enhanced\nmalware detection. However, deploying LLMs for malware detection directly at\nedge devices raises several challenges, including ensuring accuracy in\nconstrained environments and addressing edge devices' energy and computational\nlimits. To tackle these challenges, this paper proposes an architecture\nleveraging lightweight LLMs' strengths while addressing limitations like\nreduced accuracy and insufficient computational power. To evaluate the\neffectiveness of the proposed lightweight LLM-based approach for edge\ncomputing, we perform an extensive experimental evaluation using several\nstate-of-the-art lightweight LLMs. We test them with several publicly available\ndatasets specifically designed for edge and IoT scenarios and different edge\nnodes with varying computational power and characteristics.\n","authors":["Christian Rondanini","Barbara Carminati","Elena Ferrari","Antonio Gaudiano","Ashish Kundu"],"pdf_url":"https://arxiv.org/pdf/2503.04302v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.01924v2","updated":"2025-03-06T10:39:48Z","published":"2024-05-03T08:34:13Z","title":"Semi-Parametric Retrieval via Binary Bag-of-Tokens Index","summary":"  Information retrieval has transitioned from standalone systems into essential\ncomponents across broader applications, with indexing efficiency,\ncost-effectiveness, and freshness becoming increasingly critical yet often\noverlooked. In this paper, we introduce SemI-parametric Disentangled Retrieval\n(SiDR), a bi-encoder retrieval framework that decouples retrieval index from\nneural parameters to enable efficient, low-cost, and parameter-agnostic\nindexing for emerging use cases. Specifically, in addition to using embeddings\nas indexes like existing neural retrieval methods, SiDR supports a\nnon-parametric tokenization index for search, achieving BM25-like indexing\ncomplexity with significantly better effectiveness. Our comprehensive\nevaluation across 16 retrieval benchmarks demonstrates that SiDR outperforms\nboth neural and term-based retrieval baselines under the same indexing\nworkload: (i) When using an embedding-based index, SiDR exceeds the performance\nof conventional neural retrievers while maintaining similar training\ncomplexity; (ii) When using a tokenization-based index, SiDR drastically\nreduces indexing cost and time, matching the complexity of traditional\nterm-based retrieval, while consistently outperforming BM25 on all in-domain\ndatasets; (iii) Additionally, we introduce a late parametric mechanism that\nmatches BM25 index preparation time while outperforming other neural retrieval\nbaselines in effectiveness.\n","authors":["Jiawei Zhou","Li Dong","Furu Wei","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2405.01924v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04299v1","updated":"2025-03-06T10:39:47Z","published":"2025-03-06T10:39:47Z","title":"Mapping AI Benchmark Data to Quantitative Risk Estimates Through Expert\n  Elicitation","summary":"  The literature and multiple experts point to many potential risks from large\nlanguage models (LLMs), but there are still very few direct measurements of the\nactual harms posed. AI risk assessment has so far focused on measuring the\nmodels' capabilities, but the capabilities of models are only indicators of\nrisk, not measures of risk. Better modeling and quantification of AI risk\nscenarios can help bridge this disconnect and link the capabilities of LLMs to\ntangible real-world harm. This paper makes an early contribution to this field\nby demonstrating how existing AI benchmarks can be used to facilitate the\ncreation of risk estimates. We describe the results of a pilot study in which\nexperts use information from Cybench, an AI benchmark, to generate probability\nestimates. We show that the methodology seems promising for this purpose, while\nnoting improvements that can be made to further strengthen its application in\nquantitative AI risk assessment.\n","authors":["Malcolm Murray","Henry Papadatos","Otter Quarks","Pierre-François Gimenez","Simeon Campos"],"pdf_url":"https://arxiv.org/pdf/2503.04299v1.pdf","comment":"23 pages, 4 figures"},{"id":"http://arxiv.org/abs/2408.14507v3","updated":"2025-03-06T10:26:32Z","published":"2024-08-24T16:54:08Z","title":"Prompt-Matcher: Leveraging Large Models to Reduce Uncertainty in Schema\n  Matching Results","summary":"  Schema matching is the process of identifying correspondences between the\nelements of two given schemata, essential for database management systems, data\nintegration, and data warehousing. For datasets across different scenarios, the\noptimal schema matching algorithm is different. For single algorithm,\nhyperparameter tuning also cases multiple results. All results assigned equal\nprobabilities are stored in probabilistic databases to facilitate uncertainty\nmanagement. The substantial degree of uncertainty diminishes the efficiency and\nreliability of data processing, thereby precluding the provision of more\naccurate information for decision-makers. To address this problem, we introduce\na new approach based on fine-grained correspondence verification with specific\nprompt of Large Language Model.\n  Our approach is an iterative loop that consists of three main components: (1)\nthe correspondence selection algorithm, (2) correspondence verification, and\n(3) the update of probability distribution. The core idea is that\ncorrespondences intersect across multiple results, thereby linking the\nverification of correspondences to the reduction of uncertainty in candidate\nresults.\n  The task of selecting an optimal correspondence set to maximize the\nanticipated uncertainty reduction within a fixed budgetary framework is\nestablished as an NP-hard problem. We propose a novel $(1-1/e)$-approximation\nalgorithm that significantly outperforms brute algorithm in terms of\ncomputational efficiency. To enhance correspondence verification, we have\ndeveloped two prompt templates that enable GPT-4 to achieve state-of-the-art\nperformance across two established benchmark datasets. Our comprehensive\nexperimental evaluation demonstrates the superior effectiveness and robustness\nof the proposed approach.\n","authors":["Longyu Feng","Huahang Li","Chen Jason Zhang"],"pdf_url":"https://arxiv.org/pdf/2408.14507v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04291v1","updated":"2025-03-06T10:19:01Z","published":"2025-03-06T10:19:01Z","title":"MathMistake Checker: A Comprehensive Demonstration for Step-by-Step Math\n  Problem Mistake Finding by Prompt-Guided LLMs","summary":"  We propose a novel system, MathMistake Checker, designed to automate\nstep-by-step mistake finding in mathematical problems with lengthy answers\nthrough a two-stage process. The system aims to simplify grading, increase\nefficiency, and enhance learning experiences from a pedagogical perspective. It\nintegrates advanced technologies, including computer vision and the\nchain-of-thought capabilities of the latest large language models (LLMs). Our\nsystem supports open-ended grading without reference answers and promotes\npersonalized learning by providing targeted feedback. We demonstrate its\neffectiveness across various types of math problems, such as calculation and\nword problems.\n","authors":["Tianyang Zhang","Zhuoxuan Jiang","Haotian Zhang","Lin Lin","Shaohua Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.04291v1.pdf","comment":"Published in AAAI 2025"},{"id":"http://arxiv.org/abs/2503.04290v1","updated":"2025-03-06T10:17:52Z","published":"2025-03-06T10:17:52Z","title":"How Do Hackathons Foster Creativity? Towards AI Collaborative Evaluation\n  of Creativity at Scale","summary":"  Hackathons have become popular collaborative events for accelerating the\ndevelopment of creative ideas and prototypes. There are several case studies\nshowcasing creative outcomes across domains such as industry, education, and\nresearch. However, there are no large-scale studies on creativity in hackathons\nwhich can advance theory on how hackathon formats lead to creative outcomes. We\nconducted a computational analysis of 193,353 hackathon projects. By\noperationalizing creativity through usefulness and novelty, we refined our\ndataset to 10,363 projects, allowing us to analyze how participant\ncharacteristics, collaboration patterns, and hackathon setups influence the\ndevelopment of creative projects. The contribution of our paper is twofold: We\nidentified means for organizers to foster creativity in hackathons. We also\nexplore the use of large language models (LLMs) to augment the evaluation of\ncreative outcomes and discuss challenges and opportunities of doing this, which\nhas implications for creativity research at large.\n","authors":["Jeanette Falk","Yiyi Chen","Janet Rafner","Mike Zhang","Johannes Bjerva","Alexander Nolte"],"pdf_url":"https://arxiv.org/pdf/2503.04290v1.pdf","comment":"Accepted in Proceedings of the 2025 CHI Conference on Human Factors\n  in Computing Systems"},{"id":"http://arxiv.org/abs/2503.04283v1","updated":"2025-03-06T10:09:20Z","published":"2025-03-06T10:09:20Z","title":"Explainable AI in Time-Sensitive Scenarios: Prefetched Offline\n  Explanation Model","summary":"  As predictive machine learning models become increasingly adopted and\nadvanced, their role has evolved from merely predicting outcomes to actively\nshaping them. This evolution has underscored the importance of Trustworthy AI,\nhighlighting the necessity to extend our focus beyond mere accuracy and toward\na comprehensive understanding of these models' behaviors within the specific\ncontexts of their applications. To further progress in explainability, we\nintroduce Poem, Prefetched Offline Explanation Model, a model-agnostic, local\nexplainability algorithm for image data. The algorithm generates exemplars,\ncounterexemplars and saliency maps to provide quick and effective explanations\nsuitable for time-sensitive scenarios. Leveraging an existing local algorithm,\n\\poem{} infers factual and counterfactual rules from data to create\nillustrative examples and opposite scenarios with an enhanced stability by\ndesign. A novel mechanism then matches incoming test points with an explanation\nbase and produces diverse exemplars, informative saliency maps and believable\ncounterexemplars. Experimental results indicate that Poem outperforms its\npredecessor Abele in speed and ability to generate more nuanced and varied\nexemplars alongside more insightful saliency maps and valuable\ncounterexemplars.\n","authors":["Fabio Michele Russo","Carlo Metta","Anna Monreale","Salvatore Rinzivillo","Fabio Pinelli"],"pdf_url":"https://arxiv.org/pdf/2503.04283v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04280v1","updated":"2025-03-06T10:08:44Z","published":"2025-03-06T10:08:44Z","title":"Towards Autonomous Reinforcement Learning for Real-World Robotic\n  Manipulation with Large Language Models","summary":"  Recent advancements in Large Language Models (LLMs) and Visual Language\nModels (VLMs) have significantly impacted robotics, enabling high-level\nsemantic motion planning applications. Reinforcement Learning (RL), a\ncomplementary paradigm, enables agents to autonomously optimize complex\nbehaviors through interaction and reward signals. However, designing effective\nreward functions for RL remains challenging, especially in real-world tasks\nwhere sparse rewards are insufficient and dense rewards require elaborate\ndesign. In this work, we propose Autonomous Reinforcement learning for Complex\nHumanInformed Environments (ARCHIE), an unsupervised pipeline leveraging GPT-4,\na pre-trained LLM, to generate reward functions directly from natural language\ntask descriptions. The rewards are used to train RL agents in simulated\nenvironments, where we formalize the reward generation process to enhance\nfeasibility. Additionally, GPT-4 automates the coding of task success criteria,\ncreating a fully automated, one-shot procedure for translating human-readable\ntext into deployable robot skills. Our approach is validated through extensive\nsimulated experiments on single-arm and bi-manual manipulation tasks using an\nABB YuMi collaborative robot, highlighting its practicality and effectiveness.\nTasks are demonstrated on the real robot setup.\n","authors":["Niccolò Turcato","Matteo Iovino","Aris Synodinos","Alberto Dalla Libera","Ruggero Carli","Pietro Falco"],"pdf_url":"https://arxiv.org/pdf/2503.04280v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04267v1","updated":"2025-03-06T09:56:07Z","published":"2025-03-06T09:56:07Z","title":"Prompt Programming: A Platform for Dialogue-based Computational Problem\n  Solving with Generative AI Models","summary":"  Computing students increasingly rely on generative AI tools for programming\nassistance, often without formal instruction or guidance. This highlights a\nneed to teach students how to effectively interact with AI models, particularly\nthrough natural language prompts, to generate and critically evaluate code for\nsolving computational tasks. To address this, we developed a novel platform for\nprompt programming that enables authentic dialogue-based interactions, supports\nproblems involving multiple interdependent functions, and offers on-request\nexecution of generated code. Data analysis from over 900 students in an\nintroductory programming course revealed high engagement, with the majority of\nprompts occurring within multi-turn dialogues. Problems with multiple\ninterdependent functions encouraged iterative refinement, with progression\ngraphs highlighting several common strategies. Students were highly selective\nabout the code they chose to test, suggesting that on-request execution of\ngenerated code promoted critical thinking. Given the growing importance of\nlearning dialogue-based programming with AI, we provide this tool as a publicly\naccessible resource, accompanied by a corpus of programming problems for\neducational use.\n","authors":["Victor-Alexandru Pădurean","Paul Denny","Alkis Gotovos","Adish Singla"],"pdf_url":"https://arxiv.org/pdf/2503.04267v1.pdf","comment":"Preprint of the ITiCSE'25 paper"},{"id":"http://arxiv.org/abs/2503.04262v1","updated":"2025-03-06T09:46:16Z","published":"2025-03-06T09:46:16Z","title":"Guidelines for Applying RL and MARL in Cybersecurity Applications","summary":"  Reinforcement Learning (RL) and Multi-Agent Reinforcement Learning (MARL)\nhave emerged as promising methodologies for addressing challenges in automated\ncyber defence (ACD). These techniques offer adaptive decision-making\ncapabilities in high-dimensional, adversarial environments. This report\nprovides a structured set of guidelines for cybersecurity professionals and\nresearchers to assess the suitability of RL and MARL for specific use cases,\nconsidering factors such as explainability, exploration needs, and the\ncomplexity of multi-agent coordination. It also discusses key algorithmic\napproaches, implementation challenges, and real-world constraints, such as data\nscarcity and adversarial interference. The report further outlines open\nresearch questions, including policy optimality, agent cooperation levels, and\nthe integration of MARL systems into operational cybersecurity frameworks. By\nbridging theoretical advancements and practical deployment, these guidelines\naim to enhance the effectiveness of AI-driven cyber defence strategies.\n","authors":["Vasilios Mavroudis","Gregory Palmer","Sara Farmer","Kez Smithson Whitehead","David Foster","Adam Price","Ian Miles","Alberto Caron","Stephen Pasteris"],"pdf_url":"https://arxiv.org/pdf/2503.04262v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04261v1","updated":"2025-03-06T09:44:18Z","published":"2025-03-06T09:44:18Z","title":"VirtualXAI: A User-Centric Framework for Explainability Assessment\n  Leveraging GPT-Generated Personas","summary":"  In today's data-driven era, computational systems generate vast amounts of\ndata that drive the digital transformation of industries, where Artificial\nIntelligence (AI) plays a key role. Currently, the demand for eXplainable AI\n(XAI) has increased to enhance the interpretability, transparency, and\ntrustworthiness of AI models. However, evaluating XAI methods remains\nchallenging: existing evaluation frameworks typically focus on quantitative\nproperties such as fidelity, consistency, and stability without taking into\naccount qualitative characteristics such as satisfaction and interpretability.\nIn addition, practitioners face a lack of guidance in selecting appropriate\ndatasets, AI models, and XAI methods -a major hurdle in human-AI collaboration.\nTo address these gaps, we propose a framework that integrates quantitative\nbenchmarking with qualitative user assessments through virtual personas based\non the \"Anthology\" of backstories of the Large Language Model (LLM). Our\nframework also incorporates a content-based recommender system that leverages\ndataset-specific characteristics to match new input data with a repository of\nbenchmarked datasets. This yields an estimated XAI score and provides tailored\nrecommendations for both the optimal AI model and the XAI method for a given\nscenario.\n","authors":["Georgios Makridis","Vasileios Koukos","Georgios Fatouros","Dimosthenis Kyriazis"],"pdf_url":"https://arxiv.org/pdf/2503.04261v1.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2503.04258v1","updated":"2025-03-06T09:39:36Z","published":"2025-03-06T09:39:36Z","title":"TAIL: Text-Audio Incremental Learning","summary":"  Many studies combine text and audio to capture multi-modal information but\nthey overlook the model's generalization ability on new datasets. Introducing\nnew datasets may affect the feature space of the original dataset, leading to\ncatastrophic forgetting. Meanwhile, large model parameters can significantly\nimpact training performance. To address these limitations, we introduce a novel\ntask called Text-Audio Incremental Learning (TAIL) task for text-audio\nretrieval, and propose a new method, PTAT, Prompt Tuning for Audio-Text\nincremental learning. This method utilizes prompt tuning to optimize the model\nparameters while incorporating an audio-text similarity and feature\ndistillation module to effectively mitigate catastrophic forgetting. We\nbenchmark our method and previous incremental learning methods on AudioCaps,\nClotho, BBC Sound Effects and Audioset datasets, and our method outperforms\nprevious methods significantly, particularly demonstrating stronger resistance\nto forgetting on older datasets. Compared to the full-parameters Finetune\n(Sequential) method, our model only requires 2.42\\% of its parameters,\nachieving 4.46\\% higher performance.\n","authors":["Yingfei Sun","Xu Gu","Wei Ji","Hanbin Zhao","Hao Fei","Yifang Yin","Roger Zimmermann"],"pdf_url":"https://arxiv.org/pdf/2503.04258v1.pdf","comment":"4 figures, 5 tables"},{"id":"http://arxiv.org/abs/2503.04257v1","updated":"2025-03-06T09:39:09Z","published":"2025-03-06T09:39:09Z","title":"How to Move Your Dragon: Text-to-Motion Synthesis for Large-Vocabulary\n  Objects","summary":"  Motion synthesis for diverse object categories holds great potential for 3D\ncontent creation but remains underexplored due to two key challenges: (1) the\nlack of comprehensive motion datasets that include a wide range of high-quality\nmotions and annotations, and (2) the absence of methods capable of handling\nheterogeneous skeletal templates from diverse objects. To address these\nchallenges, we contribute the following: First, we augment the Truebones Zoo\ndataset, a high-quality animal motion dataset covering over 70 species, by\nannotating it with detailed text descriptions, making it suitable for\ntext-based motion synthesis. Second, we introduce rig augmentation techniques\nthat generate diverse motion data while preserving consistent dynamics,\nenabling models to adapt to various skeletal configurations. Finally, we\nredesign existing motion diffusion models to dynamically adapt to arbitrary\nskeletal templates, enabling motion synthesis for a diverse range of objects\nwith varying structures. Experiments show that our method learns to generate\nhigh-fidelity motions from textual descriptions for diverse and even unseen\nobjects, setting a strong foundation for motion synthesis across diverse object\ncategories and skeletal templates. Qualitative results are available on this\nlink: t2m4lvo.github.io\n","authors":["Wonkwang Lee","Jongwon Jeong","Taehong Moon","Hyeon-Jong Kim","Jaehyeon Kim","Gunhee Kim","Byeong-Uk Lee"],"pdf_url":"https://arxiv.org/pdf/2503.04257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04256v1","updated":"2025-03-06T09:38:14Z","published":"2025-03-06T09:38:14Z","title":"Knowledge Retention for Continual Model-Based Reinforcement Learning","summary":"  We propose DRAGO, a novel approach for continual model-based reinforcement\nlearning aimed at improving the incremental development of world models across\na sequence of tasks that differ in their reward functions but not the state\nspace or dynamics. DRAGO comprises two key components: Synthetic Experience\nRehearsal, which leverages generative models to create synthetic experiences\nfrom past tasks, allowing the agent to reinforce previously learned dynamics\nwithout storing data, and Regaining Memories Through Exploration, which\nintroduces an intrinsic reward mechanism to guide the agent toward revisiting\nrelevant states from prior tasks. Together, these components enable the agent\nto maintain a comprehensive and continually developing world model,\nfacilitating more effective learning and adaptation across diverse\nenvironments. Empirical evaluations demonstrate that DRAGO is able to preserve\nknowledge across tasks, achieving superior performance in various continual\nlearning scenarios.\n","authors":["Yixiang Sun","Haotian Fu","Michael Littman","George Konidaris"],"pdf_url":"https://arxiv.org/pdf/2503.04256v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04249v1","updated":"2025-03-06T09:32:39Z","published":"2025-03-06T09:32:39Z","title":"How to Mitigate Overfitting in Weak-to-strong Generalization?","summary":"  Aligning powerful AI models on tasks that surpass human evaluation\ncapabilities is the central problem of \\textbf{superalignment}. To address this\nproblem, weak-to-strong generalization aims to elicit the capabilities of\nstrong models through weak supervisors and ensure that the behavior of strong\nmodels aligns with the intentions of weak supervisors without unsafe behaviors\nsuch as deception. Although weak-to-strong generalization exhibiting certain\ngeneralization capabilities, strong models exhibit significant overfitting in\nweak-to-strong generalization: Due to the strong fit ability of strong models,\nerroneous labels from weak supervisors may lead to overfitting in strong\nmodels. In addition, simply filtering out incorrect labels may lead to a\ndegeneration in question quality, resulting in a weak generalization ability of\nstrong models on hard questions. To mitigate overfitting in weak-to-strong\ngeneralization, we propose a two-stage framework that simultaneously improves\nthe quality of supervision signals and the quality of input questions.\nExperimental results in three series of large language models and two\nmathematical benchmarks demonstrate that our framework significantly improves\nPGR compared to naive weak-to-strong generalization, even achieving up to 100\\%\nPGR on some models.\n","authors":["Junhao Shi","Qinyuan Cheng","Zhaoye Fei","Yining Zheng","Qipeng Guo","Xipeng Qiu"],"pdf_url":"https://arxiv.org/pdf/2503.04249v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07076v2","updated":"2025-03-06T09:13:28Z","published":"2024-11-11T15:51:48Z","title":"StoryTeller: Improving Long Video Description through Global\n  Audio-Visual Character Identification","summary":"  Existing large vision-language models (LVLMs) are largely limited to\nprocessing short, seconds-long videos and struggle with generating coherent\ndescriptions for extended video spanning minutes or more. Long video\ndescription introduces new challenges, such as consistent character\nidentification and plot-level descriptions incorporating both visual and audio\ninformation. To address these, we figure out audio-visual character\nidentification, matching character names to each dialogue, as a key factor. We\npropose StoryTeller, a system for generating dense descriptions of long videos,\nincorporating both low-level visual concepts and high-level plot information.\nStoryTeller uses a multimodal large language model that integrates visual,\naudio, and text modalities to perform audio-visual character identification on\nminute-long video clips. The results are then fed into a LVLM to enhance\nconsistency of video description. We validate our approach on movie description\ntasks and introduce MovieStory101, a dataset with dense descriptions for\nthree-minute movie clips. To evaluate long video descriptions, we create\nStoryQA, a large set of multiple-choice questions for MovieStory101 test set.\nWe assess descriptions by inputting them into GPT-4 to answer these questions,\nusing accuracy as an automatic evaluation metric. Experiments show that\nStoryTeller outperforms all open and closed-source baselines on StoryQA,\nachieving 9.5% higher accuracy than the strongest baseline, Gemini-1.5-pro, and\ndemonstrating a +15.56% advantage in human side-by-side evaluations.\nAdditionally, incorporating audio-visual character identification from\nStoryTeller improves the performance of all video description models, with\nGemini-1.5-pro and GPT-4o showing relative improvement of 5.5% and 13.0%,\nrespectively, in accuracy on StoryQA.\n","authors":["Yichen He","Yuan Lin","Jianchao Wu","Hanchong Zhang","Yuchen Zhang","Ruicheng Le"],"pdf_url":"https://arxiv.org/pdf/2411.07076v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04231v1","updated":"2025-03-06T09:12:43Z","published":"2025-03-06T09:12:43Z","title":"One-Shot Clustering for Federated Learning","summary":"  Federated Learning (FL) is a widespread and well adopted paradigm of\ndecentralized learning that allows training one model from multiple sources\nwithout the need to directly transfer data between participating clients. Since\nits inception in 2015, it has been divided into numerous sub-fields that deal\nwith application-specific issues, be it data heterogeneity or resource\nallocation. One such sub-field, Clustered Federated Learning (CFL), is dealing\nwith the problem of clustering the population of clients into separate cohorts\nto deliver personalized models. Although few remarkable works have been\npublished in this domain, the problem is still largely unexplored, as its basic\nassumption and settings are slightly different from standard FL. In this work,\nwe present One-Shot Clustered Federated Learning (OCFL), a clustering-agnostic\nalgorithm that can automatically detect the earliest suitable moment for\nclustering. Our algorithm is based on the computation of cosine similarity\nbetween gradients of the clients and a temperature measure that detects when\nthe federated model starts to converge. We empirically evaluate our methodology\nby testing various one-shot clustering algorithms for over thirty different\ntasks on three benchmark datasets. Our experiments showcase the good\nperformance of our approach when used to perform CFL in an automated manner\nwithout the need to adjust hyperparameters.\n","authors":["Maciej Krzysztof Zuziak","Roberto Pellungrini","Salvatore Rinzivillo"],"pdf_url":"https://arxiv.org/pdf/2503.04231v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14153v3","updated":"2025-03-06T09:00:18Z","published":"2024-08-26T09:55:34Z","title":"Explaining Caption-Image Interactions in CLIP models with Second-Order\n  Attributions","summary":"  Dual encoder architectures like CLIP models map two types of inputs into a\nshared embedding space and predict similarities between them. Despite their\nsuccess, it is, however, not understood how these models compare their two\ninputs. Common first-order feature-attribution methods can only provide limited\ninsights into dual-encoders since their predictions depend on\nfeature-interactions rather than on individual features. In this paper, we\nfirst derive a second-order method enabling the attribution of predictions by\nany differentiable dual encoder onto feature-interactions between its inputs.\nSecond, we apply our method to CLIP models and show that they learn\nfine-grained correspondences between parts of captions and regions in images.\nThey match objects across input modes also account for mismatches. This\nvisual-linguistic grounding ability, however, varies heavily between object\nclasses and exhibits pronounced out-of-domain effects. We can identify\nindividual errors as well as systematic failure categories including object\ncoverage, unusual scenes and correlated contexts.\n","authors":["Lucas Möller","Pascal Tilli","Ngoc Thang Vu","Sebastian Padó"],"pdf_url":"https://arxiv.org/pdf/2408.14153v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04219v1","updated":"2025-03-06T08:54:31Z","published":"2025-03-06T08:54:31Z","title":"Quantum-Inspired Reinforcement Learning in the Presence of Epistemic\n  Ambivalence","summary":"  The complexity of online decision-making under uncertainty stems from the\nrequirement of finding a balance between exploiting known strategies and\nexploring new possibilities. Naturally, the uncertainty type plays a crucial\nrole in developing decision-making strategies that manage complexity\neffectively. In this paper, we focus on a specific form of uncertainty known as\nepistemic ambivalence (EA), which emerges from conflicting pieces of evidence\nor contradictory experiences. It creates a delicate interplay between\nuncertainty and confidence, distinguishing it from epistemic uncertainty that\ntypically diminishes with new information. Indeed, ambivalence can persist even\nafter additional knowledge is acquired. To address this phenomenon, we propose\na novel framework, called the epistemically ambivalent Markov decision process\n(EA-MDP), aiming to understand and control EA in decision-making processes.\nThis framework incorporates the concept of a quantum state from the quantum\nmechanics formalism, and its core is to assess the probability and reward of\nevery possible outcome. We calculate the reward function using quantum\nmeasurement techniques and prove the existence of an optimal policy and an\noptimal value function in the EA-MDP framework. We also propose the\nEA-epsilon-greedy Q-learning algorithm. To evaluate the impact of EA on\ndecision-making and the expedience of our framework, we study two distinct\nexperimental setups, namely the two-state problem and the lattice problem. Our\nresults show that using our methods, the agent converges to the optimal policy\nin the presence of EA.\n","authors":["Alireza Habibi","Saeed Ghoorchian","Setareh Maghsudi"],"pdf_url":"https://arxiv.org/pdf/2503.04219v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02495v2","updated":"2025-03-06T08:51:47Z","published":"2025-03-04T11:01:25Z","title":"Union of Experts: Adapting Hierarchical Routing to Equivalently\n  Decomposed Transformer","summary":"  We propose Union-of-Experts (UoE), which decomposes transformer into an\nequitant group of experts, and then implement selective routing on input data\nand experts. Our approach advances MoE design with four key innovations: (1) We\nconducted equitant expert decomposition on both MLP blocks and attention blocks\nbased on matrix partition in tensor parallelism. (2) We developed two routing\nparadigms: patch-wise data selection and expert selection, to apply routing\nacross different levels. (3) We design the architecture of UoE model, including\nSelective Multi-Head Attention (SMHA) and Union-of-MLP-Experts (UoME). (4) We\ndevelop parallel implementation of UoE's routing and computation operation, and\noptimize efficiency based on the hardware processing analysis. The experiments\ndemonstrate that the UoE model surpass Full Attention, state-of-art MoEs and\nefficient transformers (including the model architecture of recently proposed\nDeepSeek-V3) in several tasks across image and natural language domains. In\nlanguage modeling tasks, we achieve an average reduction of 2.38 in perplexity\ncompared to the best-performed MoE method with an average of 76% FLOPs. In Long\nRange Arena benchmark, we recorded an average score that is at least 0.68%\nhigher than all comparison models including Full Attention, MoEs, and\ntransformer variants, with only 50% FLOPs of the best MoE method. In image\nclassification, our model yielded an average accuracy improvement of 1.75% than\nthe best model while maintaining comparable FLOPs. The source codes are\navailable at https://github.com/YujiaoYang-work/UoE.\n","authors":["Yujiao Yang","Jing Lian","Linhui Li"],"pdf_url":"https://arxiv.org/pdf/2503.02495v2.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2410.07009v2","updated":"2025-03-06T08:51:05Z","published":"2024-10-09T15:52:48Z","title":"Pap2Pat: Benchmarking Outline-Guided Long-Text Patent Generation with\n  Patent-Paper Pairs","summary":"  Dealing with long and highly complex technical text is a challenge for Large\nLanguage Models (LLMs), which still have to unfold their potential in\nsupporting expensive and timeintensive processes like patent drafting. Within\npatents, the description constitutes more than 90% of the document on average.\nYet, its automatic generation remains understudied. When drafting patent\napplications, patent attorneys typically receive invention reports (IRs), which\nare usually confidential, hindering research on LLM-supported patent drafting.\nOften, prepublication research papers serve as IRs. We leverage this duality to\nbuild PAP2PAT, an open and realistic benchmark for patent drafting consisting\nof 1.8k patent-paper pairs describing the same inventions. To address the\ncomplex longdocument patent generation task, we propose chunk-based\noutline-guided generation using the research paper as invention specification.\nOur extensive evaluation using PAP2PAT and a human case study show that LLMs\ncan effectively leverage information from the paper, but still struggle to\nprovide the necessary level of detail. Fine-tuning leads to more patent-style\nlanguage, but also to more hallucination. We release our data and code\nhttps://github.com/boschresearch/Pap2Pat.\n","authors":["Valentin Knappich","Simon Razniewski","Anna Hätty","Annemarie Friedrich"],"pdf_url":"https://arxiv.org/pdf/2410.07009v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04201v1","updated":"2025-03-06T08:28:44Z","published":"2025-03-06T08:28:44Z","title":"Knowledge-Decoupled Synergetic Learning: An MLLM based Collaborative\n  Approach to Few-shot Multimodal Dialogue Intention Recognition","summary":"  Few-shot multimodal dialogue intention recognition is a critical challenge in\nthe e-commerce domainn. Previous methods have primarily enhanced model\nclassification capabilities through post-training techniques. However, our\nanalysis reveals that training for few-shot multimodal dialogue intention\nrecognition involves two interconnected tasks, leading to a seesaw effect in\nmulti-task learning. This phenomenon is attributed to knowledge interference\nstemming from the superposition of weight matrix updates during the training\nprocess. To address these challenges, we propose Knowledge-Decoupled Synergetic\nLearning (KDSL), which mitigates these issues by utilizing smaller models to\ntransform knowledge into interpretable rules, while applying the post-training\nof larger models. By facilitating collaboration between the large and small\nmultimodal large language models for prediction, our approach demonstrates\nsignificant improvements. Notably, we achieve outstanding results on two real\nTaobao datasets, with enhancements of 6.37\\% and 6.28\\% in online weighted F1\nscores compared to the state-of-the-art method, thereby validating the efficacy\nof our framework.\n","authors":["Bin Chen","Yu Zhang","Hongfei Ye","Ziyi Huang","Hongyang Chen"],"pdf_url":"https://arxiv.org/pdf/2503.04201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04199v1","updated":"2025-03-06T08:27:51Z","published":"2025-03-06T08:27:51Z","title":"MASTER: Multimodal Segmentation with Text Prompts","summary":"  RGB-Thermal fusion is a potential solution for various weather and light\nconditions in challenging scenarios. However, plenty of studies focus on\ndesigning complex modules to fuse different modalities. With the widespread\napplication of large language models (LLMs), valuable information can be more\neffectively extracted from natural language. Therefore, we aim to leverage the\nadvantages of large language models to design a structurally simple and highly\nadaptable multimodal fusion model architecture. We proposed MultimodAl\nSegmentation with TExt PRompts (MASTER) architecture, which integrates LLM into\nthe fusion of RGB-Thermal multimodal data and allows complex query text to\nparticipate in the fusion process. Our model utilizes a dual-path structure to\nextract information from different modalities of images. Additionally, we\nemploy LLM as the core module for multimodal fusion, enabling the model to\ngenerate learnable codebook tokens from RGB, thermal images, and textual\ninformation. A lightweight image decoder is used to obtain semantic\nsegmentation results. The proposed MASTER performs exceptionally well in\nbenchmark tests across various automated driving scenarios, yielding promising\nresults.\n","authors":["Fuyang Liu","Shun Lu","Jilin Mei","Yu Hu"],"pdf_url":"https://arxiv.org/pdf/2503.04199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.12106v3","updated":"2025-03-06T08:18:50Z","published":"2024-09-18T16:26:22Z","title":"Measuring Human and AI Values Based on Generative Psychometrics with\n  Large Language Models","summary":"  Human values and their measurement are long-standing interdisciplinary\ninquiry. Recent advances in AI have sparked renewed interest in this area, with\nlarge language models (LLMs) emerging as both tools and subjects of value\nmeasurement. This work introduces Generative Psychometrics for Values (GPV), an\nLLM-based, data-driven value measurement paradigm, theoretically grounded in\ntext-revealed selective perceptions. The core idea is to dynamically parse\nunstructured texts into perceptions akin to static stimuli in traditional\npsychometrics, measure the value orientations they reveal, and aggregate the\nresults. Applying GPV to human-authored blogs, we demonstrate its stability,\nvalidity, and superiority over prior psychological tools. Then, extending GPV\nto LLM value measurement, we advance the current art with 1) a psychometric\nmethodology that measures LLM values based on their scalable and free-form\noutputs, enabling context-specific measurement; 2) a comparative analysis of\nmeasurement paradigms, indicating response biases of prior methods; and 3) an\nattempt to bridge LLM values and their safety, revealing the predictive power\nof different value systems and the impacts of various values on LLM safety.\nThrough interdisciplinary efforts, we aim to leverage AI for next-generation\npsychometrics and psychometrics for value-aligned AI.\n","authors":["Haoran Ye","Yuhang Xie","Yuanyi Ren","Hanjun Fang","Xin Zhang","Guojie Song"],"pdf_url":"https://arxiv.org/pdf/2409.12106v3.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2503.04184v1","updated":"2025-03-06T07:53:24Z","published":"2025-03-06T07:53:24Z","title":"Large-Scale AI in Telecom: Charting the Roadmap for Innovation,\n  Scalability, and Enhanced Digital Experiences","summary":"  This white paper discusses the role of large-scale AI in the\ntelecommunications industry, with a specific focus on the potential of\ngenerative AI to revolutionize network functions and user experiences,\nespecially in the context of 6G systems. It highlights the development and\ndeployment of Large Telecom Models (LTMs), which are tailored AI models\ndesigned to address the complex challenges faced by modern telecom networks.\nThe paper covers a wide range of topics, from the architecture and deployment\nstrategies of LTMs to their applications in network management, resource\nallocation, and optimization. It also explores the regulatory, ethical, and\nstandardization considerations for LTMs, offering insights into their future\nintegration into telecom infrastructure. The goal is to provide a comprehensive\nroadmap for the adoption of LTMs to enhance scalability, performance, and\nuser-centric innovation in telecom networks.\n","authors":["Adnan Shahid","Adrian Kliks","Ahmed Al-Tahmeesschi","Ahmed Elbakary","Alexandros Nikou","Ali Maatouk","Ali Mokh","Amirreza Kazemi","Antonio De Domenico","Athanasios Karapantelakis","Bo Cheng","Bo Yang","Bohao Wang","Carlo Fischione","Chao Zhang","Chaouki Ben Issaid","Chau Yuen","Chenghui Peng","Chongwen Huang","Christina Chaccour","Christo Kurisummoottil Thomas","Dheeraj Sharma","Dimitris Kalogiros","Dusit Niyato","Eli De Poorter","Elissa Mhanna","Emilio Calvanese Strinati","Faouzi Bader","Fathi Abdeldayem","Fei Wang","Fenghao Zhu","Gianluca Fontanesi","Giovanni Geraci","Haibo Zhou","Hakimeh Purmehdi","Hamed Ahmadi","Hang Zou","Hongyang Du","Hoon Lee","Howard H. Yang","Iacopo Poli","Igor Carron","Ilias Chatzistefanidis","Inkyu Lee","Ioannis Pitsiorlas","Jaron Fontaine","Jiajun Wu","Jie Zeng","Jinan Li","Jinane Karam","Johny Gemayel","Juan Deng","Julien Frison","Kaibin Huang","Kehai Qiu","Keith Ball","Kezhi Wang","Kun Guo","Leandros Tassiulas","Lecorve Gwenole","Liexiang Yue","Lina Bariah","Louis Powell","Marcin Dryjanski","Maria Amparo Canaveras Galdon","Marios Kountouris","Maryam Hafeez","Maxime Elkael","Mehdi Bennis","Mehdi Boudjelli","Meiling Dai","Merouane Debbah","Michele Polese","Mohamad Assaad","Mohamed Benzaghta","Mohammad Al Refai","Moussab Djerrab","Mubeen Syed","Muhammad Amir","Na Yan","Najla Alkaabi","Nan Li","Nassim Sehad","Navid Nikaein","Omar Hashash","Pawel Sroka","Qianqian Yang","Qiyang Zhao","Rasoul Nikbakht Silab","Rex Ying","Roberto Morabito","Rongpeng Li","Ryad Madi","Salah Eddine El Ayoubi","Salvatore D'Oro","Samson Lasaulce","Serveh Shalmashi","Sige Liu","Sihem Cherrared","Swarna Bindu Chetty","Swastika Dutta","Syed A. R. Zaidi","Tianjiao Chen","Timothy Murphy","Tommaso Melodia","Tony Q. S. Quek","Vishnu Ram","Walid Saad","Wassim Hamidouche","Weilong Chen","Xiaoou Liu","Xiaoxue Yu","Xijun Wang","Xingyu Shang","Xinquan Wang","Xuelin Cao","Yang Su","Yanping Liang","Yansha Deng","Yifan Yang","Yingping Cui","Yu Sun","Yuxuan Chen","Yvan Pointurier","Zeinab Nehme","Zeinab Nezami","Zhaohui Yang","Zhaoyang Zhang","Zhe Liu","Zhenyu Yang","Zhu Han","Zhuang Zhou","Zihan Chen","Zirui Chen","Zitao Shuai"],"pdf_url":"https://arxiv.org/pdf/2503.04184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04183v1","updated":"2025-03-06T07:52:20Z","published":"2025-03-06T07:52:20Z","title":"CrowdHMTware: A Cross-level Co-adaptation Middleware for Context-aware\n  Mobile DL Deployment","summary":"  There are many deep learning (DL) powered mobile and wearable applications\ntoday continuously and unobtrusively sensing the ambient surroundings to\nenhance all aspects of human lives.To enable robust and private mobile sensing,\nDL models are often deployed locally on resource-constrained mobile devices\nusing techniques such as model compression or offloading.However, existing\nmethods, either front-end algorithm level (i.e. DL model\ncompression/partitioning) or back-end scheduling level (i.e. operator/resource\nscheduling), cannot be locally online because they require offline retraining\nto ensure accuracy or rely on manually pre-defined strategies, struggle with\ndynamic adaptability.The primary challenge lies in feeding back runtime\nperformance from the back-end level to the front-end level optimization\ndecision. Moreover, the adaptive mobile DL model porting middleware with\ncross-level co-adaptation is less explored, particularly in mobile environments\nwith diversity and dynamics. In response, we introduce CrowdHMTware, a dynamic\ncontext-adaptive DL model deployment middleware for heterogeneous mobile\ndevices. It establishes an automated adaptation loop between cross-level\nfunctional components, i.e. elastic inference, scalable offloading, and\nmodel-adaptive engine, enhancing scalability and adaptability. Experiments with\nfour typical tasks across 15 platforms and a real-world case study demonstrate\nthat CrowdHMTware can effectively scale DL model, offloading, and engine\nactions across diverse platforms and tasks. It hides run-time system issues\nfrom developers, reducing the required developer expertise.\n","authors":["Sicong Liu","Bin Guo","Shiyan Luo","Yuzhan Wang","Hao Luo","Cheng Fang","Yuan Xu","Ke Ma","Yao Li","Zhiwen Yu"],"pdf_url":"https://arxiv.org/pdf/2503.04183v1.pdf","comment":"This paper is accepted by IEEE Transactions on Mobile Computing"},{"id":"http://arxiv.org/abs/2411.11006v2","updated":"2025-03-06T07:50:21Z","published":"2024-11-17T09:01:55Z","title":"BackdoorMBTI: A Backdoor Learning Multimodal Benchmark Tool Kit for\n  Backdoor Defense Evaluation","summary":"  Over the past few years, the emergence of backdoor attacks has presented\nsignificant challenges to deep learning systems, allowing attackers to insert\nbackdoors into neural networks. When data with a trigger is processed by a\nbackdoor model, it can lead to mispredictions targeted by attackers, whereas\nnormal data yields regular results. The scope of backdoor attacks is expanding\nbeyond computer vision and encroaching into areas such as natural language\nprocessing and speech recognition. Nevertheless, existing backdoor defense\nmethods are typically tailored to specific data modalities, restricting their\napplication in multimodal contexts. While multimodal learning proves highly\napplicable in facial recognition, sentiment analysis, action recognition,\nvisual question answering, the security of these models remains a crucial\nconcern. Specifically, there are no existing backdoor benchmarks targeting\nmultimodal applications or related tasks.\n  In order to facilitate the research in multimodal backdoor, we introduce\nBackdoorMBTI, the first backdoor learning toolkit and benchmark designed for\nmultimodal evaluation across three representative modalities from eleven\ncommonly used datasets. BackdoorMBTI provides a systematic backdoor learning\npipeline, encompassing data processing, data poisoning, backdoor training, and\nevaluation. The generated poison datasets and backdoor models enable detailed\nevaluation of backdoor defenses. Given the diversity of modalities,\nBackdoorMBTI facilitates systematic evaluation across different data types.\nFurthermore, BackdoorMBTI offers a standardized approach to handling practical\nfactors in backdoor learning, such as issues related to data quality and\nerroneous labels. We anticipate that BackdoorMBTI will expedite future research\nin backdoor defense methods within a multimodal context. Code is available at\nhttps://github.com/SJTUHaiyangYu/BackdoorMBTI.\n","authors":["Haiyang Yu","Tian Xie","Jiaping Gui","Pengyang Wang","Ping Yi","Yue Wu"],"pdf_url":"https://arxiv.org/pdf/2411.11006v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04176v1","updated":"2025-03-06T07:44:17Z","published":"2025-03-06T07:44:17Z","title":"TIMER: Temporal Instruction Modeling and Evaluation for Longitudinal\n  Clinical Records","summary":"  Large language models (LLMs) have emerged as promising tools for assisting in\nmedical tasks, yet processing Electronic Health Records (EHRs) presents unique\nchallenges due to their longitudinal nature. While LLMs' capabilities to\nperform medical tasks continue to improve, their ability to reason over\ntemporal dependencies across multiple patient visits and time frames remains\nunexplored. We introduce TIMER (Temporal Instruction Modeling and Evaluation\nfor Longitudinal Clinical Records), a framework that incorporate\ninstruction-response pairs grounding to different parts of a patient's record\nas a critical dimension in both instruction evaluation and tuning for\nlongitudinal clinical records. We develop TIMER-Bench, the first time-aware\nbenchmark that evaluates temporal reasoning capabilities over longitudinal\nEHRs, as well as TIMER-Instruct, an instruction-tuning methodology for LLMs to\nlearn reasoning over time. We demonstrate that models fine-tuned with\nTIMER-Instruct improve performance by 7.3% on human-generated benchmarks and\n9.2% on TIMER-Bench, indicating that temporal instruction-tuning improves model\nperformance for reasoning over EHR.\n","authors":["Hejie Cui","Alyssa Unell","Bowen Chen","Jason Alan Fries","Emily Alsentzer","Sanmi Koyejo","Nigam Shah"],"pdf_url":"https://arxiv.org/pdf/2503.04176v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2410.06665v3","updated":"2025-03-06T07:41:25Z","published":"2024-10-09T08:19:31Z","title":"Revisiting Multi-Permutation Equivariance through the Lens of\n  Irreducible Representations","summary":"  This paper explores the characterization of equivariant linear layers for\nrepresentations of permutations and related groups. Unlike traditional\napproaches, which address these problems using parameter-sharing, we consider\nan alternative methodology based on irreducible representations and Schur's\nlemma. Using this methodology, we obtain an alternative derivation for existing\nmodels like DeepSets, 2-IGN graph equivariant networks, and Deep Weight Space\n(DWS) networks. The derivation for DWS networks is significantly simpler than\nthat of previous results.\n  Next, we extend our approach to unaligned symmetric sets, where equivariance\nto the wreath product of groups is required. Previous works have addressed this\nproblem in a rather restrictive setting, in which almost all wreath equivariant\nlayers are Siamese. In contrast, we give a full characterization of layers in\nthis case and show that there is a vast number of additional non-Siamese layers\nin some settings. We also show empirically that these additional non-Siamese\nlayers can improve performance in tasks like graph anomaly detection, weight\nspace alignment, and learning Wasserstein distances. Our code is available at\n\\href{https://github.com/yonatansverdlov/Irreducible-Representations-of-Deep-Weight-Spaces}{GitHub}.\n","authors":["Yonatan Sverdlov","Ido Springer","Nadav Dym"],"pdf_url":"https://arxiv.org/pdf/2410.06665v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04170v1","updated":"2025-03-06T07:36:06Z","published":"2025-03-06T07:36:06Z","title":"Towards Intelligent Transportation with Pedestrians and Vehicles\n  In-the-Loop: A Surveillance Video-Assisted Federated Digital Twin Framework","summary":"  In intelligent transportation systems (ITSs), incorporating pedestrians and\nvehicles in-the-loop is crucial for developing realistic and safe traffic\nmanagement solutions. However, there is falls short of simulating complex\nreal-world ITS scenarios, primarily due to the lack of a digital twin\nimplementation framework for characterizing interactions between pedestrians\nand vehicles at different locations in different traffic environments. In this\narticle, we propose a surveillance video assisted federated digital twin\n(SV-FDT) framework to empower ITSs with pedestrians and vehicles in-the-loop.\nSpecifically, SVFDT builds comprehensive pedestrian-vehicle interaction models\nby leveraging multi-source traffic surveillance videos. Its architecture\nconsists of three layers: (i) the end layer, which collects traffic\nsurveillance videos from multiple sources; (ii) the edge layer, responsible for\nsemantic segmentation-based visual understanding, twin agent-based interaction\nmodeling, and local digital twin system (LDTS) creation in local regions; and\n(iii) the cloud layer, which integrates LDTSs across different regions to\nconstruct a global DT model in realtime. We analyze key design requirements and\nchallenges and present core guidelines for SVFDT's system implementation. A\ntestbed evaluation demonstrates its effectiveness in optimizing traffic\nmanagement. Comparisons with traditional terminal-server frameworks highlight\nSV-FDT's advantages in mirroring delays, recognition accuracy, and subjective\nevaluation. Finally, we identify some open challenges and discuss future\nresearch directions.\n","authors":["Xiaolong Li","Jianhao Wei","Haidong Wang","Li Dong","Ruoyang Chen","Changyan Yi","Jun Cai","Dusit Niyato"," Xuemin"," Shen"],"pdf_url":"https://arxiv.org/pdf/2503.04170v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.02318v3","updated":"2025-03-06T07:29:44Z","published":"2024-04-18T00:20:48Z","title":"Autoformalizing Natural Language to First-Order Logic: A Case Study in\n  Logical Fallacy Detection","summary":"  Translating natural language into formal language such as First-Order Logic\n(FOL) is a foundational challenge in NLP with wide-ranging applications in\nautomated reasoning, misinformation tracking, and knowledge validation. In this\npaper, we introduce Natural Language to First-Order Logic (NL2FOL), a framework\nto autoformalize natural language to FOL step by step using Large Language\nModels (LLMs). Our approach addresses key challenges in this translation\nprocess, including the integration of implicit background knowledge. By\nleveraging structured representations generated by NL2FOL, we use\nSatisfiability Modulo Theory (SMT) solvers to reason about the logical validity\nof natural language statements. We present logical fallacy detection as a case\nstudy to evaluate the efficacy of NL2FOL. Being neurosymbolic, our approach\nalso provides interpretable insights into the reasoning process and\ndemonstrates robustness without requiring model fine-tuning or labeled training\ndata. Our framework achieves strong performance on multiple datasets. On the\nLOGIC dataset, NL2FOL achieves an F1-score of 78%, while generalizing\neffectively to the LOGICCLIMATE dataset with an F1-score of 80%.\n","authors":["Abhinav Lalwani","Tasha Kim","Lovish Chopra","Christopher Hahn","Zhijing Jin","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2405.02318v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04167v1","updated":"2025-03-06T07:29:33Z","published":"2025-03-06T07:29:33Z","title":"The Role of Visual Modality in Multimodal Mathematical Reasoning:\n  Challenges and Insights","summary":"  Recent research has increasingly focused on multimodal mathematical\nreasoning, particularly emphasizing the creation of relevant datasets and\nbenchmarks. Despite this, the role of visual information in reasoning has been\nunderexplored. Our findings show that existing multimodal mathematical models\nminimally leverage visual information, and model performance remains largely\nunaffected by changes to or removal of images in the dataset. We attribute this\nto the dominance of textual information and answer options that inadvertently\nguide the model to correct answers. To improve evaluation methods, we introduce\nthe HC-M3D dataset, specifically designed to require image reliance for\nproblem-solving and to challenge models with similar, yet distinct, images that\nchange the correct answer. In testing leading models, their failure to detect\nthese subtle visual differences suggests limitations in current visual\nperception capabilities. Additionally, we observe that the common approach of\nimproving general VQA capabilities by combining various types of image encoders\ndoes not contribute to math reasoning performance. This finding also presents a\nchallenge to enhancing visual reliance during math reasoning. Our benchmark and\ncode would be available at\n\\href{https://github.com/Yufang-Liu/visual_modality_role}{https://github.com/Yufang-Liu/visual\\_modality\\_role}.\n","authors":["Yufang Liu","Yao Du","Tao Ji","Jianing Wang","Yang Liu","Yuanbin Wu","Aimin Zhou","Mengdi Zhang","Xunliang Cai"],"pdf_url":"https://arxiv.org/pdf/2503.04167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04162v1","updated":"2025-03-06T07:25:19Z","published":"2025-03-06T07:25:19Z","title":"Semantic Retrieval Augmented Contrastive Learning for Sequential\n  Recommendation","summary":"  Sequential recommendation aims to model user preferences based on historical\nbehavior sequences, which is crucial for various online platforms. Data\nsparsity remains a significant challenge in this area as most users have\nlimited interactions and many items receive little attention. To mitigate this\nissue, contrastive learning has been widely adopted. By constructing positive\nsample pairs from the data itself and maximizing their agreement in the\nembedding space,it can leverage available data more effectively. Constructing\nreasonable positive sample pairs is crucial for the success of contrastive\nlearning. However, current approaches struggle to generate reliable positive\npairs as they either rely on representations learned from inherently sparse\ncollaborative signals or use random perturbations which introduce significant\nuncertainty. To address these limitations, we propose a novel approach named\nSemantic Retrieval Augmented Contrastive Learning (SRA-CL), which leverages\nsemantic information to improve the reliability of contrastive samples. SRA-CL\ncomprises two main components: (1) Cross-Sequence Contrastive Learning via User\nSemantic Retrieval, which utilizes large language models (LLMs) to understand\ndiverse user preferences and retrieve semantically similar users to form\nreliable positive samples through a learnable sample synthesis method; and (2)\nIntra-Sequence Contrastive Learning via Item Semantic Retrieval, which employs\nLLMs to comprehend items and retrieve similar items to perform semantic-based\nitem substitution, thereby creating semantically consistent augmented views for\ncontrastive learning. SRA-CL is plug-and-play and can be integrated into\nstandard sequential recommendation models. Extensive experiments on four public\ndatasets demonstrate the effectiveness and generalizability of the proposed\napproach.\n","authors":["Ziqiang Cui","Yunpeng Weng","Xing Tang","Xiaokun Zhang","Dugang Liu","Shiwei Li","Peiyang Liu","Bowei He","Weihong Luo","Xiuqiang He","Chen Ma"],"pdf_url":"https://arxiv.org/pdf/2503.04162v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04160v1","updated":"2025-03-06T07:23:44Z","published":"2025-03-06T07:23:44Z","title":"Unseen Fake News Detection Through Casual Debiasing","summary":"  The widespread dissemination of fake news on social media poses significant\nrisks, necessitating timely and accurate detection. However, existing methods\nstruggle with unseen news due to their reliance on training data from past\nevents and domains, leaving the challenge of detecting novel fake news largely\nunresolved. To address this, we identify biases in training data tied to\nspecific domains and propose a debiasing solution FNDCD. Originating from\ncausal analysis, FNDCD employs a reweighting strategy based on classification\nconfidence and propagation structure regularization to reduce the influence of\ndomain-specific biases, enhancing the detection of unseen fake news.\nExperiments on real-world datasets with non-overlapping news domains\ndemonstrate FNDCD's effectiveness in improving generalization across domains.\n","authors":["Shuzhi Gong","Richard Sinnott","Jianzhong Qi","Cecile Paris"],"pdf_url":"https://arxiv.org/pdf/2503.04160v1.pdf","comment":"2025 The Web Conference, 6 pages, 4 figures"},{"id":"http://arxiv.org/abs/2502.13681v2","updated":"2025-03-06T07:17:09Z","published":"2025-02-19T12:51:35Z","title":"An LLM-based Agent for Reliable Docker Environment Configuration","summary":"  Environment configuration is a critical yet time-consuming step in software\ndevelopment, especially when dealing with unfamiliar code repositories. While\nLarge Language Models (LLMs) demonstrate the potential to accomplish software\nengineering tasks, existing methods for environment configuration often rely on\nmanual efforts or fragile scripts, leading to inefficiencies and unreliable\noutcomes. We introduce Repo2Run, the first LLM-based agent designed to fully\nautomate environment configuration and generate executable Dockerfiles for\narbitrary Python repositories. We address two major challenges: (1) enabling\nthe LLM agent to configure environments within isolated Docker containers, and\n(2) ensuring the successful configuration process is recorded and accurately\ntransferred to a Dockerfile without error. To achieve this, we propose atomic\nconfiguration synthesis, featuring a dual-environment architecture (internal\nand external environment) with a rollback mechanism to prevent environment\n\"pollution\" from failed commands, guaranteeing atomic execution (execute fully\nor not at all) and a Dockerfile generator to transfer successful configuration\nsteps into runnable Dockerfiles. We evaluate Repo2Run~on our proposed benchmark\nof 420 recent Python repositories with unit tests, where it achieves an 86.0%\nsuccess rate, outperforming the best baseline by 63.9%. Repo2Run is available\nat https://github.com/bytedance/Repo2Run.\n","authors":["Ruida Hu","Chao Peng","Xinchen Wang","Cuiyun Gao"],"pdf_url":"https://arxiv.org/pdf/2502.13681v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04154v1","updated":"2025-03-06T07:02:13Z","published":"2025-03-06T07:02:13Z","title":"CA-W3D: Leveraging Context-Aware Knowledge for Weakly Supervised\n  Monocular 3D Detection","summary":"  Weakly supervised monocular 3D detection, while less annotation-intensive,\noften struggles to capture the global context required for reliable 3D\nreasoning. Conventional label-efficient methods focus on object-centric\nfeatures, neglecting contextual semantic relationships that are critical in\ncomplex scenes. In this work, we propose a Context-Aware Weak Supervision for\nMonocular 3D object detection, namely CA-W3D, to address this limitation in a\ntwo-stage training paradigm. Specifically, we first introduce a pre-training\nstage employing Region-wise Object Contrastive Matching (ROCM), which aligns\nregional object embeddings derived from a trainable monocular 3D encoder and a\nfrozen open-vocabulary 2D visual grounding model. This alignment encourages the\nmonocular encoder to discriminate scene-specific attributes and acquire richer\ncontextual knowledge. In the second stage, we incorporate a pseudo-label\ntraining process with a Dual-to-One Distillation (D2OD) mechanism, which\neffectively transfers contextual priors into the monocular encoder while\npreserving spatial fidelity and maintaining computational efficiency during\ninference. Extensive experiments conducted on the public KITTI benchmark\ndemonstrate the effectiveness of our approach, surpassing the SoTA method over\nall metrics, highlighting the importance of contextual-aware knowledge in\nweakly-supervised monocular 3D detection.\n","authors":["Chupeng Liu","Runkai Zhao","Weidong Cai"],"pdf_url":"https://arxiv.org/pdf/2503.04154v1.pdf","comment":"The paper includes 8 pages, 6 figures and 4 tables"},{"id":"http://arxiv.org/abs/2503.04153v1","updated":"2025-03-06T07:01:36Z","published":"2025-03-06T07:01:36Z","title":"KidneyTalk-open: No-code Deployment of a Private Large Language Model\n  with Medical Documentation-Enhanced Knowledge Database for Kidney Disease","summary":"  Privacy-preserving medical decision support for kidney disease requires\nlocalized deployment of large language models (LLMs) while maintaining clinical\nreasoning capabilities. Current solutions face three challenges: 1) Cloud-based\nLLMs pose data security risks; 2) Local model deployment demands technical\nexpertise; 3) General LLMs lack mechanisms to integrate medical knowledge.\nRetrieval-augmented systems also struggle with medical document processing and\nclinical usability. We developed KidneyTalk-open, a desktop system integrating\nthree technical components: 1) No-code deployment of state-of-the-art (SOTA)\nopen-source LLMs (such as DeepSeek-r1, Qwen2.5) via local inference engine; 2)\nMedical document processing pipeline combining context-aware chunking and\nintelligent filtering; 3) Adaptive Retrieval and Augmentation Pipeline (AddRep)\nemploying agents collaboration for improving the recall rate of medical\ndocuments. A graphical interface was designed to enable clinicians to manage\nmedical documents and conduct AI-powered consultations without technical\nexpertise. Experimental validation on 1,455 challenging nephrology exam\nquestions demonstrates AddRep's effectiveness: achieving 29.1% accuracy (+8.1%\nover baseline) with intelligent knowledge integration, while maintaining\nrobustness through 4.9% rejection rate to suppress hallucinations. Comparative\ncase studies with the mainstream products (AnythingLLM, Chatbox, GPT4ALL)\ndemonstrate KidneyTalk-open's superior performance in real clinical query.\nKidneyTalk-open represents the first no-code medical LLM system enabling secure\ndocumentation-enhanced medical Q&A on desktop. Its designs establishes a new\nframework for privacy-sensitive clinical AI applications. The system\nsignificantly lowers technical barriers while improving evidence traceability,\nenabling more medical staff or patients to use SOTA open-source LLMs\nconveniently.\n","authors":["Yongchao Long","Chao Yang","Gongzheng Tang","Jinwei Wang","Zhun Sui","Yuxi Zhou","Shenda Hong","Luxia Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.04153v1.pdf","comment":"Corresponding authors: zhanglx@bjmu.edu.cn; joy_yuxi@pku.edu.cn;\n  hongshenda@pku.edu.cn"},{"id":"http://arxiv.org/abs/2503.04151v1","updated":"2025-03-06T07:01:08Z","published":"2025-03-06T07:01:08Z","title":"Robust Multi-View Learning via Representation Fusion of Sample-Level\n  Attention and Alignment of Simulated Perturbation","summary":"  Recently, multi-view learning (MVL) has garnered significant attention due to\nits ability to fuse discriminative information from multiple views. However,\nreal-world multi-view datasets are often heterogeneous and imperfect, which\nusually makes MVL methods designed for specific combinations of views lack\napplication potential and limits their effectiveness. To address this issue, we\npropose a novel robust MVL method (namely RML) with simultaneous representation\nfusion and alignment. Specifically, we introduce a simple yet effective\nmulti-view transformer fusion network where we transform heterogeneous\nmulti-view data into homogeneous word embeddings, and then integrate multiple\nviews by the sample-level attention mechanism to obtain a fused representation.\nFurthermore, we propose a simulated perturbation based multi-view contrastive\nlearning framework that dynamically generates the noise and unusable\nperturbations for simulating imperfect data conditions. The simulated noisy and\nunusable data obtain two distinct fused representations, and we utilize\ncontrastive learning to align them for learning discriminative and robust\nrepresentations. Our RML is self-supervised and can also be applied for\ndownstream tasks as a regularization. In experiments, we employ it in\nunsupervised multi-view clustering, noise-label classification, and as a\nplug-and-play module for cross-modal hashing retrieval. Extensive comparison\nexperiments and ablation studies validate the effectiveness of RML.\n","authors":["Jie Xu","Na Zhao","Gang Niu","Masashi Sugiyama","Xiaofeng Zhu"],"pdf_url":"https://arxiv.org/pdf/2503.04151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04150v1","updated":"2025-03-06T06:59:09Z","published":"2025-03-06T06:59:09Z","title":"Ticktack : Long Span Temporal Alignment of Large Language Models\n  Leveraging Sexagenary Cycle Time Expression","summary":"  Large language models (LLMs) suffer from temporal misalignment issues\nespecially across long span of time. The issue arises from knowing that LLMs\nare trained on large amounts of data where temporal information is rather\nsparse over long times, such as thousands of years, resulting in insufficient\nlearning or catastrophic forgetting by the LLMs. This paper proposes a\nmethodology named \"Ticktack\" for addressing the LLM's long-time span\nmisalignment in a yearly setting. Specifically, we first propose to utilize the\nsexagenary year expression instead of the Gregorian year expression employed by\nLLMs, achieving a more uniform distribution in yearly granularity. Then, we\nemploy polar coordinates to model the sexagenary cycle of 60 terms and the year\norder within each term, with additional temporal encoding to ensure LLMs\nunderstand them. Finally, we present a temporal representational alignment\napproach for post-training LLMs that effectively distinguishes time points with\nrelevant knowledge, hence improving performance on time-related tasks,\nparticularly over a long period. We also create a long time span benchmark for\nevaluation. Experimental results prove the effectiveness of our proposal.\n","authors":["Xue Han","Qian Hu","Yitong Wang","Wenchun Gao","Lianlian Zhang","Qing Wang","Lijun Mei","Chao Deng","Junlan Feng"],"pdf_url":"https://arxiv.org/pdf/2503.04150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04149v1","updated":"2025-03-06T06:56:59Z","published":"2025-03-06T06:56:59Z","title":"Dynamic Benchmarking of Reasoning Capabilities in Code Large Language\n  Models Under Data Contamination","summary":"  The rapid evolution of code largelanguage models underscores the need for\neffective and transparent benchmarking of their reasoning capabilities.\nHowever, the current benchmarking approach heavily depends on publicly\navailable, human-created datasets. The widespread use of these fixed benchmark\ndatasets makes the benchmarking process to be static and thus particularly\nsusceptible to data contamination, an unavoidable consequence of the extensive\ndata collection processes used to train Code LLMs. Existing approaches that\naddress data contamination often suffer from human effort limitations and\nimbalanced problem complexity. To tackle these challenges, we propose \\tool, a\nnovel benchmarking suite for evaluating Code LLMs under potential data\ncontamination. Given a seed programming problem, \\tool employs multiple agents\nto extract and modify the context without altering the core logic, generating\nsemantically equivalent variations. We introduce a dynamic data generation\nmethods and conduct empirical studies on two seed datasets across 21 Code LLMs.\nResults show that \\tool effectively benchmarks reasoning capabilities under\ncontamination risks while generating diverse problem sets to ensure consistent\nand reliable evaluations.\n","authors":["Simin Chen","Pranav Pusarla","Baishakhi Ray"],"pdf_url":"https://arxiv.org/pdf/2503.04149v1.pdf","comment":"https://codekaleidoscope.github.io/dycodeeval.html"},{"id":"http://arxiv.org/abs/2502.12767v2","updated":"2025-03-06T06:41:40Z","published":"2025-02-18T11:31:52Z","title":"R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on\n  Knowledge Graphs","summary":"  Recent studies have combined Large Language Models (LLMs) with Knowledge\nGraphs (KGs) to enhance reasoning, improving inference accuracy without\nadditional training while mitigating hallucination. However, existing\nframeworks are often rigid, struggling to adapt to KG or task changes. They\nalso rely heavily on powerful LLMs for reliable (i.e., trustworthy) reasoning.\nTo address this, We introduce R2-KG, a plug-and-play, dual-agent framework that\nseparates reasoning into two roles: an Operator (a low-capacity LLM) that\ngathers evidence and a Supervisor (a high-capacity LLM) that makes final\njudgments. This design is cost-efficient for LLM inference while still\nmaintaining strong reasoning accuracy. Additionally, R2-KG employs an\nAbstention mechanism, generating answers only when sufficient evidence is\ncollected from KG, which significantly enhances reliability. Experiments across\nmultiple KG-based reasoning tasks show that R2-KG consistently outperforms\nbaselines in both accuracy and reliability, regardless of the inherent\ncapability of LLMs used as the Operator. Further experiments reveal that the\nsingle-agent version of R2-KG, equipped with a strict self-consistency\nstrategy, achieves significantly higher-than-baseline reliability while\nreducing inference cost. However, it also leads to a higher abstention rate in\ncomplex KGs. Our findings establish R2-KG as a flexible and cost-effective\nsolution for KG-based reasoning. It reduces reliance on high-capacity LLMs\nwhile ensuring trustworthy inference.\n","authors":["Sumin Jo","Junseong Choi","Jiho Kim","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2502.12767v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04144v1","updated":"2025-03-06T06:41:38Z","published":"2025-03-06T06:41:38Z","title":"DM-Adapter: Domain-Aware Mixture-of-Adapters for Text-Based Person\n  Retrieval","summary":"  Text-based person retrieval (TPR) has gained significant attention as a\nfine-grained and challenging task that closely aligns with practical\napplications. Tailoring CLIP to person domain is now a emerging research topic\ndue to the abundant knowledge of vision-language pretraining, but challenges\nstill remain during fine-tuning: (i) Previous full-model fine-tuning in TPR is\ncomputationally expensive and prone to overfitting.(ii) Existing\nparameter-efficient transfer learning (PETL) for TPR lacks of fine-grained\nfeature extraction. To address these issues, we propose Domain-Aware\nMixture-of-Adapters (DM-Adapter), which unifies Mixture-of-Experts (MOE) and\nPETL to enhance fine-grained feature representations while maintaining\nefficiency. Specifically, Sparse Mixture-of-Adapters is designed in parallel to\nMLP layers in both vision and language branches, where different experts\nspecialize in distinct aspects of person knowledge to handle features more\nfinely. To promote the router to exploit domain information effectively and\nalleviate the routing imbalance, Domain-Aware Router is then developed by\nbuilding a novel gating function and injecting learnable domain-aware prompts.\nExtensive experiments show that our DM-Adapter achieves state-of-the-art\nperformance, outperforming previous methods by a significant margin.\n","authors":["Yating Liu","Zimo Liu","Xiangyuan Lan","Wenming Yang","Yaowei Li","Qingmin Liao"],"pdf_url":"https://arxiv.org/pdf/2503.04144v1.pdf","comment":"9 pages, 5 figures, accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2503.04143v1","updated":"2025-03-06T06:41:17Z","published":"2025-03-06T06:41:17Z","title":"MTS: A Deep Reinforcement Learning Portfolio Management Framework with\n  Time-Awareness and Short-Selling","summary":"  Portfolio management remains a crucial challenge in finance, with traditional\nmethods often falling short in complex and volatile market environments. While\ndeep reinforcement approaches have shown promise, they still face limitations\nin dynamic risk management, exploitation of temporal markets, and incorporation\nof complex trading strategies such as short-selling. These limitations can lead\nto suboptimal portfolio performance, increased vulnerability to market\nvolatility, and missed opportunities in capturing potential returns from\ndiverse market conditions. This paper introduces a Deep Reinforcement Learning\nPortfolio Management Framework with Time-Awareness and Short-Selling (MTS),\noffering a robust and adaptive strategy for sustainable investment performance.\nThis framework utilizes a novel encoder-attention mechanism to address the\nlimitations by incorporating temporal market characteristics, a parallel\nstrategy for automated short-selling based on market trends, and risk\nmanagement through innovative Incremental Conditional Value at Risk, enhancing\nadaptability and performance. Experimental validation on five diverse datasets\nfrom 2019 to 2023 demonstrates MTS's superiority over traditional algorithms\nand advanced machine learning techniques. MTS consistently achieves higher\ncumulative returns, Sharpe, Omega, and Sortino ratios, underscoring its\neffectiveness in balancing risk and return while adapting to market dynamics.\nMTS demonstrates an average relative increase of 30.67% in cumulative returns\nand 29.33% in Sharpe ratio compared to the next best-performing strategies\nacross various datasets.\n","authors":["Fengchen Gu","Zhengyong Jiang","Ángel F. García-Fernández","Angelos Stefanidis","Jionglong Su","Huakang Li"],"pdf_url":"https://arxiv.org/pdf/2503.04143v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17635v2","updated":"2025-03-06T06:39:56Z","published":"2024-10-23T07:53:29Z","title":"Markov Chain of Thought for Efficient Mathematical Reasoning","summary":"  Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, \"derive, then reduce\", we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the $\\texttt{MCoTInstruct}$ dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs. The\ncode is available at https://github.com/james-yw/Markov-Chain-of-Thought\n","authors":["Wen Yang","Minpeng Liao","Kai Fan"],"pdf_url":"https://arxiv.org/pdf/2410.17635v2.pdf","comment":"Camera ready version for NAACL 2025 Main"},{"id":"http://arxiv.org/abs/2502.14074v2","updated":"2025-03-06T06:32:54Z","published":"2025-02-19T19:59:16Z","title":"Investigating Non-Transitivity in LLM-as-a-Judge","summary":"  Automatic evaluation methods based on large language models (LLMs) are\nemerging as the standard tool for assessing the instruction-following abilities\nof LLM-based agents. The most common method in this paradigm, pairwise\ncomparisons with a baseline model, critically depends on the assumption of\ntransitive preferences. However, the validity of this assumption remains\nlargely unexplored. In this study, we investigate the presence of\nnon-transitivity within the AlpacaEval framework and analyze its effects on\nmodel rankings. We find that LLM judges exhibit non-transitive preferences,\nleading to rankings that are sensitive to the choice of the baseline model. To\nmitigate this issue, we show that round-robin tournaments combined with\nBradley-Terry models of preference can produce more reliable rankings. Notably,\nour method increases both the Spearman correlation and the Kendall correlation\nwith Chatbot Arena (95.0% -> 96.4% and 82.1% -> 86.3% respectively). To address\nthe computational cost of round-robin tournaments, we propose Swiss-Wise\nIterative Matchmaking (Swim) tournaments, using a dynamic matching strategy to\ncapture the benefits of round-robin tournaments while maintaining computational\nefficiency.\n","authors":["Yi Xu","Laura Ruis","Tim Rocktäschel","Robert Kirk"],"pdf_url":"https://arxiv.org/pdf/2502.14074v2.pdf","comment":"8 pages, 6 figures, 2 tables (30 pages, 11 figures, 8 tables\n  including references and appendices)"},{"id":"http://arxiv.org/abs/2503.04128v1","updated":"2025-03-06T06:14:27Z","published":"2025-03-06T06:14:27Z","title":"Artificial Intelligence in Pronunciation Teaching: Use and Beliefs of\n  Foreign Language Teachers","summary":"  Pronunciation instruction in foreign language classrooms has often been an\noverlooked area of focus. With the widespread adoption of Artificial\nIntelligence (AI) and its potential benefits, investigating how AI is utilized\nin pronunciation teaching and understanding the beliefs of teachers about this\ntool is essential for improving learning outcomes. This study aims to examine\nhow AI use for pronunciation instruction varies across different demographic\nand professional factors among teachers, and how these factors, including AI\nuse, influence the beliefs of teachers about AI. The study involved 117 English\nas a Foreign Language (EFL) in-service teachers working in Cyprus, who\ncompleted an online survey designed to assess their beliefs about the\neffectiveness of AI, its drawbacks, and their willingness to integrate AI into\ntheir teaching practices. The results revealed that teachers were significantly\nmore likely to agree on the perceived effectiveness of AI and their willingness\nto adopt it, compared to their concerns about its use. Furthermore, teachers\nworking in higher education and adult education, as well as those who had\nreceived more extensive training, reported using AI more frequently in their\nteaching. Teachers who utilized AI more often expressed stronger agreement with\nits effectiveness, while those who had received more training were less likely\nto express concerns about its integration. Given the limited training that many\nteachers currently receive, these findings demonstrate the need for tailored\ntraining sessions that address the specific needs and concerns of educators,\nultimately fostering the adoption of AI in pronunciation instruction.\n","authors":["Georgios P. Georgiou"],"pdf_url":"https://arxiv.org/pdf/2503.04128v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18125v3","updated":"2025-03-06T06:10:12Z","published":"2024-10-16T07:45:31Z","title":"Towards Edge General Intelligence via Large Language Models:\n  Opportunities and Challenges","summary":"  Edge Intelligence (EI) has been instrumental in delivering real-time,\nlocalized services by leveraging the computational capabilities of edge\nnetworks. The integration of Large Language Models (LLMs) empowers EI to evolve\ninto the next stage: Edge General Intelligence (EGI), enabling more adaptive\nand versatile applications that require advanced understanding and reasoning\ncapabilities. However, systematic exploration in this area remains\ninsufficient. This survey delineates the distinctions between EGI and\ntraditional EI, categorizing LLM-empowered EGI into three conceptual systems:\ncentralized, hybrid, and decentralized. For each system, we detail the\nframework designs and review existing implementations. Furthermore, we evaluate\nthe performance and throughput of various Small Language Models (SLMs) that are\nmore suitable for development on edge devices. This survey provides researchers\nwith a comprehensive vision of EGI, offering insights into its vast potential\nand establishing a foundation for future advancements in this rapidly evolving\nfield.\n","authors":["Handi Chen","Weipeng Deng","Shuo Yang","Jinfeng Xu","Zhihan Jiang","Edith C. H. Ngai","Jiangchuan Liu","Xue Liu"],"pdf_url":"https://arxiv.org/pdf/2410.18125v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04121v1","updated":"2025-03-06T05:58:41Z","published":"2025-03-06T05:58:41Z","title":"Simple Self Organizing Map with Visual Transformer","summary":"  Vision Transformers (ViTs) have demonstrated exceptional performance in\nvarious vision tasks. However, they tend to underperform on smaller datasets\ndue to their inherent lack of inductive biases. Current approaches address this\nlimitation implicitly-often by pairing ViTs with pretext tasks or by distilling\nknowledge from convolutional neural networks (CNNs) to strengthen the prior. In\ncontrast, Self-Organizing Maps (SOMs), a widely adopted self-supervised\nframework, are inherently structured to preserve topology and spatial\norganization, making them a promising candidate to directly address the\nlimitations of ViTs in limited or small training datasets. Despite this\npotential, equipping SOMs with modern deep learning architectures remains\nlargely unexplored. In this study, we conduct a novel exploration on how Vision\nTransformers (ViTs) and Self-Organizing Maps (SOMs) can empower each other,\naiming to bridge this critical research gap. Our findings demonstrate that\nthese architectures can synergistically enhance each other, leading to\nsignificantly improved performance in both unsupervised and supervised tasks.\nCode will be publicly available.\n","authors":["Alan Luo","Kaiwen Yuan"],"pdf_url":"https://arxiv.org/pdf/2503.04121v1.pdf","comment":"5 pages, 4 figures. Submitted to IEEE. All experiments and code work\n  were performed by the first author, with the second author serving in a\n  PI/mentor role, guiding the progression of the work"},{"id":"http://arxiv.org/abs/2411.18104v2","updated":"2025-03-06T05:54:29Z","published":"2024-11-27T07:32:56Z","title":"Training and Evaluating Language Models with Template-based Data\n  Generation","summary":"  The rapid advancement of large language models (LLMs) such as GPT-3, PaLM,\nand Llama has significantly transformed natural language processing, showcasing\nremarkable capabilities in understanding and generating language. However,\nthese models often struggle with tasks requiring complex reasoning,\nparticularly in mathematical problem-solving, due in part to the scarcity of\nlarge-scale, high-quality, domain-specific datasets necessary for training\nsophisticated reasoning abilities. To address this limitation, we introduce\nTemplate-based Data Generation (TDG), a novel approach that leverages LLMs\n(GPT-4) to automatically generate parameterized meta-templates, which are then\nused to synthesize a vast array of high-quality problems and solutions.\nLeveraging TDG, we create TemplateMath Part I: TemplateGSM, a dataset\ncomprising over 7 million synthetically generated grade school math\nproblems--each accompanied by code-based and natural language solutions--with\nthe potential to generate an effectively unlimited number more. This dataset\nalleviates the scarcity of large-scale mathematical datasets and serves as a\nvaluable resource for pre-training, fine-tuning, and evaluating LLMs in\nmathematical reasoning. Our method not only enables the generation of virtually\ninfinite data but also elevates data augmentation to a new level by using GPT-4\nfor meta-template generation, ensuring diverse and high-quality problem\nstructures. The TemplateMath Part I: TemplateGSM dataset is publicly available\nat https://huggingface.co/datasets/math-ai/TemplateGSM. The code is available\nat https://github.com/iiis-ai/TemplateMath.\n","authors":["Yifan Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.18104v2.pdf","comment":"9 pages, 2 figures"},{"id":"http://arxiv.org/abs/2409.07055v2","updated":"2025-03-06T05:48:54Z","published":"2024-09-11T07:01:08Z","title":"Legal Fact Prediction: The Missing Piece in Legal Judgment Prediction","summary":"  Legal judgment prediction (LJP), which enables litigants and their lawyers to\nforecast judgment outcomes and refine litigation strategies, has emerged as a\ncrucial legal NLP task. Existing studies typically utilize legal facts, i.e.,\nfacts that have been established by evidence and determined by the judge, to\npredict the judgment. However, legal facts are often difficult to obtain in the\nearly stages of litigation, significantly limiting the practical applicability\nof fact-based LJP. To address this limitation, we propose a novel legal NLP\ntask: \\textit{legal fact prediction} (LFP), which takes the evidence submitted\nby litigants for trial as input to predict legal facts, thereby empowering\nfact-based LJP technologies to perform prediction in the absence of\nground-truth legal facts. We also propose the first benchmark dataset,\nLFPBench, for evaluating the LFP task. Our extensive experiments on LFPBench\ndemonstrate the effectiveness of LFP-empowered LJP and highlight promising\nresearch directions for LFP. Our code and data are available at\nhttps://github.com/HPRCEST/LFPBench.\n","authors":["Junkai Liu","Yujie Tong","Hui Huang","Bowen Zheng","Yiran Hu","Peicheng Wu","Chuan Xiao","Makoto Onizuka","Muyun Yang","Shuyuan Zheng"],"pdf_url":"https://arxiv.org/pdf/2409.07055v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02398v2","updated":"2025-03-06T05:46:40Z","published":"2024-11-04T18:59:51Z","title":"Prompting with Phonemes: Enhancing LLMs' Multilinguality for Non-Latin\n  Script Languages","summary":"  Although multilingual LLMs have achieved remarkable performance across\nbenchmarks, we find they continue to underperform on non-Latin script languages\nacross contemporary LLM families. This discrepancy arises from the fact that\nLLMs are pretrained with orthographic scripts, which are dominated by Latin\ncharacters that obscure their shared phonology with non-Latin scripts. We\npropose leveraging phonemic transcriptions as complementary signals to induce\nscript-invariant representations. Our study demonstrates that integrating\nphonemic signals improves performance across both non-Latin and Latin script\nlanguages, with a particularly significant impact on closing the performance\ngap between the two. Through detailed experiments, we show that phonemic and\northographic scripts retrieve distinct examples for in-context learning (ICL).\nThis motivates our proposed Mixed-ICL retrieval strategy, where further\naggregation from both leads to our significant performance improvements for\nboth Latin script languages (up to 12.6%) and non-Latin script languages (up to\n15.1%) compared to randomized ICL retrieval.\n","authors":["Hoang H Nguyen","Khyati Mahajan","Vikas Yadav","Julian Salazar","Philip S. Yu","Masoud Hashemi","Rishabh Maheshwary"],"pdf_url":"https://arxiv.org/pdf/2411.02398v2.pdf","comment":"Accepted for NAACL 2025 (Main Conference)"},{"id":"http://arxiv.org/abs/2402.04355v2","updated":"2025-03-06T05:43:48Z","published":"2024-02-06T19:39:26Z","title":"PQMass: Probabilistic Assessment of the Quality of Generative Models\n  using Probability Mass Estimation","summary":"  We propose a likelihood-free method for comparing two distributions given\nsamples from each, with the goal of assessing the quality of generative models.\nThe proposed approach, PQMass, provides a statistically rigorous method for\nassessing the performance of a single generative model or the comparison of\nmultiple competing models. PQMass divides the sample space into non-overlapping\nregions and applies chi-squared tests to the number of data samples that fall\nwithin each region, giving a p-value that measures the probability that the bin\ncounts derived from two sets of samples are drawn from the same multinomial\ndistribution. PQMass does not depend on assumptions regarding the density of\nthe true distribution, nor does it rely on training or fitting any auxiliary\nmodels. We evaluate PQMass on data of various modalities and dimensions,\ndemonstrating its effectiveness in assessing the quality, novelty, and\ndiversity of generated samples. We further show that PQMass scales well to\nmoderately high-dimensional data and thus obviates the need for feature\nextraction in practical applications.\n","authors":["Pablo Lemos","Sammy Sharief","Nikolay Malkin","Salma Salhi","Conner Stone","Laurence Perreault-Levasseur","Yashar Hezaveh"],"pdf_url":"https://arxiv.org/pdf/2402.04355v2.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2410.16024v3","updated":"2025-03-06T05:38:52Z","published":"2024-10-21T13:58:38Z","title":"SMAC-R1: The Emergence of Intelligence in Decision-Making Tasks","summary":"  StarCraft Multi-Agent Challenge (SMAC) has been one of the most commonly used\nexperimental environments in multi-agent reinforcement learning (MARL), where\nthe specific task is to control a set number of allied units to defeat enemy\nforces. Traditional MARL algorithms often require interacting with the\nenvironment for millions of steps to train a parametric model, of which the\nresulting policies are typically non-interpretable with weak transferability.\nIn this paper, we introduce SMAC-R1 which is based on the Qwen2.5-7B-Base LLM\ndistilled from DeepSeek-Coder-v2.5-236B. Similar to online reinforcement\nlearning after behavior cloning in offline learning process, in our pipeline,\nagents leverage the DeepSeek LLM to generate decision tree code by providing\ntask descriptions, and the agents are further self-reflected using feedback\nfrom the rewards provided by the environment. Based on that, we augment the\ngenerated scripts to fine-tune a small LLM, Qwen2.5-7B-Base, to distill the\ndecision-making ability via Supervised Fine-Tuning (SFT) and enhance the script\ngeneration ability by the Group Relative Policy Optimization (GRPO) algorithm.\nWe conduct experiments in the original 23 SMAC tasks and 10 newly-designed\ntasks to demonstrate that our method can produce high-quality, interpretable\ndecision trees with minimal environmental exploration. Moreover, these scripts\nexhibit strong transferability, successfully applying to homogeneous SMAC\nenvironments without modification. We believe this approach offers a new\ndirection for solving decision-making tasks and domain-specific LLM training\npipelines in the future.\n","authors":["Yue Deng","Weiyu Ma","Yuxin Fan","Ruyi Song","Yin Zhang","Haifeng Zhang","Jian Zhao"],"pdf_url":"https://arxiv.org/pdf/2410.16024v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04111v1","updated":"2025-03-06T05:36:35Z","published":"2025-03-06T05:36:35Z","title":"Generalizability of Neural Networks Minimizing Empirical Risk Based on\n  Expressive Ability","summary":"  The primary objective of learning methods is generalization. Classic uniform\ngeneralization bounds, which rely on VC-dimension or Rademacher complexity,\nfail to explain the significant attribute that over-parameterized models in\ndeep learning exhibit nice generalizability. On the other hand,\nalgorithm-dependent generalization bounds, like stability bounds, often rely on\nstrict assumptions. To establish generalizability under less stringent\nassumptions, this paper investigates the generalizability of neural networks\nthat minimize or approximately minimize empirical risk. We establish a lower\nbound for population accuracy based on the expressiveness of these networks,\nwhich indicates that with an adequate large number of training samples and\nnetwork sizes, these networks, including over-parameterized ones, can\ngeneralize effectively. Additionally, we provide a necessary condition for\ngeneralization, demonstrating that, for certain data distributions, the\nquantity of training data required to ensure generalization exceeds the network\nsize needed to represent the corresponding data distribution. Finally, we\nprovide theoretical insights into several phenomena in deep learning, including\nrobust generalization, importance of over-parameterization, and effect of loss\nfunction on generalization.\n","authors":["Lijia Yu","Yibo Miao","Yifan Zhu","Xiao-Shan Gao","Lijun Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.04111v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.04355v2","updated":"2025-03-06T05:43:48Z","published":"2024-02-06T19:39:26Z","title":"PQMass: Probabilistic Assessment of the Quality of Generative Models\n  using Probability Mass Estimation","summary":"  We propose a likelihood-free method for comparing two distributions given\nsamples from each, with the goal of assessing the quality of generative models.\nThe proposed approach, PQMass, provides a statistically rigorous method for\nassessing the performance of a single generative model or the comparison of\nmultiple competing models. PQMass divides the sample space into non-overlapping\nregions and applies chi-squared tests to the number of data samples that fall\nwithin each region, giving a p-value that measures the probability that the bin\ncounts derived from two sets of samples are drawn from the same multinomial\ndistribution. PQMass does not depend on assumptions regarding the density of\nthe true distribution, nor does it rely on training or fitting any auxiliary\nmodels. We evaluate PQMass on data of various modalities and dimensions,\ndemonstrating its effectiveness in assessing the quality, novelty, and\ndiversity of generated samples. We further show that PQMass scales well to\nmoderately high-dimensional data and thus obviates the need for feature\nextraction in practical applications.\n","authors":["Pablo Lemos","Sammy Sharief","Nikolay Malkin","Salma Salhi","Connor Stone","Laurence Perreault-Levasseur","Yashar Hezaveh"],"pdf_url":"https://arxiv.org/pdf/2402.04355v2.pdf","comment":"Published as a conference paper at ICLR 2025"}]}}