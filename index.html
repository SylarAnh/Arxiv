<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                Jcwang News
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2025-03-06T00:00:00Z">2025-03-06</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">71</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Floxels: Fast Unsupervised Voxel Based Scene Flow Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David T. Hoffmann, Syed Haseeb Raza, Hanqiu Jiang, Denis Tananaev, Steffen Klingenhoefer, Martin Meinke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene flow estimation is a foundational task for many robotic applications,
including robust dynamic object detection, automatic labeling, and sensor
synchronization. Two types of approaches to the problem have evolved: 1)
Supervised and 2) optimization-based methods. Supervised methods are fast
during inference and achieve high-quality results, however, they are limited by
the need for large amounts of labeled training data and are susceptible to
domain gaps. In contrast, unsupervised test-time optimization methods do not
face the problem of domain gaps but usually suffer from substantial runtime,
exhibit artifacts, or fail to converge to the right solution. In this work, we
mitigate several limitations of existing optimization-based methods. To this
end, we 1) introduce a simple voxel grid-based model that improves over the
standard MLP-based formulation in multiple dimensions and 2) introduce a new
multiframe loss formulation. 3) We combine both contributions in our new
method, termed Floxels. On the Argoverse 2 benchmark, Floxels is surpassed only
by EulerFlow among unsupervised methods while achieving comparable performance
at a fraction of the computational cost. Floxels achieves a massive speedup of
more than ~60 - 140x over EulerFlow, reducing the runtime from a day to 10
minutes per sequence. Over the faster but low-quality baseline, NSFP, Floxels
achieves a speedup of ~14x.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Multi</span>-Agent Inverse Q-Learning from Demonstrations <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathaniel Haynam, Adam Khoja, Dhruv Kumar, Vivek Myers, Erdem Bıyık
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When reward functions are hand-designed, deep reinforcement learning
algorithms often suffer from reward misspecification, causing them to learn
suboptimal policies in terms of the intended task objectives. In the
single-agent case, inverse reinforcement learning (IRL) techniques attempt to
address this issue by inferring the reward function from expert demonstrations.
However, in multi-agent problems, misalignment between the learned and true
objectives is exacerbated due to increased environment non-stationarity and
variance that scales with multiple agents. As such, in multi-agent general-sum
games, multi-agent IRL algorithms have difficulty balancing cooperative and
competitive objectives. To address these issues, we propose Multi-Agent
Marginal Q-Learning from Demonstrations (MAMQL), a novel sample-efficient
framework for multi-agent IRL. For each agent, MAMQL learns a critic
marginalized over the other agents' policies, allowing for a well-motivated use
of Boltzmann policies in the multi-agent context. We identify a connection
between optimal marginalized critics and single-agent soft-Q IRL, allowing us
to apply a direct, simple optimization criterion from the single-agent domain.
Across our experiments on three different simulated domains, MAMQL
significantly outperforms previous multi-agent methods in average reward,
sample efficiency, and reward recovery by often more than 2-5x. We make our
code available at https://sites.google.com/view/mamql .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, 2 tables. Published at the International
  Conference on Robotics and Automation (ICRA) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 3HANDS <span class="highlight-title">Dataset</span>: Learning from Humans for Generating Naturalistic
  Handovers with Supernumerary Robotic Limbs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04635v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04635v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Artin Saberpour Abadian, Yi-Chi Liao, Ata Otaran, Rishabh Dabral, Marie Muehlhaus, Christian Theobalt, Martin Schmitz, Jürgen Steimle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supernumerary robotic limbs (SRLs) are robotic structures integrated closely
with the user's body, which augment human physical capabilities and necessitate
seamless, naturalistic human-machine interaction. For effective assistance in
physical tasks, enabling SRLs to hand over objects to humans is crucial. Yet,
designing heuristic-based policies for robots is time-consuming, difficult to
generalize across tasks, and results in less human-like motion. When trained
with proper datasets, generative models are powerful alternatives for creating
naturalistic handover motions. We introduce 3HANDS, a novel dataset of object
handover interactions between a participant performing a daily activity and
another participant enacting a hip-mounted SRL in a naturalistic manner. 3HANDS
captures the unique characteristics of SRL interactions: operating in intimate
personal space with asymmetric object origins, implicit motion synchronization,
and the user's engagement in a primary task during the handover. To demonstrate
the effectiveness of our dataset, we present three models: one that generates
naturalistic handover trajectories, another that determines the appropriate
handover endpoints, and a third that predicts the moment to initiate a
handover. In a user study (N=10), we compare the handover interaction performed
with our method compared to a baseline. The findings show that our method was
perceived as significantly more natural, less physically demanding, and more
comfortable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CHI '25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Whole-Body Model-Predictive Control of Legged Robots with MuJoCo 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Z. Zhang, Taylor A. Howell, Zeji Yi, Chaoyi Pan, Guanya Shi, Guannan Qu, Tom Erez, Yuval Tassa, Zachary Manchester
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We demonstrate the surprising real-world effectiveness of a very simple
approach to whole-body model-predictive control (MPC) of quadruped and humanoid
robots: the iterative LQR (iLQR) algorithm with MuJoCo dynamics and
finite-difference approximated derivatives. Building upon the previous success
of model-based behavior synthesis and control of locomotion and manipulation
tasks with MuJoCo in simulation, we show that these policies can easily
generalize to the real world with few sim-to-real considerations. Our baseline
method achieves real-time whole-body MPC on a variety of hardware experiments,
including dynamic quadruped locomotion, quadruped walking on two legs, and
full-sized humanoid bipedal locomotion. We hope this easy-to-reproduce hardware
baseline lowers the barrier to entry for real-world whole-body MPC research and
contributes to accelerating research velocity in the community. Our code and
experiment videos will be available online
at:https://johnzhang3.github.io/mujoco_ilqr
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ExoNav II: Design of a Robotic Tool with Follow-the-Leader Motion
  Capability for Lateral and Ventral Spinal Cord Stimulation (SCS) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04603v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04603v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Behnam Moradkhani, Pejman Kheradmand, Harshith Jella, Joseph Klein, Ajmal Zemmar, Yash Chitalia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spinal cord stimulation (SCS) electrodes are traditionally placed in the
dorsal epidural space to stimulate the dorsal column fibers for pain therapy.
Recently, SCS has gained attention in restoring gait. However, the motor fibers
triggering locomotion are located in the ventral and lateral spinal cord.
Currently, SCS electrodes are steered manually, making it difficult to navigate
them to the lateral and ventral motor fibers in the spinal cord. In this work,
we propose a helically micro-machined continuum robot that can bend in a
helical shape when subjected to actuation tendon forces. Using a stiff outer
tube and adding translational and rotational degrees of freedom, this helical
continuum robot can perform follow-the-leader (FTL) motion. We propose a
kinematic model to relate tendon stroke and geometric parameters of the robot's
helical shape to its acquired trajectory and end-effector position. We evaluate
the proposed kinematic model and the robot's FTL motion capability
experimentally. The stroke-based method, which links tendon stroke values to
the robot's shape, showed inaccuracies with a 19.84 mm deviation and an RMSE of
14.42 mm for 63.6 mm of robot's length bending. The position-based method,
using kinematic equations to map joint space to task space, performed better
with a 10.54 mm deviation and an RMSE of 8.04 mm. Follow-the-leader experiments
showed deviations of 11.24 mm and 7.32 mm, with RMSE values of 8.67 mm and 5.18
mm for the stroke-based and position-based methods, respectively. Furthermore,
end-effector trajectories in two FTL motion trials are compared to confirm the
robot's repeatable behavior. Finally, we demonstrate the robot's operation on a
3D-printed spinal cord phantom model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DogLegs: Robust Proprioceptive State Estimation for Legged Robots Using
  <span class="highlight-title">Multi</span>ple Leg-Mounted IMUs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibin Wu, Jian Kuang, Shahram Khorshidi, Xiaoji Niu, Lasse Klingbeil, Maren Bennewitz, Heiner Kuhlmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust and accurate proprioceptive state estimation of the main body is
crucial for legged robots to execute tasks in extreme environments where
exteroceptive sensors, such as LiDARs and cameras may become unreliable. In
this paper, we propose DogLegs, a state estimation system for legged robots
that fuses the measurements from a body-mounted inertial measurement unit
(Body-IMU), joint encoders, and multiple leg-mounted IMUs (Leg-IMU) using an
extended Kalman filter (EKF). The filter system contains the error states of
all IMU frames. The Leg-IMUs are used to detect foot contact, thereby providing
zero velocity measurements to update the state of the Leg-IMU frames.
Additionally, we compute the relative position constraints between the Body-IMU
and Leg-IMUs by the leg kinematics and use them to update the main body state
and reduce the error drift of the individual IMU frames. Field experimental
results have shown that our proposed system can achieve better state estimation
accuracy compared to the traditional leg odometry method (using only Body-IMU
and joint encoders) across different terrains. We make our datasets publicly
available to benefit the research community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-augmented Learning of Geodesic Distances in Irregular Domains
  through Soner Boundary Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael I. Cabral Muchacho, Florian T. Pokorny
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Geodesic distances play a fundamental role in robotics, as they efficiently
encode global geometric information of the domain. Recent methods use neural
networks to approximate geodesic distances by solving the Eikonal equation
through physics-informed approaches. While effective, these approaches often
suffer from unstable convergence during training in complex environments. We
propose a framework to learn geodesic distances in irregular domains by using
the Soner boundary condition, and systematically evaluate the impact of data
losses on training stability and solution accuracy. Our experiments demonstrate
that incorporating data losses significantly improves convergence robustness,
reducing training instabilities and sensitivity to initialization. These
findings suggest that hybrid data-physics approaches can effectively enhance
the reliability of learning-based geodesic distance solvers with sparse data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Omnidirectional <span class="highlight-title">Multi</span>-Object Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04565v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04565v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Luo, Hao Shi, Sheng Wu, Fei Teng, Mengfei Duan, Chang Huang, Yuhang Wang, Kaiwei Wang, Kailun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Panoramic imagery, with its 360{\deg} field of view, offers comprehensive
information to support Multi-Object Tracking (MOT) in capturing spatial and
temporal relationships of surrounding objects. However, most MOT algorithms are
tailored for pinhole images with limited views, impairing their effectiveness
in panoramic settings. Additionally, panoramic image distortions, such as
resolution loss, geometric deformation, and uneven lighting, hinder direct
adaptation of existing MOT methods, leading to significant performance
degradation. To address these challenges, we propose OmniTrack, an
omnidirectional MOT framework that incorporates Tracklet Management to
introduce temporal cues, FlexiTrack Instances for object localization and
association, and the CircularStatE Module to alleviate image and geometric
distortions. This integration enables tracking in large field-of-view
scenarios, even under rapid sensor motion. To mitigate the lack of panoramic
MOT datasets, we introduce the QuadTrack dataset--a comprehensive panoramic
dataset collected by a quadruped robot, featuring diverse challenges such as
wide fields of view, intense motion, and complex environments. Extensive
experiments on the public JRDB dataset and the newly introduced QuadTrack
benchmark demonstrate the state-of-the-art performance of the proposed
framework. OmniTrack achieves a HOTA score of 26.92% on JRDB, representing an
improvement of 3.43%, and further achieves 23.45% on QuadTrack, surpassing the
baseline by 6.81%. The dataset and code will be made publicly available at
https://github.com/xifen523/OmniTrack.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2025. The dataset and code will be made publicly
  available at https://github.com/xifen523/OmniTrack</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Occlusion-Aware Consistent Model Predictive Control for Robot Navigation
  in Occluded Obstacle-Dense Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minzhe Zheng, Lei Zheng, Lei Zhu, Jun Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring safety and motion consistency for robot navigation in occluded,
obstacle-dense environments is a critical challenge. In this context, this
study presents an occlusion-aware Consistent Model Predictive Control (CMPC)
strategy. To account for the occluded obstacles, it incorporates adjustable
risk regions that represent their potential future locations. Subsequently,
dynamic risk boundary constraints are developed online to ensure safety. The
CMPC then constructs multiple locally optimal trajectory branches (each
tailored to different risk regions) to balance between exploitation and
exploration. A shared consensus trunk is generated to ensure smooth transitions
between branches without significant velocity fluctuations, further preserving
motion consistency. To facilitate high computational efficiency and ensure
coordination across local trajectories, we use the alternating direction method
of multipliers (ADMM) to decompose the CMPC into manageable sub-problems for
parallel solving. The proposed strategy is validated through simulation and
real-world experiments on an Ackermann-steering robot platform. The results
demonstrate the effectiveness of the proposed CMPC strategy through comparisons
with baseline approaches in occluded, obstacle-dense environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Generalizable Language-Conditioned Cloth Manipulation from Long
  Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanyi Zhao, Jinxuan Zhu, Zihao Yan, Yichen Li, Yuhong Deng, Xueqian Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-step cloth manipulation is a challenging problem for robots due to the
high-dimensional state spaces and the dynamics of cloth. Despite recent
significant advances in end-to-end imitation learning for multi-step cloth
manipulation skills, these methods fail to generalize to unseen tasks. Our
insight in tackling the challenge of generalizable multi-step cloth
manipulation is decomposition. We propose a novel pipeline that autonomously
learns basic skills from long demonstrations and composes learned basic skills
to generalize to unseen tasks. Specifically, our method first discovers and
learns basic skills from the existing long demonstration benchmark with the
commonsense knowledge of a large language model (LLM). Then, leveraging a
high-level LLM-based task planner, these basic skills can be composed to
complete unseen tasks. Experimental results demonstrate that our method
outperforms baseline methods in learning multi-step cloth manipulation skills
for both seen and unseen tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ViT-VS: On the Applicability of Pretrained Vision Transformer Features
  for Generalizable Visual Servoing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Scherl, Stefan Thalhammer, Bernhard Neuberger, Wilfried Wöber, José Gracía-Rodríguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual servoing enables robots to precisely position their end-effector
relative to a target object. While classical methods rely on hand-crafted
features and thus are universally applicable without task-specific training,
they often struggle with occlusions and environmental variations, whereas
learning-based approaches improve robustness but typically require extensive
training. We present a visual servoing approach that leverages pretrained
vision transformers for semantic feature extraction, combining the advantages
of both paradigms while also being able to generalize beyond the provided
sample. Our approach achieves full convergence in unperturbed scenarios and
surpasses classical image-based visual servoing by up to 31.2\% relative
improvement in perturbed scenarios. Even the convergence rates of
learning-based methods are matched despite requiring no task- or
object-specific training. Real-world evaluations confirm robust performance in
end-effector positioning, industrial box manipulation, and grasping of unseen
objects using only a reference from the same category. Our code and simulation
environment are available at: https://alessandroscherl.github.io/ViT-VS/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SRSA: Skill Retrieval and Adaptation for Robotic Assembly Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04538v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04538v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijie Guo, Bingjie Tang, Iretiayo Akinola, Dieter Fox, Abhishek Gupta, Yashraj Narang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enabling robots to learn novel tasks in a data-efficient manner is a
long-standing challenge. Common strategies involve carefully leveraging prior
experiences, especially transition data collected on related tasks. Although
much progress has been made for general pick-and-place manipulation, far fewer
studies have investigated contact-rich assembly tasks, where precise control is
essential. We introduce SRSA (Skill Retrieval and Skill Adaptation), a novel
framework designed to address this problem by utilizing a pre-existing skill
library containing policies for diverse assembly tasks. The challenge lies in
identifying which skill from the library is most relevant for fine-tuning on a
new task. Our key hypothesis is that skills showing higher zero-shot success
rates on a new task are better suited for rapid and effective fine-tuning on
that task. To this end, we propose to predict the transfer success for all
skills in the skill library on a novel task, and then use this prediction to
guide the skill retrieval process. We establish a framework that jointly
captures features of object geometry, physical dynamics, and expert actions to
represent the tasks, allowing us to efficiently learn the transfer success
predictor. Extensive experiments demonstrate that SRSA significantly
outperforms the leading baseline. When retrieving and fine-tuning skills on
unseen tasks, SRSA achieves a 19% relative improvement in success rate,
exhibits 2.6x lower standard deviation across random seeds, and requires 2.4x
fewer transition samples to reach a satisfactory success rate, compared to the
baseline. Furthermore, policies trained with SRSA in simulation achieve a 90%
mean success rate when deployed in the real world. Please visit our project
webpage https://srsa2024.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ForestLPR: LiDAR Place Recognition in Forests Attentioning <span class="highlight-title">Multi</span>ple BEV
  Density Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanqing Shen, Turcan Tuna, Marco Hutter, Cesar Cadena, Nanning Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Place recognition is essential to maintain global consistency in large-scale
localization systems. While research in urban environments has progressed
significantly using LiDARs or cameras, applications in natural forest-like
environments remain largely under-explored. Furthermore, forests present
particular challenges due to high self-similarity and substantial variations in
vegetation growth over time. In this work, we propose a robust LiDAR-based
place recognition method for natural forests, ForestLPR. We hypothesize that a
set of cross-sectional images of the forest's geometry at different heights
contains the information needed to recognize revisiting a place. The
cross-sectional images are represented by \ac{bev} density images of horizontal
slices of the point cloud at different heights. Our approach utilizes a visual
transformer as the shared backbone to produce sets of local descriptors and
introduces a multi-BEV interaction module to attend to information at different
heights adaptively. It is followed by an aggregation layer that produces a
rotation-invariant place descriptor. We evaluated the efficacy of our method
extensively on real-world data from public benchmarks as well as robotic
datasets and compared it against the state-of-the-art (SOTA) methods. The
results indicate that ForestLPR has consistently good performance on all
evaluations and achieves an average increase of 7.38\% and 9.11\% on Recall@1
over the closest competitor on intra-sequence loop closure detection and
inter-sequence re-localization, respectively, validating our hypothesis
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by CVPR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PALo: Learning Posture-Aware Locomotion for Quadruped Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04462v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04462v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Miao, Jun Sun, Hang Lai, Xinpeng Di, Jiahang Cao, Yong Yu, Weinan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of embodied intelligence, locomotion control of
quadruped robots on complex terrains has become a research hotspot. Unlike
traditional locomotion control approaches focusing solely on velocity tracking,
we pursue to balance the agility and robustness of quadruped robots on diverse
and complex terrains. To this end, we propose an end-to-end deep reinforcement
learning framework for posture-aware locomotion named PALo, which manages to
handle simultaneous linear and angular velocity tracking and real-time
adjustments of body height, pitch, and roll angles. In PALo, the locomotion
control problem is formulated as a partially observable Markov decision
process, and an asymmetric actor-critic architecture is adopted to overcome the
sim-to-real challenge. Further, by incorporating customized training curricula,
PALo achieves agile posture-aware locomotion control in simulated environments
and successfully transfers to real-world settings without fine-tuning, allowing
real-time control of the quadruped robot's locomotion and body posture across
challenging terrains. Through in-depth experimental analysis, we identify the
key components of PALo that contribute to its performance, further validating
the effectiveness of the proposed method. The results of this study provide new
possibilities for the low-level locomotion control of quadruped robots in
higher dimensional command spaces and lay the foundation for future research on
upper-level modules for embodied intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EvidMTL: Evidential <span class="highlight-title">Multi</span>-Task Learning for Uncertainty-Aware Semantic
  Surface Mapping from Monocular RGB Images <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohit Menon, Nils Dengler, Sicong Pan, Gokul Krishna Chenchani, Maren Bennewitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For scene understanding in unstructured environments, an accurate and
uncertainty-aware metric-semantic mapping is required to enable informed action
selection by autonomous systems.Existing mapping methods often suffer from
overconfident semantic predictions, and sparse and noisy depth sensing, leading
to inconsistent map representations. In this paper, we therefore introduce
EvidMTL, a multi-task learning framework that uses evidential heads for depth
estimation and semantic segmentation, enabling uncertainty-aware inference from
monocular RGB images. To enable uncertainty-calibrated evidential multi-task
learning, we propose a novel evidential depth loss function that jointly
optimizes the belief strength of the depth prediction in conjunction with
evidential segmentation loss. Building on this, we present EvidKimera, an
uncertainty-aware semantic surface mapping framework, which uses evidential
depth and semantics prediction for improved 3D metric-semantic consistency. We
train and evaluate EvidMTL on the NYUDepthV2 and assess its zero-shot
performance on ScanNetV2, demonstrating superior uncertainty estimation
compared to conventional approaches while maintaining comparable depth
estimation and semantic segmentation. In zero-shot mapping tests on ScanNetV2,
EvidKimera outperforms Kimera in semantic surface mapping accuracy and
consistency, highlighting the benefits of uncertainty-aware mapping and
underscoring its potential for real-world robotic applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS 2025 Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Analysis of Stability, Sensitivity and Transparency in Variable
  Admittance Control for pHRI Enhanced by Virtual Fixtures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04414v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04414v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Tebaldi, Dario Onfiani, Luigi Biagiotti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The interest in Physical Human-Robot Interaction (pHRI) has significantly
increased over the last two decades thanks to the availability of collaborative
robots that guarantee user safety during force exchanges. For this reason,
stability concerns have been addressed extensively in the literature while
proposing new control schemes for pHRI applications. Because of the nonlinear
nature of robots, stability analyses generally leverage passivity concepts. On
the other hand, the proposed algorithms generally consider ideal models of
robot manipulators. For this reason, the primary objective of this paper is to
conduct a detailed analysis of the sources of instability for a class of pHRI
control schemes, namely proxy-based constrained admittance controllers, by
considering parasitic effects such as transmission elasticity, motor velocity
saturation, and actuation delay. Next, a sensitivity analysis supported by
experimental results is carried out, in order to identify how the control
parameters affect the stability of the overall system. Finally, an adaptation
technique for the proxy parameters is proposed with the goal of maximizing
transparency in pHRI. The proposed adaptation method is validated through both
simulations and experimental tests.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SeGMan: Sequential and Guided Manipulation Planner for Robust Planning
  in 2D Constrained Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04409v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04409v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cankut Bora Tuncer, Dilruba Sultan Haliloglu, Ozgur S. Oguz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present SeGMan, a hybrid motion planning framework that
integrates sampling-based and optimization-based techniques with a guided
forward search to address complex, constrained sequential manipulation
challenges, such as pick-and-place puzzles. SeGMan incorporates an adaptive
subgoal selection method that adjusts the granularity of subgoals, enhancing
overall efficiency. Furthermore, proposed generalizable heuristics guide the
forward search in a more targeted manner. Extensive evaluations in maze-like
tasks populated with numerous objects and obstacles demonstrate that SeGMan is
capable of generating not only consistent and computationally efficient
manipulation plans but also outperform state-of-the-art approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Energy Consumption of Robotic Arm with the Local Reduction Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Halima Ibrahim Kure, Jishna Retnakumari, Lucian Nita, Saeed Sharif, Hamed Balogun, Augustine O. Nwajana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Energy consumption in robotic arms is a significant concern in industrial
automation due to rising operational costs and environmental impact. This study
investigates the use of a local reduction method to optimize energy efficiency
in robotic systems without compromising performance. The approach refines
movement parameters, minimizing energy use while maintaining precision and
operational reliability. A three-joint robotic arm model was tested using
simulation over a 30-second period for various tasks, including pick-and-place
and trajectory-following operations. The results revealed that the local
reduction method reduced energy consumption by up to 25% compared to
traditional techniques such as Model Predictive Control (MPC) and Genetic
Algorithms (GA). Unlike MPC, which requires significant computational
resources, and GA, which has slow convergence rates, the local reduction method
demonstrated superior adaptability and computational efficiency in real-time
applications. The study highlights the scalability and simplicity of the local
reduction approach, making it an attractive option for industries seeking
sustainable and cost-effective solutions. Additionally, this method can
integrate seamlessly with emerging technologies like Artificial Intelligence
(AI), further enhancing its application in dynamic and complex environments.
This research underscores the potential of the local reduction method as a
practical tool for optimizing robotic arm operations, reducing energy demands,
and contributing to sustainability in industrial automation. Future work will
focus on extending the approach to real-world scenarios and incorporating
AI-driven adjustments for more dynamic adaptability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 3 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shaken, Not Stirred: A Novel <span class="highlight-title">Dataset</span> for Visual Understanding of Glasses
  in Human-Robot Bartending Tasks <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04308v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04308v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukáš Gajdošech, Hassan Ali, Jan-Gerrit Habekost, Martin Madaras, Matthias Kerzel, Stefan Wermter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Datasets for object detection often do not account for enough variety of
glasses, due to their transparent and reflective properties. Specifically,
open-vocabulary object detectors, widely used in embodied robotic agents, fail
to distinguish subclasses of glasses. This scientific gap poses an issue to
robotic applications that suffer from accumulating errors between detection,
planning, and action execution. The paper introduces a novel method for the
acquisition of real-world data from RGB-D sensors that minimizes human effort.
We propose an auto-labeling pipeline that generates labels for all the acquired
frames based on the depth measurements. We provide a novel real-world glass
object dataset that was collected on the Neuro-Inspired COLlaborator (NICOL), a
humanoid robot platform. The data set consists of 7850 images recorded from
five different cameras. We show that our trained baseline model outperforms
state-of-the-art open-vocabulary approaches. In addition, we deploy our
baseline model in an embodied agent approach to the NICOL platform, on which it
achieves a success rate of 81% in a human-robot bartending scenario.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Manipulation of Elasto-Flexible Cables with Single or <span class="highlight-title">Multi</span>ple UAVs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chiara Gabellieri, Lars Teeuwen, Yaolei Shen, Antonio Franchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work considers a large class of systems composed of multiple quadrotors
manipulating deformable and extensible cables. The cable is described via a
discretized representation, which decomposes it into linear springs
interconnected through lumped-mass passive spherical joints. Sets of flat
outputs are found for the systems. Numerical simulations support the findings
by showing cable manipulation relying on flatness-based trajectories.
Eventually, we present an experimental validation of the effectiveness of the
proposed discretized cable model for a two-robot example. Moreover, a
closed-loop controller based on the identified model and using cable-output
feedback is experimentally tested.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Autonomous Reinforcement Learning for Real-Wo<span class="highlight-title">rl</span>d Robotic
  Manipulation with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niccolò Turcato, Matteo Iovino, Aris Synodinos, Alberto Dalla Libera, Ruggero Carli, Pietro Falco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) and Visual Language
Models (VLMs) have significantly impacted robotics, enabling high-level
semantic motion planning applications. Reinforcement Learning (RL), a
complementary paradigm, enables agents to autonomously optimize complex
behaviors through interaction and reward signals. However, designing effective
reward functions for RL remains challenging, especially in real-world tasks
where sparse rewards are insufficient and dense rewards require elaborate
design. In this work, we propose Autonomous Reinforcement learning for Complex
HumanInformed Environments (ARCHIE), an unsupervised pipeline leveraging GPT-4,
a pre-trained LLM, to generate reward functions directly from natural language
task descriptions. The rewards are used to train RL agents in simulated
environments, where we formalize the reward generation process to enhance
feasibility. Additionally, GPT-4 automates the coding of task success criteria,
creating a fully automated, one-shot procedure for translating human-readable
text into deployable robot skills. Our approach is validated through extensive
simulated experiments on single-arm and bi-manual manipulation tasks using an
ABB YuMi collaborative robot, highlighting its practicality and effectiveness.
Tasks are demonstrated on the real robot setup.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VLA Model-Expert Collaboration for Bi-directional Manipulation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04163v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04163v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tian-Yu Xiang, Ao-Qun Jin, Xiao-Hu Zhou, Mei-Jiang Gui, Xiao-Liang Xie, Shi-Qi Liu, Shuang-Yi Wang, Sheng-Bin Duang, Si-Cheng Wang, Zheng Lei, Zeng-Guang Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of vision-language-action (VLA) models has given rise to
foundation models for robot manipulation. Although these models have achieved
significant improvements, their generalization in multi-task manipulation
remains limited. This study proposes a VLA model-expert collaboration framework
that leverages a limited number of expert actions to enhance VLA model
performance. This approach reduces expert workload relative to manual operation
while simultaneously improving the reliability and generalization of VLA
models. Furthermore, manipulation data collected during collaboration can
further refine the VLA model, while human participants concurrently enhance
their skills. This bi-directional learning loop boosts the overall performance
of the collaboration system. Experimental results across various VLA models
demonstrate the effectiveness of the proposed system in collaborative
manipulation and learning, as evidenced by improved success rates across tasks.
Additionally, validation using a brain-computer interface (BCI) indicates that
the collaboration system enhances the efficiency of low-speed action systems by
involving VLA model during manipulation. These promising results pave the way
for advancing human-robot interaction in the era of foundation models for
robotics. (Project website: https://aoqunjin.github.io/Expert-VLA/)
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simulation-based Analysis Of Highway Trajectory Planning Using
  High-Order Polynomial For Highly Automated Driving Function 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04159v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04159v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milin Patel, Marzana Khatun, Rolf Jung, Michael Glaß
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the fundamental tasks of autonomous driving is safe trajectory
planning, the task of deciding where the vehicle needs to drive, while avoiding
obstacles, obeying safety rules, and respecting the fundamental limits of road.
Real-world application of such a method involves consideration of surrounding
environment conditions and movements such as Lane Change, collision avoidance,
and lane merge. The focus of the paper is to develop and implement safe
collision free highway Lane Change trajectory using high order polynomial for
Highly Automated Driving Function (HADF). Planning is often considered as a
higher-level process than control. Behavior Planning Module (BPM) is designed
that plans the high-level driving actions like Lane Change maneuver to safely
achieve the functionality of transverse guidance ensuring safety of the vehicle
using motion planning in a scenario including environmental situation. Based on
the recommendation received from the (BPM), the function will generate a desire
corresponding trajectory. The proposed planning system is situation specific
with polynomial based algorithm for same direction two lane highway scenario.
To support the trajectory system polynomial curve can be used to reduces
overall complexity and thereby allows rapid computation. The proposed Lane
Change scenario is modeled, and results has been analyzed (verified and
validate) through the MATLAB simulation environment. The method proposed in
this paper has achieved a significant improvement in safety and stability of
Lane Changing maneuver.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Real-time Spatial-temporal Traversability Assessment via Feature-based
  Sparse Gaussian Process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04134v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04134v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Senming Tan, Zhenyu Hou, Zhihao Zhang, Long Xu, Mengke Zhang, Zhaoqi He, Chao Xu, <span class="highlight-author">Fei Gao</span>, Yanjun Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Terrain analysis is critical for the practical application of ground mobile
robots in real-world tasks, especially in outdoor unstructured environments. In
this paper, we propose a novel spatial-temporal traversability assessment
method, which aims to enable autonomous robots to effectively navigate through
complex terrains. Our approach utilizes sparse Gaussian processes (SGP) to
extract geometric features (curvature, gradient, elevation, etc.) directly from
point cloud scans. These features are then used to construct a high-resolution
local traversability map. Then, we design a spatial-temporal Bayesian Gaussian
kernel (BGK) inference method to dynamically evaluate traversability scores,
integrating historical and real-time data while considering factors such as
slope, flatness, gradient, and uncertainty metrics. GPU acceleration is applied
in the feature extraction step, and the system achieves real-time performance.
Extensive simulation experiments across diverse terrain scenarios demonstrate
that our method outperforms SOTA approaches in both accuracy and computational
efficiency. Additionally, we develop an autonomous navigation framework
integrated with the traversability map and validate it with a differential
driven vehicle in complex outdoor environments. Our code will be open-source
for further research and development by the community,
https://github.com/ZJU-FAST-Lab/FSGP_BGK.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DVM-<span class="highlight-title">SLAM</span>: <span class="highlight-title">Decentralized</span> Visual Monocular Simultaneous Localization and
  Mapping for <span class="highlight-title">Multi</span>-Agent Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04126v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04126v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Bird, Jan Blumenkamp, Amanda Prorok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cooperative Simultaneous Localization and Mapping (C-SLAM) enables multiple
agents to work together in mapping unknown environments while simultaneously
estimating their own positions. This approach enhances robustness, scalability,
and accuracy by sharing information between agents, reducing drift, and
enabling collective exploration of larger areas. In this paper, we present
Decentralized Visual Monocular SLAM (DVM-SLAM), the first open-source
decentralized monocular C-SLAM system. By only utilizing low-cost and
light-weight monocular vision sensors, our system is well suited for small
robots and micro aerial vehicles (MAVs). DVM-SLAM's real-world applicability is
validated on physical robots with a custom collision avoidance framework,
showcasing its potential in real-time multi-agent autonomous navigation
scenarios. We also demonstrate comparable accuracy to state-of-the-art
centralized monocular C-SLAM systems. We open-source our code and provide
supplementary material online.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GAGrasp: Geometric Algebra Diffusion for Dexterous Grasping <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Zhong, Christine Allen-Blanchette
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose GAGrasp, a novel framework for dexterous grasp generation that
leverages geometric algebra representations to enforce equivariance to SE(3)
transformations. By encoding the SE(3) symmetry constraint directly into the
architecture, our method improves data and parameter efficiency while enabling
robust grasp generation across diverse object poses. Additionally, we
incorporate a differentiable physics-informed refinement layer, which ensures
that generated grasps are physically plausible and stable. Extensive
experiments demonstrate the model's superior performance in generalization,
stability, and adaptability compared to existing methods. Additional details at
https://gagrasp.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Spinning Blimp: Design and Control of a Novel Minimalist Aerial
  Vehicle Leveraging Rotational Dynamics and Locomotion <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Santens, Diego S. D'Antonio, Shuhang Hou, David Saldaña
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the Spinning Blimp, a novel lighter-than-air (LTA) aerial
vehicle designed for low-energy stable flight. Utilizing an oblate spheroid
helium balloon for buoyancy, the vehicle achieves minimal energy consumption
while maintaining prolonged airborne states. The unique and low-cost design
employs a passively arranged wing coupled with a propeller to induce a spinning
behavior, providing inherent pendulum-like stabilization. We propose a control
strategy that takes advantage of the continuous revolving nature of the
spinning blimp to control translational motion. The cost-effectiveness of the
vehicle makes it highly suitable for a variety of applications, such as
patrolling, localization, air and turbulence monitoring, and domestic
surveillance. Experimental evaluations affirm the design's efficacy and
underscore its potential as a versatile and economically viable solution for
aerial applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the IEEE international conference on robotics and
  automation(ICRA 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image-Based Relocalization and Alignment for Long-Term Monitoring of
  Dynamic Underwater Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04096v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04096v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beverley Gorry, Tobias Fischer, Michael Milford, Alejandro Fontan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective monitoring of underwater ecosystems is crucial for tracking
environmental changes, guiding conservation efforts, and ensuring long-term
ecosystem health. However, automating underwater ecosystem management with
robotic platforms remains challenging due to the complexities of underwater
imagery, which pose significant difficulties for traditional visual
localization methods. We propose an integrated pipeline that combines Visual
Place Recognition (VPR), feature matching, and image segmentation on
video-derived images. This method enables robust identification of revisited
areas, estimation of rigid transformations, and downstream analysis of
ecosystem changes. Furthermore, we introduce the SQUIDLE+ VPR Benchmark-the
first large-scale underwater VPR benchmark designed to leverage an extensive
collection of unstructured data from multiple robotic platforms, spanning time
intervals from days to years. The dataset encompasses diverse trajectories,
arbitrary overlap and diverse seafloor types captured under varying
environmental conditions, including differences in depth, lighting, and
turbidity. Our code is available at: https://github.com/bev-gorry/underloc
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OPG-Policy: Occluded Push-Grasp Policy Learning with Amodal Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04089v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04089v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Ding, Yiming Zeng, Zhaoliang Wan, Hui Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Goal-oriented grasping in dense clutter, a fundamental challenge in robotics,
demands an adaptive policy to handle occluded target objects and diverse
configurations. Previous methods typically learn policies based on partially
observable segments of the occluded target to generate motions. However, these
policies often struggle to generate optimal motions due to uncertainties
regarding the invisible portions of different occluded target objects across
various scenes, resulting in low motion efficiency. To this end, we propose
OPG-Policy, a novel framework that leverages amodal segmentation to predict
occluded portions of the target and develop an adaptive push-grasp policy for
cluttered scenarios where the target object is partially observed.
Specifically, our approach trains a dedicated amodal segmentation module for
diverse target objects to generate amodal masks. These masks and scene
observations are mapped to the future rewards of grasp and push motion
primitives via deep Q-learning to learn the motion critic. Afterward, the push
and grasp motion candidates predicted by the critic, along with the relevant
domain knowledge, are fed into the coordinator to generate the optimal motion
implemented by the robot. Extensive experiments conducted in both simulated and
real-world environments demonstrate the effectiveness of our approach in
generating motion sequences for retrieving occluded targets, outperforming
other baseline methods in success rate and motion efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Instrument-Splatting: Controllable Photorealistic Reconstruction of
  Surgical Instruments Using Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04082v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04082v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuojue Yang, Zijian Wu, Mingxuan Hong, Qian Li, Daiyun Shen, Septimiu E. Salcudean, Yueming Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real2Sim is becoming increasingly important with the rapid development of
surgical artificial intelligence (AI) and autonomy. In this work, we propose a
novel Real2Sim methodology, \textit{Instrument-Splatting}, that leverages 3D
Gaussian Splatting to provide fully controllable 3D reconstruction of surgical
instruments from monocular surgical videos. To maintain both high visual
fidelity and manipulability, we introduce a geometry pre-training to bind
Gaussian point clouds on part mesh with accurate geometric priors and define a
forward kinematics to control the Gaussians as flexible as real instruments.
Afterward, to handle unposed videos, we design a novel instrument pose tracking
method leveraging semantics-embedded Gaussians to robustly refine per-frame
instrument poses and joint states in a render-and-compare manner, which allows
our instrument Gaussian to accurately learn textures and reach photorealistic
rendering. We validated our method on 2 publicly released surgical videos and 4
videos collected on ex vivo tissues and green screens. Quantitative and
qualitative evaluations demonstrate the effectiveness and superiority of the
proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Music-Driven Legged Robots: Synchronized Walking to Rhythmic Beats <span class="chip">ICRA2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taixian Hou, Yueqi Zhang, Xiaoyi Wei, Zhiyan Dong, Jiafu Yi, Peng Zhai, Lihua Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the challenge of effectively controlling the locomotion of legged
robots by incorporating precise frequency and phase characteristics, which is
often ignored in locomotion policies that do not account for the periodic
nature of walking. We propose a hierarchical architecture that integrates a
low-level phase tracker, oscillators, and a high-level phase modulator. This
controller allows quadruped robots to walk in a natural manner that is
synchronized with external musical rhythms. Our method generates diverse gaits
across different frequencies and achieves real-time synchronization with music
in the physical world. This research establishes a foundational framework for
enabling real-time execution of accurate rhythmic motions in legged robots.
Video is available at website: https://music-walker.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA2025 accepted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RA-DP: Rapid Adaptive Diffusion Policy for Training-Free High-frequency
  Robotics Replanning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04051v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04051v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Ye, Rui Heng Yang, Jun Jin, Yinchuan Li, Amir Rasouli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models exhibit impressive scalability in robotic task learning, yet
they struggle to adapt to novel, highly dynamic environments. This limitation
primarily stems from their constrained replanning ability: they either operate
at a low frequency due to a time-consuming iterative sampling process, or are
unable to adapt to unforeseen feedback in case of rapid replanning. To address
these challenges, we propose RA-DP, a novel diffusion policy framework with
training-free high-frequency replanning ability that solves the above
limitations in adapting to unforeseen dynamic environments. Specifically, our
method integrates guidance signals which are often easily obtained in the new
environment during the diffusion sampling process, and utilizes a novel action
queue mechanism to generate replanned actions at every denoising step without
retraining, thus forming a complete training-free framework for robot motion
adaptation in unseen environments. Extensive evaluations have been conducted in
both well-recognized simulation benchmarks and real robot tasks. Results show
that RA-DP outperforms the state-of-the-art diffusion-based methods in terms of
replanning frequency and success rate. Moreover, we show that our framework is
theoretically compatible with any training-free guidance signal.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Object State Estimation Through Robotic Active Interaction for
  Biological Autonomous Drilling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04043v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04043v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Lin, Enduo Zhao, Saúl Alexis Heredia Pérez, Kanako Harada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the state of biological specimens is challenging due to limited
observation through microscopic vision. For instance, during mouse skull
drilling, the appearance alters little when thinning bone tissue because of its
semi-transparent property and the high-magnification microscopic vision. To
obtain the object's state, we introduce an object state estimation method for
biological specimens through active interaction based on the deflection. The
method is integrated to enhance the autonomous drilling system developed in our
previous work. The method and integrated system were evaluated through 12
autonomous eggshell drilling experiment trials. The results show that the
system achieved a 91.7% successful ratio and 75% detachable ratio, showcasing
its potential applicability in more complex surgical procedures such as mouse
skull craniotomy. This research paves the way for further development of
autonomous robotic systems capable of estimating the object's state through
active interaction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first and second authors contribute equally to this research. 6
  pages, 5 figures, submitted to RA-L</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Autonomous Robotic Bone Micro-Milling System with Automatic Calibration
  and 3D Surface Fitting <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04038v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04038v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enduo Zhao, Xiaofeng Lin, Yifan Wang, Kanako Harada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automating bone micro-milling using a robotic system presents challenges due
to the uncertainties in both the external and internal features of bone tissue.
For example, during a mouse cranial window creation, a circular path with a
radius of 2 to 4 mm needs to be milled on the mouse skull using a microdrill.
The uneven surface and non-uniform thickness of the mouse skull make it
difficult to fully automate this process, requiring the system to possess
advanced perceptual and adaptive capabilities. In this study, we propose an
automatic calibration and 3D surface fitting method and integrate it into an
autonomous robotic bone micro-milling system, enabling it to quickly, in
real-time, and accurately perceive and adapt to the uneven surface and
non-uniform thickness of the target without human assistance. Validation
experiments on euthanized mice demonstrate that the improved system achieves a
success rate of 85.7 % and an average milling time of 2.1 minutes, showing not
only significant performance improvements over the previous system but also
exceptional accuracy, speed, and stability compared to human operators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures, submitted to IROS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dexterous Hand Manipulation via Efficient Imitation-Bootstrapped Online
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04014v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04014v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongchi Huang, Tianle Zhang, Yihang Li, Ling Zhao, Jiayi Li, Zhirui Fang, Chunhe Xia, Lusong Li, Xiaodong He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dexterous hand manipulation in real-world scenarios presents considerable
challenges due to its demands for both dexterity and precision. While imitation
learning approaches have thoroughly examined these challenges, they still
require a significant number of expert demonstrations and are limited by a
constrained performance upper bound. In this paper, we propose a novel and
efficient Imitation-Bootstrapped Online Reinforcement Learning (IBORL) method
tailored for robotic dexterous hand manipulation in real-world environments.
Specifically, we pretrain the policy using a limited set of expert
demonstrations and subsequently finetune this policy through direct
reinforcement learning in the real world. To address the catastrophic
forgetting issues that arise from the distribution shift between expert
demonstrations and real-world environments, we design a regularization term
that balances the exploration of novel behaviors with the preservation of the
pretrained policy. Our experiments with real-world tasks demonstrate that our
method significantly outperforms existing approaches, achieving an almost 100%
success rate and a 23% improvement in cycle time. Furthermore, by finetuning
with online reinforcement learning, our method surpasses expert demonstrations
and uncovers superior policies. Our code and empirical results are available in
https://hggforget.github.io/iborl.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Planning and Control for Deformable Linear Object Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04007v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04007v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Burak Aksoy, John Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manipulating a deformable linear object (DLO) such as wire, cable, and rope
is a common yet challenging task due to their high degrees of freedom and
complex deformation behaviors, especially in an environment with obstacles.
Existing local control methods are efficient but prone to failure in complex
scenarios, while precise global planners are computationally intensive and
difficult to deploy. This paper presents an efficient, easy-to-deploy framework
for collision-free DLO manipulation using mobile manipulators. We demonstrate
the effectiveness of leveraging standard planning tools for high-dimensional
DLO manipulation without requiring custom planners or extensive data-driven
models. Our approach combines an off-the-shelf global planner with a real-time
local controller. The global planner approximates the DLO as a series of rigid
links connected by spherical joints, enabling rapid path planning without the
need for problem-specific planners or large datasets. The local controller
employs control barrier functions (CBFs) to enforce safety constraints,
maintain the DLO integrity, prevent overstress, and handle obstacle avoidance.
It compensates for modeling inaccuracies by using a state-of-the-art
position-based dynamics technique that approximates physical properties like
Young's and shear moduli. We validate our framework through extensive
simulations and real-world demonstrations. In complex obstacle
scenarios-including tent pole transport, corridor navigation, and tasks
requiring varied stiffness-our method achieves a 100% success rate over
thousands of trials, with significantly reduced planning times compared to
state-of-the-art techniques. Real-world experiments include transportation of a
tent pole and a rope using mobile manipulators. We share our ROS-based
implementation to facilitate adoption in various applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SUBMITTED TO IEEE TRANSACTIONS ON AUTOMATION SCIENCE AND ENGINEERING
  (T-ASE)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robotic Compliant Object Prying Using Diffusion Policy Guided by Vision
  and Force Observations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03998v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03998v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeon Ho Kang, Sagar Joshi, Ruopeng Huang, Satyandra K. Gupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing adoption of batteries in the electric vehicle industry and
various consumer products has created an urgent need for effective recycling
solutions. These products often contain a mix of compliant and rigid
components, making robotic disassembly a critical step toward achieving
scalable recycling processes. Diffusion policy has emerged as a promising
approach for learning low-level skills in robotics. To effectively apply
diffusion policy to contact-rich tasks, incorporating force as feedback is
essential. In this paper, we apply diffusion policy with vision and force in a
compliant object prying task. However, when combining low-dimensional contact
force with high-dimensional image, the force information may be diluted. To
address this issue, we propose a method that effectively integrates force with
image data for diffusion policy observations. We validate our approach on a
battery prying task that demands high precision and multi-step execution. Our
model achieves a 96\% success rate in diverse scenarios, marking a 57\%
improvement over the vision-only baseline. Our method also demonstrates
zero-shot transfer capability to handle unseen objects and battery types.
Supplementary videos and implementation codes are available on our project
website. https://rros-lab.github.io/diffusion-with-force.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE RA-L. (C) 2025 IEEE. Personal use of this material
  is permitted. Permission from IEEE must be obtained for all other uses, in
  any current or future media. 8 pages with 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GeoFIK: A Fast and Reliable Geometric Solver for the IK of the Franka
  Arm based on Screw Theory Enabling <span class="highlight-title">Multi</span>ple Redundancy Parameters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo C. Lopez-Custodio, Yuhe Gong, Luis F. C. Figueredo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern robotics applications require an inverse kinematics (IK) solver that
is fast, robust and consistent, and that provides all possible solutions.
Currently, the Franka robot arm is the most widely used manipulator in robotics
research. With 7 DOFs, the IK of this robot is not only complex due to its
1-DOF redundancy, but also due to the link offsets at the wrist and elbow. Due
to this complexity, none of the Franka IK solvers available in the literature
provide satisfactory results when used in real-world applications. Therefore,
in this paper we introduce GeoFIK (Geometric Franka IK), an analytical IK
solver that allows the use of different joint variables to resolve the
redundancy. The approach uses screw theory to describe the entire geometry of
the robot, allowing the computation of the Jacobian matrix prior to computation
of joint angles. All singularities are identified and handled. As an example of
how the geometric elements obtained by the IK can be exploited, a solver with
the swivel angle as the free variable is provided. Several experiments are
carried out to validate the speed, robustness and reliability of the GeoFIK
against two state-of-the-art solvers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GRaD-Nav: Efficiently Learning Visual Drone Navigation with Gaussian
  Radiance Fields and Differentiable Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03984v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03984v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianzhong Chen, Jiankai Sun, Naixiang Gao, JunEn Low, Timothy Chen, Mac Schwager
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous visual navigation is an essential element in robot autonomy.
Reinforcement learning (RL) offers a promising policy training paradigm.
However existing RL methods suffer from high sample complexity, poor
sim-to-real transfer, and limited runtime adaptability to navigation scenarios
not seen during training. These problems are particularly challenging for
drones, with complex nonlinear and unstable dynamics, and strong dynamic
coupling between control and perception. In this paper, we propose a novel
framework that integrates 3D Gaussian Splatting (3DGS) with differentiable deep
reinforcement learning (DDRL) to train vision-based drone navigation policies.
By leveraging high-fidelity 3D scene representations and differentiable
simulation, our method improves sample efficiency and sim-to-real transfer.
Additionally, we incorporate a Context-aided Estimator Network (CENet) to adapt
to environmental variations at runtime. Moreover, by curriculum training in a
mixture of different surrounding environments, we achieve in-task
generalization, the ability to solve new instances of a task not seen during
training. Drone hardware experiments demonstrate our method's high training
efficiency compared to state-of-the-art RL methods, zero shot sim-to-real
transfer for real robot deployment without fine tuning, and ability to adapt to
new instances within the same task class (e.g. to fly through a gate at
different locations with different distractors in the environment).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DEFT: Differentiable Branched Discrete Elastic Rods for Modeling
  Furcated DLOs in Real-Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15037v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15037v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhou Chen, Xiaoyue Wu, Yeheng Zong, Anran Li, Yuzhen Chen, Julie Wu, Bohao Zhang, Ram Vasudevan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous wire harness assembly requires robots to manipulate complex
branched cables with high precision and reliability. A key challenge in
automating this process is predicting how these flexible and branched
structures behave under manipulation. Without accurate predictions, it is
difficult for robots to reliably plan or execute assembly operations. While
existing research has made progress in modeling single-threaded Deformable
Linear Objects (DLOs), extending these approaches to Branched Deformable Linear
Objects (BDLOs) presents fundamental challenges. The junction points in BDLOs
create complex force interactions and strain propagation patterns that cannot
be adequately captured by simply connecting multiple single-DLO models. To
address these challenges, this paper presents Differentiable discrete branched
Elastic rods for modeling Furcated DLOs in real-Time (DEFT), a novel framework
that combines a differentiable physics-based model with a learning framework
to: 1) accurately model BDLO dynamics, including dynamic propagation at
junction points and grasping in the middle of a BDLO, 2) achieve efficient
computation for real-time inference, and 3) enable planning to demonstrate
dexterous BDLO manipulation. A comprehensive series of real-world experiments
demonstrates DEFT's efficacy in terms of accuracy, computational speed, and
generalizability compared to state-of-the-art alternatives. Project
page:https://roahmlab.github.io/DEFT/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentiable Composite Neural Signed Distance Fields for Robot
  Navigation in Dynamic Indoor Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02664v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02664v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. Talha Bukhari, Daniel Lawson, Ahmed H. Qureshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Signed Distance Fields (SDFs) provide a differentiable environment
representation to readily obtain collision checks and well-defined gradients
for robot navigation tasks. However, updating neural SDFs as the scene evolves
entails re-training, which is tedious, time consuming, and inefficient, making
it unsuitable for robot navigation with limited field-of-view in dynamic
environments. Towards this objective, we propose a compositional framework of
neural SDFs to solve robot navigation in indoor environments using only an
onboard RGB-D sensor. Our framework embodies a dual mode procedure for
trajectory optimization, with different modes using complementary methods of
modeling collision costs and collision avoidance gradients. The primary stage
queries the robot body's SDF, swept along the route to goal, at the obstacle
point cloud, enabling swift local optimization of trajectories. The secondary
stage infers the visible scene's SDF by aligning and composing the SDF
representations of its constituents, providing better informed costs and
gradients for trajectory optimization. The dual mode procedure combines the
best of both stages, achieving a success rate of 98%, 14.4% higher than
baseline with comparable amortized plan time on iGibson 2.0. We also
demonstrate its effectiveness in adapting to real-world indoor scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compact LED-Based Displacement Sensing for Robot Fingers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03481v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03481v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amr El-Azizi, Sharfin Islam, Pedro Piacenza, Kai Jiang, Ioannis Kymissis, Matei Ciocarlie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a sensor designed for integration in robot
fingers, where it can provide information on the displacements induced by
external contact. Our sensor uses LEDs to sense the displacement between two
plates connected by a transparent elastomer; when a force is applied to the
finger, the elastomer displaces and the LED signals change. We show that using
LEDs as both light emitters an receivers in this context provides high
sensitivity, allowing such an emitter and receiver pairs to detect very small
displacements. We characterize the standalone performance of the sensor by
testing the ability of a supervised learning model to predict complete force
and torque data from its raw signals, and obtain a mean error between 0.05 and
0.07 N across the three directions of force applied to the finger. Our method
allows for finger-size packaging with no amplification electronics, low cost
manufacturing, easy integration into a complete hand, and high overload shear
forces and bending torques, suggesting future applicability to complete
manipulation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AdaptBot: Combining <span class="highlight-title">LLM</span> with Knowledge Graphs and Human Input for
  Generic-to-Specific Task Decomposition and Knowledge Refinement <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02067v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02067v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivam Singh, Karthik Swaminathan, Nabanita Dash, Ramandeep Singh, Snehasis Banerjee, Mohan Sridharan, Madhava Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An embodied agent assisting humans is often asked to complete new tasks, and
there may not be sufficient time or labeled examples to train the agent to
perform these new tasks. Large Language Models (LLMs) trained on considerable
knowledge across many domains can be used to predict a sequence of abstract
actions for completing such tasks, although the agent may not be able to
execute this sequence due to task-, agent-, or domain-specific constraints. Our
framework addresses these challenges by leveraging the generic predictions
provided by LLM and the prior domain knowledge encoded in a Knowledge Graph
(KG), enabling an agent to quickly adapt to new tasks. The robot also solicits
and uses human input as needed to refine its existing knowledge. Based on
experimental evaluation in the context of cooking and cleaning tasks in
simulation domains, we demonstrate that the interplay between LLM, KG, and
human input leads to substantial performance gains compared with just using the
LLM. Project website{\S}: https://sssshivvvv.github.io/adaptbot/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE International Conference on Robotics and Automation
  (ICRA) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Quadrotor Control From Visual Features Using Differentiable
  Simulation <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15979v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15979v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes Heeg, Yunlong Song, Davide Scaramuzza
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The sample inefficiency of reinforcement learning (RL) remains a significant
challenge in robotics. RL requires large-scale simulation and can still cause
long training times, slowing research and innovation. This issue is
particularly pronounced in vision-based control tasks where reliable state
estimates are not accessible. Differentiable simulation offers an alternative
by enabling gradient back-propagation through the dynamics model, providing
low-variance analytical policy gradients and, hence, higher sample efficiency.
However, its usage for real-world robotic tasks has yet been limited. This work
demonstrates the great potential of differentiable simulation for learning
quadrotor control. We show that training in differentiable simulation
significantly outperforms model-free RL in terms of both sample efficiency and
training time, allowing a policy to learn to recover a quadrotor in seconds
when providing vehicle states and in minutes when relying solely on visual
features. The key to our success is two-fold. First, the use of a simple
surrogate model for gradient computation greatly accelerates training without
sacrificing control performance. Second, combining state representation
learning with policy learning enhances convergence speed in tasks where only
visual features are observable. These findings highlight the potential of
differentiable simulation for real-world robotics and offer a compelling
alternative to conventional RL approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for presentation at the IEEE International Conference on
  Robotics and Automation (ICRA) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TacDiffusion: Force-domain Diffusion Policy for Precise Tactile
  Manipulation <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11047v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11047v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yansong Wu, Zongxie Chen, Fan Wu, Lingyun Chen, Liding Zhang, Zhenshan Bing, Abdalla Swikir, Sami Haddadin, Alois Knoll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assembly is a crucial skill for robots in both modern manufacturing and
service robotics. However, mastering transferable insertion skills that can
handle a variety of high-precision assembly tasks remains a significant
challenge. This paper presents a novel framework that utilizes diffusion models
to generate 6D wrench for high-precision tactile robotic insertion tasks. It
learns from demonstrations performed on a single task and achieves a zero-shot
transfer success rate of 95.7% across various novel high-precision tasks. Our
method effectively inherits the self-adaptability demonstrated by our previous
work. In this framework, we address the frequency misalignment between the
diffusion policy and the real-time control loop with a dynamic system-based
filter, significantly improving the task success rate by 9.15%. Furthermore, we
provide a practical guideline regarding the trade-off between diffusion models'
inference ability and speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages. Accepted to ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Word2Wave: Language Driven Mission Programming for Efficient Subsea
  Deployments of Marine Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18405v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18405v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruo Chen, David Blow, Adnan Abdullah, Md Jahidul Islam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the design and development of a language-based interface
for dynamic mission programming of autonomous underwater vehicles (AUVs). The
proposed `Word2Wave' (W2W) framework enables interactive programming and
parameter configuration of AUVs for remote subsea missions. The W2W framework
includes: (i) a set of novel language rules and command structures for
efficient language-to-mission mapping; (ii) a GPT-based prompt engineering
module for training data generation; (iii) a small language model (SLM)-based
sequence-to-sequence learning pipeline for mission command generation from
human speech or text; and (iv) a novel user interface for 2D mission map
visualization and human-machine interfacing. The proposed learning pipeline
adapts an SLM named T5-Small that can learn language-to-mission mapping from
processed language data effectively, providing robust and efficient
performance. In addition to a benchmark evaluation with state-of-the-art, we
conduct a user interaction study to demonstrate the effectiveness of W2W over
commercial AUV programming interfaces. Across participants, W2W-based
programming required less than 10\% time for mission programming compared to
traditional interfaces; it is deemed to be a simpler and more natural paradigm
for subsea mission programming with a usability score of 76.25. W2W opens up
promising future research opportunities on hands-free AUV mission programming
for efficient subsea deployments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Data-Driven Aggressive Autonomous Racing Framework Utilizing Local
  Trajectory Planning with Velocity Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11570v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11570v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhouheng Li, Bei Zhou, Cheng Hu, Lei Xie, Hongye Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of autonomous driving has boosted the research on autonomous
racing. However, existing local trajectory planning methods have difficulty
planning trajectories with optimal velocity profiles at racetracks with sharp
corners, thus weakening the performance of autonomous racing. To address this
problem, we propose a local trajectory planning method that integrates Velocity
Prediction based on Model Predictive Contouring Control (VPMPCC). The optimal
parameters of VPMPCC are learned through Bayesian Optimization (BO) based on a
proposed novel Objective Function adapted to Racing (OFR). Specifically, VPMPCC
achieves velocity prediction by encoding the racetrack as a reference velocity
profile and incorporating it into the optimization problem. This method
optimizes the velocity profile of local trajectories, especially at corners
with significant curvature. The proposed OFR balances racing performance with
vehicle safety, ensuring safe and efficient BO training. In the simulation, the
number of training iterations for OFR-based BO is reduced by 42.86% compared to
the state-of-the-art method. The optimal simulation-trained parameters are then
applied to a real-world F1TENTH vehicle without retraining. During prolonged
racing on a custom-built racetrack featuring significant sharp corners, the
mean projected velocity of VPMPCC reaches 93.18% of the vehicle's handling
limits. The released code is available at https://github.com/zhouhengli/VPMPCC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pretrained Embeddings as a Behavior Specification Mechanism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02012v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02012v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parv Kapoor, Abigail Hammer, Ashish Kapoor, Karen Leung, Eunsuk Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an approach to formally specifying the behavioral properties of
systems that rely on a perception model for interactions with the physical
world. The key idea is to introduce embeddings -- mathematical representations
of a real-world concept -- as a first-class construct in a specification
language, where properties are expressed in terms of distances between a pair
of ideal and observed embeddings. To realize this approach, we propose a new
type of temporal logic called Embedding Temporal Logic (ETL), and describe how
it can be used to express a wider range of properties about AI-enabled systems
than previously possible. We demonstrate the applicability of ETL through a
preliminary evaluation involving planning tasks in robots that are driven by
foundation models; the results are promising, showing that embedding-based
specifications can be used to steer a system towards desirable behaviors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FRNet: Frustum-Range Networks for Scalable LiDAR Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04484v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04484v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Xu, Lingdong Kong, Hui Shuai, Qingshan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LiDAR segmentation has become a crucial component of advanced autonomous
driving systems. Recent range-view LiDAR segmentation approaches show promise
for real-time processing. However, they inevitably suffer from corrupted
contextual information and rely heavily on post-processing techniques for
prediction refinement. In this work, we propose FRNet, a simple yet powerful
method aimed at restoring the contextual information of range image pixels
using corresponding frustum LiDAR points. First, a frustum feature encoder
module is used to extract per-point features within the frustum region, which
preserves scene consistency and is critical for point-level predictions. Next,
a frustum-point fusion module is introduced to update per-point features
hierarchically, enabling each point to extract more surrounding information
through the frustum features. Finally, a head fusion module is used to fuse
features at different levels for final semantic predictions. Extensive
experiments conducted on four popular LiDAR segmentation benchmarks under
various task setups demonstrate the superiority of FRNet. Notably, FRNet
achieves 73.3% and 82.5% mIoU scores on the testing sets of SemanticKITTI and
nuScenes. While achieving competitive performance, FRNet operates 5 times
faster than state-of-the-art approaches. Such high efficiency opens up new
possibilities for more scalable LiDAR segmentation. The code has been made
publicly available at https://github.com/Xiangxu-0103/FRNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TIP 2025; 18 pages, 11 figures, 14 tables; Code at
  https://github.com/Xiangxu-0103/FRNet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Dataset</span> and Benchmark for Shape Completion of Fruits for Agricultural
  Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13304v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13304v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Magistri, Thomas Läbe, Elias Marks, Sumanth Nagulavancha, Yue Pan, Claus Smitt, Lasse Klingbeil, Michael Halstead, Heiner Kuhlmann, Chris McCool, Jens Behley, Cyrill Stachniss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the world population is expected to reach 10 billion by 2050, our
agricultural production system needs to double its productivity despite a
decline of human workforce in the agricultural sector. Autonomous robotic
systems are one promising pathway to increase productivity by taking over
labor-intensive manual tasks like fruit picking. To be effective, such systems
need to monitor and interact with plants and fruits precisely, which is
challenging due to the cluttered nature of agricultural environments causing,
for example, strong occlusions. Thus, being able to estimate the complete 3D
shapes of objects in presence of occlusions is crucial for automating
operations such as fruit harvesting. In this paper, we propose the first
publicly available 3D shape completion dataset for agricultural vision systems.
We provide an RGB-D dataset for estimating the 3D shape of fruits.
Specifically, our dataset contains RGB-D frames of single sweet peppers in lab
conditions but also in a commercial greenhouse. For each fruit, we additionally
collected high-precision point clouds that we use as ground truth. For
acquiring the ground truth shape, we developed a measuring process that allows
us to record data of real sweet pepper plants, both in the lab and in the
greenhouse with high precision, and determine the shape of the sensed fruits.
We release our dataset, consisting of almost 7,000 RGB-D frames belonging to
more than 100 different fruits. We provide segmented RGB-D frames, with camera
intrinsics to easily obtain colored point clouds, together with the
corresponding high-precision, occlusion-free point clouds obtained with a
high-precision laser scanner. We additionally enable evaluation of shape
completion approaches on a hidden test set through a public challenge on a
benchmark server.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tracking Control of Euler-Lagrangian Systems with Prescribed State,
  Input, and Temporal Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01866v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01866v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chidre Shravista Kashyap, Pushpak Jagtap, Jishnu Keshavan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The synthesis of a smooth tracking control policy for Euler-Lagrangian (EL)
systems with stringent regions of operation induced by state, input and
temporal (SIT) constraints is a very challenging task. In contrast with
existing methods that utilize prior knowledge of EL model parameters and
uncertainty bounds, this study proposes an approximation-free adaptive barrier
function-based control policy to ensure local prescribed time convergence of
tracking error under state and input constraints. The proposed control policy
accomplishes this by utilizing smooth time-based generator functions embedded
in the filtered tracking error, which is combined with a saturation function
that limits control action and confines states within the prescribed limits by
enforcing the time-varying bounds on the filtered tracking error. Importantly,
corresponding feasibility conditions pertaining to the minimum control
authority, maximum disturbance rejection capability of the control policy, and
the viable set of initial conditions are derived, illuminating the narrow
operating domain of the EL systems arising from the interplay of SIT
constraints. Numerical validation studies with three different robotic
manipulators are employed to demonstrate the efficacy of the proposed scheme. A
detailed performance comparison study with leading alternative designs is also
undertaken to illustrate the superior performance of the proposed scheme.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Open-Source and Modular Space Systems with ATMOS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16973v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16973v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Roque, Sujet Phodapol, Elias Krantz, Jaeyoung Lim, Joris Verhagen, Frank J. Jiang, David Dörner, Huina Mao, Gunnar Tibert, Roland Siegwart, Ivan Stenius, Jana Tumova, Christer Fuglesang, Dimos V. Dimarogonas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the near future, autonomous space systems will compose many of the
deployed spacecraft. Their tasks will involve autonomous rendezvous and
proximity operations with large structures, such as inspections, assembly, and
maintenance of orbiting space stations, as well as human-assistance tasks over
shared workspaces. To promote replicable and reliable scientific results for
autonomous control of spacecraft, we present the design of a space robotics
laboratory based on open-source and modular software and hardware. The
simulation software provides a software-in-the-loop architecture that
seamlessly transfers simulated results to the hardware. Our results provide an
insight into such a system, including comparisons of hardware and software
results, as well as control and planning methodologies for controlling
free-flying platforms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TACO: General Acrobatic Flight Control via Target-and-Command-Oriented
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01125v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01125v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zikang Yin, Canlun Zheng, Shiliang Guo, Zhikun Wang, Shiyu Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although acrobatic flight control has been studied extensively, one key
limitation of the existing methods is that they are usually restricted to
specific maneuver tasks and cannot change flight pattern parameters online. In
this work, we propose a target-and-command-oriented reinforcement learning
(TACO) framework, which can handle different maneuver tasks in a unified way
and allows online parameter changes. Additionally, we propose a spectral
normalization method with input-output rescaling to enhance the policy's
temporal and spatial smoothness, independence, and symmetry, thereby overcoming
the sim-to-real gap. We validate the TACO approach through extensive simulation
and real-world experiments, demonstrating its capability to achieve high-speed
circular flights and continuous multi-flips.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>For the experiment video, please refer to
  https://www.youtube.com/watch?v=4tX_25BcMJw&ab_channel=WINDYLab</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Backbone for Long-Horizon Robot Task Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01334v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01334v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoshuai Chen, Wei Chen, Dongmyoung Lee, Yukun Ge, Nicolas Rojas, Petar Kormushev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end robot learning, particularly for long-horizon tasks, often results
in unpredictable outcomes and poor generalization. To address these challenges,
we propose a novel Therblig-Based Backbone Framework (TBBF) as a fundamental
structure to enhance interpretability, data efficiency, and generalization in
robotic systems. TBBF utilizes expert demonstrations to enable therblig-level
task decomposition, facilitate efficient action-object mapping, and generate
adaptive trajectories for new scenarios. The approach consists of two stages:
offline training and online testing. During the offline training stage, we
developed the Meta-RGate SynerFusion (MGSF) network for accurate therblig
segmentation across various tasks. In the online testing stage, after a
one-shot demonstration of a new task is collected, our MGSF network extracts
high-level knowledge, which is then encoded into the image using Action
Registration (ActionREG). Additionally, Large Language Model (LLM)-Alignment
Policy for Visual Correction (LAP-VC) is employed to ensure precise action
registration, facilitating trajectory transfer in novel robot scenarios.
Experimental results validate these methods, achieving 94.37% recall in
therblig segmentation and success rates of 94.4% and 80% in real-world online
robot testing for simple and complex scenarios, respectively. Supplementary
material is available at:
https://sites.google.com/view/therbligsbasedbackbone/home
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures. This work has been published by IEEE Robotics and
  Automation Letters (RA-L)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Deterministic Policy Gradient for Disturbance Attenuation and Its
  Application to Quadrotor Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21057v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21057v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taeho Lee, Donghwan Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Practical control systems pose significant challenges in identifying optimal
control policies due to uncertainties in the system model and external
disturbances. While $H_\infty$ control techniques are commonly used to design
robust controllers that mitigate the effects of disturbances, these methods
often require complex and computationally intensive calculations. To address
this issue, this paper proposes a reinforcement learning algorithm called
Robust Deterministic Policy Gradient (RDPG), which formulates the $H_\infty$
control problem as a two-player zero-sum dynamic game. In this formulation, one
player (the user) aims to minimize the cost, while the other player (the
adversary) seeks to maximize it. We then employ deterministic policy gradient
(DPG) and its deep reinforcement learning counterpart to train a robust control
policy with effective disturbance attenuation. In particular, for practical
implementation, we introduce an algorithm called robust deep deterministic
policy gradient (RDDPG), which employs a deep neural network architecture and
integrates techniques from the twin-delayed deep deterministic policy gradient
(TD3) to enhance stability and learning efficiency. To evaluate the proposed
algorithm, we implement it on an unmanned aerial vehicle (UAV) tasked with
following a predefined path in a disturbance-prone environment. The
experimental results demonstrate that the proposed method outperforms other
control approaches in terms of robustness against disturbances, enabling
precise real-time tracking of moving targets even under severe disturbance
conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Collision Sensitivity for Efficient and Safe Human-Robot
  Collaboration <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.20184v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.20184v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Rustler, Matej Misar, Matej Hoffmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  What is considered safe for a robot operator during physical human-robot
collaboration (HRC) is specified in corresponding HRC standards (e.g., ISO/TS
15066). The regime that allows collisions between the moving robot and the
operator, called Power and Force Limiting (PFL), restricts the permissible
contact forces. Using the same fixed contact thresholds on the entire robot
surface results in significant and unnecessary productivity losses, as the
robot needs to stop even when impact forces are within limits. Here we present
a framework that decides whether the robot should interrupt or continue its
motion based on estimated collision force computed individually for different
parts of the robot body and dynamically on the fly, based on the effective mass
of each robot link and the link velocity. We performed experiments on simulated
and real 6-axis collaborative robot arm (UR10e) with sensitive skin (AIRSKIN)
for collision detection and isolation. To demonstrate the generality of our
method, we added experiments on the simulated KUKA LBR iiwa robot, where
collision detection and isolation draws on joint torque sensing. On a mock
pick-and-place scenario with both transient and quasi-static collisions, we
demonstrate how sensitivity to collisions influences the task performance and
number of stops. We show an increase in productivity over 45% from using the
standard approach that interrupts the tasks during every collision. While
reducing the cycle time and the number of interruptions, our framework also
ensures the safety of human operators. The method is applicable to any robot
for which the effective mass can be calculated.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Don't Shake the Wheel: Momentum-Aware Planning in End-to-End Autonomous
  Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03125v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03125v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziying Song, Caiyan Jia, Lin Liu, Hongyu Pan, Yongchang Zhang, Junming Wang, Xingyu Zhang, Shaoqing Xu, Lei Yang, Yadan Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end autonomous driving frameworks enable seamless integration of
perception and planning but often rely on one-shot trajectory prediction, which
may lead to unstable control and vulnerability to occlusions in single-frame
perception. To address this, we propose the Momentum-Aware Driving (MomAD)
framework, which introduces trajectory momentum and perception momentum to
stabilize and refine trajectory predictions. MomAD comprises two core
components: (1) Topological Trajectory Matching (TTM) employs Hausdorff
Distance to select the optimal planning query that aligns with prior paths to
ensure coherence;(2) Momentum Planning Interactor (MPI) cross-attends the
selected planning query with historical queries to expand static and dynamic
perception files. This enriched query, in turn, helps regenerate long-horizon
trajectory and reduce collision risks. To mitigate noise arising from dynamic
environments and detection errors, we introduce robust instance denoising
during training, enabling the planning model to focus on critical signals and
improve its robustness. We also propose a novel Trajectory Prediction
Consistency (TPC) metric to quantitatively assess planning stability.
Experiments on the nuScenes dataset demonstrate that MomAD achieves superior
long-term consistency (>=3s) compared to SOTA methods. Moreover, evaluations on
the curated Turning-nuScenes shows that MomAD reduces the collision rate by 26%
and improves TPC by 0.97m (33.45%) over a 6s prediction horizon, while
closedloop on Bench2Drive demonstrates an up to 16.3% improvement in success
rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPIBOT: A Drone-Tethered Mobile Gripper for Robust Aerial Object
  Retrieval in Dynamic Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16181v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16181v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gyuree Kang, Ozan Güneş, Seungwook Lee, Maulana Bisyir Azhari, David Hyunchul Shim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world field operations, aerial grasping systems face significant
challenges in dynamic environments due to strong winds, shifting surfaces, and
the need to handle heavy loads. Particularly when dealing with heavy objects,
the powerful propellers of the drone can inadvertently blow the target object
away as it approaches, making the task even more difficult. To address these
challenges, we introduce SPIBOT, a novel drone-tethered mobile gripper system
designed for robust and stable autonomous target retrieval. SPIBOT operates via
a tether, much like a spider, allowing the drone to maintain a safe distance
from the target. To ensure both stable mobility and secure grasping
capabilities, SPIBOT is equipped with six legs and sensors to estimate the
robot's and mission's states. It is designed with a reduced volume and weight
compared to other hexapod robots, allowing it to be easily stowed under the
drone and reeled in as needed. Designed for the 2024 MBZIRC Maritime Grand
Challenge, SPIBOT is built to retrieve a 1kg target object in the highly
dynamic conditions of the moving deck of a ship. This system integrates a
real-time action selection algorithm that dynamically adjusts the robot's
actions based on proximity to the mission goal and environmental conditions,
enabling rapid and robust mission execution. Experimental results across
various terrains, including a pontoon on a lake, a grass field, and rubber mats
on coastal sand, demonstrate SPIBOT's ability to efficiently and reliably
retrieve targets. SPIBOT swiftly converges on the target and completes its
mission, even when dealing with irregular initial states and noisy information
introduced by the drone.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HOVER: Versatile Neural Whole-Body Controller for Humanoid Robots <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21229v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21229v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tairan He, Wenli Xiao, Toru Lin, Zhengyi Luo, Zhenjia Xu, Zhenyu Jiang, Jan Kautz, Changliu Liu, Guanya Shi, Xiaolong Wang, Linxi Fan, Yuke Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humanoid whole-body control requires adapting to diverse tasks such as
navigation, loco-manipulation, and tabletop manipulation, each demanding a
different mode of control. For example, navigation relies on root velocity
tracking, while tabletop manipulation prioritizes upper-body joint angle
tracking. Existing approaches typically train individual policies tailored to a
specific command space, limiting their transferability across modes. We present
the key insight that full-body kinematic motion imitation can serve as a common
abstraction for all these tasks and provide general-purpose motor skills for
learning multiple modes of whole-body control. Building on this, we propose
HOVER (Humanoid Versatile Controller), a multi-mode policy distillation
framework that consolidates diverse control modes into a unified policy. HOVER
enables seamless transitions between control modes while preserving the
distinct advantages of each, offering a robust and scalable solution for
humanoid control across a wide range of modes. By eliminating the need for
policy retraining for each control mode, our approach improves efficiency and
flexibility for future humanoid applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICRA 2025, Project Page: see
  https://hover-versatile-humanoid.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Large-Scale <span class="highlight-title">UWB</span> Anchor Calibration and One-Shot Localization Using
  Gaussian Process <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16880v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16880v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenghai Yuan, Boyang Lou, Thien-Minh Nguyen, Pengyu Yin, Muqing Cao, Xinghang Xu, Jianping Li, Jie Xu, Siyu Chen, <span class="highlight-author">Lihua Xie</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ultra-wideband (UWB) is gaining popularity with devices like AirTags for
precise home item localization but faces significant challenges when scaled to
large environments like seaports. The main challenges are calibration and
localization in obstructed conditions, which are common in logistics
environments. Traditional calibration methods, dependent on line-of-sight
(LoS), are slow, costly, and unreliable in seaports and warehouses, making
large-scale localization a significant pain point in the industry. To overcome
these challenges, we propose a UWB-LiDAR fusion-based calibration and one-shot
localization framework. Our method uses Gaussian Processes to estimate anchor
position from continuous-time LiDAR Inertial Odometry with sampled UWB ranges.
This approach ensures accurate and reliable calibration with just one round of
sampling in large-scale areas, I.e., 600x450 square meter. With the LoS issues,
UWB-only localization can be problematic, even when anchor positions are known.
We demonstrate that by applying a UWB-range filter, the search range for LiDAR
loop closure descriptors is significantly reduced, improving both accuracy and
speed. This concept can be applied to other loop closure detection methods,
enabling cost-effective localization in large-scale warehouses and seaports. It
significantly improves precision in challenging environments where UWB-only and
LiDAR-Inertial methods fall short, as shown in the video
(https://youtu.be/oY8jQKdM7lU). We will open-source our datasets and
calibration codes for community use.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted to IEEE International Conference on
  Robotics and Automation (ICRA) @ 2025 IEEE. Personal use of this material is
  permitted. Permission from IEEE must be obtained for all other uses,
  including reprinting/redistribution, creating new works, or reuse of any
  copyrighted components of this work in other media</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tensegrity Robot Proprioceptive State Estimation with Geometric
  Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24226v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24226v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenzhe Tong, Tzu-Yuan Lin, Jonathan Mi, Yicheng Jiang, Maani Ghaffari, Xiaonan Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tensegrity robots, characterized by a synergistic assembly of rigid rods and
elastic cables, form robust structures that are resistant to impacts. However,
this design introduces complexities in kinematics and dynamics, complicating
control and state estimation. This work presents a novel proprioceptive state
estimator for tensegrity robots. The estimator initially uses the geometric
constraints of 3-bar prism tensegrity structures, combined with IMU and motor
encoder measurements, to reconstruct the robot's shape and orientation. It then
employs a contact-aided invariant extended Kalman filter with forward
kinematics to estimate the global position and orientation of the tensegrity
robot. The state estimator's accuracy is assessed against ground truth data in
both simulated environments and real-world tensegrity robot applications. It
achieves an average drift percentage of 4.2%, comparable to the state
estimation performance of traditional rigid robots. This state estimator
advances the state of the art in tensegrity robot state estimation and has the
potential to run in real-time using onboard sensors, paving the way for full
autonomy of tensegrity robots in unstructured environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint; 8 pages, 12 figures, 2 tables; Code at
  https://github.com/Jonathan-Twz/tensegrity-robot-state-estimator</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dur360BEV: A Real-wo<span class="highlight-title">rl</span>d 360-degree Single Camera <span class="highlight-title">Dataset</span> and Benchmark
  for Bird-Eye View Mapping in Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.00675v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.00675v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenke E, Chao Yuan, Li Li, Yixin Sun, Yona Falinie A. Gaus, Amir Atapour-Abarghouei, Toby P. Breckon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Dur360BEV, a novel spherical camera autonomous driving dataset
equipped with a high-resolution 128-channel 3D LiDAR and a RTK-refined GNSS/INS
system, along with a benchmark architecture designed to generate Bird-Eye-View
(BEV) maps using only a single spherical camera. This dataset and benchmark
address the challenges of BEV generation in autonomous driving, particularly by
reducing hardware complexity through the use of a single 360-degree camera
instead of multiple perspective cameras. Within our benchmark architecture, we
propose a novel spherical-image-to-BEV module that leverages spherical imagery
and a refined sampling strategy to project features from 2D to 3D. Our approach
also includes an innovative application of focal loss, specifically adapted to
address the extreme class imbalance often encountered in BEV segmentation
tasks, that demonstrates improved segmentation performance on the Dur360BEV
dataset. The results show that our benchmark not only simplifies the sensor
setup but also achieves competitive performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DexMimicGen: Automated Data Generation for Bimanual Dexterous
  Manipulation via Imitation Learning <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.24185v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.24185v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Jiang, Yuqi Xie, Kevin Lin, Zhenjia Xu, Weikang Wan, Ajay Mandlekar, Linxi Fan, Yuke Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning from human demonstrations is an effective means to teach
robots manipulation skills. But data acquisition is a major bottleneck in
applying this paradigm more broadly, due to the amount of cost and human effort
involved. There has been significant interest in imitation learning for
bimanual dexterous robots, like humanoids. Unfortunately, data collection is
even more challenging here due to the challenges of simultaneously controlling
multiple arms and multi-fingered hands. Automated data generation in simulation
is a compelling, scalable alternative to fuel this need for data. To this end,
we introduce DexMimicGen, a large-scale automated data generation system that
synthesizes trajectories from a handful of human demonstrations for humanoid
robots with dexterous hands. We present a collection of simulation environments
in the setting of bimanual dexterous manipulation, spanning a range of
manipulation behaviors and different requirements for coordination among the
two arms. We generate 21K demos across these tasks from just 60 source human
demos and study the effect of several data generation and policy learning
decisions on agent performance. Finally, we present a real-to-sim-to-real
pipeline and deploy it on a real-world humanoid can sorting task. Generated
datasets, simulation environments and additional results are at
https://dexmimicgen.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2025. Project website: https://dexmimicgen.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Select before Act: Spatially Decoupled Action Repetition for Continuous
  Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06919v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06919v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Buqing Nie, Yangqing Fu, Yue Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) has achieved remarkable success in various
continuous control tasks, such as robot manipulation and locomotion. Different
to mainstream RL which makes decisions at individual steps, recent studies have
incorporated action repetition into RL, achieving enhanced action persistence
with improved sample efficiency and superior performance. However, existing
methods treat all action dimensions as a whole during repetition, ignoring
variations among them. This constraint leads to inflexibility in decisions,
which reduces policy agility with inferior effectiveness. In this work, we
propose a novel repetition framework called SDAR, which implements Spatially
Decoupled Action Repetition through performing closed-loop act-or-repeat
selection for each action dimension individually. SDAR achieves more flexible
repetition strategies, leading to an improved balance between action
persistence and diversity. Compared to existing repetition frameworks, SDAR is
more sample efficient with higher policy performance and reduced action
fluctuation. Experiments are conducted on various continuous control scenarios,
demonstrating the effectiveness of spatially decoupled repetition design
proposed in this work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Search-Based Path Planning in Interactive Environments among Movable
  Obstacles <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18333v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18333v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongqiang Ren, Bunyod Suvonov, Guofei Chen, Botao He, Yijie Liao, Cornelia Fermuller, Ji Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates Path planning Among Movable Obstacles (PAMO), which
seeks a minimum cost collision-free path among static obstacles from start to
goal while allowing the robot to push away movable obstacles (i.e., objects)
along its path when needed. To develop planners that are complete and optimal
for PAMO, the planner has to search a giant state space involving both the
location of the robot as well as the locations of the objects, which grows
exponentially with respect to the number of objects. This paper leverages a
simple yet under-explored idea that, only a small fraction of this giant state
space needs to be searched during planning as guided by a heuristic, and most
of the objects far away from the robot are intact, which thus leads to runtime
efficient algorithms. Based on this idea, this paper introduces two PAMO
formulations, i.e., bi-objective and resource constrained problems in an
occupancy grid, and develops PAMO*, a planning method with completeness and
solution optimality guarantees, to solve the two problems. We then further
extend PAMO* to hybrid-state PAMO* to plan in continuous spaces with
high-fidelity interaction between the robot and the objects. Our results show
that, PAMO* can often find optimal solutions within a second in cluttered maps
with up to 400 objects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ iWalker: Imperative Visual Planning for Walking Humanoid Robot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18361v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18361v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Lin, Yuhao Huang, Taimeng Fu, Xiaobin Xiong, Chen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humanoid robots, designed to operate in human-centric environments, serve as
a fundamental platform for a broad range of tasks. Although humanoid robots
have been extensively studied for decades, a majority of existing humanoid
robots still heavily rely on complex modular frameworks, leading to
inflexibility and potential compounded errors from independent sensing,
planning, and acting components. In response, we propose an end-to-end humanoid
sense-plan-act walking system, enabling vision-based obstacle avoidance and
footstep planning for whole body balancing simultaneously. We designed two
imperative learning (IL)-based bilevel optimizations for model-predictive step
planning and whole body balancing, respectively, to achieve self-supervised
learning for humanoid robot walking. This enables the robot to learn from
arbitrary unlabeled data, improving its adaptability and generalization
capabilities. We refer to our method as iWalker and demonstrate its
effectiveness in both simulated and real-world environments, representing a
significant advancement toward autonomous humanoid robots.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Path Planning in Complex Environments with Superquadrics and
  Voronoi-Based Orientation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05279v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05279v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lin Yang, Ganesh Iyer, Baichuan Lou, Sri Harsha Turlapati, Chen Lv, Domenico Campolo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Path planning in narrow passages is a challenging problem in various
applications. Traditional planning algorithms often face challenges in complex
environments like mazes and traps, where narrow entrances require special
orientation control for successful navigation. In this work, we present a novel
approach that combines superquadrics (SQ) representation and Voronoi diagrams
to solve the narrow passage problem in both 2D and 3D environment. Our method
utilizes the SQ formulation to expand obstacles, eliminating impassable
passages, while Voronoi hyperplane ensures maximum clearance path.
Additionally, the hyperplane provides a natural reference for robot
orientation, aligning its long axis with the passage direction. We validate our
framework through a 2D object retrieval task and 3D drone simulation,
demonstrating that our approach outperforms classical planners and a
cutting-edge drone planner by ensuring passable trajectories with maximum
clearance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BODex: Scalable and Efficient Robotic Dexterous Grasp Synthesis Using
  Bilevel Optimization <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16490v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16490v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Chen, Yubin Ke, He Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic dexterous grasping is important for interacting with the environment.
To unleash the potential of data-driven models for dexterous grasping, a
large-scale, high-quality dataset is essential. While gradient-based
optimization offers a promising way for constructing such datasets, previous
works suffer from limitations, such as inefficiency, strong assumptions in the
grasp quality energy, or limited object sets for experiments. Moreover, the
lack of a standard benchmark for comparing different methods and datasets
hinders progress in this field. To address these challenges, we develop a
highly efficient synthesis system and a comprehensive benchmark with MuJoCo for
dexterous grasping. We formulate grasp synthesis as a bilevel optimization
problem, combining a novel lower-level quadratic programming (QP) with an
upper-level gradient descent process. By leveraging recent advances in
CUDA-accelerated robotic libraries and GPU-based QP solvers, our system can
parallelize thousands of grasps and synthesize over 49 grasps per second on a
single 3090 GPU. Our synthesized grasps for Shadow, Allegro, and Leap hands all
achieve a success rate above 75% in simulation, with a penetration depth under
1 mm, outperforming existing baselines on nearly all metrics. Compared to the
previous large-scale dataset, DexGraspNet, our dataset significantly improves
the performance of learning models, with a success rate from around 40% to 80%
in simulation. Real-world testing of the trained model on the Shadow Hand
achieves an 81% success rate across 20 diverse objects. The codes and datasets
are released on our project page: https://pku-epic.github.io/BODex.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Skeleton-Based Topological Planner for Exploration in Complex Unknown
  Environments <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.13664v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.13664v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haochen Niu, Xingwu Ji, Lantao Zhang, Fei Wen, Rendong Ying, Peilin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The capability of autonomous exploration in complex, unknown environments is
important in many robotic applications. While recent research on autonomous
exploration have achieved much progress, there are still limitations, e.g.,
existing methods relying on greedy heuristics or optimal path planning are
often hindered by repetitive paths and high computational demands. To address
such limitations, we propose a novel exploration framework that utilizes the
global topology information of observed environment to improve exploration
efficiency while reducing computational overhead. Specifically, global
information is utilized based on a skeletal topological graph representation of
the environment geometry. We first propose an incremental skeleton extraction
method based on wavefront propagation, based on which we then design an
approach to generate a lightweight topological graph that can effectively
capture the environment's structural characteristics. Building upon this, we
introduce a finite state machine that leverages the topological structure to
efficiently plan coverage paths, which can substantially mitigate the
back-and-forth maneuvers (BFMs) problem. Experimental results demonstrate the
superiority of our method in comparison with state-of-the-art methods. The
source code will be made publicly available at:
https://github.com/Haochen-Niu/STGPlanner.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures. Accepted to be presented at the ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Joint-repositionable Inner-wireless Planar Snake Robot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.13916v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.13916v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayato Kanada, Ryo Takahashi, Keito Hayashi, Ryusuke Hosaka, Wakako Yukita, Yasutaka Nakashima, Tomoyuki Yokota, Takao Someya, Mitsuhiro Kamezaki, Yoshihiro Kawahara, Motoji Yamamoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bio-inspired multi-joint snake robots offer the advantages of terrain
adaptability due to their limbless structure and high flexibility. However, a
series of dozens of motor units in typical multiple-joint snake robots results
in a heavy body structure and hundreds of watts of high power consumption. This
paper presents a joint-repositionable, inner-wireless snake robot that enables
multi-joint-like locomotion using a low-powered underactuated mechanism. The
snake robot, consisting of a series of flexible passive links, can dynamically
change its joint coupling configuration by repositioning motor-driven joint
units along rack gears inside the robot. Additionally, a soft robot skin
wirelessly powers the internal joint units, avoiding the risk of wire tangling
and disconnection caused by the movable joint units. The combination of the
joint-repositionable mechanism and the wireless-charging-enabled soft skin
achieves a high degree of bending, along with a lightweight structure of 1.3 kg
and energy-efficient wireless power transmission of 7.6 watts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TeraSim: Uncovering Unknown Unsafe Events for Autonomous Vehicles
  through Generative Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03629v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03629v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowei Sun, Xintao Yan, Zhijie Qiao, Haojie Zhu, Yihao Sun, Jiawei Wang, Shengyin Shen, Darian Hogue, Rajanikant Ananta, Derek Johnson, Greg Stevens, Greg McGuire, Yifan Wei, Wei Zheng, Yong Sun, Yasuo Fukai, Henry X. Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traffic simulation is essential for autonomous vehicle (AV) development,
enabling comprehensive safety evaluation across diverse driving conditions.
However, traditional rule-based simulators struggle to capture complex human
interactions, while data-driven approaches often fail to maintain long-term
behavioral realism or generate diverse safety-critical events. To address these
challenges, we propose TeraSim, an open-source, high-fidelity traffic
simulation platform designed to uncover unknown unsafe events and efficiently
estimate AV statistical performance metrics, such as crash rates. TeraSim is
designed for seamless integration with third-party physics simulators and
standalone AV stacks, to construct a complete AV simulation system.
Experimental results demonstrate its effectiveness in generating diverse
safety-critical events involving both static and dynamic agents, identifying
hidden deficiencies in AV systems, and enabling statistical performance
evaluation. These findings highlight TeraSim's potential as a practical tool
for AV safety assessment, benefiting researchers, developers, and policymakers.
The code is available at https://github.com/mcity/TeraSim.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence <span class="chip" style="font-size: 60%">151</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ L$^2$M: Mutual Information Scaling Law for Long-Context Language
  Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuo Chen, Oriol Mayné i Comas, Zhuotao Jin, Di Luo, Marin Soljačić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We rigorously establish a bipartite mutual information scaling law in natural
language that governs long-range dependencies. This scaling law, which we show
is distinct from and scales independently of the conventional two-point mutual
information, is the key to understanding long-context language modeling. Using
this scaling law, we formulate the Long-context Language Modeling (L$^2$M)
condition, which relates a model's capacity for effective long context length
modeling to the scaling of its latent state size for storing past information.
Our results are validated through experiments on both transformers and state
space models. This work establishes a theoretical foundation that guides the
development of large language models toward longer context lengths.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 12 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shifting Long-Context <span class="highlight-title">LLM</span>s Research from Input to Output 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04723v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04723v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Wu, Yushi Bai, Zhiqing Hu, Shangqing Tu, Ming Shan Hee, Juanzi Li, Roy Ka-Wei Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in long-context Large Language Models (LLMs) have
primarily concentrated on processing extended input contexts, resulting in
significant strides in long-context comprehension. However, the equally
critical aspect of generating long-form outputs has received comparatively less
attention. This paper advocates for a paradigm shift in NLP research toward
addressing the challenges of long-output generation. Tasks such as novel
writing, long-term planning, and complex reasoning require models to understand
extensive contexts and produce coherent, contextually rich, and logically
consistent extended text. These demands highlight a critical gap in current LLM
capabilities. We underscore the importance of this under-explored domain and
call for focused efforts to develop foundational LLMs tailored for generating
high-quality, long-form outputs, which hold immense potential for real-world
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enough Coin Flips Can Make <span class="highlight-title">LLM</span>s Act Bayesian 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ritwik Gupta, Rodolfo Corona, Jiaxin Ge, Eric Wang, Dan Klein, Trevor Darrell, David M. Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) exhibit the ability to generalize given few-shot
examples in their input prompt, an emergent capability known as in-context
learning (ICL). We investigate whether LLMs utilize ICL to perform structured
reasoning in ways that are consistent with a Bayesian framework or rely on
pattern matching. Using a controlled setting of biased coin flips, we find
that: (1) LLMs often possess biased priors, causing initial divergence in
zero-shot settings, (2) in-context evidence outweighs explicit bias
instructions, (3) LLMs broadly follow Bayesian posterior updates, with
deviations primarily due to miscalibrated priors rather than flawed updates,
and (4) attention magnitude has negligible effect on Bayesian inference. With
sufficient demonstrations of biased coin flips via ICL, LLMs update their
priors in a Bayesian manner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large
  Language Model Pretraining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Houyi Li, Wenzheng Zheng, Jingcheng Hu, Qiufeng Wang, Hanshan Zhang, Zili Wang, Yangshijie Xu, Shuigeng Zhou, Xiangyu Zhang, Daxin Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The impressive capabilities of Large Language Models (LLMs) across diverse
tasks are now well-established, yet their effective deployment necessitates
careful hyperparameter optimization. Through extensive empirical studies
involving grid searches across diverse configurations, we discover universal
scaling laws governing these hyperparameters: optimal learning rate follows a
power-law relationship with both model parameters and data sizes, while optimal
batch size scales primarily with data sizes. Our analysis reveals a convex
optimization landscape for hyperparameters under fixed models and data size
conditions. This convexity implies an optimal hyperparameter plateau. We
contribute a universal, plug-and-play optimal hyperparameter tool for the
community. Its estimated values on the test set are merely 0.07\% away from the
globally optimal LLM performance found via an exhaustive search. These laws
demonstrate remarkable robustness across variations in model sparsity, training
data distribution, and model shape. To our best known, this is the first work
that unifies different model shapes and structures, such as Mixture-of-Experts
models and dense transformers, as well as establishes optimal hyperparameter
scaling laws across diverse data distributions. This exhaustive optimization
process demands substantial computational resources, utilizing nearly one
million NVIDIA H800 GPU hours to train 3,700 LLMs of varying sizes and
hyperparameters from scratch and consuming approximately 100 trillion tokens in
total. To facilitate reproducibility and further research, we will
progressively release all loss measurements and model checkpoints through our
designated repository https://step-law.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Rich Style-Prompted Text-to-Speech <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04713v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04713v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anuj Diwan, Zhisheng Zheng, David Harwath, Eunsol Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Paralinguistic Speech Captions (ParaSpeechCaps), a large-scale
dataset that annotates speech utterances with rich style captions. While rich
abstract tags (e.g. guttural, nasal, pained) have been explored in small-scale
human-annotated datasets, existing large-scale datasets only cover basic tags
(e.g. low-pitched, slow, loud). We combine off-the-shelf text and speech
embedders, classifiers and an audio language model to automatically scale rich
tag annotations for the first time. ParaSpeechCaps covers a total of 59 style
tags, including both speaker-level intrinsic tags and utterance-level
situational tags. It consists of 342 hours of human-labelled data (PSC-Base)
and 2427 hours of automatically annotated data (PSC-Scaled). We finetune
Parler-TTS, an open-source style-prompted TTS model, on ParaSpeechCaps, and
achieve improved style consistency (+7.9% Consistency MOS) and speech quality
(+15.5% Naturalness MOS) over the best performing baseline that combines
existing rich style tag datasets. We ablate several of our dataset design
choices to lay the foundation for future work in this space. Our dataset,
models and code are released at https://github.com/ajd12342/paraspeechcaps .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Supervised Models for Phoneme Recognition: Applications in
  Children's Speech for Reading Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04710v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04710v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Block Medin, Thomas Pellegrini, Lucile Gelin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Child speech recognition is still an underdeveloped area of research due to
the lack of data (especially on non-English languages) and the specific
difficulties of this task. Having explored various architectures for child
speech recognition in previous work, in this article we tackle recent
self-supervised models. We first compare wav2vec 2.0, HuBERT and WavLM models
adapted to phoneme recognition in French child speech, and continue our
experiments with the best of them, WavLM base+. We then further adapt it by
unfreezing its transformer blocks during fine-tuning on child speech, which
greatly improves its performance and makes it significantly outperform our base
model, a Transformer+CTC. Finally, we study in detail the behaviour of these
two models under the real conditions of our application, and show that WavLM
base+ is more robust to various reading tasks and noise levels. Index Terms:
speech recognition, child speech, self-supervised learning
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was originally published in the Proceedings of Interspeech
  2024. DOI: 10.21437/Interspeech.2024-1095</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Universality of Layer-Level Entropy-Weighted Quantization Beyond Model
  Architecture and Size 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04704v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04704v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Behtash, Marijan Fofonjka, Ethan Baird, Tyler Mauer, Hossein Moghimifam, David Stout, Joel Dennison
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel approach to selective model quantization that transcends
the limitations of architecture-specific and size-dependent compression methods
for Large Language Models (LLMs) using Entropy-Weighted Quantization (EWQ). By
analyzing the entropy distribution across transformer blocks, EWQ determines
which blocks can be safely quantized without causing significant performance
degradation, independent of model architecture or size. Our method outperforms
uniform quantization approaches, maintaining Massive Multitask Language
Understanding (MMLU) accuracy scores within 0.5% of unquantized models while
reducing memory usage by up to 18%. We demonstrate the effectiveness of EWQ
across multiple architectures-from 1.6B to 70B parameters-showcasing consistent
improvements in the quality-compression trade-off regardless of model scale or
architectural design. A surprising finding of EWQ is its ability to reduce
perplexity compared to unquantized models, suggesting the presence of
beneficial regularization through selective precision reduction. This
improvement holds across different model families, indicating a fundamental
relationship between layer-level entropy and optimal precision requirements.
Additionally, we introduce FastEWQ, a rapid method for entropy distribution
analysis that eliminates the need for loading model weights. This technique
leverages universal characteristics of entropy distribution that persist across
various architectures and scales, enabling near-instantaneous quantization
decisions while maintaining 80% classification accuracy with full entropy
analysis. Our results demonstrate that effective quantization strategies can be
developed independently of specific architectural choices or model sizes,
opening new possibilities for efficient LLM deployment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 7 figures, 14 tables; Comments are welcome</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ L1: Controlling How Long A Reasoning Model Thinks With Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranjal Aggarwal, Sean Welleck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reasoning language models have shown an uncanny ability to improve
performance at test-time by ``thinking longer''-that is, by generating longer
chain-of-thought sequences and hence using more compute. However, the length of
their chain-of-thought reasoning is not controllable, making it impossible to
allocate test-time compute to achieve a desired level of performance. We
introduce Length Controlled Policy Optimization (LCPO), a simple reinforcement
learning method that optimizes for accuracy and adherence to user-specified
length constraints. We use LCPO to train L1, a reasoning language model that
produces outputs satisfying a length constraint given in its prompt. L1's
length control allows for smoothly trading off computational cost and accuracy
on a wide range of tasks, and outperforms the state-of-the-art S1 method for
length control. Furthermore, we uncover an unexpected short chain-of-thought
capability in models trained with LCPO. For instance, our 1.5B L1 model
surpasses GPT-4o at equal reasoning lengths. Overall, LCPO enables precise
control over reasoning length, allowing for fine-grained allocation of
test-time compute and accuracy. We release code and models at
https://www.cmu-l3.github.io/l1
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Matrix Factorization for Inferring Associations and Missing Links 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan Barron, Maksim E. Eren, Duc P. Truong, Cynthia Matuszek, James Wendelberger, Mary F. Dorn, Boian Alexandrov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Missing link prediction is a method for network analysis, with applications
in recommender systems, biology, social sciences, cybersecurity, information
retrieval, and Artificial Intelligence (AI) reasoning in Knowledge Graphs.
Missing link prediction identifies unseen but potentially existing connections
in a network by analyzing the observed patterns and relationships. In
proliferation detection, this supports efforts to identify and characterize
attempts by state and non-state actors to acquire nuclear weapons or associated
technology - a notoriously challenging but vital mission for global security.
Dimensionality reduction techniques like Non-Negative Matrix Factorization
(NMF) and Logistic Matrix Factorization (LMF) are effective but require
selection of the matrix rank parameter, that is, of the number of hidden
features, k, to avoid over/under-fitting. We introduce novel Weighted (WNMFk),
Boolean (BNMFk), and Recommender (RNMFk) matrix factorization methods, along
with ensemble variants incorporating logistic factorization, for link
prediction. Our methods integrate automatic model determination for rank
estimation by evaluating stability and accuracy using a modified bootstrap
methodology and uncertainty quantification (UQ), assessing prediction
reliability under random perturbations. We incorporate Otsu threshold selection
and k-means clustering for Boolean matrix factorization, comparing them to
coordinate descent-based Boolean thresholding. Our experiments highlight the
impact of rank k selection, evaluate model performance under varying test-set
sizes, and demonstrate the benefits of UQ for reliable predictions using
abstention. We validate our methods on three synthetic datasets (Boolean and
uniformly distributed) and benchmark them against LMF and symmetric LMF
(symLMF) on five real-world protein-protein interaction networks, showcasing an
improved prediction performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 14 figures, 3 tables, 1 algorithm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Multi</span>-Agent Inverse Q-Learning from Demonstrations <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathaniel Haynam, Adam Khoja, Dhruv Kumar, Vivek Myers, Erdem Bıyık
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When reward functions are hand-designed, deep reinforcement learning
algorithms often suffer from reward misspecification, causing them to learn
suboptimal policies in terms of the intended task objectives. In the
single-agent case, inverse reinforcement learning (IRL) techniques attempt to
address this issue by inferring the reward function from expert demonstrations.
However, in multi-agent problems, misalignment between the learned and true
objectives is exacerbated due to increased environment non-stationarity and
variance that scales with multiple agents. As such, in multi-agent general-sum
games, multi-agent IRL algorithms have difficulty balancing cooperative and
competitive objectives. To address these issues, we propose Multi-Agent
Marginal Q-Learning from Demonstrations (MAMQL), a novel sample-efficient
framework for multi-agent IRL. For each agent, MAMQL learns a critic
marginalized over the other agents' policies, allowing for a well-motivated use
of Boltzmann policies in the multi-agent context. We identify a connection
between optimal marginalized critics and single-agent soft-Q IRL, allowing us
to apply a direct, simple optimization criterion from the single-agent domain.
Across our experiments on three different simulated domains, MAMQL
significantly outperforms previous multi-agent methods in average reward,
sample efficiency, and reward recovery by often more than 2-5x. We make our
code available at https://sites.google.com/view/mamql .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 figures, 2 tables. Published at the International
  Conference on Robotics and Automation (ICRA) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implicit Cross-Lingual Rewarding for Efficient <span class="highlight-title">Multi</span>lingual Preference
  Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04647v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04647v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen Yang, Junhong Wu, Chen Wang, Chengqing Zong, Jiajun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Direct Preference Optimization (DPO) has become a prominent method for
aligning Large Language Models (LLMs) with human preferences. While DPO has
enabled significant progress in aligning English LLMs, multilingual preference
alignment is hampered by data scarcity. To address this, we propose a novel
approach that $\textit{captures}$ learned preferences from well-aligned English
models by implicit rewards and $\textit{transfers}$ them to other languages
through iterative training. Specifically, we derive an implicit reward model
from the logits of an English DPO-aligned model and its corresponding reference
model. This reward model is then leveraged to annotate preference relations in
cross-lingual instruction-following pairs, using English instructions to
evaluate multilingual responses. The annotated data is subsequently used for
multilingual DPO fine-tuning, facilitating preference knowledge transfer from
English to other languages. Fine-tuning Llama3 for two iterations resulted in a
12.72% average improvement in Win Rate and a 5.97% increase in Length Control
Win Rate across all training languages on the X-AlpacaEval leaderboard. Our
findings demonstrate that leveraging existing English-aligned models can enable
efficient and effective multilingual preference alignment, significantly
reducing the need for extensive multilingual preference data. The code is
available at https://github.com/ZNLP/Implicit-Cross-Lingual-Rewarding
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simulating the Real Wo<span class="highlight-title">rl</span>d: A Unified <span class="highlight-title">Survey</span> of <span class="highlight-title">Multi</span>modal Generative
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqi Hu, Longguang Wang, Xian Liu, Ling-Hao Chen, Yuwei Guo, Yukai Shi, Ce Liu, Anyi Rao, Zeyu Wang, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and replicating the real world is a critical challenge in
Artificial General Intelligence (AGI) research. To achieve this, many existing
approaches, such as world models, aim to capture the fundamental principles
governing the physical world, enabling more accurate simulations and meaningful
interactions. However, current methods often treat different modalities,
including 2D (images), videos, 3D, and 4D representations, as independent
domains, overlooking their interdependencies. Additionally, these methods
typically focus on isolated dimensions of reality without systematically
integrating their connections. In this survey, we present a unified survey for
multimodal generative models that investigate the progression of data
dimensionality in real-world simulation. Specifically, this survey starts from
2D generation (appearance), then moves to video (appearance+dynamics) and 3D
generation (appearance+geometry), and finally culminates in 4D generation that
integrate all dimensions. To the best of our knowledge, this is the first
attempt to systematically unify the study of 2D, video, 3D and 4D generation
within a single framework. To guide future research, we provide a comprehensive
review of datasets, evaluation metrics and future directions, and fostering
insights for newcomers. This survey serves as a bridge to advance the study of
multimodal generative models and real-world simulation within a unified
framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Repository for the related papers at
  https://github.com/ALEEEHU/World-Simulator</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mark Your <span class="highlight-title">LLM</span>: Detecting the Misuse of Open-Source Large Language Models
  via Watermarking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04636v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04636v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijie Xu, Aiwei Liu, Xuming Hu, Lijie Wen, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As open-source large language models (LLMs) like Llama3 become more capable,
it is crucial to develop watermarking techniques to detect their potential
misuse. Existing watermarking methods either add watermarks during LLM
inference, which is unsuitable for open-source LLMs, or primarily target
classification LLMs rather than recent generative LLMs. Adapting these
watermarks to open-source LLMs for misuse detection remains an open challenge.
This work defines two misuse scenarios for open-source LLMs: intellectual
property (IP) violation and LLM Usage Violation. Then, we explore the
application of inference-time watermark distillation and backdoor watermarking
in these contexts. We propose comprehensive evaluation methods to assess the
impact of various real-world further fine-tuning scenarios on watermarks and
the effect of these watermarks on LLM performance. Our experiments reveal that
backdoor watermarking could effectively detect IP Violation, while
inference-time watermark distillation is applicable in both scenarios but less
robust to further fine-tuning and has a more significant impact on LLM
performance compared to backdoor watermarking. Exploring more advanced
watermarking methods for open-source LLMs to detect their misuse should be an
important future direction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 1st Workshop on GenAI Watermarking, collocated with
  ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IDInit: A Universal and Stable Initialization Method for Neural Network
  Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Pan, Chaozheng Wang, Zekai Wu, Qifan Wang, Min Zhang, Zenglin Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have achieved remarkable accomplishments in practice.
The success of these networks hinges on effective initialization methods, which
are vital for ensuring stable and rapid convergence during training. Recently,
initialization methods that maintain identity transition within layers have
shown good efficiency in network training. These techniques (e.g., Fixup) set
specific weights to zero to achieve identity control. However, settings of
remaining weight (e.g., Fixup uses random values to initialize non-zero
weights) will affect the inductive bias that is achieved only by a zero weight,
which may be harmful to training. Addressing this concern, we introduce fully
identical initialization (IDInit), a novel method that preserves identity in
both the main and sub-stem layers of residual networks. IDInit employs a padded
identity-like matrix to overcome rank constraints in non-square weight
matrices. Furthermore, we show the convergence problem of an identity matrix
can be solved by stochastic gradient descent. Additionally, we enhance the
universality of IDInit by processing higher-order weights and addressing dead
neuron problems. IDInit is a straightforward yet effective initialization
method, with improved convergence, stability, and performance across various
settings, including large-scale datasets and deep models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Best of Both Wo<span class="highlight-title">rl</span>ds: Integrating Language Models and Diffusion
  Models for Video Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aoxiong Yin, Kai Shen, Yichong Leng, Xu Tan, Xinyu Zhou, Juncheng Li, Siliang Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in text-to-video (T2V) generation have been driven by two
competing paradigms: autoregressive language models and diffusion models.
However, each paradigm has intrinsic limitations: language models struggle with
visual quality and error accumulation, while diffusion models lack semantic
understanding and causal modeling. In this work, we propose LanDiff, a hybrid
framework that synergizes the strengths of both paradigms through
coarse-to-fine generation. Our architecture introduces three key innovations:
(1) a semantic tokenizer that compresses 3D visual features into compact 1D
discrete representations through efficient semantic compression, achieving a
$\sim$14,000$\times$ compression ratio; (2) a language model that generates
semantic tokens with high-level semantic relationships; (3) a streaming
diffusion model that refines coarse semantics into high-fidelity videos.
Experiments show that LanDiff, a 5B model, achieves a score of 85.43 on the
VBench T2V benchmark, surpassing the state-of-the-art open-source models
Hunyuan Video (13B) and other commercial models such as Sora, Keling, and
Hailuo. Furthermore, our model also achieves state-of-the-art performance in
long video generation, surpassing other open-source models in this field. Our
demo can be viewed at https://landiff.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid
  Normalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijian Zhuo, Yutao Zeng, Ya Wang, Sijun Zhang, Jian Yang, Xiaoqing Li, Xun Zhou, Jinwen Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have become the de facto architecture for a wide range of
machine learning tasks, particularly in large language models (LLMs). Despite
their remarkable performance, challenges remain in training deep transformer
networks, especially regarding the location of layer normalization. While
Pre-Norm structures facilitate easier training due to their more prominent
identity path, they often yield suboptimal performance compared to Post-Norm.
In this paper, we propose $\textbf{HybridNorm}$, a straightforward yet
effective hybrid normalization strategy that integrates the advantages of both
Pre-Norm and Post-Norm approaches. Specifically, HybridNorm employs QKV
normalization within the attention mechanism and Post-Norm in the feed-forward
network (FFN) of each transformer block. This design not only stabilizes
training but also enhances performance, particularly in the context of LLMs.
Comprehensive experiments in both dense and sparse architectures show that
HybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches,
achieving state-of-the-art results across various benchmarks. These findings
highlight the potential of HybridNorm as a more stable and effective technique
for improving the training and performance of deep transformer models. %Code
will be made publicly available. Code is available at
https://github.com/BryceZhuo/HybridNorm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Next Frontier of <span class="highlight-title">LLM</span> Applications: Open Ecosystems and Hardware
  Synergy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Hou, Yanjie Zhao, Haoyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model (LLM) applications, including LLM app stores and
autonomous agents, are shaping the future of AI ecosystems. However, platform
silos, fragmented hardware integration, and the absence of standardized
interfaces limit scalability, interoperability, and resource efficiency. While
LLM app stores democratize AI, their closed ecosystems restrict modular AI
reuse and cross-platform portability. Meanwhile, agent-based frameworks offer
flexibility but often lack seamless integration across diverse environments.
This paper envisions the future of LLM applications and proposes a three-layer
decoupled architecture grounded in software engineering principles such as
layered system design, service-oriented architectures, and hardware-software
co-design. This architecture separates application logic, communication
protocols, and hardware execution, enhancing modularity, efficiency, and
cross-platform compatibility. Beyond architecture, we highlight key security
and privacy challenges for safe, scalable AI deployment and outline research
directions in software and security engineering. This vision aims to foster
open, secure, and interoperable LLM ecosystems, guiding future advancements in
AI applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ValuePilot: A Two-Phase Framework for Value-Driven Decision-Making 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yitong Luo, Hou Hei Lam, Ziang Chen, Zhenliang Zhang, Xue Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent advances in artificial intelligence (AI), it poses challenges
to ensure personalized decision-making in tasks that are not considered in
training datasets. To address this issue, we propose ValuePilot, a two-phase
value-driven decision-making framework comprising a dataset generation toolkit
DGT and a decision-making module DMM trained on the generated data. DGT is
capable of generating scenarios based on value dimensions and closely mirroring
real-world tasks, with automated filtering techniques and human curation to
ensure the validity of the dataset. In the generated dataset, DMM learns to
recognize the inherent values of scenarios, computes action feasibility and
navigates the trade-offs between multiple value dimensions to make personalized
decisions. Extensive experiments demonstrate that, given human value
preferences, our DMM most closely aligns with human decisions, outperforming
Claude-3.5-Sonnet, Gemini-2-flash, Llama-3.1-405b and GPT-4o. This research is
a preliminary exploration of value-driven decision-making. We hope it will
stimulate interest in value-driven decision-making and personalized
decision-making within the community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fundamental Limits of Hierarchical Secure Aggregation with Cyclic User
  Association 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04564v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04564v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Zhang, Zhou Li, Kai Wan, Hua Sun, Mingyue Ji, Giuseppe Caire
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Secure aggregation is motivated by federated learning (FL) where a cloud
server aims to compute an averaged model (i.e., weights of deep neural
networks) of the locally-trained models of numerous clients, while adhering to
data security requirements. Hierarchical secure aggregation (HSA) extends this
concept to a three-layer network, where clustered users communicate with the
server through an intermediate layer of relays. In HSA, beyond conventional
server security, relay security is also enforced to ensure that the relays
remain oblivious to the users' inputs (an abstraction of the local models in
FL). Existing study on HSA assumes that each user is associated with only one
relay, limiting opportunities for coding across inter-cluster users to achieve
efficient communication and key generation. In this paper, we consider HSA with
a cyclic association pattern where each user is connected to $B$ consecutive
relays in a wrap-around manner. We propose an efficient aggregation scheme
which includes a message design for the inputs inspired by gradient coding-a
well-known technique for efficient communication in distributed computing-along
with a highly nontrivial security key design. We also derive novel converse
bounds on the minimum achievable communication and key rates using
information-theoretic arguments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Compositional Causal Reasoning Evaluation in Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacqueline R. M. A. Maasch, Alihan Hüyük, Xinnuo Xu, Aditya V. Nori, Javier Gonzalez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal reasoning and compositional reasoning are two core aspirations in
generative AI. Measuring the extent of these behaviors requires principled
evaluation methods. We explore a unified perspective that considers both
behaviors simultaneously, termed compositional causal reasoning (CCR): the
ability to infer how causal measures compose and, equivalently, how causal
quantities propagate through graphs. We instantiate a framework for the
systematic evaluation of CCR for the average treatment effect and the
probability of necessity and sufficiency. As proof of concept, we demonstrate
the design of CCR tasks for language models in the LLama, Phi, and GPT
families. On a math word problem, our framework revealed a range of
taxonomically distinct error patterns. Additionally, CCR errors increased with
the complexity of causal paths for all models except o1.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Reasoning Robustness in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Yu, Yongcheng Jing, Xikun Zhang, Wentao Jiang, Wenjie Wu, Yingjie Wang, Wenbin Hu, Bo Du, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the recent success of large language models (LLMs) in reasoning such
as DeepSeek, we for the first time identify a key dilemma in reasoning
robustness and generalization: significant performance degradation on novel or
incomplete data, suggesting a reliance on memorized patterns rather than
systematic reasoning. Our closer examination reveals four key unique
limitations underlying this issue:(1) Positional bias--models favor earlier
queries in multi-query inputs but answering the wrong one in the latter (e.g.,
GPT-4o's accuracy drops from 75.8 percent to 72.8 percent); (2) Instruction
sensitivity--performance declines by 5.0 to 7.5 percent in the Qwen2.5 Series
and by 5.0 percent in DeepSeek-V3 with auxiliary guidance; (3) Numerical
fragility--value substitution sharply reduces accuracy (e.g., GPT-4o drops from
97.5 percent to 82.5 percent, GPT-o1-mini drops from 97.5 percent to 92.5
percent); and (4) Memory dependence--models resort to guesswork when missing
critical data. These findings further highlight the reliance on heuristic
recall over rigorous logical inference, demonstrating challenges in reasoning
robustness. To comprehensively investigate these robustness challenges, this
paper introduces a novel benchmark, termed as Math-RoB, that exploits
hallucinations triggered by missing information to expose reasoning gaps. This
is achieved by an instruction-based approach to generate diverse datasets that
closely resemble training distributions, facilitating a holistic robustness
assessment and advancing the development of more robust reasoning frameworks.
Bad character(s) in field Abstract.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Keeping Yourself is Important in Downstream Tuning <span class="highlight-title">Multi</span>modal Large
  Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04543v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04543v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenke Huang, Jian Liang, Xianda Guo, Yiyang Fang, Guancheng Wan, Xuankun Rong, Chi Wen, Zekun Shi, Qingyun Li, Didi Zhu, Yanbiao Ma, Ke Liang, Bin Yang, He Li, Jiawei Shao, Mang Ye, Bo Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal Large Language Models (MLLMs) integrate visual and linguistic
reasoning to address complex tasks such as image captioning and visual question
answering. While MLLMs demonstrate remarkable versatility, MLLMs appears
limited performance on special applications. But tuning MLLMs for downstream
tasks encounters two key challenges: Task-Expert Specialization, where
distribution shifts between pre-training and target datasets constrain target
performance, and Open-World Stabilization, where catastrophic forgetting erases
the model general knowledge. In this work, we systematically review recent
advancements in MLLM tuning methodologies, classifying them into three
paradigms: (I) Selective Tuning, (II) Additive Tuning, and (III)
Reparameterization Tuning. Furthermore, we benchmark these tuning strategies
across popular MLLM architectures and diverse downstream tasks to establish
standardized evaluation analysis and systematic tuning principles. Finally, we
highlight several open challenges in this domain and propose future research
directions. To facilitate ongoing progress in this rapidly evolving field, we
provide a public repository that continuously tracks developments:
https://github.com/WenkeHuang/Awesome-MLLM-Tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SOLAR: Scalable Optimization of Large-scale Architecture for Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Li, Yinyi Luo, Anudeep Bolimera, Marios Savvides
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) excel in reasoning but remain constrained by
their Chain-of-Thought (CoT) approach, which struggles with complex tasks
requiring more nuanced topological reasoning. We introduce SOLAR, Scalable
Optimization of Large-scale Architecture for Reasoning, a framework that
dynamically optimizes various reasoning topologies to enhance accuracy and
efficiency.
  Our Topological Annotation Generation (TAG) system automates topological
dataset creation and segmentation, improving post-training and evaluation.
Additionally, we propose Topological-Scaling, a reward-driven framework that
aligns training and inference scaling, equipping LLMs with adaptive, task-aware
reasoning.
  SOLAR achieves substantial gains on MATH and GSM8K: +5% accuracy with
Topological Tuning, +9% with Topological Reward, and +10.02% with Hybrid
Scaling. It also reduces response length by over 5% for complex problems,
lowering inference latency.
  To foster the reward system, we train a multi-task Topological Reward Model
(M-TRM), which autonomously selects the best reasoning topology and answer in a
single pass, eliminating the need for training and inference on multiple
single-task TRMs (S-TRMs), thus reducing both training cost and inference
latency. In addition, in terms of performance, M-TRM surpasses all S-TRMs,
improving accuracy by +10% and rank correlation by +9%.
  To the best of our knowledge, SOLAR sets a new benchmark for scalable,
high-precision LLM reasoning while introducing an automated annotation process
and a dynamic reasoning topology competition mechanism.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Pricing for On-Demand DNN Inference in the Edge-AI Market 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songyuan Li, Jia Hu, Geyong Min, Haojun Huang, Jiwei Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The convergence of edge computing and AI gives rise to Edge-AI, which enables
the deployment of real-time AI applications and services at the network edge.
One of the fundamental research issues in Edge-AI is edge inference
acceleration, which aims to realize low-latency high-accuracy DNN inference
services by leveraging the fine-grained offloading of partitioned inference
tasks from end devices to edge servers. However, existing research has yet to
adopt a practical Edge-AI market perspective, which would systematically
explore the personalized inference needs of AI users (e.g., inference accuracy,
latency, and task complexity), the revenue incentives for AI service providers
that offer edge inference services, and multi-stakeholder governance within a
market-oriented context. To bridge this gap, we propose an Auction-based Edge
Inference Pricing Mechanism (AERIA) for revenue maximization to tackle the
multi-dimensional optimization problem of DNN model partition, edge inference
pricing, and resource allocation. We investigate the multi-exit device-edge
synergistic inference scheme for on-demand DNN inference acceleration, and
analyse the auction dynamics amongst the AI service providers, AI users and
edge infrastructure provider. Owing to the strategic mechanism design via
randomized consensus estimate and cost sharing techniques, the Edge-AI market
attains several desirable properties, including competitiveness in revenue
maximization, incentive compatibility, and envy-freeness, which are crucial to
maintain the effectiveness, truthfulness, and fairness of our auction outcomes.
The extensive simulation experiments based on four representative DNN inference
workloads demonstrate that our AERIA mechanism significantly outperforms
several state-of-the-art approaches in revenue maximization, demonstrating the
efficacy of AERIA for on-demand DNN inference in the Edge-AI market.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Index Terms: Edge-AI, DNN Inference Offloading, Resource Management,
  Dynamic Pricing, Auction Mechanism</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STX-Search: Explanation Search for Continuous Dynamic Spatio-Temporal
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saif Anwar, Nathan Griffiths, Thomas Popham, Abhir Bhalerao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent improvements in the expressive power of spatio-temporal models have
led to performance gains in many real-world applications, such as traffic
forecasting and social network modelling. However, understanding the
predictions from a model is crucial to ensure reliability and trustworthiness,
particularly for high-risk applications, such as healthcare and transport. Few
existing methods are able to generate explanations for models trained on
continuous-time dynamic graph data and, of these, the computational complexity
and lack of suitable explanation objectives pose challenges. In this paper, we
propose $\textbf{S}$patio-$\textbf{T}$emporal E$\textbf{X}$planation
$\textbf{Search}$ (STX-Search), a novel method for generating instance-level
explanations that is applicable to static and dynamic temporal graph
structures. We introduce a novel search strategy and objective function, to
find explanations that are highly faithful and interpretable. When compared
with existing methods, STX-Search produces explanations of higher fidelity
whilst optimising explanation size to maintain interpretability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Multi</span>-modal Summarization in Model-Based Engineering: Automotive
  Software Development Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nenad Petrovic, Yurui Zhang, Moaad Maaroufi, Kuo-Yi Chao, Lukasz Mazur, Fengjunjie Pan, Vahid Zolfaghari, Alois Knoll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal summarization integrating information from diverse data modalities
presents a promising solution to aid the understanding of information within
various processes. However, the application and advantages of multimodal
summarization have not received much attention in model-based engineering
(MBE), where it has become a cornerstone in the design and development of
complex systems, leveraging formal models to improve understanding, validation
and automation throughout the engineering lifecycle. UML and EMF diagrams in
model-based engineering contain a large amount of multimodal information and
intricate relational data. Hence, our study explores the application of
multimodal large language models within the domain of model-based engineering
to evaluate their capacity for understanding and identifying relationships,
features, and functionalities embedded in UML and EMF diagrams. We aim to
demonstrate the transformative potential benefits and limitations of multimodal
summarization in improving productivity and accuracy in MBE practices. The
proposed approach is evaluated within the context of automotive software
development, while many promising state-of-art models were taken into account.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Conference paper accepted for IntelliSys2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Transformation and Analysis of Timelines through Learning
  via Surprisability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Osnat Mokryn, Teddy Lazebnik, Hagit Ben Shoshan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The analysis of high-dimensional timeline data and the identification of
outliers and anomalies is critical across diverse domains, including sensor
readings, biological and medical data, historical records, and global
statistics. However, conventional analysis techniques often struggle with
challenges such as high dimensionality, complex distributions, and sparsity.
These limitations hinder the ability to extract meaningful insights from
complex temporal datasets, making it difficult to identify trending features,
outliers, and anomalies effectively. Inspired by surprisability -- a cognitive
science concept describing how humans instinctively focus on unexpected
deviations - we propose Learning via Surprisability (LvS), a novel approach for
transforming high-dimensional timeline data. LvS quantifies and prioritizes
anomalies in time-series data by formalizing deviations from expected behavior.
LvS bridges cognitive theories of attention with computational methods,
enabling the detection of anomalies and shifts in a way that preserves critical
context, offering a new lens for interpreting complex datasets. We demonstrate
the usefulness of LvS on three high-dimensional timeline use cases: a time
series of sensor data, a global dataset of mortality causes over multiple
years, and a textual corpus containing over two centuries of State of the Union
Addresses by U.S. presidents. Our results show that the LvS transformation
enables efficient and interpretable identification of outliers, anomalies, and
the most variable features along the timeline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ReynoldsFlow: Exquisite Flow Estimation via Reynolds Transport Theorem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04500v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04500v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Hsi Chen, Chin-Tien Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optical flow is a fundamental technique for motion estimation, widely applied
in video stabilization, interpolation, and object tracking. Recent advancements
in artificial intelligence (AI) have enabled deep learning models to leverage
optical flow as an important feature for motion analysis. However, traditional
optical flow methods rely on restrictive assumptions, such as brightness
constancy and slow motion constraints, limiting their effectiveness in complex
scenes. Deep learning-based approaches require extensive training on large
domain-specific datasets, making them computationally demanding. Furthermore,
optical flow is typically visualized in the HSV color space, which introduces
nonlinear distortions when converted to RGB and is highly sensitive to noise,
degrading motion representation accuracy. These limitations inherently
constrain the performance of downstream models, potentially hindering object
tracking and motion analysis tasks. To address these challenges, we propose
Reynolds flow, a novel training-free flow estimation inspired by the Reynolds
transport theorem, offering a principled approach to modeling complex motion
dynamics. Beyond the conventional HSV-based visualization, denoted
ReynoldsFlow, we introduce an alternative representation, ReynoldsFlow+,
designed to improve flow visualization. We evaluate ReynoldsFlow and
ReynoldsFlow+ across three video-based benchmarks: tiny object detection on
UAVDB, infrared object detection on Anti-UAV, and pose estimation on GolfDB.
Experimental results demonstrate that networks trained with ReynoldsFlow+
achieve state-of-the-art (SOTA) performance, exhibiting improved robustness and
efficiency across all tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalized Interpolating Discrete Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitri von Rütte, Janis Fluri, Yuhui Ding, Antonio Orvieto, Bernhard Schölkopf, Thomas Hofmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While state-of-the-art language models achieve impressive results through
next-token prediction, they have inherent limitations such as the inability to
revise already generated tokens. This has prompted exploration of alternative
approaches such as discrete diffusion. However, masked diffusion, which has
emerged as a popular choice due to its simplicity and effectiveness,
reintroduces this inability to revise words. To overcome this, we generalize
masked diffusion and derive the theoretical backbone of a family of general
interpolating discrete diffusion (GIDD) processes offering greater flexibility
in the design of the noising processes. Leveraging a novel diffusion ELBO, we
achieve compute-matched state-of-the-art performance in diffusion language
modeling. Exploiting GIDD's flexibility, we explore a hybrid approach combining
masking and uniform noise, leading to improved sample quality and unlocking the
ability for the model to correct its own mistakes, an area where autoregressive
models notoriously have struggled. Our code and models are open-source:
https://github.com/dvruette/gidd/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ToolFuzz -- Automated Agent Tool Testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04479v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04479v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivan Milev, Mislav Balunović, Maximilian Baader, Martin Vechev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model (LLM) Agents leverage the advanced reasoning
capabilities of LLMs in real-world applications. To interface with an
environment, these agents often rely on tools, such as web search or database
APIs. As the agent provides the LLM with tool documentation along the user
query, the completeness and correctness of this documentation is critical.
However, tool documentation is often over-, under-, or ill-specified, impeding
the agent's accuracy. Standard software testing approaches struggle to identify
these errors as they are expressed in natural language. Thus, despite its
importance, there currently exists no automated method to test the tool
documentation for agents. To address this issue, we present ToolFuzz, the first
method for automated testing of tool documentations. ToolFuzz is designed to
discover two types of errors: (1) user queries leading to tool runtime errors
and (2) user queries that lead to incorrect agent responses. ToolFuzz can
generate a large and diverse set of natural inputs, effectively finding tool
description errors at a low false positive rate. Further, we present two
straightforward prompt-engineering approaches. We evaluate all three tool
testing approaches on 32 common LangChain tools and 35 newly created custom
tools and 2 novel benchmarks to further strengthen the assessment. We find that
many publicly available tools suffer from underspecification. Specifically, we
show that ToolFuzz identifies 20x more erroneous inputs compared to the
prompt-engineering approaches, making it a key component for building reliable
AI agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DAST: Difficulty-Adaptive Slow-Thinking for Large Reasoning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04472v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04472v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Shen, Jian Zhang, Jieyun Huang, Shuming Shi, Wenjing Zhang, Jiangze Yan, Ning Wang, Kai Wang, Shiguo Lian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in slow-thinking reasoning models have shown exceptional
performance in complex reasoning tasks. However, these models often exhibit
overthinking-generating redundant reasoning steps for simple problems, leading
to excessive computational resource usage. While current mitigation strategies
uniformly reduce reasoning tokens, they risk degrading performance on
challenging tasks that require extended reasoning. This paper introduces
Difficulty-Adaptive Slow-Thinking (DAST), a novel framework that enables models
to autonomously adjust the length of Chain-of-Thought(CoT) based on problem
difficulty. We first propose a Token Length Budget (TLB) metric to quantify
difficulty, then leveraging length-aware reward shaping and length preference
optimization to implement DAST. DAST penalizes overlong responses for simple
tasks while incentivizing sufficient reasoning for complex problems.
Experiments on diverse datasets and model scales demonstrate that DAST
effectively mitigates overthinking (reducing token usage by over 30\% on
average) while preserving reasoning accuracy on complex problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>working in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TPC: Cross-Temporal Prediction Connection for Vision-Language Model
  Hallucination Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Wang, Weiwei Fu, Yang Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language models (VLMs) have achieved remarkable advancements,
capitalizing on the impressive capabilities of large language models (LLMs)
across diverse tasks. Despite this, a critical challenge known as hallucination
occurs when models overconfidently describe objects or attributes absent from
the image, a problem exacerbated by the tendency of VLMs to rely on linguistic
priors. This limitation reduces model reliability in high-stakes applications.
In this work, we have observed the characteristic of logits' continuity
consistency enhancement and introduced a straightforward and efficient method,
Cross-Temporal Prediction Connection (TPC), designed to enhance the semantic
consistency of logits by connecting them temporally across timesteps. TPC
amplifies information flow and improves coherence, effectively reducing
hallucination. Extensive experiments show that TPC surpasses existing
representatives, delivering superior performance in both accuracy and
efficiency while maintaining robustness in open-ended text generation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Privacy Preserving and Robust Aggregation for Cross-Silo Federated
  Learning in Non-IID Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04451v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04451v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Arazzi, Mert Cihangiroglu, Antonino Nocera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Averaging remains the most widely used aggregation strategy in
federated learning due to its simplicity and scalability. However, its
performance degrades significantly in non-IID data settings, where client
distributions are highly imbalanced or skewed. Additionally, it relies on
clients transmitting metadata, specifically the number of training samples,
which introduces privacy risks and may conflict with regulatory frameworks like
the European GDPR. In this paper, we propose a novel aggregation strategy that
addresses these challenges by introducing class-aware gradient masking. Unlike
traditional approaches, our method relies solely on gradient updates,
eliminating the need for any additional client metadata, thereby enhancing
privacy protection. Furthermore, our approach validates and dynamically weights
client contributions based on class-specific importance, ensuring robustness
against non-IID distributions, convergence prevention, and backdoor attacks.
Extensive experiments on benchmark datasets demonstrate that our method not
only outperforms FedAvg and other widely accepted aggregation strategies in
non-IID settings but also preserves model integrity in adversarial scenarios.
Our results establish the effectiveness of gradient masking as a practical and
secure solution for federated learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Activation Space Interventions Can Be Transferred Between Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Narmeen Oozeer, Dhruv Nathawani, Nirmalendu Prakash, Michael Lan, Abir Harrasse, Amirali Abdullah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of representation universality in AI models reveals growing
convergence across domains, modalities, and architectures. However, the
practical applications of representation universality remain largely
unexplored. We bridge this gap by demonstrating that safety interventions can
be transferred between models through learned mappings of their shared
activation spaces. We demonstrate this approach on two well-established AI
safety tasks: backdoor removal and refusal of harmful prompts, showing
successful transfer of steering vectors that alter the models' outputs in a
predictable way. Additionally, we propose a new task, \textit{corrupted
capabilities}, where models are fine-tuned to embed knowledge tied to a
backdoor. This tests their ability to separate useful skills from backdoors,
reflecting real-world challenges. Extensive experiments across Llama, Qwen and
Gemma model families show that our method enables using smaller models to
efficiently align larger ones. Furthermore, we demonstrate that autoencoder
mappings between base and fine-tuned models can serve as reliable ``lightweight
safety switches", allowing dynamic toggling between model behaviors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>68 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PDX: A Data Layout for Vector Similarity Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04422v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04422v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonardo Kuffo, Elena Krippner, Peter Boncz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Partition Dimensions Across (PDX), a data layout for vectors
(e.g., embeddings) that, similar to PAX [6], stores multiple vectors in one
block, using a vertical layout for the dimensions (Figure 1). PDX accelerates
exact and approximate similarity search thanks to its dimension-by-dimension
search strategy that operates on multiple-vectors-at-a-time in tight loops. It
beats SIMD-optimized distance kernels on standard horizontal vector storage
(avg 40% faster), only relying on scalar code that gets auto-vectorized. We
combined the PDX layout with recent dimension-pruning algorithms ADSampling
[19] and BSA [52] that accelerate approximate vector search. We found that
these algorithms on the horizontal vector layout can lose to SIMD-optimized
linear scans, even if they are SIMD-optimized. However, when used on PDX, their
benefit is restored to 2-7x. We find that search on PDX is especially fast if a
limited number of dimensions has to be scanned fully, which is what the
dimension-pruning approaches do. We finally introduce PDX-BOND, an even more
flexible dimension-pruning strategy, with good performance on exact search and
reasonable performance on approximate search. Unlike previous pruning
algorithms, it can work on vector data "as-is" without preprocessing; making it
attractive for vector databases with frequent updates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in Proceedings of The 2025 International Conference
  on Management of Data (SIGMOD '25). For associated code, see
  https://github.com/cwida/PDX</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Idea to CAD: A Language Model-Driven <span class="highlight-title">Multi</span>-Agent System for
  Collaborative Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Ocker, Stefan Menzel, Ahmed Sadik, Thiago Rios
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Creating digital models using Computer Aided Design (CAD) is a process that
requires in-depth expertise. In industrial product development, this process
typically involves entire teams of engineers, spanning requirements
engineering, CAD itself, and quality assurance. We present an approach that
mirrors this team structure with a Vision Language Model (VLM)-based Multi
Agent System, with access to parametric CAD tooling and tool documentation.
Combining agents for requirements engineering, CAD engineering, and
vision-based quality assurance, a model is generated automatically from
sketches and/ or textual descriptions. The resulting model can be refined
collaboratively in an iterative validation loop with the user. Our approach has
the potential to increase the effectiveness of design processes, both for
industry experts and for hobbyists who create models for 3D printing. We
demonstrate the potential of the architecture at the example of various design
tasks and provide several ablations that show the benefits of the
architecture's individual components.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Transformer-based Wo<span class="highlight-title">rl</span>d Models with Contrastive Predictive
  Coding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04416v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04416v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maxime Burchi, Radu Timofte
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The DreamerV3 algorithm recently obtained remarkable performance across
diverse environment domains by learning an accurate world model based on
Recurrent Neural Networks (RNNs). Following the success of model-based
reinforcement learning algorithms and the rapid adoption of the Transformer
architecture for its superior training efficiency and favorable scaling
properties, recent works such as STORM have proposed replacing RNN-based world
models with Transformer-based world models using masked self-attention.
However, despite the improved training efficiency of these methods, their
impact on performance remains limited compared to the Dreamer algorithm,
struggling to learn competitive Transformer-based world models. In this work,
we show that the next state prediction objective adopted in previous approaches
is insufficient to fully exploit the representation capabilities of
Transformers. We propose to extend world model predictions to longer time
horizons by introducing TWISTER (Transformer-based World model wIth contraSTivE
Representations), a world model using action-conditioned Contrastive Predictive
Coding to learn high-level temporal feature representations and improve the
agent performance. TWISTER achieves a human-normalized mean score of 162% on
the Atari 100k benchmark, setting a new record among state-of-the-art methods
that do not employ look-ahead search.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wider or Deeper? Scaling <span class="highlight-title">LLM</span> Inference-Time Compute with Adaptive
  Branching Tree Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kou Misaki, Yuichi Inoue, Yuki Imajuku, So Kuroki, Taishi Nakamura, Takuya Akiba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances demonstrate that increasing inference-time computation can
significantly boost the reasoning capabilities of large language models (LLMs).
Although repeated sampling (i.e., generating multiple candidate outputs) is a
highly effective strategy, it does not leverage external feedback signals for
refinement, which are often available in tasks like coding. In this work, we
propose $\textit{Adaptive Branching Monte Carlo Tree Search (AB-MCTS)}$, a
novel inference-time framework that generalizes repeated sampling with
principled multi-turn exploration and exploitation. At each node in the search
tree, AB-MCTS dynamically decides whether to "go wider" by expanding new
candidate responses or "go deeper" by revisiting existing ones based on
external feedback signals. We evaluate our method on complex coding and
engineering tasks using frontier models. Empirical results show that AB-MCTS
consistently outperforms both repeated sampling and standard MCTS, underscoring
the importance of combining the response diversity of LLMs with multi-turn
solution refinement for effective inference-time scaling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at ICLR 2025 Workshop on Foundation Models in the Wild</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training-Free Graph Filtering via <span class="highlight-title">Multi</span>modal Feature Refinement for
  Extremely Fast <span class="highlight-title">Multi</span>modal Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Seung Roh, Joo-Young Kim, Jin-Duk Park, Won-Yong Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal recommender systems improve the performance of canonical
recommender systems with no item features by utilizing diverse content types
such as text, images, and videos, while alleviating inherent sparsity of
user-item interactions and accelerating user engagement. However, current
neural network-based models often incur significant computational overhead due
to the complex training process required to learn and integrate information
from multiple modalities. To overcome this limitation, we propose
MultiModal-Graph Filtering (MM-GF), a training-free method based on the notion
of graph filtering (GF) for efficient and accurate multimodal recommendations.
Specifically, MM-GF first constructs multiple similarity graphs through
nontrivial multimodal feature refinement such as robust scaling and vector
shifting by addressing the heterogeneous characteristics across modalities.
Then, MM-GF optimally fuses multimodal information using linear low-pass
filters across different modalities. Extensive experiments on real-world
benchmark datasets demonstrate that MM-GF not only improves recommendation
accuracy by up to 13.35% compared to the best competitor but also dramatically
reduces computational costs by achieving the runtime of less than 10 seconds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speculative MoE: Communication Efficient Parallel MoE Inference with
  Speculative Token and Expert Pre-scheduling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Li, Pengfei Zheng, Shuang Chen, Zewei Xu, Yunfei Du, Zhengang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  MoE (Mixture of Experts) prevails as a neural architecture that can scale
modern transformer-based LLMs (Large Language Models) to unprecedented scales.
Nevertheless, large MoEs' great demands of computing power, memory capacity and
memory bandwidth make scalable serving a fundamental challenge and efficient
parallel inference has become a requisite to attain adequate throughput under
latency constraints. DeepSpeed-MoE, one state-of-the-art MoE inference
framework, adopts a 3D-parallel paradigm including EP (Expert Parallelism), TP
(Tensor Parallel) and DP (Data Parallelism). However, our analysis shows
DeepSpeed-MoE's inference efficiency is largely bottlenecked by EP, which is
implemented with costly all-to-all collectives to route token activation. Our
work aims to boost DeepSpeed-MoE by strategically reducing EP's communication
overhead with a technique named Speculative MoE. Speculative MoE has two
speculative parallelization schemes, speculative token shuffling and
speculative expert grouping, which predict outstanding tokens' expert routing
paths and pre-schedule tokens and experts across devices to losslessly trim
EP's communication volume. Besides DeepSpeed-MoE, we also build Speculative MoE
into a prevailing MoE inference engine SGLang. Experiments show Speculative MoE
can significantly boost state-of-the-art MoE inference frameworks on fast
homogeneous and slow heterogeneous interconnects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AgentSafe: Safeguarding Large Language Model-based <span class="highlight-title">Multi</span>-agent Systems
  via Hierarchical Data Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyuan Mao, Fanci Meng, Yifan Duan, Miao Yu, Xiaojun Jia, Junfeng Fang, Yuxuan Liang, Kun Wang, Qingsong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model based multi-agent systems are revolutionizing autonomous
communication and collaboration, yet they remain vulnerable to security threats
like unauthorized access and data breaches. To address this, we introduce
AgentSafe, a novel framework that enhances MAS security through hierarchical
information management and memory protection. AgentSafe classifies information
by security levels, restricting sensitive data access to authorized agents.
AgentSafe incorporates two components: ThreatSieve, which secures communication
by verifying information authority and preventing impersonation, and
HierarCache, an adaptive memory management system that defends against
unauthorized access and malicious poisoning, representing the first systematic
defense for agent memory. Experiments across various LLMs show that AgentSafe
significantly boosts system resilience, achieving defense success rates above
80% under adversarial conditions. Additionally, AgentSafe demonstrates
scalability, maintaining robust performance as agent numbers and information
complexity grow. Results underscore effectiveness of AgentSafe in securing MAS
and its potential for real-world application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dedicated Feedback and Edit Models Empower Inference-Time Scaling for
  Open-Ended General-Domain Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhilin Wang, Jiaqi Zeng, Olivier Delalleau, Daniel Egert, Ellie Evans, Hoo-Chang Shin, Felipe Soares, Yi Dong, Oleksii Kuchaiev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inference-Time Scaling has been critical to the success of recent models such
as OpenAI o1 and DeepSeek R1. However, many techniques used to train models for
inference-time scaling require tasks to have answers that can be verified,
limiting their application to domains such as math, coding and logical
reasoning. We take inspiration from how humans make first attempts, ask for
detailed feedback from others and make improvements based on such feedback
across a wide spectrum of open-ended endeavors. To this end, we collect data
for and train dedicated Feedback and Edit Models that are capable of performing
inference-time scaling for open-ended general-domain tasks. In our setup, one
model generates an initial response, which are given feedback by a second
model, that are then used by a third model to edit the response. We show that
performance on Arena Hard, a benchmark strongly predictive of Chatbot Arena Elo
can be boosted by scaling the number of initial response drafts, effective
feedback and edited responses. When scaled optimally, our setup based on 70B
models from the Llama 3 family can reach SoTA performance on Arena Hard at 92.7
as of 5 Mar 2025, surpassing OpenAI o1-preview-2024-09-12 with 90.4 and
DeepSeek R1 with 92.3.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causally Reliable Concept Bottleneck Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giovanni De Felice, Arianna Casanova Flores, Francesco De Santis, Silvia Santini, Johannes Schneider, Pietro Barbiero, Alberto Termine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concept-based models are an emerging paradigm in deep learning that
constrains the inference process to operate through human-interpretable
concepts, facilitating explainability and human interaction. However, these
architectures, on par with popular opaque neural models, fail to account for
the true causal mechanisms underlying the target phenomena represented in the
data. This hampers their ability to support causal reasoning tasks, limits
out-of-distribution generalization, and hinders the implementation of fairness
constraints. To overcome these issues, we propose \emph{Causally reliable
Concept Bottleneck Models} (C$^2$BMs), a class of concept-based architectures
that enforce reasoning through a bottleneck of concepts structured according to
a model of the real-world causal mechanisms. We also introduce a pipeline to
automatically learn this structure from observational data and
\emph{unstructured} background knowledge (e.g., scientific literature).
Experimental evidence suggest that C$^2$BM are more interpretable, causally
reliable, and improve responsiveness to interventions w.r.t. standard opaque
and concept-based models, while maintaining their accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Generalist Cross-Domain Molecular Learning Framework for
  Structure-Based Drug Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04362v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04362v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiheng Zhu, Mingyang Li, Junlong Liu, Kun Fu, Jiansheng Wu, Qiuyi Li, Mingze Yin, Jieping Ye, Jian Wu, Zheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structure-based drug discovery (SBDD) is a systematic scientific process that
develops new drugs by leveraging the detailed physical structure of the target
protein. Recent advancements in pre-trained models for biomolecules have
demonstrated remarkable success across various biochemical applications,
including drug discovery and protein engineering. However, in most approaches,
the pre-trained models primarily focus on the characteristics of either small
molecules or proteins, without delving into their binding interactions which
are essential cross-domain relationships pivotal to SBDD. To fill this gap, we
propose a general-purpose foundation model named BIT (an abbreviation for
Biomolecular Interaction Transformer), which is capable of encoding a range of
biochemical entities, including small molecules, proteins, and protein-ligand
complexes, as well as various data formats, encompassing both 2D and 3D
structures. Specifically, we introduce Mixture-of-Domain-Experts (MoDE) to
handle the biomolecules from diverse biochemical domains and
Mixture-of-Structure-Experts (MoSE) to capture positional dependencies in the
molecular structures. The proposed mixture-of-experts approach enables BIT to
achieve both deep fusion and domain-specific encoding, effectively capturing
fine-grained molecular interactions within protein-ligand complexes. Then, we
perform cross-domain pre-training on the shared Transformer backbone via
several unified self-supervised denoising tasks. Experimental results on
various benchmarks demonstrate that BIT achieves exceptional performance in
downstream tasks, including binding affinity prediction, structure-based
virtual screening, and molecular property prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ scDD: Latent Codes Based scRNA-seq <span class="highlight-title">Dataset</span> Distillation with Foundation
  Model Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04357v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04357v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Yu, Jianan Han, Yang Liu, Qingchao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Single-cell RNA sequencing (scRNA-seq) technology has profiled hundreds of
millions of human cells across organs, diseases, development and perturbations
to date. However, the high-dimensional sparsity, batch effect noise, category
imbalance, and ever-increasing data scale of the original sequencing data pose
significant challenges for multi-center knowledge transfer, data fusion, and
cross-validation between scRNA-seq datasets. To address these barriers, (1) we
first propose a latent codes-based scRNA-seq dataset distillation framework
named scDD, which transfers and distills foundation model knowledge and
original dataset information into a compact latent space and generates
synthetic scRNA-seq dataset by a generator to replace the original dataset.
Then, (2) we propose a single-step conditional diffusion generator named SCDG,
which perform single-step gradient back-propagation to help scDD optimize
distillation quality and avoid gradient decay caused by multi-step
back-propagation. Meanwhile, SCDG ensures the scRNA-seq data characteristics
and inter-class discriminability of the synthetic dataset through flexible
conditional control and generation quality assurance. Finally, we propose a
comprehensive benchmark to evaluate the performance of scRNA-seq dataset
distillation in different data analysis tasks. It is validated that our
proposed method can achieve 7.61% absolute and 15.70% relative improvement over
previous state-of-the-art methods on average task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Talking Back -- human input and explanations to interactive AI systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alan Dix, Tommaso Turchi, Ben Wilson, Anna Monreale, Matt Roach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While XAI focuses on providing AI explanations to humans, can the reverse -
humans explaining their judgments to AI - foster richer, synergistic human-AI
systems? This paper explores various forms of human inputs to AI and examines
how human explanations can guide machine learning models toward automated
judgments and explanations that align more closely with human concepts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Solving Word-Sense Disambiguation and Word-Sense Induction with
  Dictionary Examples 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tadej Škvorc, Marko Robnik-Šikonja
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many less-resourced languages struggle with a lack of large, task-specific
datasets that are required for solving relevant tasks with modern
transformer-based large language models (LLMs). On the other hand, many
linguistic resources, such as dictionaries, are rarely used in this context
despite their large information contents. We show how LLMs can be used to
extend existing language resources in less-resourced languages for two
important tasks: word-sense disambiguation (WSD) and word-sense induction
(WSI). We approach the two tasks through the related but much more accessible
word-in-context (WiC) task where, given a pair of sentences and a target word,
a classification model is tasked with predicting whether the sense of a given
word differs between sentences. We demonstrate that a well-trained model for
this task can distinguish between different word senses and can be adapted to
solve the WSD and WSI tasks. The advantage of using the WiC task, instead of
directly predicting senses, is that the WiC task does not need pre-constructed
sense inventories with a sufficient number of examples for each sense, which
are rarely available in less-resourced languages. We show that sentence pairs
for the WiC task can be successfully generated from dictionary examples using
LLMs. The resulting prediction models outperform existing models on WiC, WSD,
and WSI tasks. We demonstrate our methodology on the Slovene language, where a
monolingual dictionary is available, but word-sense resources are tiny.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Provable Robust Overfitting Mitigation in Wasserstein Distributionally
  Robust Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04315v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04315v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuang Liu, Yihan Wang, Yifan Zhu, Yibo Miao, Xiao-Shan Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wasserstein distributionally robust optimization (WDRO) optimizes against
worst-case distributional shifts within a specified uncertainty set, leading to
enhanced generalization on unseen adversarial examples, compared to standard
adversarial training which focuses on pointwise adversarial perturbations.
However, WDRO still suffers fundamentally from the robust overfitting problem,
as it does not consider statistical error. We address this gap by proposing a
novel robust optimization framework under a new uncertainty set for adversarial
noise via Wasserstein distance and statistical error via Kullback-Leibler
divergence, called the Statistically Robust WDRO. We establish a robust
generalization bound for the new optimization framework, implying that
out-of-distribution adversarial performance is at least as good as the
statistically robust training loss with high probability. Furthermore, we
derive conditions under which Stackelberg and Nash equilibria exist between the
learner and the adversary, giving an optimal robust model in certain sense.
Finally, through extensive experiments, we demonstrate that our method
significantly mitigates robust overfitting and enhances robustness within the
framework of WDRO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Malware Detection at the Edge with Lightweight <span class="highlight-title">LLM</span>s: A Performance
  Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Rondanini, Barbara Carminati, Elena Ferrari, Antonio Gaudiano, Ashish Kundu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid evolution of malware attacks calls for the development of
innovative detection methods, especially in resource-constrained edge
computing. Traditional detection techniques struggle to keep up with modern
malware's sophistication and adaptability, prompting a shift towards advanced
methodologies like those leveraging Large Language Models (LLMs) for enhanced
malware detection. However, deploying LLMs for malware detection directly at
edge devices raises several challenges, including ensuring accuracy in
constrained environments and addressing edge devices' energy and computational
limits. To tackle these challenges, this paper proposes an architecture
leveraging lightweight LLMs' strengths while addressing limitations like
reduced accuracy and insufficient computational power. To evaluate the
effectiveness of the proposed lightweight LLM-based approach for edge
computing, we perform an extensive experimental evaluation using several
state-of-the-art lightweight LLMs. We test them with several publicly available
datasets specifically designed for edge and IoT scenarios and different edge
nodes with varying computational power and characteristics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mapping AI Benchmark Data to Quantitative Risk Estimates Through Expert
  Elicitation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04299v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04299v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Malcolm Murray, Henry Papadatos, Otter Quarks, Pierre-François Gimenez, Simeon Campos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The literature and multiple experts point to many potential risks from large
language models (LLMs), but there are still very few direct measurements of the
actual harms posed. AI risk assessment has so far focused on measuring the
models' capabilities, but the capabilities of models are only indicators of
risk, not measures of risk. Better modeling and quantification of AI risk
scenarios can help bridge this disconnect and link the capabilities of LLMs to
tangible real-world harm. This paper makes an early contribution to this field
by demonstrating how existing AI benchmarks can be used to facilitate the
creation of risk estimates. We describe the results of a pilot study in which
experts use information from Cybench, an AI benchmark, to generate probability
estimates. We show that the methodology seems promising for this purpose, while
noting improvements that can be made to further strengthen its application in
quantitative AI risk assessment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MathMistake Checker: A Comprehensive Demonstration for Step-by-Step Math
  Problem Mistake Finding by Prompt-Guided <span class="highlight-title">LLM</span>s <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04291v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04291v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyang Zhang, Zhuoxuan Jiang, Haotian Zhang, Lin Lin, Shaohua Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel system, MathMistake Checker, designed to automate
step-by-step mistake finding in mathematical problems with lengthy answers
through a two-stage process. The system aims to simplify grading, increase
efficiency, and enhance learning experiences from a pedagogical perspective. It
integrates advanced technologies, including computer vision and the
chain-of-thought capabilities of the latest large language models (LLMs). Our
system supports open-ended grading without reference answers and promotes
personalized learning by providing targeted feedback. We demonstrate its
effectiveness across various types of math problems, such as calculation and
word problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Do Hackathons Foster Creativity? Towards AI Collaborative Evaluation
  of Creativity at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04290v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04290v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeanette Falk, Yiyi Chen, Janet Rafner, Mike Zhang, Johannes Bjerva, Alexander Nolte
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hackathons have become popular collaborative events for accelerating the
development of creative ideas and prototypes. There are several case studies
showcasing creative outcomes across domains such as industry, education, and
research. However, there are no large-scale studies on creativity in hackathons
which can advance theory on how hackathon formats lead to creative outcomes. We
conducted a computational analysis of 193,353 hackathon projects. By
operationalizing creativity through usefulness and novelty, we refined our
dataset to 10,363 projects, allowing us to analyze how participant
characteristics, collaboration patterns, and hackathon setups influence the
development of creative projects. The contribution of our paper is twofold: We
identified means for organizers to foster creativity in hackathons. We also
explore the use of large language models (LLMs) to augment the evaluation of
creative outcomes and discuss challenges and opportunities of doing this, which
has implications for creativity research at large.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Proceedings of the 2025 CHI Conference on Human Factors
  in Computing Systems</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable AI in Time-Sensitive Scenarios: Prefetched Offline
  Explanation Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04283v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04283v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabio Michele Russo, Carlo Metta, Anna Monreale, Salvatore Rinzivillo, Fabio Pinelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As predictive machine learning models become increasingly adopted and
advanced, their role has evolved from merely predicting outcomes to actively
shaping them. This evolution has underscored the importance of Trustworthy AI,
highlighting the necessity to extend our focus beyond mere accuracy and toward
a comprehensive understanding of these models' behaviors within the specific
contexts of their applications. To further progress in explainability, we
introduce Poem, Prefetched Offline Explanation Model, a model-agnostic, local
explainability algorithm for image data. The algorithm generates exemplars,
counterexemplars and saliency maps to provide quick and effective explanations
suitable for time-sensitive scenarios. Leveraging an existing local algorithm,
\poem{} infers factual and counterfactual rules from data to create
illustrative examples and opposite scenarios with an enhanced stability by
design. A novel mechanism then matches incoming test points with an explanation
base and produces diverse exemplars, informative saliency maps and believable
counterexemplars. Experimental results indicate that Poem outperforms its
predecessor Abele in speed and ability to generate more nuanced and varied
exemplars alongside more insightful saliency maps and valuable
counterexemplars.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Autonomous Reinforcement Learning for Real-Wo<span class="highlight-title">rl</span>d Robotic
  Manipulation with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niccolò Turcato, Matteo Iovino, Aris Synodinos, Alberto Dalla Libera, Ruggero Carli, Pietro Falco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Models (LLMs) and Visual Language
Models (VLMs) have significantly impacted robotics, enabling high-level
semantic motion planning applications. Reinforcement Learning (RL), a
complementary paradigm, enables agents to autonomously optimize complex
behaviors through interaction and reward signals. However, designing effective
reward functions for RL remains challenging, especially in real-world tasks
where sparse rewards are insufficient and dense rewards require elaborate
design. In this work, we propose Autonomous Reinforcement learning for Complex
HumanInformed Environments (ARCHIE), an unsupervised pipeline leveraging GPT-4,
a pre-trained LLM, to generate reward functions directly from natural language
task descriptions. The rewards are used to train RL agents in simulated
environments, where we formalize the reward generation process to enhance
feasibility. Additionally, GPT-4 automates the coding of task success criteria,
creating a fully automated, one-shot procedure for translating human-readable
text into deployable robot skills. Our approach is validated through extensive
simulated experiments on single-arm and bi-manual manipulation tasks using an
ABB YuMi collaborative robot, highlighting its practicality and effectiveness.
Tasks are demonstrated on the real robot setup.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prompt Programming: A Platform for Dialogue-based Computational Problem
  Solving with Generative AI Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04267v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04267v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor-Alexandru Pădurean, Paul Denny, Alkis Gotovos, Adish Singla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computing students increasingly rely on generative AI tools for programming
assistance, often without formal instruction or guidance. This highlights a
need to teach students how to effectively interact with AI models, particularly
through natural language prompts, to generate and critically evaluate code for
solving computational tasks. To address this, we developed a novel platform for
prompt programming that enables authentic dialogue-based interactions, supports
problems involving multiple interdependent functions, and offers on-request
execution of generated code. Data analysis from over 900 students in an
introductory programming course revealed high engagement, with the majority of
prompts occurring within multi-turn dialogues. Problems with multiple
interdependent functions encouraged iterative refinement, with progression
graphs highlighting several common strategies. Students were highly selective
about the code they chose to test, suggesting that on-request execution of
generated code promoted critical thinking. Given the growing importance of
learning dialogue-based programming with AI, we provide this tool as a publicly
accessible resource, accompanied by a corpus of programming problems for
educational use.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint of the ITiCSE'25 paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Guidelines for Applying <span class="highlight-title">RL</span> and MA<span class="highlight-title">RL</span> in Cybersecurity Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vasilios Mavroudis, Gregory Palmer, Sara Farmer, Kez Smithson Whitehead, David Foster, Adam Price, Ian Miles, Alberto Caron, Stephen Pasteris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) and Multi-Agent Reinforcement Learning (MARL)
have emerged as promising methodologies for addressing challenges in automated
cyber defence (ACD). These techniques offer adaptive decision-making
capabilities in high-dimensional, adversarial environments. This report
provides a structured set of guidelines for cybersecurity professionals and
researchers to assess the suitability of RL and MARL for specific use cases,
considering factors such as explainability, exploration needs, and the
complexity of multi-agent coordination. It also discusses key algorithmic
approaches, implementation challenges, and real-world constraints, such as data
scarcity and adversarial interference. The report further outlines open
research questions, including policy optimality, agent cooperation levels, and
the integration of MARL systems into operational cybersecurity frameworks. By
bridging theoretical advancements and practical deployment, these guidelines
aim to enhance the effectiveness of AI-driven cyber defence strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VirtualXAI: A User-Centric Framework for Explainability Assessment
  Leveraging GPT-Generated Personas 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04261v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04261v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios Makridis, Vasileios Koukos, Georgios Fatouros, Dimosthenis Kyriazis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In today's data-driven era, computational systems generate vast amounts of
data that drive the digital transformation of industries, where Artificial
Intelligence (AI) plays a key role. Currently, the demand for eXplainable AI
(XAI) has increased to enhance the interpretability, transparency, and
trustworthiness of AI models. However, evaluating XAI methods remains
challenging: existing evaluation frameworks typically focus on quantitative
properties such as fidelity, consistency, and stability without taking into
account qualitative characteristics such as satisfaction and interpretability.
In addition, practitioners face a lack of guidance in selecting appropriate
datasets, AI models, and XAI methods -a major hurdle in human-AI collaboration.
To address these gaps, we propose a framework that integrates quantitative
benchmarking with qualitative user assessments through virtual personas based
on the "Anthology" of backstories of the Large Language Model (LLM). Our
framework also incorporates a content-based recommender system that leverages
dataset-specific characteristics to match new input data with a repository of
benchmarked datasets. This yields an estimated XAI score and provides tailored
recommendations for both the optimal AI model and the XAI method for a given
scenario.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TAIL: Text-Audio Incremental Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingfei Sun, Xu Gu, Wei Ji, Hanbin Zhao, Hao Fei, Yifang Yin, Roger Zimmermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many studies combine text and audio to capture multi-modal information but
they overlook the model's generalization ability on new datasets. Introducing
new datasets may affect the feature space of the original dataset, leading to
catastrophic forgetting. Meanwhile, large model parameters can significantly
impact training performance. To address these limitations, we introduce a novel
task called Text-Audio Incremental Learning (TAIL) task for text-audio
retrieval, and propose a new method, PTAT, Prompt Tuning for Audio-Text
incremental learning. This method utilizes prompt tuning to optimize the model
parameters while incorporating an audio-text similarity and feature
distillation module to effectively mitigate catastrophic forgetting. We
benchmark our method and previous incremental learning methods on AudioCaps,
Clotho, BBC Sound Effects and Audioset datasets, and our method outperforms
previous methods significantly, particularly demonstrating stronger resistance
to forgetting on older datasets. Compared to the full-parameters Finetune
(Sequential) method, our model only requires 2.42\% of its parameters,
achieving 4.46\% higher performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How to Move Your Dragon: Text-to-Motion Synthesis for Large-Vocabulary
  Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04257v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04257v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wonkwang Lee, Jongwon Jeong, Taehong Moon, Hyeon-Jong Kim, Jaehyeon Kim, Gunhee Kim, Byeong-Uk Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion synthesis for diverse object categories holds great potential for 3D
content creation but remains underexplored due to two key challenges: (1) the
lack of comprehensive motion datasets that include a wide range of high-quality
motions and annotations, and (2) the absence of methods capable of handling
heterogeneous skeletal templates from diverse objects. To address these
challenges, we contribute the following: First, we augment the Truebones Zoo
dataset, a high-quality animal motion dataset covering over 70 species, by
annotating it with detailed text descriptions, making it suitable for
text-based motion synthesis. Second, we introduce rig augmentation techniques
that generate diverse motion data while preserving consistent dynamics,
enabling models to adapt to various skeletal configurations. Finally, we
redesign existing motion diffusion models to dynamically adapt to arbitrary
skeletal templates, enabling motion synthesis for a diverse range of objects
with varying structures. Experiments show that our method learns to generate
high-fidelity motions from textual descriptions for diverse and even unseen
objects, setting a strong foundation for motion synthesis across diverse object
categories and skeletal templates. Qualitative results are available on this
link: t2m4lvo.github.io
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge Retention for Continual Model-Based Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixiang Sun, Haotian Fu, Michael Littman, George Konidaris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose DRAGO, a novel approach for continual model-based reinforcement
learning aimed at improving the incremental development of world models across
a sequence of tasks that differ in their reward functions but not the state
space or dynamics. DRAGO comprises two key components: Synthetic Experience
Rehearsal, which leverages generative models to create synthetic experiences
from past tasks, allowing the agent to reinforce previously learned dynamics
without storing data, and Regaining Memories Through Exploration, which
introduces an intrinsic reward mechanism to guide the agent toward revisiting
relevant states from prior tasks. Together, these components enable the agent
to maintain a comprehensive and continually developing world model,
facilitating more effective learning and adaptation across diverse
environments. Empirical evaluations demonstrate that DRAGO is able to preserve
knowledge across tasks, achieving superior performance in various continual
learning scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How to Mitigate Overfitting in Weak-to-strong Generalization? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junhao Shi, Qinyuan Cheng, Zhaoye Fei, Yining Zheng, Qipeng Guo, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning powerful AI models on tasks that surpass human evaluation
capabilities is the central problem of \textbf{superalignment}. To address this
problem, weak-to-strong generalization aims to elicit the capabilities of
strong models through weak supervisors and ensure that the behavior of strong
models aligns with the intentions of weak supervisors without unsafe behaviors
such as deception. Although weak-to-strong generalization exhibiting certain
generalization capabilities, strong models exhibit significant overfitting in
weak-to-strong generalization: Due to the strong fit ability of strong models,
erroneous labels from weak supervisors may lead to overfitting in strong
models. In addition, simply filtering out incorrect labels may lead to a
degeneration in question quality, resulting in a weak generalization ability of
strong models on hard questions. To mitigate overfitting in weak-to-strong
generalization, we propose a two-stage framework that simultaneously improves
the quality of supervision signals and the quality of input questions.
Experimental results in three series of large language models and two
mathematical benchmarks demonstrate that our framework significantly improves
PGR compared to naive weak-to-strong generalization, even achieving up to 100\%
PGR on some models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One-Shot Clustering for Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maciej Krzysztof Zuziak, Roberto Pellungrini, Salvatore Rinzivillo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is a widespread and well adopted paradigm of
decentralized learning that allows training one model from multiple sources
without the need to directly transfer data between participating clients. Since
its inception in 2015, it has been divided into numerous sub-fields that deal
with application-specific issues, be it data heterogeneity or resource
allocation. One such sub-field, Clustered Federated Learning (CFL), is dealing
with the problem of clustering the population of clients into separate cohorts
to deliver personalized models. Although few remarkable works have been
published in this domain, the problem is still largely unexplored, as its basic
assumption and settings are slightly different from standard FL. In this work,
we present One-Shot Clustered Federated Learning (OCFL), a clustering-agnostic
algorithm that can automatically detect the earliest suitable moment for
clustering. Our algorithm is based on the computation of cosine similarity
between gradients of the clients and a temperature measure that detects when
the federated model starts to converge. We empirically evaluate our methodology
by testing various one-shot clustering algorithms for over thirty different
tasks on three benchmark datasets. Our experiments showcase the good
performance of our approach when used to perform CFL in an automated manner
without the need to adjust hyperparameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantum-Inspired Reinforcement Learning in the Presence of Epistemic
  Ambivalence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alireza Habibi, Saeed Ghoorchian, Setareh Maghsudi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The complexity of online decision-making under uncertainty stems from the
requirement of finding a balance between exploiting known strategies and
exploring new possibilities. Naturally, the uncertainty type plays a crucial
role in developing decision-making strategies that manage complexity
effectively. In this paper, we focus on a specific form of uncertainty known as
epistemic ambivalence (EA), which emerges from conflicting pieces of evidence
or contradictory experiences. It creates a delicate interplay between
uncertainty and confidence, distinguishing it from epistemic uncertainty that
typically diminishes with new information. Indeed, ambivalence can persist even
after additional knowledge is acquired. To address this phenomenon, we propose
a novel framework, called the epistemically ambivalent Markov decision process
(EA-MDP), aiming to understand and control EA in decision-making processes.
This framework incorporates the concept of a quantum state from the quantum
mechanics formalism, and its core is to assess the probability and reward of
every possible outcome. We calculate the reward function using quantum
measurement techniques and prove the existence of an optimal policy and an
optimal value function in the EA-MDP framework. We also propose the
EA-epsilon-greedy Q-learning algorithm. To evaluate the impact of EA on
decision-making and the expedience of our framework, we study two distinct
experimental setups, namely the two-state problem and the lattice problem. Our
results show that using our methods, the agent converges to the optimal policy
in the presence of EA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge-Decoupled Synergetic Learning: An M<span class="highlight-title">LLM</span> based Collaborative
  Approach to Few-shot <span class="highlight-title">Multi</span>modal Dialogue Intention Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04201v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04201v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Chen, Yu Zhang, Hongfei Ye, Ziyi Huang, Hongyang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot multimodal dialogue intention recognition is a critical challenge in
the e-commerce domainn. Previous methods have primarily enhanced model
classification capabilities through post-training techniques. However, our
analysis reveals that training for few-shot multimodal dialogue intention
recognition involves two interconnected tasks, leading to a seesaw effect in
multi-task learning. This phenomenon is attributed to knowledge interference
stemming from the superposition of weight matrix updates during the training
process. To address these challenges, we propose Knowledge-Decoupled Synergetic
Learning (KDSL), which mitigates these issues by utilizing smaller models to
transform knowledge into interpretable rules, while applying the post-training
of larger models. By facilitating collaboration between the large and small
multimodal large language models for prediction, our approach demonstrates
significant improvements. Notably, we achieve outstanding results on two real
Taobao datasets, with enhancements of 6.37\% and 6.28\% in online weighted F1
scores compared to the state-of-the-art method, thereby validating the efficacy
of our framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MASTER: <span class="highlight-title">Multi</span>modal Segmentation with Text Prompts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04199v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04199v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fuyang Liu, Shun Lu, Jilin Mei, Yu Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  RGB-Thermal fusion is a potential solution for various weather and light
conditions in challenging scenarios. However, plenty of studies focus on
designing complex modules to fuse different modalities. With the widespread
application of large language models (LLMs), valuable information can be more
effectively extracted from natural language. Therefore, we aim to leverage the
advantages of large language models to design a structurally simple and highly
adaptable multimodal fusion model architecture. We proposed MultimodAl
Segmentation with TExt PRompts (MASTER) architecture, which integrates LLM into
the fusion of RGB-Thermal multimodal data and allows complex query text to
participate in the fusion process. Our model utilizes a dual-path structure to
extract information from different modalities of images. Additionally, we
employ LLM as the core module for multimodal fusion, enabling the model to
generate learnable codebook tokens from RGB, thermal images, and textual
information. A lightweight image decoder is used to obtain semantic
segmentation results. The proposed MASTER performs exceptionally well in
benchmark tests across various automated driving scenarios, yielding promising
results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large-Scale AI in Telecom: Charting the Roadmap for Innovation,
  Scalability, and Enhanced Digital Experiences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04184v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04184v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adnan Shahid, Adrian Kliks, Ahmed Al-Tahmeesschi, Ahmed Elbakary, Alexandros Nikou, Ali Maatouk, Ali Mokh, Amirreza Kazemi, Antonio De Domenico, Athanasios Karapantelakis, Bo Cheng, Bo Yang, Bohao Wang, Carlo Fischione, Chao Zhang, Chaouki Ben Issaid, Chau Yuen, Chenghui Peng, Chongwen Huang, Christina Chaccour, Christo Kurisummoottil Thomas, Dheeraj Sharma, Dimitris Kalogiros, Dusit Niyato, Eli De Poorter, Elissa Mhanna, Emilio Calvanese Strinati, Faouzi Bader, Fathi Abdeldayem, Fei Wang, Fenghao Zhu, Gianluca Fontanesi, Giovanni Geraci, Haibo Zhou, Hakimeh Purmehdi, Hamed Ahmadi, Hang Zou, Hongyang Du, Hoon Lee, Howard H. Yang, Iacopo Poli, Igor Carron, Ilias Chatzistefanidis, Inkyu Lee, Ioannis Pitsiorlas, Jaron Fontaine, Jiajun Wu, Jie Zeng, Jinan Li, Jinane Karam, Johny Gemayel, Juan Deng, Julien Frison, Kaibin Huang, Kehai Qiu, Keith Ball, Kezhi Wang, Kun Guo, Leandros Tassiulas, Lecorve Gwenole, Liexiang Yue, Lina Bariah, Louis Powell, Marcin Dryjanski, Maria Amparo Canaveras Galdon, Marios Kountouris, Maryam Hafeez, Maxime Elkael, Mehdi Bennis, Mehdi Boudjelli, Meiling Dai, Merouane Debbah, Michele Polese, Mohamad Assaad, Mohamed Benzaghta, Mohammad Al Refai, Moussab Djerrab, Mubeen Syed, Muhammad Amir, Na Yan, Najla Alkaabi, Nan Li, Nassim Sehad, Navid Nikaein, Omar Hashash, Pawel Sroka, Qianqian Yang, Qiyang Zhao, Rasoul Nikbakht Silab, Rex Ying, Roberto Morabito, Rongpeng Li, Ryad Madi, Salah Eddine El Ayoubi, Salvatore D'Oro, Samson Lasaulce, Serveh Shalmashi, Sige Liu, Sihem Cherrared, Swarna Bindu Chetty, Swastika Dutta, Syed A. R. Zaidi, Tianjiao Chen, Timothy Murphy, Tommaso Melodia, Tony Q. S. Quek, Vishnu Ram, Walid Saad, Wassim Hamidouche, Weilong Chen, Xiaoou Liu, Xiaoxue Yu, Xijun Wang, Xingyu Shang, Xinquan Wang, Xuelin Cao, Yang Su, Yanping Liang, Yansha Deng, Yifan Yang, Yingping Cui, Yu Sun, Yuxuan Chen, Yvan Pointurier, Zeinab Nehme, Zeinab Nezami, Zhaohui Yang, Zhaoyang Zhang, Zhe Liu, Zhenyu Yang, Zhu Han, Zhuang Zhou, Zihan Chen, Zirui Chen, Zitao Shuai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This white paper discusses the role of large-scale AI in the
telecommunications industry, with a specific focus on the potential of
generative AI to revolutionize network functions and user experiences,
especially in the context of 6G systems. It highlights the development and
deployment of Large Telecom Models (LTMs), which are tailored AI models
designed to address the complex challenges faced by modern telecom networks.
The paper covers a wide range of topics, from the architecture and deployment
strategies of LTMs to their applications in network management, resource
allocation, and optimization. It also explores the regulatory, ethical, and
standardization considerations for LTMs, offering insights into their future
integration into telecom infrastructure. The goal is to provide a comprehensive
roadmap for the adoption of LTMs to enhance scalability, performance, and
user-centric innovation in telecom networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CrowdHMTware: A Cross-level Co-adaptation Middleware for Context-aware
  Mobile DL Deployment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sicong Liu, Bin Guo, Shiyan Luo, Yuzhan Wang, Hao Luo, Cheng Fang, Yuan Xu, Ke Ma, Yao Li, Zhiwen Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There are many deep learning (DL) powered mobile and wearable applications
today continuously and unobtrusively sensing the ambient surroundings to
enhance all aspects of human lives.To enable robust and private mobile sensing,
DL models are often deployed locally on resource-constrained mobile devices
using techniques such as model compression or offloading.However, existing
methods, either front-end algorithm level (i.e. DL model
compression/partitioning) or back-end scheduling level (i.e. operator/resource
scheduling), cannot be locally online because they require offline retraining
to ensure accuracy or rely on manually pre-defined strategies, struggle with
dynamic adaptability.The primary challenge lies in feeding back runtime
performance from the back-end level to the front-end level optimization
decision. Moreover, the adaptive mobile DL model porting middleware with
cross-level co-adaptation is less explored, particularly in mobile environments
with diversity and dynamics. In response, we introduce CrowdHMTware, a dynamic
context-adaptive DL model deployment middleware for heterogeneous mobile
devices. It establishes an automated adaptation loop between cross-level
functional components, i.e. elastic inference, scalable offloading, and
model-adaptive engine, enhancing scalability and adaptability. Experiments with
four typical tasks across 15 platforms and a real-world case study demonstrate
that CrowdHMTware can effectively scale DL model, offloading, and engine
actions across diverse platforms and tasks. It hides run-time system issues
from developers, reducing the required developer expertise.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by IEEE Transactions on Mobile Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TIMER: Temporal Instruction Modeling and Evaluation for Longitudinal
  Clinical Records 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hejie Cui, Alyssa Unell, Bowen Chen, Jason Alan Fries, Emily Alsentzer, Sanmi Koyejo, Nigam Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have emerged as promising tools for assisting in
medical tasks, yet processing Electronic Health Records (EHRs) presents unique
challenges due to their longitudinal nature. While LLMs' capabilities to
perform medical tasks continue to improve, their ability to reason over
temporal dependencies across multiple patient visits and time frames remains
unexplored. We introduce TIMER (Temporal Instruction Modeling and Evaluation
for Longitudinal Clinical Records), a framework that incorporate
instruction-response pairs grounding to different parts of a patient's record
as a critical dimension in both instruction evaluation and tuning for
longitudinal clinical records. We develop TIMER-Bench, the first time-aware
benchmark that evaluates temporal reasoning capabilities over longitudinal
EHRs, as well as TIMER-Instruct, an instruction-tuning methodology for LLMs to
learn reasoning over time. We demonstrate that models fine-tuned with
TIMER-Instruct improve performance by 7.3% on human-generated benchmarks and
9.2% on TIMER-Bench, indicating that temporal instruction-tuning improves model
performance for reasoning over EHR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Intelligent Transportation with Pedestrians and Vehicles
  In-the-Loop: A Surveillance Video-Assisted Federated Digital Twin Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04170v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04170v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaolong Li, Jianhao Wei, Haidong Wang, Li Dong, Ruoyang Chen, Changyan Yi, Jun Cai, Dusit Niyato,  Xuemin,  Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In intelligent transportation systems (ITSs), incorporating pedestrians and
vehicles in-the-loop is crucial for developing realistic and safe traffic
management solutions. However, there is falls short of simulating complex
real-world ITS scenarios, primarily due to the lack of a digital twin
implementation framework for characterizing interactions between pedestrians
and vehicles at different locations in different traffic environments. In this
article, we propose a surveillance video assisted federated digital twin
(SV-FDT) framework to empower ITSs with pedestrians and vehicles in-the-loop.
Specifically, SVFDT builds comprehensive pedestrian-vehicle interaction models
by leveraging multi-source traffic surveillance videos. Its architecture
consists of three layers: (i) the end layer, which collects traffic
surveillance videos from multiple sources; (ii) the edge layer, responsible for
semantic segmentation-based visual understanding, twin agent-based interaction
modeling, and local digital twin system (LDTS) creation in local regions; and
(iii) the cloud layer, which integrates LDTSs across different regions to
construct a global DT model in realtime. We analyze key design requirements and
challenges and present core guidelines for SVFDT's system implementation. A
testbed evaluation demonstrates its effectiveness in optimizing traffic
management. Comparisons with traditional terminal-server frameworks highlight
SV-FDT's advantages in mirroring delays, recognition accuracy, and subjective
evaluation. Finally, we identify some open challenges and discuss future
research directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Role of Visual Modality in <span class="highlight-title">Multi</span>modal Mathematical Reasoning:
  Challenges and Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yufang Liu, Yao Du, Tao Ji, Jianing Wang, Yang Liu, Yuanbin Wu, Aimin Zhou, Mengdi Zhang, Xunliang Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has increasingly focused on multimodal mathematical
reasoning, particularly emphasizing the creation of relevant datasets and
benchmarks. Despite this, the role of visual information in reasoning has been
underexplored. Our findings show that existing multimodal mathematical models
minimally leverage visual information, and model performance remains largely
unaffected by changes to or removal of images in the dataset. We attribute this
to the dominance of textual information and answer options that inadvertently
guide the model to correct answers. To improve evaluation methods, we introduce
the HC-M3D dataset, specifically designed to require image reliance for
problem-solving and to challenge models with similar, yet distinct, images that
change the correct answer. In testing leading models, their failure to detect
these subtle visual differences suggests limitations in current visual
perception capabilities. Additionally, we observe that the common approach of
improving general VQA capabilities by combining various types of image encoders
does not contribute to math reasoning performance. This finding also presents a
challenge to enhancing visual reliance during math reasoning. Our benchmark and
code would be available at
\href{https://github.com/Yufang-Liu/visual_modality_role}{https://github.com/Yufang-Liu/visual\_modality\_role}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Semantic Retrieval Augmented Contrastive Learning for Sequential
  Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04162v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04162v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqiang Cui, Yunpeng Weng, Xing Tang, Xiaokun Zhang, Dugang Liu, Shiwei Li, Peiyang Liu, Bowei He, Weihong Luo, Xiuqiang He, Chen Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommendation aims to model user preferences based on historical
behavior sequences, which is crucial for various online platforms. Data
sparsity remains a significant challenge in this area as most users have
limited interactions and many items receive little attention. To mitigate this
issue, contrastive learning has been widely adopted. By constructing positive
sample pairs from the data itself and maximizing their agreement in the
embedding space,it can leverage available data more effectively. Constructing
reasonable positive sample pairs is crucial for the success of contrastive
learning. However, current approaches struggle to generate reliable positive
pairs as they either rely on representations learned from inherently sparse
collaborative signals or use random perturbations which introduce significant
uncertainty. To address these limitations, we propose a novel approach named
Semantic Retrieval Augmented Contrastive Learning (SRA-CL), which leverages
semantic information to improve the reliability of contrastive samples. SRA-CL
comprises two main components: (1) Cross-Sequence Contrastive Learning via User
Semantic Retrieval, which utilizes large language models (LLMs) to understand
diverse user preferences and retrieve semantically similar users to form
reliable positive samples through a learnable sample synthesis method; and (2)
Intra-Sequence Contrastive Learning via Item Semantic Retrieval, which employs
LLMs to comprehend items and retrieve similar items to perform semantic-based
item substitution, thereby creating semantically consistent augmented views for
contrastive learning. SRA-CL is plug-and-play and can be integrated into
standard sequential recommendation models. Extensive experiments on four public
datasets demonstrate the effectiveness and generalizability of the proposed
approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unseen Fake News Detection Through Casual Debiasing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuzhi Gong, Richard Sinnott, Jianzhong Qi, Cecile Paris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread dissemination of fake news on social media poses significant
risks, necessitating timely and accurate detection. However, existing methods
struggle with unseen news due to their reliance on training data from past
events and domains, leaving the challenge of detecting novel fake news largely
unresolved. To address this, we identify biases in training data tied to
specific domains and propose a debiasing solution FNDCD. Originating from
causal analysis, FNDCD employs a reweighting strategy based on classification
confidence and propagation structure regularization to reduce the influence of
domain-specific biases, enhancing the detection of unseen fake news.
Experiments on real-world datasets with non-overlapping news domains
demonstrate FNDCD's effectiveness in improving generalization across domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2025 The Web Conference, 6 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CA-W3D: Leveraging Context-Aware Knowledge for Weakly Supervised
  Monocular 3D Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04154v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04154v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chupeng Liu, Runkai Zhao, Weidong Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weakly supervised monocular 3D detection, while less annotation-intensive,
often struggles to capture the global context required for reliable 3D
reasoning. Conventional label-efficient methods focus on object-centric
features, neglecting contextual semantic relationships that are critical in
complex scenes. In this work, we propose a Context-Aware Weak Supervision for
Monocular 3D object detection, namely CA-W3D, to address this limitation in a
two-stage training paradigm. Specifically, we first introduce a pre-training
stage employing Region-wise Object Contrastive Matching (ROCM), which aligns
regional object embeddings derived from a trainable monocular 3D encoder and a
frozen open-vocabulary 2D visual grounding model. This alignment encourages the
monocular encoder to discriminate scene-specific attributes and acquire richer
contextual knowledge. In the second stage, we incorporate a pseudo-label
training process with a Dual-to-One Distillation (D2OD) mechanism, which
effectively transfers contextual priors into the monocular encoder while
preserving spatial fidelity and maintaining computational efficiency during
inference. Extensive experiments conducted on the public KITTI benchmark
demonstrate the effectiveness of our approach, surpassing the SoTA method over
all metrics, highlighting the importance of contextual-aware knowledge in
weakly-supervised monocular 3D detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper includes 8 pages, 6 figures and 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KidneyTalk-open: No-code Deployment of a Private Large Language Model
  with Medical Documentation-Enhanced Knowledge Database for Kidney Disease 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04153v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04153v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchao Long, Chao Yang, Gongzheng Tang, Jinwei Wang, Zhun Sui, Yuxi Zhou, Shenda Hong, Luxia Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Privacy-preserving medical decision support for kidney disease requires
localized deployment of large language models (LLMs) while maintaining clinical
reasoning capabilities. Current solutions face three challenges: 1) Cloud-based
LLMs pose data security risks; 2) Local model deployment demands technical
expertise; 3) General LLMs lack mechanisms to integrate medical knowledge.
Retrieval-augmented systems also struggle with medical document processing and
clinical usability. We developed KidneyTalk-open, a desktop system integrating
three technical components: 1) No-code deployment of state-of-the-art (SOTA)
open-source LLMs (such as DeepSeek-r1, Qwen2.5) via local inference engine; 2)
Medical document processing pipeline combining context-aware chunking and
intelligent filtering; 3) Adaptive Retrieval and Augmentation Pipeline (AddRep)
employing agents collaboration for improving the recall rate of medical
documents. A graphical interface was designed to enable clinicians to manage
medical documents and conduct AI-powered consultations without technical
expertise. Experimental validation on 1,455 challenging nephrology exam
questions demonstrates AddRep's effectiveness: achieving 29.1% accuracy (+8.1%
over baseline) with intelligent knowledge integration, while maintaining
robustness through 4.9% rejection rate to suppress hallucinations. Comparative
case studies with the mainstream products (AnythingLLM, Chatbox, GPT4ALL)
demonstrate KidneyTalk-open's superior performance in real clinical query.
KidneyTalk-open represents the first no-code medical LLM system enabling secure
documentation-enhanced medical Q&A on desktop. Its designs establishes a new
framework for privacy-sensitive clinical AI applications. The system
significantly lowers technical barriers while improving evidence traceability,
enabling more medical staff or patients to use SOTA open-source LLMs
conveniently.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Corresponding authors: zhanglx@bjmu.edu.cn; joy_yuxi@pku.edu.cn;
  hongshenda@pku.edu.cn</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust <span class="highlight-title">Multi</span>-View Learning via Representation Fusion of Sample-Level
  Attention and Alignment of Simulated Perturbation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04151v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04151v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Xu, Na Zhao, Gang Niu, Masashi Sugiyama, Xiaofeng Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, multi-view learning (MVL) has garnered significant attention due to
its ability to fuse discriminative information from multiple views. However,
real-world multi-view datasets are often heterogeneous and imperfect, which
usually makes MVL methods designed for specific combinations of views lack
application potential and limits their effectiveness. To address this issue, we
propose a novel robust MVL method (namely RML) with simultaneous representation
fusion and alignment. Specifically, we introduce a simple yet effective
multi-view transformer fusion network where we transform heterogeneous
multi-view data into homogeneous word embeddings, and then integrate multiple
views by the sample-level attention mechanism to obtain a fused representation.
Furthermore, we propose a simulated perturbation based multi-view contrastive
learning framework that dynamically generates the noise and unusable
perturbations for simulating imperfect data conditions. The simulated noisy and
unusable data obtain two distinct fused representations, and we utilize
contrastive learning to align them for learning discriminative and robust
representations. Our RML is self-supervised and can also be applied for
downstream tasks as a regularization. In experiments, we employ it in
unsupervised multi-view clustering, noise-label classification, and as a
plug-and-play module for cross-modal hashing retrieval. Extensive comparison
experiments and ablation studies validate the effectiveness of RML.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ticktack : Long Span Temporal Alignment of Large Language Models
  Leveraging Sexagenary Cycle Time Expression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xue Han, Qian Hu, Yitong Wang, Wenchun Gao, Lianlian Zhang, Qing Wang, Lijun Mei, Chao Deng, Junlan Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) suffer from temporal misalignment issues
especially across long span of time. The issue arises from knowing that LLMs
are trained on large amounts of data where temporal information is rather
sparse over long times, such as thousands of years, resulting in insufficient
learning or catastrophic forgetting by the LLMs. This paper proposes a
methodology named "Ticktack" for addressing the LLM's long-time span
misalignment in a yearly setting. Specifically, we first propose to utilize the
sexagenary year expression instead of the Gregorian year expression employed by
LLMs, achieving a more uniform distribution in yearly granularity. Then, we
employ polar coordinates to model the sexagenary cycle of 60 terms and the year
order within each term, with additional temporal encoding to ensure LLMs
understand them. Finally, we present a temporal representational alignment
approach for post-training LLMs that effectively distinguishes time points with
relevant knowledge, hence improving performance on time-related tasks,
particularly over a long period. We also create a long time span benchmark for
evaluation. Experimental results prove the effectiveness of our proposal.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dynamic Benchmarking of Reasoning Capabilities in Code Large Language
  Models Under Data Contamination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04149v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04149v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simin Chen, Pranav Pusarla, Baishakhi Ray
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid evolution of code largelanguage models underscores the need for
effective and transparent benchmarking of their reasoning capabilities.
However, the current benchmarking approach heavily depends on publicly
available, human-created datasets. The widespread use of these fixed benchmark
datasets makes the benchmarking process to be static and thus particularly
susceptible to data contamination, an unavoidable consequence of the extensive
data collection processes used to train Code LLMs. Existing approaches that
address data contamination often suffer from human effort limitations and
imbalanced problem complexity. To tackle these challenges, we propose \tool, a
novel benchmarking suite for evaluating Code LLMs under potential data
contamination. Given a seed programming problem, \tool employs multiple agents
to extract and modify the context without altering the core logic, generating
semantically equivalent variations. We introduce a dynamic data generation
methods and conduct empirical studies on two seed datasets across 21 Code LLMs.
Results show that \tool effectively benchmarks reasoning capabilities under
contamination risks while generating diverse problem sets to ensure consistent
and reliable evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://codekaleidoscope.github.io/dycodeeval.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DM-Adapter: Domain-Aware Mixture-of-Adapters for Text-Based Person
  Retrieval <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yating Liu, Zimo Liu, Xiangyuan Lan, Wenming Yang, Yaowei Li, Qingmin Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-based person retrieval (TPR) has gained significant attention as a
fine-grained and challenging task that closely aligns with practical
applications. Tailoring CLIP to person domain is now a emerging research topic
due to the abundant knowledge of vision-language pretraining, but challenges
still remain during fine-tuning: (i) Previous full-model fine-tuning in TPR is
computationally expensive and prone to overfitting.(ii) Existing
parameter-efficient transfer learning (PETL) for TPR lacks of fine-grained
feature extraction. To address these issues, we propose Domain-Aware
Mixture-of-Adapters (DM-Adapter), which unifies Mixture-of-Experts (MOE) and
PETL to enhance fine-grained feature representations while maintaining
efficiency. Specifically, Sparse Mixture-of-Adapters is designed in parallel to
MLP layers in both vision and language branches, where different experts
specialize in distinct aspects of person knowledge to handle features more
finely. To promote the router to exploit domain information effectively and
alleviate the routing imbalance, Domain-Aware Router is then developed by
building a novel gating function and injecting learnable domain-aware prompts.
Extensive experiments show that our DM-Adapter achieves state-of-the-art
performance, outperforming previous methods by a significant margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 5 figures, accepted by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MTS: A Deep Reinforcement Learning Portfolio Management Framework with
  Time-Awareness and Short-Selling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengchen Gu, Zhengyong Jiang, Ángel F. García-Fernández, Angelos Stefanidis, Jionglong Su, Huakang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Portfolio management remains a crucial challenge in finance, with traditional
methods often falling short in complex and volatile market environments. While
deep reinforcement approaches have shown promise, they still face limitations
in dynamic risk management, exploitation of temporal markets, and incorporation
of complex trading strategies such as short-selling. These limitations can lead
to suboptimal portfolio performance, increased vulnerability to market
volatility, and missed opportunities in capturing potential returns from
diverse market conditions. This paper introduces a Deep Reinforcement Learning
Portfolio Management Framework with Time-Awareness and Short-Selling (MTS),
offering a robust and adaptive strategy for sustainable investment performance.
This framework utilizes a novel encoder-attention mechanism to address the
limitations by incorporating temporal market characteristics, a parallel
strategy for automated short-selling based on market trends, and risk
management through innovative Incremental Conditional Value at Risk, enhancing
adaptability and performance. Experimental validation on five diverse datasets
from 2019 to 2023 demonstrates MTS's superiority over traditional algorithms
and advanced machine learning techniques. MTS consistently achieves higher
cumulative returns, Sharpe, Omega, and Sortino ratios, underscoring its
effectiveness in balancing risk and return while adapting to market dynamics.
MTS demonstrates an average relative increase of 30.67% in cumulative returns
and 29.33% in Sharpe ratio compared to the next best-performing strategies
across various datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Artificial Intelligence in Pronunciation Teaching: Use and Beliefs of
  Foreign Language Teachers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04128v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04128v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georgios P. Georgiou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pronunciation instruction in foreign language classrooms has often been an
overlooked area of focus. With the widespread adoption of Artificial
Intelligence (AI) and its potential benefits, investigating how AI is utilized
in pronunciation teaching and understanding the beliefs of teachers about this
tool is essential for improving learning outcomes. This study aims to examine
how AI use for pronunciation instruction varies across different demographic
and professional factors among teachers, and how these factors, including AI
use, influence the beliefs of teachers about AI. The study involved 117 English
as a Foreign Language (EFL) in-service teachers working in Cyprus, who
completed an online survey designed to assess their beliefs about the
effectiveness of AI, its drawbacks, and their willingness to integrate AI into
their teaching practices. The results revealed that teachers were significantly
more likely to agree on the perceived effectiveness of AI and their willingness
to adopt it, compared to their concerns about its use. Furthermore, teachers
working in higher education and adult education, as well as those who had
received more extensive training, reported using AI more frequently in their
teaching. Teachers who utilized AI more often expressed stronger agreement with
its effectiveness, while those who had received more training were less likely
to express concerns about its integration. Given the limited training that many
teachers currently receive, these findings demonstrate the need for tailored
training sessions that address the specific needs and concerns of educators,
ultimately fostering the adoption of AI in pronunciation instruction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simple Self Organizing Map with Visual Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04121v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04121v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alan Luo, Kaiwen Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformers (ViTs) have demonstrated exceptional performance in
various vision tasks. However, they tend to underperform on smaller datasets
due to their inherent lack of inductive biases. Current approaches address this
limitation implicitly-often by pairing ViTs with pretext tasks or by distilling
knowledge from convolutional neural networks (CNNs) to strengthen the prior. In
contrast, Self-Organizing Maps (SOMs), a widely adopted self-supervised
framework, are inherently structured to preserve topology and spatial
organization, making them a promising candidate to directly address the
limitations of ViTs in limited or small training datasets. Despite this
potential, equipping SOMs with modern deep learning architectures remains
largely unexplored. In this study, we conduct a novel exploration on how Vision
Transformers (ViTs) and Self-Organizing Maps (SOMs) can empower each other,
aiming to bridge this critical research gap. Our findings demonstrate that
these architectures can synergistically enhance each other, leading to
significantly improved performance in both unsupervised and supervised tasks.
Code will be publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures. Submitted to IEEE. All experiments and code work
  were performed by the first author, with the second author serving in a
  PI/mentor role, guiding the progression of the work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalizability of Neural Networks Minimizing Empirical Risk Based on
  Expressive Ability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.04111v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.04111v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lijia Yu, Yibo Miao, Yifan Zhu, Xiao-Shan Gao, Lijun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The primary objective of learning methods is generalization. Classic uniform
generalization bounds, which rely on VC-dimension or Rademacher complexity,
fail to explain the significant attribute that over-parameterized models in
deep learning exhibit nice generalizability. On the other hand,
algorithm-dependent generalization bounds, like stability bounds, often rely on
strict assumptions. To establish generalizability under less stringent
assumptions, this paper investigates the generalizability of neural networks
that minimize or approximately minimize empirical risk. We establish a lower
bound for population accuracy based on the expressiveness of these networks,
which indicates that with an adequate large number of training samples and
network sizes, these networks, including over-parameterized ones, can
generalize effectively. Additionally, we provide a necessary condition for
generalization, demonstrating that, for certain data distributions, the
quantity of training data required to ensure generalization exceeds the network
size needed to represent the corresponding data distribution. Finally, we
provide theoretical insights into several phenomena in deep learning, including
robust generalization, importance of over-parameterization, and effect of loss
function on generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Far Are We on the Decision-Making of <span class="highlight-title">LLM</span>s? Evaluating <span class="highlight-title">LLM</span>s' Gaming
  Ability in <span class="highlight-title">Multi</span>-Agent Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.11807v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.11807v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jen-tse Huang, Eric John Li, Man Ho Lam, Tian Liang, Wenxuan Wang, Youliang Yuan, Wenxiang Jiao, Xing Wang, Zhaopeng Tu, Michael R. Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decision-making is a complex process requiring diverse abilities, making it
an excellent framework for evaluating Large Language Models (LLMs). Researchers
have examined LLMs' decision-making through the lens of Game Theory. However,
existing evaluation mainly focus on two-player scenarios where an LLM competes
against another. Additionally, previous benchmarks suffer from test set leakage
due to their static design. We introduce GAMA($\gamma$)-Bench, a new framework
for evaluating LLMs' Gaming Ability in Multi-Agent environments. It includes
eight classical game theory scenarios and a dynamic scoring scheme specially
designed to quantitatively assess LLMs' performance. $\gamma$-Bench allows
flexible game settings and adapts the scoring system to different game
parameters, enabling comprehensive evaluation of robustness, generalizability,
and strategies for improvement. Our results indicate that GPT-3.5 demonstrates
strong robustness but limited generalizability, which can be enhanced using
methods like Chain-of-Thought. We also evaluate 13 LLMs from 6 model families,
including GPT-3.5, GPT-4, Gemini, LLaMA-3.1, Mixtral, and Qwen-2.
Gemini-1.5-Pro outperforms others, scoring of $69.8$ out of $100$, followed by
LLaMA-3.1-70B ($65.9$) and Mixtral-8x22B ($62.4$). Our code and experimental
results are publicly available at https://github.com/CUHK-ARISE/GAMABench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025; 11 pages of main text; 26 pages of appendices;
  Included models: GPT-3.5-{0613, 1106, 0125}, GPT-4-0125, GPT-4o-0806,
  Gemini-{1.0, 1.5)-Pro, LLaMA-3.1-{7, 70, 405}B, Mixtral-8x{7, 22}B,
  Qwen-2-72B</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DEFT: Differentiable Branched Discrete Elastic Rods for Modeling
  Furcated DLOs in Real-Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15037v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15037v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhou Chen, Xiaoyue Wu, Yeheng Zong, Anran Li, Yuzhen Chen, Julie Wu, Bohao Zhang, Ram Vasudevan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous wire harness assembly requires robots to manipulate complex
branched cables with high precision and reliability. A key challenge in
automating this process is predicting how these flexible and branched
structures behave under manipulation. Without accurate predictions, it is
difficult for robots to reliably plan or execute assembly operations. While
existing research has made progress in modeling single-threaded Deformable
Linear Objects (DLOs), extending these approaches to Branched Deformable Linear
Objects (BDLOs) presents fundamental challenges. The junction points in BDLOs
create complex force interactions and strain propagation patterns that cannot
be adequately captured by simply connecting multiple single-DLO models. To
address these challenges, this paper presents Differentiable discrete branched
Elastic rods for modeling Furcated DLOs in real-Time (DEFT), a novel framework
that combines a differentiable physics-based model with a learning framework
to: 1) accurately model BDLO dynamics, including dynamic propagation at
junction points and grasping in the middle of a BDLO, 2) achieve efficient
computation for real-time inference, and 3) enable planning to demonstrate
dexterous BDLO manipulation. A comprehensive series of real-world experiments
demonstrates DEFT's efficacy in terms of accuracy, computational speed, and
generalizability compared to state-of-the-art alternatives. Project
page:https://roahmlab.github.io/DEFT/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Not Trust Licenses You See -- <span class="highlight-title">Dataset</span> Compliance Requires
  Massive-Scale AI-Powered Lifecycle Tracing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02784v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02784v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaekyeom Kim, Sungryull Sohn, Gerrard Jeongwon Jo, Jihoon Choi, Kyunghoon Bae, Hwayoung Lee, Yongmin Park, Honglak Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper argues that a dataset's legal risk cannot be accurately assessed
by its license terms alone; instead, tracking dataset redistribution and its
full lifecycle is essential. However, this process is too complex for legal
experts to handle manually at scale. Tracking dataset provenance, verifying
redistribution rights, and assessing evolving legal risks across multiple
stages require a level of precision and efficiency that exceeds human
capabilities. Addressing this challenge effectively demands AI agents that can
systematically trace dataset redistribution, analyze compliance, and identify
legal risks. We develop an automated data compliance system called NEXUS and
show that AI can perform these tasks with higher accuracy, efficiency, and
cost-effectiveness than human experts. Our massive legal analysis of 17,429
unique entities and 8,072 license terms using this approach reveals the
discrepancies in legal rights between the original datasets before
redistribution and their redistributed subsets, underscoring the necessity of
the data lifecycle-aware compliance. For instance, we find that out of 2,852
datasets with commercially viable individual license terms, only 605 (21%) are
legally permissible for commercialization. This work sets a new standard for AI
data governance, advocating for a framework that systematically examines the
entire lifecycle of dataset redistribution to ensure transparent, legal, and
responsible dataset management.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HELMET: How to Evaluate Long-Context Language Models Effectively and
  Thoroughly 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02694v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02694v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, Danqi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many benchmarks exist for evaluating long-context language models (LCLMs),
yet developers often rely on synthetic tasks such as needle-in-a-haystack
(NIAH) or an arbitrary subset of tasks. However, it remains unclear whether
these benchmarks reflect the diverse downstream applications of LCLMs, and such
inconsistencies further complicate model comparison. We investigate the
underlying reasons behind these practices and find that existing benchmarks
often provide noisy signals due to limited coverage of applications,
insufficient context lengths, unreliable metrics, and incompatibility with base
models. In this work, we introduce HELMET (How to Evaluate Long-context Models
Effectively and Thoroughly), a comprehensive benchmark encompassing seven
diverse, application-centric categories. We also address several issues in
previous benchmarks by adding controllable lengths up to 128K tokens,
model-based evaluation for reliable metrics, and few-shot prompting for
robustly evaluating base models. Consequently, we demonstrate that HELMET
offers more reliable and consistent rankings of frontier LCLMs. Through a
comprehensive study of 59 LCLMs, we find that (1) synthetic tasks like NIAH do
not reliably predict downstream performance; (2) the diverse categories in
HELMET exhibit distinct trends and low correlations with each other; and (3)
while most LCLMs achieve perfect NIAH scores, open-source models significantly
lag behind closed ones when tasks require full-context reasoning or following
complex instructions -- the gap widens as length increases. Finally, we
recommend using our RAG tasks for fast model development, as they are easy to
run and better predict other downstream performance; ultimately, we advocate
for a holistic evaluation across diverse tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025. Project page: https://princeton-nlp.github.io/HELMET/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AdaptBot: Combining <span class="highlight-title">LLM</span> with Knowledge Graphs and Human Input for
  Generic-to-Specific Task Decomposition and Knowledge Refinement <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02067v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02067v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shivam Singh, Karthik Swaminathan, Nabanita Dash, Ramandeep Singh, Snehasis Banerjee, Mohan Sridharan, Madhava Krishna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An embodied agent assisting humans is often asked to complete new tasks, and
there may not be sufficient time or labeled examples to train the agent to
perform these new tasks. Large Language Models (LLMs) trained on considerable
knowledge across many domains can be used to predict a sequence of abstract
actions for completing such tasks, although the agent may not be able to
execute this sequence due to task-, agent-, or domain-specific constraints. Our
framework addresses these challenges by leveraging the generic predictions
provided by LLM and the prior domain knowledge encoded in a Knowledge Graph
(KG), enabling an agent to quickly adapt to new tasks. The robot also solicits
and uses human input as needed to refine its existing knowledge. Based on
experimental evaluation in the context of cooking and cleaning tasks in
simulation domains, we demonstrate that the interplay between LLM, KG, and
human input leads to substantial performance gains compared with just using the
LLM. Project website{\S}: https://sssshivvvv.github.io/adaptbot/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE International Conference on Robotics and Automation
  (ICRA) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting Systematic Weaknesses in Vision Models along Predefined
  Human-Understandable Dimensions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12360v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12360v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sujan Sai Gannamaneni, Rohil Prakash Rao, Michael Mock, Maram Akila, Stefan Wrobel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Slice discovery methods (SDMs) are prominent algorithms for finding
systematic weaknesses in DNNs. They identify top-k semantically coherent
slices/subsets of data where a DNN-under-test has low performance. For being
directly useful, slices should be aligned with human-understandable and
relevant dimensions, which, for example, are defined by safety and domain
experts as part of the operational design domain (ODD). While SDMs can be
applied effectively on structured data, their application on image data is
complicated by the lack of semantic metadata. To address these issues, we
present an algorithm that combines foundation models for zero-shot image
classification to generate semantic metadata with methods for combinatorial
search to find systematic weaknesses in images. In contrast to existing
approaches, ours identifies weak slices that are in line with pre-defined
human-understandable dimensions. As the algorithm includes foundation models,
its intermediate and final results may not always be exact. Therefore, we
include an approach to address the impact of noisy metadata. We validate our
algorithm on both synthetic and real-world datasets, demonstrating its ability
to recover human-understandable systematic weaknesses. Furthermore, using our
approach, we identify systematic weaknesses of multiple pre-trained and
publicly available state-of-the-art computer vision DNNs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Back Home: A Machine Learning Approach to Seashell Classification and
  Ecosystem Restoration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04873v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04873v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Valverde, Luis Solano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Costa Rica, an average of 5 tons of seashells are extracted from
ecosystems annually. Confiscated seashells, cannot be returned to their
ecosystems due to the lack of origin recognition. To address this issue, we
developed a convolutional neural network (CNN) specifically for seashell
identification. We built a dataset from scratch, consisting of approximately
19000 images from the Pacific and Caribbean coasts. Using this dataset, the
model achieved a classification accuracy exceeding 85%. The model has been
integrated into a user-friendly application, which has classified over 36,000
seashells to date, delivering real-time results within 3 seconds per image. To
further enhance the system's accuracy, an anomaly detection mechanism was
incorporated to filter out irrelevant or anomalous inputs, ensuring only valid
seashell images are processed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tutorial on amortized optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.00665v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.00665v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brandon Amos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimization is a ubiquitous modeling tool and is often deployed in settings
which repeatedly solve similar instances of the same problem. Amortized
optimization methods use learning to predict the solutions to problems in these
settings, exploiting the shared structure between similar problem instances.
These methods have been crucial in variational inference and reinforcement
learning and are capable of solving optimization problems many orders of
magnitudes times faster than traditional optimization methods that do not use
amortization. This tutorial presents an introduction to the amortized
optimization foundations behind these advancements and overviews their
applications in variational inference, sparse coding, gradient-based
meta-learning, control, reinforcement learning, convex optimization, optimal
transport, and deep equilibrium networks. The source code for this tutorial is
available at
https://github.com/facebookresearch/amortized-optimization-tutorial.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Foundations and Trends in Machine Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Simple and Effective Reinforcement Learning Method for Text-to-Image
  Diffusion Fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.00897v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.00897v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashank Gupta, Chaitanya Ahuja, Tsung-Yu Lin, Sreya Dutta Roy, Harrie Oosterhuis, Maarten de Rijke, Satya Narayan Shukla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL)-based fine-tuning has emerged as a powerful
approach for aligning diffusion models with black-box objectives. Proximal
policy optimization (PPO) is the most popular choice of method for policy
optimization. While effective in terms of performance, PPO is highly sensitive
to hyper-parameters and involves substantial computational overhead. REINFORCE,
on the other hand, mitigates some computational complexities such as high
memory overhead and sensitive hyper-parameter tuning, but has suboptimal
performance due to high-variance and sample inefficiency. While the variance of
the REINFORCE can be reduced by sampling multiple actions per input prompt and
using a baseline correction term, it still suffers from sample inefficiency. To
address these challenges, we systematically analyze the
efficiency-effectiveness trade-off between REINFORCE and PPO, and propose
leave-one-out PPO (LOOP), a novel RL for diffusion fine-tuning method. LOOP
combines variance reduction techniques from REINFORCE, such as sampling
multiple actions per input prompt and a baseline correction term, with the
robustness and sample efficiency of PPO via clipping and importance sampling.
Our results demonstrate that LOOP effectively improves diffusion models on
various black-box objectives, and achieves a better balance between
computational efficiency and performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Human-Feedback Efficient Reinforcement Learning for Online Diffusion
  Model Finetuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05116v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05116v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayano Hiranaka, Shang-Fu Chen, Chieh-Hsin Lai, Dongjun Kim, Naoki Murata, Takashi Shibuya, Wei-Hsiang Liao, Shao-Hua Sun, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Controllable generation through Stable Diffusion (SD) fine-tuning aims to
improve fidelity, safety, and alignment with human guidance. Existing
reinforcement learning from human feedback methods usually rely on predefined
heuristic reward functions or pretrained reward models built on large-scale
datasets, limiting their applicability to scenarios where collecting such data
is costly or difficult. To effectively and efficiently utilize human feedback,
we develop a framework, HERO, which leverages online human feedback collected
on the fly during model learning. Specifically, HERO features two key
mechanisms: (1) Feedback-Aligned Representation Learning, an online training
method that captures human feedback and provides informative learning signals
for fine-tuning, and (2) Feedback-Guided Image Generation, which involves
generating images from SD's refined initialization samples, enabling faster
convergence towards the evaluator's intent. We demonstrate that HERO is 4x more
efficient in online feedback for body part anomaly correction compared to the
best existing method. Additionally, experiments show that HERO can effectively
handle tasks like reasoning, counting, personalization, and reducing NSFW
content with only 0.5K online feedback.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in International Conference on Learning Representations
  (ICLR) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-supervised pre-training with diffusion model for few-shot landmark
  detection in x-ray images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18125v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18125v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roberto Di Via, Francesca Odone, Vito Paolo Pastore
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have been extensively applied in the medical domain for
various tasks, including image classification, segmentation, and landmark
detection. However, their application is often hindered by data scarcity, both
in terms of available annotations and images. This study introduces a novel
application of denoising diffusion probabilistic models (DDPMs) to the landmark
detection task, specifically addressing the challenge of limited annotated data
in x-ray imaging. Our key innovation lies in leveraging DDPMs for
self-supervised pre-training in landmark detection, a previously unexplored
approach in this domain. This method enables accurate landmark detection with
minimal annotated training data (as few as 50 images), surpassing both ImageNet
supervised pre-training and traditional self-supervised techniques across three
popular x-ray benchmark datasets. To our knowledge, this work represents the
first application of diffusion models for self-supervised learning in landmark
detection, which may offer a valuable pre-training approach in few-shot
regimes, for mitigating data scarcity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ACC-Collab: An Actor-Critic Approach to <span class="highlight-title">Multi</span>-Agent <span class="highlight-title">LLM</span> Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00053v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00053v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Estornell, Jean-Francois Ton, Yuanshun Yao, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated a remarkable ability to serve
as general-purpose tools for various language-based tasks. Recent works have
demonstrated that the efficacy of such models can be improved through iterative
dialog between multiple models. While these paradigms show promise in improving
model efficacy, most works in this area treat collaboration as an emergent
behavior, rather than a learned behavior. In doing so, current multi-agent
frameworks rely on collaborative behaviors to have been sufficiently trained
into off-the-shelf models. To address this limitation, we propose ACC-Collab,
an Actor-Critic based learning framework to produce a two-agent team (an
actor-agent and a critic-agent) specialized in collaboration. We demonstrate
that ACC-Collab outperforms SotA multi-agent techniques on a wide array of
benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards One Model for Classical Dimensionality Reduction: A
  Probabilistic Perspective on UMAP and t-SNE 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17412v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17412v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Ravuri, Neil D. Lawrence
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper shows that dimensionality reduction methods such as UMAP and
t-SNE, can be approximately recast as MAP inference methods corresponding to a
model introduced in ProbDR, that describes the graph Laplacian (an estimate of
the data precision matrix) using a Wishart distribution, with a mean given by a
non-linear covariance function evaluated on the latents. This interpretation
offers deeper theoretical and semantic insights into such algorithms, by
showing that variances corresponding to these covariances are low (potentially
misspecified), and forging a connection to Gaussian process latent variable
models by showing that well-known kernels can be used to describe covariances
implied by graph Laplacians. We also introduce tools with which similar
dimensionality reduction methods can be studied.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic
  Templatisation and Orthographic Obfuscation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02972v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02972v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jude Khouja, Karolina Korgul, Simi Hellsten, Lingyi Yang, Vlad Neacs, Harry Mayne, Ryan Kearns, Andrew Bean, Adam Mahdi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Assessing the reasoning capabilities of large language models (LLMs) is
susceptible to overestimation due to data exposure of evaluation benchmarks. We
introduce a framework for producing linguistic reasoning problems that reduces
the effect of memorisation in model performance estimates and apply this
framework to develop LINGOLY-TOO, a challenging benchmark for linguistic
reasoning. By developing orthographic templates, we dynamically obfuscate the
writing systems of real languages to generate numerousquestion variations.
These variations preserve the reasoning steps required for each solution while
reducing the likelihood of specific problem instances appearing in model
training data. Our experiments demonstrate that frontier models, including
Claud 3.7 Sonnet, o1-preview and DeepSeek R1, struggle with advanced reasoning.
Our analysis also shows that LLMs exhibit noticeable variance in accuracy
across permutations of the same problem, and on average perform better on
questions appearing in their original orthography. Our findings highlight the
opaque nature of response generation in LLMs and provide evidence that prior
data exposure contributes to over estimating the reasoning capabilities of
frontier models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Protein Large Language Models: A Comprehensive <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17504v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17504v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijia Xiao, Wanjia Zhao, Junkai Zhang, Yiqiao Jin, Han Zhang, Zhicheng Ren, Renliang Sun, Haixin Wang, Guancheng Wan, Pan Lu, Xiao Luo, Yu Zhang, James Zou, Yizhou Sun, Wei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Protein-specific large language models (Protein LLMs) are revolutionizing
protein science by enabling more efficient protein structure prediction,
function annotation, and design. While existing surveys focus on specific
aspects or applications, this work provides the first comprehensive overview of
Protein LLMs, covering their architectures, training datasets, evaluation
metrics, and diverse applications. Through a systematic analysis of over 100
articles, we propose a structured taxonomy of state-of-the-art Protein LLMs,
analyze how they leverage large-scale protein sequence data for improved
accuracy, and explore their potential in advancing protein engineering and
biomedical research. Additionally, we discuss key challenges and future
directions, positioning Protein LLMs as essential tools for scientific
discovery in protein science. Resources are maintained at
https://github.com/Yijia-Xiao/Protein-LLM-Survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 4 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $\texttt{SEM-CT<span class="highlight-title">RL</span>}$: Semantically Controlled Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01804v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01804v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Albinhassan, Pranava Madhyastha, Alessandra Russo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring both syntactic and semantic correctness in Large Language Model
(LLM) outputs remains a significant challenge, despite being critical for
real-world deployment. In this paper, we introduce $\texttt{SEM-CTRL}$, a
unified approach that enforces rich context-sensitive constraints and task- and
instance-specific semantics directly on an LLM decoder. Our approach integrates
token-level MCTS, which is guided by specific syntactic and semantic
constraints. The constraints over the desired outputs are expressed using
Answer Set Grammars -- a logic-based formalism that generalizes
context-sensitive grammars while incorporating background knowledge to
represent task-specific semantics. We show that our approach guarantees correct
completions for any off-the-shelf LLM without the need for fine-tuning. We
evaluate $\texttt{SEM-CTRL}$ on a range of tasks, including synthetic grammar
synthesis, combinatorial reasoning, and planning. Our results demonstrate that
$\texttt{SEM-CTRL}$ allows small pre-trained LLMs to efficiently outperform
larger variants and state-of-the-art reasoning models (e.g., o1-preview) while
simultaneously guaranteeing solution correctness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Single Concept Vector: Modeling Concept Subspace in <span class="highlight-title">LLM</span>s with
  Gaussian Distribution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00153v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00153v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiyan Zhao, Heng Zhao, Bo Shen, Ali Payani, Fan Yang, Mengnan Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Probing learned concepts in large language models (LLMs) is crucial for
understanding how semantic knowledge is encoded internally. Training linear
classifiers on probing tasks is a principle approach to denote the vector of a
certain concept in the representation space. However, the single vector
identified for a concept varies with both data and training, making it less
robust and weakening its effectiveness in real-world applications. To address
this challenge, we propose an approach to approximate the subspace representing
a specific concept. Built on linear probing classifiers, we extend the concept
vectors into Gaussian Concept Subspace (GCS). We demonstrate GCS's
effectiveness through measuring its faithfulness and plausibility across
multiple LLMs with different sizes and architectures. Additionally, we use
representation intervention tasks to showcase its efficacy in real-world
applications such as emotion steering. Experimental results indicate that GCS
concept vectors have the potential to balance steering performance and
maintaining the fluency in natural language generation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Neural Networks for Virtual Sensing in Complex Systems: Addressing
  Heterogeneous Temporal Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.18691v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.18691v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengjie Zhao, Cees Taal, Stephan Baggerohr, Olga Fink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-time condition monitoring is crucial for the reliable and efficient
operation of complex systems. However, relying solely on physical sensors can
be limited due to their cost, placement constraints, or inability to directly
measure certain critical parameters. Virtual sensing addresses these
limitations by leveraging readily available sensor data and system knowledge to
estimate inaccessible parameters or infer system states. The increasing
complexity of industrial systems necessitates deployments of sensors with
diverse modalities to provide a comprehensive understanding of system states.
These sensors capture data at varying frequencies to monitor both rapid and
slowly varying system dynamics, as well as local and global state evolutions of
the systems. This leads to heterogeneous temporal dynamics, which, particularly
under varying operational end environmental conditions, pose a significant
challenge for accurate virtual sensing. To address this, we propose a
Heterogeneous Temporal Graph Neural Network (HTGNN) framework. HTGNN explicitly
models signals from diverse sensors and integrates operating conditions into
the model architecture. We evaluate HTGNN using two newly released datasets: a
bearing dataset with diverse load conditions for bearing load prediction and a
year-long simulated dataset for predicting bridge live loads. Our results
demonstrate that HTGNN significantly outperforms established baseline methods
in both tasks, particularly under highly varying operating conditions. These
results highlight HTGNN's potential as a robust and accurate virtual sensing
approach for complex systems, paving the way for improved monitoring,
predictive maintenance, and enhanced system performance. Our code and data are
available under https://github.com/EPFL-IMOS/htgnn.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper extends our previous conference paper (Best Paper at
  European Conference of the PHM Society 2024,
  https://doi.org/10.36001/phme.2024.v8i1.3998). Accepted by Mechanical Systems
  and Signal Processing (MSSP)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ X-Boundary: Establishing Exact Safety Boundary to Shield <span class="highlight-title">LLM</span>s from
  <span class="highlight-title">Multi</span>-Turn Jailbreaks without Compromising Usability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09990v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09990v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoya Lu, Dongrui Liu, Yi Yu, Luxin Xu, Jing Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the rapid development of safety alignment techniques for LLMs,
defending against multi-turn jailbreaks is still a challenging task. In this
paper, we conduct a comprehensive comparison, revealing that some existing
defense methods can improve the robustness of LLMs against multi-turn
jailbreaks but compromise usability, i.e., reducing general capabilities or
causing the over-refusal problem. From the perspective of mechanism
interpretability of LLMs, we discover that these methods fail to establish a
boundary that exactly distinguishes safe and harmful feature representations.
Therefore, boundary-safe representations close to harmful representations are
inevitably disrupted, leading to a decline in usability. To address this issue,
we propose X-Boundary to push harmful representations away from boundary-safe
representations and obtain an exact distinction boundary. In this way, harmful
representations can be precisely erased without disrupting safe ones.
Experimental results show that X-Boundary achieves state-of-the-art defense
performance against multi-turn jailbreaks, while reducing the over-refusal rate
by about 20% and maintaining nearly complete general capability. Furthermore,
we theoretically prove and empirically verify that X-Boundary can accelerate
the convergence process during training. Please see our code at:
https://github.com/AI45Lab/X-Boundary.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UoR-NCL at SemEval-2025 Task 1: Using Generative <span class="highlight-title">LLM</span>s and CLIP Models
  for <span class="highlight-title">Multi</span>lingual <span class="highlight-title">Multi</span>modal Idiomaticity Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20984v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20984v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thanet Markchom, Tong Wu, Liting Huang, Huizhi Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SemEval-2025 Task 1 focuses on ranking images based on their alignment with a
given nominal compound that may carry idiomatic meaning in both English and
Brazilian Portuguese. To address this challenge, this work uses generative
large language models (LLMs) and multilingual CLIP models to enhance idiomatic
compound representations. LLMs generate idiomatic meanings for potentially
idiomatic compounds, enriching their semantic interpretation. These meanings
are then encoded using multilingual CLIP models, serving as representations for
image ranking. Contrastive learning and data augmentation techniques are
applied to fine-tune these embeddings for improved performance. Experimental
results show that multimodal representations extracted through this method
outperformed those based solely on the original nominal compounds. The
fine-tuning approach shows promising outcomes but is less effective than using
embeddings without fine-tuning. The source code used in this paper is available
at https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Challenges and Opportunities in Generative AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.00025v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.00025v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Manduchi, Kushagra Pandey, Clara Meister, Robert Bamler, Ryan Cotterell, Sina Däubener, Sophie Fellenz, Asja Fischer, Thomas Gärtner, Matthias Kirchler, Marius Kloft, Yingzhen Li, Christoph Lippert, Gerard de Melo, Eric Nalisnick, Björn Ommer, Rajesh Ranganath, Maja Rudolph, Karen Ullrich, Guy Van den Broeck, Julia E Vogt, Yixin Wang, Florian Wenzel, Frank Wood, Stephan Mandt, Vincent Fortuin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of deep generative modeling has grown rapidly in the last few
years. With the availability of massive amounts of training data coupled with
advances in scalable unsupervised learning paradigms, recent large-scale
generative models show tremendous promise in synthesizing high-resolution
images and text, as well as structured data such as videos and molecules.
However, we argue that current large-scale generative AI models exhibit several
fundamental shortcomings that hinder their widespread adoption across domains.
In this work, our objective is to identify these issues and highlight key
unresolved challenges in modern generative AI paradigms that should be
addressed to further enhance their capabilities, versatility, and reliability.
By identifying these challenges, we aim to provide researchers with insights
for exploring fruitful research directions, thus fostering the development of
more robust and accessible generative AI solutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gumbel Counterfactual Generation From Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07180v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07180v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shauli Ravfogel, Anej Svete, Vésteinn Snæbjarnarson, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and manipulating the causal generation mechanisms in language
models is essential for controlling their behavior. Previous work has primarily
relied on techniques such as representation surgery -- e.g., model ablations or
manipulation of linear subspaces tied to specific concepts -- to
\emph{intervene} on these models. To understand the impact of interventions
precisely, it is useful to examine \emph{counterfactuals} -- e.g., how a given
sentence would have appeared had it been generated by the model following a
specific intervention. We highlight that counterfactual reasoning is
conceptually distinct from interventions, as articulated in Pearl's causal
hierarchy. Based on this observation, we propose a framework for generating
true string counterfactuals by reformulating language models as a structural
equation model using the Gumbel-max trick, which we called Gumbel
counterfactual generation. This reformulation allows us to model the joint
distribution over original strings and their counterfactuals resulting from the
same instantiation of the sampling noise. We develop an algorithm based on
hindsight Gumbel sampling that allows us to infer the latent noise variables
and generate counterfactuals of observed strings. Our experiments demonstrate
that the approach produces meaningful counterfactuals while at the same time
showing that commonly used intervention techniques have considerable undesired
side effects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMGDreamer: Mixed-Modality Graph for Geometry-Controllable 3D Indoor
  Scene Generation <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05874v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05874v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhifei Yang, Keyang Lu, Chao Zhang, Jiaxing Qi, Hanqi Jiang, Ruifei Ma, Shenglin Yin, Yifan Xu, Mingzhe Xing, Zhen Xiao, Jieyi Long, Xiangde Liu, Guangyao Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Controllable 3D scene generation has extensive applications in virtual
reality and interior design, where the generated scenes should exhibit high
levels of realism and controllability in terms of geometry. Scene graphs
provide a suitable data representation that facilitates these applications.
However, current graph-based methods for scene generation are constrained to
text-based inputs and exhibit insufficient adaptability to flexible user
inputs, hindering the ability to precisely control object geometry. To address
this issue, we propose MMGDreamer, a dual-branch diffusion model for scene
generation that incorporates a novel Mixed-Modality Graph, visual enhancement
module, and relation predictor. The mixed-modality graph allows object nodes to
integrate textual and visual modalities, with optional relationships between
nodes. It enhances adaptability to flexible user inputs and enables meticulous
control over the geometry of objects in the generated scenes. The visual
enhancement module enriches the visual fidelity of text-only nodes by
constructing visual representations using text embeddings. Furthermore, our
relation predictor leverages node representations to infer absent relationships
between nodes, resulting in more coherent scene layouts. Extensive experimental
results demonstrate that MMGDreamer exhibits superior control of object
geometry, achieving state-of-the-art scene generation performance. Project
page: https://yangzhifeio.github.io/project/MMGDreamer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2025 Main Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pretrained Embeddings as a Behavior Specification Mechanism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02012v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02012v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parv Kapoor, Abigail Hammer, Ashish Kapoor, Karen Leung, Eunsuk Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an approach to formally specifying the behavioral properties of
systems that rely on a perception model for interactions with the physical
world. The key idea is to introduce embeddings -- mathematical representations
of a real-world concept -- as a first-class construct in a specification
language, where properties are expressed in terms of distances between a pair
of ideal and observed embeddings. To realize this approach, we propose a new
type of temporal logic called Embedding Temporal Logic (ETL), and describe how
it can be used to express a wider range of properties about AI-enabled systems
than previously possible. We demonstrate the applicability of ETL through a
preliminary evaluation involving planning tasks in robots that are driven by
foundation models; the results are promising, showing that embedding-based
specifications can be used to steer a system towards desirable behaviors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Decoupled Recommender Systems: Exploring Alternative Recommender
  Ecosystem Designs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03606v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03606v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anas Buhayh, Elizabeth McKinnie, Robin Burke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender ecosystems are an emerging subject of research. Such research
examines how the characteristics of algorithms, recommendation consumers, and
item providers influence system dynamics and long-term outcomes. One
architectural possibility that has not yet been widely explored in this line of
research is the consequences of a configuration in which recommendation
algorithms are decoupled from the platforms they serve. This is sometimes
called "the friendly neighborhood algorithm store" or "middleware" model. We
are particularly interested in how such architectures might offer a range of
different distributions of utility across consumers, providers, and
recommendation platforms. In this paper, we create a model of a recommendation
ecosystem that incorporates algorithm choice and examine the outcomes of such a
design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D
  Medical Image Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13524v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13524v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Dai, Jun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient evaluation of three-dimensional (3D) medical images is crucial for
diagnostic and therapeutic practices in healthcare. Recent years have seen a
substantial uptake in applying deep learning and computer vision to analyse and
interpret medical images. Traditional approaches, such as convolutional neural
networks (CNNs) and vision transformers (ViTs), face significant computational
challenges, prompting the need for architectural advancements. Recent efforts
have led to the introduction of novel architectures like the ``Mamba'' model as
alternative solutions to traditional CNNs or ViTs. The Mamba model excels in
the linear processing of one-dimensional data with low computational demands.
However, Mamba's potential for 3D medical image analysis remains underexplored
and could face significant computational challenges as the dimension increases.
This manuscript presents MobileViM, a streamlined architecture for efficient
segmentation of 3D medical images. In the MobileViM network, we invent a new
dimension-independent mechanism and a dual-direction traversing approach to
incorporate with a vision-Mamba-based framework. MobileViM also features a
cross-scale bridging technique to improve efficiency and accuracy across
various medical imaging modalities. With these enhancements, MobileViM achieves
segmentation speeds exceeding 90 frames per second (FPS) on a single graphics
processing unit (i.e., NVIDIA RTX 4090). This performance is over 24 FPS faster
than the state-of-the-art deep learning models for processing 3D images with
the same computational resources. In addition, experimental evaluations
demonstrate that MobileViM delivers superior performance, with Dice similarity
scores reaching 92.72%, 86.69%, 80.46%, and 77.43% for PENGWIN, BraTS2024,
ATLAS, and Toothfairy2 datasets, respectively, which significantly surpasses
existing models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The corresponding author disagrees with the manuscript submitted to
  arXiv</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Secure Federated Data Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13728v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13728v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Arazzi, Mert Cihangiroglu, Serena Nicolazzo, Antonino Nocera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dataset Distillation (DD) is a powerful technique for reducing large datasets
into compact, representative synthetic datasets, accelerating Machine Learning
training. However, traditional DD methods operate in a centralized manner,
which poses significant privacy threats and reduces its applicability. To
mitigate these risks, we propose a Secure Federated Data Distillation (SFDD)
framework to decentralize the distillation process while preserving privacy.
Unlike existing Federated Distillation techniques that focus on training global
models with distilled knowledge, our approach aims to produce a distilled
dataset without exposing local contributions. We leverage the
gradient-matching-based distillation method, adapting it for a distributed
setting where clients contribute to the distillation process without sharing
raw data. The central aggregator iteratively refines a synthetic dataset by
integrating client-side updates while ensuring data confidentiality. To make
our approach resilient to inference attacks perpetrated by the server that
could exploit gradient updates to reconstruct private data, we create an
optimized Local Differential Privacy approach, called LDPO-RLD. Furthermore, we
assess the framework's resilience against malicious clients executing backdoor
attacks (such as Doorping) and demonstrate robustness under the assumption of a
sufficient number of participating clients. Our experimental results
demonstrate the effectiveness of SFDD and that the proposed defense concretely
mitigates the identified vulnerabilities, with minimal impact on the
performance of the distilled dataset. By addressing the interplay between
privacy and federation in dataset distillation, this work advances the field of
privacy-preserving Machine Learning making our SFDD framework a viable solution
for sensitive data-sharing applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Which Frequencies do CNNs Need? Emergent Bottleneck Structure in Feature
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.08010v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.08010v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxiao Wen, Arthur Jacot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe the emergence of a Convolution Bottleneck (CBN) structure in
CNNs, where the network uses its first few layers to transform the input
representation into a representation that is supported only along a few
frequencies and channels, before using the last few layers to map back to the
outputs. We define the CBN rank, which describes the number and type of
frequencies that are kept inside the bottleneck, and partially prove that the
parameter norm required to represent a function $f$ scales as depth times the
CBN rank $f$. We also show that the parameter norm depends at next order on the
regularity of $f$. We show that any network with almost optimal parameter norm
will exhibit a CBN structure in both the weights and - under the assumption
that the network is stable under large learning rate - the activations, which
motivates the common practice of down-sampling; and we verify that the CBN
results still hold with down-sampling. Finally we use the CBN structure to
interpret the functions learned by CNNs on a number of tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assisting Mathematical Formalization with A Learning-based Premise
  Retriever 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13959v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13959v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yicheng Tao, Haotian Liu, Shanwen Wang, Hongteng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Premise selection is a crucial yet challenging step in mathematical
formalization, especially for users with limited experience. Due to the lack of
available formalization projects, existing approaches that leverage language
models often suffer from data scarcity. In this work, we introduce an
innovative method for training a premise retriever to support the formalization
of mathematics. Our approach employs a BERT model to embed proof states and
premises into a shared latent space. The retrieval model is trained within a
contrastive learning framework and incorporates a domain-specific tokenizer
along with a fine-grained similarity computation method. Experimental results
show that our model is highly competitive compared to existing baselines,
achieving strong performance while requiring fewer computational resources.
Performance is further enhanced through the integration of a re-ranking module.
To streamline the formalization process, we will release a search engine that
enables users to query Mathlib theorems directly using proof states,
significantly improving accessibility and efficiency. Codes are available at
https://github.com/ruc-ai4math/Premise-Retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hamiltonian Mechanics of Feature Learning: Bottleneck Structure in Leaky
  ResNets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17573v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17573v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arthur Jacot, Alexandre Kaiser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study Leaky ResNets, which interpolate between ResNets and Fully-Connected
nets depending on an 'effective depth' hyper-parameter $\tilde{L}$. In the
infinite depth limit, we study 'representation geodesics' $A_{p}$: continuous
paths in representation space (similar to NeuralODEs) from input $p=0$ to
output $p=1$ that minimize the parameter norm of the network. We give a
Lagrangian and Hamiltonian reformulation, which highlight the importance of two
terms: a kinetic energy which favors small layer derivatives
$\partial_{p}A_{p}$ and a potential energy that favors low-dimensional
representations, as measured by the 'Cost of Identity'. The balance between
these two forces offers an intuitive understanding of feature learning in
ResNets. We leverage this intuition to explain the emergence of a bottleneck
structure, as observed in previous work: for large $\tilde{L}$ the potential
energy dominates and leads to a separation of timescales, where the
representation jumps rapidly from the high dimensional inputs to a
low-dimensional representation, move slowly inside the space of low-dimensional
representations, before jumping back to the potentially high-dimensional
outputs. Inspired by this phenomenon, we train with an adaptive layer step-size
to adapt to the separation of timescales.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How DNNs break the Curse of Dimensionality: Compositionality and
  Symmetry Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.05664v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.05664v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arthur Jacot, Seok Hoan Choi, Yuxiao Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show that deep neural networks (DNNs) can efficiently learn any
composition of functions with bounded $F_{1}$-norm, which allows DNNs to break
the curse of dimensionality in ways that shallow networks cannot. More
specifically, we derive a generalization bound that combines a covering number
argument for compositionality, and the $F_{1}$-norm (or the related Barron
norm) for large width adaptivity. We show that the global minimizer of the
regularized loss of DNNs can fit for example the composition of two functions
$f^{*}=h\circ g$ from a small number of observations, assuming $g$ is
smooth/regular and reduces the dimensionality (e.g. $g$ could be the quotient
map of the symmetries of $f^{*}$), so that $h$ can be learned in spite of its
low regularity. The measures of regularity we consider is the Sobolev norm with
different levels of differentiability, which is well adapted to the $F_{1}$
norm. We compute scaling laws empirically and observe phase transitions
depending on whether $g$ or $h$ is harder to learn, as predicted by our theory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CATCH: Channel-Aware <span class="highlight-title">multi</span>variate Time Series Anomaly Detection via
  Frequency Patching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12261v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12261v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingjian Wu, Xiangfei Qiu, Zhengyu Li, Yihang Wang, Jilin Hu, Chenjuan Guo, Hui Xiong, Bin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Anomaly detection in multivariate time series is challenging as heterogeneous
subsequence anomalies may occur. Reconstruction-based methods, which focus on
learning normal patterns in the frequency domain to detect diverse abnormal
subsequences, achieve promising results, while still falling short on capturing
fine-grained frequency characteristics and channel correlations. To contend
with the limitations, we introduce CATCH, a framework based on frequency
patching. We propose to patchify the frequency domain into frequency bands,
which enhances its ability to capture fine-grained frequency characteristics.
To perceive appropriate channel correlations, we propose a Channel Fusion
Module (CFM), which features a patch-wise mask generator and a masked-attention
mechanism. Driven by a bi-level multi-objective optimization algorithm, the CFM
is encouraged to iteratively discover appropriate patch-wise channel
correlations, and to cluster relevant channels while isolating adverse effects
from irrelevant channels. Extensive experiments on 10 real-world datasets and
12 synthetic datasets demonstrate that CATCH achieves state-of-the-art
performance. We make our code and datasets available at
https://github.com/decisionintelligence/CATCH.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AfroBench: How Good are Large Language Models on African Languages? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.07978v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.07978v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jessica Ojo, Odunayo Ogundepo, Akintunde Oladipo, Kelechi Ogueji, Jimmy Lin, Pontus Stenetorp, David Ifeoluwa Adelani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale multilingual evaluations, such as MEGA, often include only a
handful of African languages due to the scarcity of high-quality evaluation
data and the limited discoverability of existing African datasets. This lack of
representation hinders comprehensive LLM evaluation across a diverse range of
languages and tasks. To address these challenges, we introduce AfroBench -- a
multi-task benchmark for evaluating the performance of LLMs across 64 African
languages, 15 tasks and 22 datasets. AfroBench consists of nine natural
language understanding datasets, six text generation datasets, six knowledge
and question answering tasks, and one mathematical reasoning task. We present
results comparing the performance of prompting LLMs to fine-tuned baselines
based on BERT and T5-style models. Our results suggest large gaps in
performance between high-resource languages, such as English, and African
languages across most tasks; but performance also varies based on the
availability of monolingual data resources. Our findings confirm that
performance on African languages continues to remain a hurdle for current LLMs,
underscoring the need for additional efforts to close this gap.
  https://mcgill-nlp.github.io/AfroBench/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OlympicArena: Benchmarking <span class="highlight-title">Multi</span>-discipline Cognitive Reasoning for
  Superintelligent AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12753v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12753v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Huang, Zengzhi Wang, Shijie Xia, Xuefeng Li, Haoyang Zou, Ruijie Xu, Run-Ze Fan, Lyumanshan Ye, Ethan Chern, Yixin Ye, Yikai Zhang, Yuqing Yang, Ting Wu, Binjie Wang, Shichao Sun, Yang Xiao, Yiyuan Li, Fan Zhou, Steffi Chern, Yiwei Qin, Yan Ma, Jiadi Su, Yixiu Liu, Yuxiang Zheng, Shaoting Zhang, Dahua Lin, Yu Qiao, Pengfei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The evolution of Artificial Intelligence (AI) has been significantly
accelerated by advancements in Large Language Models (LLMs) and Large
Multimodal Models (LMMs), gradually showcasing potential cognitive reasoning
abilities in problem-solving and scientific discovery (i.e., AI4Science) once
exclusive to human intellect. To comprehensively evaluate current models'
performance in cognitive reasoning abilities, we introduce OlympicArena, which
includes 11,163 bilingual problems across both text-only and interleaved
text-image modalities. These challenges encompass a wide range of disciplines
spanning seven fields and 62 international Olympic competitions, rigorously
examined for data leakage. We argue that the challenges in Olympic competition
problems are ideal for evaluating AI's cognitive reasoning due to their
complexity and interdisciplinary nature, which are essential for tackling
complex scientific challenges and facilitating discoveries. Beyond evaluating
performance across various disciplines using answer-only criteria, we conduct
detailed experiments and analyses from multiple perspectives. We delve into the
models' cognitive reasoning abilities, their performance across different
modalities, and their outcomes in process-level evaluations, which are vital
for tasks requiring complex reasoning with lengthy solutions. Our extensive
evaluations reveal that even advanced models like GPT-4o only achieve a 39.97%
overall accuracy, illustrating current AI limitations in complex reasoning and
multimodal integration. Through the OlympicArena, we aim to advance AI towards
superintelligence, equipping it to address more complex challenges in science
and beyond. We also provide a comprehensive set of resources to support AI
research, including a benchmark dataset, an open-source annotation platform, a
detailed evaluation tool, and a leaderboard with automatic submission features.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 360$^\circ$REA: Towards A Reusable Experience Accumulation with
  360° Assessment for <span class="highlight-title">Multi</span>-Agent System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.05569v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.05569v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shen Gao, Hao Li, Chengrui Huang, Quan Tu, Zhiliang Tian, Minlie Huang, Shuo Shang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model agents have demonstrated remarkable advancements across
various complex tasks. Recent works focus on optimizing the agent team or
employing self-reflection to iteratively solve complex tasks. Since these
agents are all based on the same LLM, only conducting self-evaluation or
removing underperforming agents does not substantively enhance the capability
of the agents. We argue that a comprehensive evaluation and accumulating
experience from evaluation feedback is an effective approach to improving
system performance. In this paper, we propose Reusable Experience Accumulation
with 360$^\circ$ Assessment (360$^\circ$REA), a hierarchical multi-agent
framework inspired by corporate organizational practices. The framework employs
a novel 360$^\circ$ performance assessment method for multi-perspective
performance evaluation with fine-grained assessment. To enhance the capability
of agents in addressing complex tasks, we introduce dual-level experience pool
for agents to accumulate experience through fine-grained assessment. Extensive
experiments on complex task datasets demonstrate the effectiveness of
360$^\circ$REA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structured Preference Optimization for Vision-Language Long-Horizon Task
  Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20742v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20742v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiwen Liang, Min Lin, Weiqi Ruan, Rongtao Xu, Yuecheng Liu, Jiaqi Chen, Bingqian Lin, Yuzheng Zhuang, Xiaodan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing methods for vision-language task planning excel in short-horizon
tasks but often fall short in complex, long-horizon planning within dynamic
environments. These challenges primarily arise from the difficulty of
effectively training models to produce high-quality reasoning processes for
long-horizon tasks. To address this, we propose Structured Preference
Optimization (SPO), which aims to enhance reasoning and action selection in
long-horizon task planning through structured preference evaluation and
optimized training strategies. Specifically, SPO introduces: 1)
Preference-Based Scoring and Optimization, which systematically evaluates
reasoning chains based on task relevance, visual grounding, and historical
consistency; and 2) Curriculum-Guided Training, where the model progressively
adapts from simple to complex tasks, improving its generalization ability in
long-horizon scenarios and enhancing reasoning robustness. To advance research
in vision-language long-horizon task planning, we introduce ExtendaBench, a
comprehensive benchmark covering 1,509 tasks across VirtualHome and Habitat
2.0, categorized into ultra-short, short, medium, and long tasks. Experimental
results demonstrate that SPO significantly improves reasoning quality and final
decision accuracy, outperforming prior methods on long-horizon tasks and
underscoring the effectiveness of preference-driven optimization in
vision-language task planning. Specifically, SPO achieves a +5.98% GCR and
+4.68% SR improvement in VirtualHome and a +3.30% GCR and +2.11% SR improvement
in Habitat over the best-performing baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting new obfuscated malware variants: A lightweight and
  interpretable machine learning approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07918v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07918v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oladipo A. Madamidola, Felix Ngobigha, Adnane Ez-zizi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning has been successfully applied in developing malware
detection systems, with a primary focus on accuracy, and increasing attention
to reducing computational overhead and improving model interpretability.
However, an important question remains underexplored: How well can machine
learning-based models detect entirely new forms of malware not present in the
training data? In this study, we present a machine learning-based system for
detecting obfuscated malware that is not only highly accurate, lightweight and
interpretable, but also capable of successfully adapting to new types of
malware attacks. Our system is capable of detecting 15 malware subtypes despite
being exclusively trained on one malware subtype, namely the Transponder from
the Spyware family. This system was built after training 15 distinct random
forest-based models, each on a different malware subtype from the
CIC-MalMem-2022 dataset. These models were evaluated against the entire range
of malware subtypes, including all unseen malware subtypes. To maintain the
system's streamlined nature, training was confined to the top five most
important features, which also enhanced interpretability. The
Transponder-focused model exhibited high accuracy, exceeding 99.8%, with an
average processing speed of 5.7 microseconds per file. We also illustrate how
the Shapley additive explanations technique can facilitate the interpretation
of the model predictions. Our research contributes to advancing malware
detection methodologies, pioneering the feasibility of detecting obfuscated
malware by exclusively training a model on a single or a few carefully selected
malware subtypes and applying it to detect unseen subtypes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages (excluding Appendix), 5 figures and 5 tables. Now published
  in Intelligent Systems with Applications
  (https://doi.org/10.1016/j.iswa.2024.200472)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stealthy Jailbreak Attacks on Large Language Models via Benign Data
  Mirroring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21083v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21083v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Honglin Mu, Han He, Yuxin Zhou, Yunlong Feng, Yang Xu, Libo Qin, Xiaoming Shi, Zeming Liu, Xudong Han, Qi Shi, Qingfu Zhu, Wanxiang Che
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language model (LLM) safety is a critical issue, with numerous studies
employing red team testing to enhance model security. Among these, jailbreak
methods explore potential vulnerabilities by crafting malicious prompts that
induce model outputs contrary to safety alignments. Existing black-box
jailbreak methods often rely on model feedback, repeatedly submitting queries
with detectable malicious instructions during the attack search process.
Although these approaches are effective, the attacks may be intercepted by
content moderators during the search process. We propose an improved transfer
attack method that guides malicious prompt construction by locally training a
mirror model of the target black-box model through benign data distillation.
This method offers enhanced stealth, as it does not involve submitting
identifiable malicious instructions to the target model during the search
phase. Our approach achieved a maximum attack success rate of 92%, or a
balanced value of 80% with an average of 1.5 detectable jailbreak queries per
sample against GPT-3.5 Turbo on a subset of AdvBench. These results underscore
the need for more robust defense mechanisms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nature Language Model: Deciphering the Language of Nature for Scientific
  Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07527v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07527v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingce Xia, Peiran Jin, Shufang Xie, Liang He, Chuan Cao, Renqian Luo, Guoqing Liu, Yue Wang, Zequn Liu, Yuan-Jyue Chen, Zekun Guo, Yeqi Bai, Pan Deng, Yaosen Min, Ziheng Lu, Hongxia Hao, Han Yang, Jielan Li, Chang Liu, Jia Zhang, Jianwei Zhu, Ran Bi, Kehan Wu, Wei Zhang, Kaiyuan Gao, Qizhi Pei, Qian Wang, Xixian Liu, Yanting Li, Houtian Zhu, Yeqing Lu, Mingqian Ma, Zun Wang, Tian Xie, Krzysztof Maziarz, Marwin Segler, Zhao Yang, Zilong Chen, Yu Shi, Shuxin Zheng, Lijun Wu, Chen Hu, Peggy Dai, Tie-Yan Liu, Haiguang Liu, Tao Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models have revolutionized natural language processing and
artificial intelligence, significantly enhancing how machines comprehend and
generate human languages. Inspired by the success of these foundation models,
researchers have developed foundation models for individual scientific domains,
including small molecules, materials, proteins, DNA, RNA and even cells.
However, these models are typically trained in isolation, lacking the ability
to integrate across different scientific domains. Recognizing that entities
within these domains can all be represented as sequences, which together form
the "language of nature", we introduce Nature Language Model (NatureLM), a
sequence-based science foundation model designed for scientific discovery.
Pre-trained with data from multiple scientific domains, NatureLM offers a
unified, versatile model that enables various applications including: (i)
generating and optimizing small molecules, proteins, RNA, and materials using
text instructions; (ii) cross-domain generation/design, such as
protein-to-molecule and protein-to-RNA generation; and (iii) top performance
across different domains, matching or surpassing state-of-the-art specialist
models. NatureLM offers a promising generalist approach for various scientific
tasks, including drug discovery (hit generation/optimization, ADMET
optimization, synthesis), novel material design, and the development of
therapeutic proteins or nucleotides. We have developed NatureLM models in
different sizes (1 billion, 8 billion, and 46.7 billion parameters) and
observed a clear improvement in performance as the model size increases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>93 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InfoDisent: Explainability of Image Classification Models by Information
  Disentanglement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10329v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10329v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Łukasz Struski, Dawid Rymarczyk, Jacek Tabor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce InfoDisent, a hybrid approach to explainability
based on the information bottleneck principle. InfoDisent enables the
disentanglement of information in the final layer of any pretrained model into
atomic concepts, which can be interpreted as prototypical parts. This approach
merges the flexibility of post-hoc methods with the concept-level modeling
capabilities of self-explainable neural networks, such as ProtoPNets. We
demonstrate the effectiveness of InfoDisent through computational experiments
and user studies across various datasets using modern backbones such as ViTs
and convolutional networks. Notably, InfoDisent generalizes the prototypical
parts approach to novel domains (ImageNet).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HelpSteer2-Preference: Complementing Ratings with Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01257v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01257v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhilin Wang, Alexander Bukharin, Olivier Delalleau, Daniel Egert, Gerald Shen, Jiaqi Zeng, Oleksii Kuchaiev, Yi Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward models are critical for aligning models to follow instructions, and
are typically trained following one of two popular paradigms: Bradley-Terry
style or Regression style. However, there is a lack of evidence that either
approach is better than the other, when adequately matched for data. This is
primarily because these approaches require data collected in different (but
incompatible) formats, meaning that adequately matched data is not available in
existing public datasets. To tackle this problem, we release preference
annotations (designed for Bradley-Terry training) to complement existing
ratings (designed for Regression style training) in the HelpSteer2 dataset. To
improve data interpretability, preference annotations are accompanied with
human-written justifications. Using this data, we conduct the first
head-to-head comparison of Bradley-Terry and Regression models when adequately
matched for data. Based on insights derived from such a comparison, we propose
a novel approach to combine Bradley-Terry and Regression reward modeling. A
Llama-3.1-70B-Instruct model tuned with this approach scores 94.1 on
RewardBench, emerging top of more than 140 reward models as of 1 Oct 2024. This
reward model can then be used with REINFORCE algorithm (RLHF) to align an
Instruct model to reach 85.0 on Arena Hard, which is No. 1 as of 1 Oct 2024. We
open-source this dataset (CC-BY-4.0 license) at
https://huggingface.co/datasets/nvidia/HelpSteer2#preferences-new -- 1-oct-2024
and openly release the trained Reward and Instruct models at
https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Reward and
https://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025; 28 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Backbone for Long-Horizon Robot Task Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01334v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01334v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoshuai Chen, Wei Chen, Dongmyoung Lee, Yukun Ge, Nicolas Rojas, Petar Kormushev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end robot learning, particularly for long-horizon tasks, often results
in unpredictable outcomes and poor generalization. To address these challenges,
we propose a novel Therblig-Based Backbone Framework (TBBF) as a fundamental
structure to enhance interpretability, data efficiency, and generalization in
robotic systems. TBBF utilizes expert demonstrations to enable therblig-level
task decomposition, facilitate efficient action-object mapping, and generate
adaptive trajectories for new scenarios. The approach consists of two stages:
offline training and online testing. During the offline training stage, we
developed the Meta-RGate SynerFusion (MGSF) network for accurate therblig
segmentation across various tasks. In the online testing stage, after a
one-shot demonstration of a new task is collected, our MGSF network extracts
high-level knowledge, which is then encoded into the image using Action
Registration (ActionREG). Additionally, Large Language Model (LLM)-Alignment
Policy for Visual Correction (LAP-VC) is employed to ensure precise action
registration, facilitating trajectory transfer in novel robot scenarios.
Experimental results validate these methods, achieving 94.37% recall in
therblig segmentation and success rates of 94.4% and 80% in real-world online
robot testing for simple and complex scenarios, respectively. Supplementary
material is available at:
https://sites.google.com/view/therbligsbasedbackbone/home
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures. This work has been published by IEEE Robotics and
  Automation Letters (RA-L)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Search Engines and Large Language Models for Answering Health
  Questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.12468v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.12468v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcos Fernández-Pichel, Juan C. Pichel, David E. Losada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Search engines (SEs) have traditionally been primary tools for information
seeking, but the new Large Language Models (LLMs) are emerging as powerful
alternatives, particularly for question-answering tasks. This study compares
the performance of four popular SEs, seven LLMs, and retrieval-augmented (RAG)
variants in answering 150 health-related questions from the TREC Health
Misinformation (HM) Track. Results reveal SEs correctly answer between 50 and
70% of questions, often hindered by many retrieval results not responding to
the health question. LLMs deliver higher accuracy, correctly answering about
80% of questions, though their performance is sensitive to input prompts. RAG
methods significantly enhance smaller LLMs' effectiveness, improving accuracy
by up to 30% by integrating retrieval evidence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Extracting Formulae in Many-Valued Logic from Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.12113v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.12113v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yani Zhang, Helmut Bölcskei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new perspective on deep ReLU networks, namely as circuit
counterparts of Lukasiewicz infinite-valued logic -- a many-valued (MV)
generalization of Boolean logic. An algorithm for extracting formulae in MV
logic from deep ReLU networks is presented. As the algorithm applies to
networks with general, in particular also real-valued, weights, it can be used
to extract logical formulae from deep ReLU networks trained on data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Signicant extension of the previous version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VISION-XL: High Definition Video Inverse Problem Solver using Latent
  Image Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00156v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00156v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taesung Kwon, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a novel framework for solving high-definition video
inverse problems using latent image diffusion models. Building on recent
advancements in spatio-temporal optimization for video inverse problems using
image diffusion models, our approach leverages latent-space diffusion models to
achieve enhanced video quality and resolution. To address the high
computational demands of processing high-resolution frames, we introduce a
pseudo-batch consistent sampling strategy, allowing efficient operation on a
single GPU. Additionally, to improve temporal consistency, we present
pseudo-batch inversion, an initialization technique that incorporates
informative latents from the measurement. By integrating with SDXL, our
framework achieves state-of-the-art video reconstruction across a wide range of
spatio-temporal inverse problems, including complex combinations of frame
averaging and various spatial degradations, such as deblurring,
super-resolution, and inpainting. Unlike previous methods, our approach
supports multiple aspect ratios (landscape, vertical, and square) and delivers
HD-resolution reconstructions (exceeding 1280x720) in under 6 seconds per frame
on a single NVIDIA 4090 GPU.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://vision-xl.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ No More Sliding Window: Efficient 3D Medical Image Segmentation with
  Differentiable Top-k Patch Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10814v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10814v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Young Seok Jeon, Hongfei Yang, Huazhu Fu, Mengling Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D models surpass 2D models in CT/MRI segmentation by effectively capturing
inter-slice relationships. However, the added depth dimension substantially
increases memory consumption. While patch-based training alleviates memory
constraints, it significantly slows down the inference speed due to the sliding
window (SW) approach. We propose No-More-Sliding-Window (NMSW), a novel
end-to-end trainable framework that enhances the efficiency of generic 3D
segmentation backbone during an inference step by eliminating the need for SW.
NMSW employs a differentiable Top-k module to selectively sample only the most
relevant patches, thereby minimizing redundant computations. When patch-level
predictions are insufficient, the framework intelligently leverages coarse
global predictions to refine results. Evaluated across 3 tasks using 3
segmentation backbones, NMSW achieves competitive accuracy compared to SW
inference while significantly reducing computational complexity by 91% (88.0 to
8.00 TMACs). Moreover, it delivers a 9.1x faster inference on the H100 GPU
(99.0 to 8.3 sec) and a 11.1x faster inference on the Xeon Gold CPU (2110 to
189 sec). NMSW is model-agnostic, further boosting efficiency when integrated
with any existing efficient segmentation backbones.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Deterministic Policy Gradient for Disturbance Attenuation and Its
  Application to Quadrotor Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21057v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21057v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taeho Lee, Donghwan Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Practical control systems pose significant challenges in identifying optimal
control policies due to uncertainties in the system model and external
disturbances. While $H_\infty$ control techniques are commonly used to design
robust controllers that mitigate the effects of disturbances, these methods
often require complex and computationally intensive calculations. To address
this issue, this paper proposes a reinforcement learning algorithm called
Robust Deterministic Policy Gradient (RDPG), which formulates the $H_\infty$
control problem as a two-player zero-sum dynamic game. In this formulation, one
player (the user) aims to minimize the cost, while the other player (the
adversary) seeks to maximize it. We then employ deterministic policy gradient
(DPG) and its deep reinforcement learning counterpart to train a robust control
policy with effective disturbance attenuation. In particular, for practical
implementation, we introduce an algorithm called robust deep deterministic
policy gradient (RDDPG), which employs a deep neural network architecture and
integrates techniques from the twin-delayed deep deterministic policy gradient
(TD3) to enhance stability and learning efficiency. To evaluate the proposed
algorithm, we implement it on an unmanned aerial vehicle (UAV) tasked with
following a predefined path in a disturbance-prone environment. The
experimental results demonstrate that the proposed method outperforms other
control approaches in terms of robustness against disturbances, enabling
precise real-time tracking of moving targets even under severe disturbance
conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Claims Evolve: Evaluating and Enhancing the Robustness of Embedding
  Models Against Misinformation Edits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03417v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03417v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jabez Magomere, Emanuele La Malfa, Manuel Tonneau, Ashkan Kazemi, Scott Hale
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online misinformation remains a critical challenge, and fact-checkers
increasingly rely on embedding-based methods to retrieve relevant fact-checks.
Yet, when debunked claims reappear in edited forms, the performance of these
methods is unclear. In this work, we introduce a taxonomy of six common
real-world misinformation edits and propose a perturbation framework that
generates valid, natural claim variations. Our multi-stage retrieval evaluation
reveals that standard embedding models struggle with user-introduced edits,
while LLM-distilled embeddings offer improved robustness at a higher
computational cost. Although a strong reranker helps mitigate some issues, it
cannot fully compensate for first-stage retrieval gaps. Addressing these
retrieval gaps, our train- and inference-time mitigation approaches enhance
in-domain robustness by up to 17 percentage points and boost out-of-domain
generalization by 10 percentage points over baseline models. Overall, our
findings provide practical improvements to claim-matching systems, enabling
more reliable fact-checking of evolving misinformation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semi-Parametric Retrieval via Binary Bag-of-Tokens Index 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.01924v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.01924v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Zhou, Li Dong, Furu Wei, Lei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information retrieval has transitioned from standalone systems into essential
components across broader applications, with indexing efficiency,
cost-effectiveness, and freshness becoming increasingly critical yet often
overlooked. In this paper, we introduce SemI-parametric Disentangled Retrieval
(SiDR), a bi-encoder retrieval framework that decouples retrieval index from
neural parameters to enable efficient, low-cost, and parameter-agnostic
indexing for emerging use cases. Specifically, in addition to using embeddings
as indexes like existing neural retrieval methods, SiDR supports a
non-parametric tokenization index for search, achieving BM25-like indexing
complexity with significantly better effectiveness. Our comprehensive
evaluation across 16 retrieval benchmarks demonstrates that SiDR outperforms
both neural and term-based retrieval baselines under the same indexing
workload: (i) When using an embedding-based index, SiDR exceeds the performance
of conventional neural retrievers while maintaining similar training
complexity; (ii) When using a tokenization-based index, SiDR drastically
reduces indexing cost and time, matching the complexity of traditional
term-based retrieval, while consistently outperforming BM25 on all in-domain
datasets; (iii) Additionally, we introduce a late parametric mechanism that
matches BM25 index preparation time while outperforming other neural retrieval
baselines in effectiveness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prompt-Matcher: Leveraging Large Models to Reduce Uncertainty in Schema
  Matching Results 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14507v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14507v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longyu Feng, Huahang Li, Chen Jason Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Schema matching is the process of identifying correspondences between the
elements of two given schemata, essential for database management systems, data
integration, and data warehousing. For datasets across different scenarios, the
optimal schema matching algorithm is different. For single algorithm,
hyperparameter tuning also cases multiple results. All results assigned equal
probabilities are stored in probabilistic databases to facilitate uncertainty
management. The substantial degree of uncertainty diminishes the efficiency and
reliability of data processing, thereby precluding the provision of more
accurate information for decision-makers. To address this problem, we introduce
a new approach based on fine-grained correspondence verification with specific
prompt of Large Language Model.
  Our approach is an iterative loop that consists of three main components: (1)
the correspondence selection algorithm, (2) correspondence verification, and
(3) the update of probability distribution. The core idea is that
correspondences intersect across multiple results, thereby linking the
verification of correspondences to the reduction of uncertainty in candidate
results.
  The task of selecting an optimal correspondence set to maximize the
anticipated uncertainty reduction within a fixed budgetary framework is
established as an NP-hard problem. We propose a novel $(1-1/e)$-approximation
algorithm that significantly outperforms brute algorithm in terms of
computational efficiency. To enhance correspondence verification, we have
developed two prompt templates that enable GPT-4 to achieve state-of-the-art
performance across two established benchmark datasets. Our comprehensive
experimental evaluation demonstrates the superior effectiveness and robustness
of the proposed approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StoryTeller: Improving Long Video Description through Global
  Audio-Visual Character Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07076v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07076v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichen He, Yuan Lin, Jianchao Wu, Hanchong Zhang, Yuchen Zhang, Ruicheng Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing large vision-language models (LVLMs) are largely limited to
processing short, seconds-long videos and struggle with generating coherent
descriptions for extended video spanning minutes or more. Long video
description introduces new challenges, such as consistent character
identification and plot-level descriptions incorporating both visual and audio
information. To address these, we figure out audio-visual character
identification, matching character names to each dialogue, as a key factor. We
propose StoryTeller, a system for generating dense descriptions of long videos,
incorporating both low-level visual concepts and high-level plot information.
StoryTeller uses a multimodal large language model that integrates visual,
audio, and text modalities to perform audio-visual character identification on
minute-long video clips. The results are then fed into a LVLM to enhance
consistency of video description. We validate our approach on movie description
tasks and introduce MovieStory101, a dataset with dense descriptions for
three-minute movie clips. To evaluate long video descriptions, we create
StoryQA, a large set of multiple-choice questions for MovieStory101 test set.
We assess descriptions by inputting them into GPT-4 to answer these questions,
using accuracy as an automatic evaluation metric. Experiments show that
StoryTeller outperforms all open and closed-source baselines on StoryQA,
achieving 9.5% higher accuracy than the strongest baseline, Gemini-1.5-pro, and
demonstrating a +15.56% advantage in human side-by-side evaluations.
Additionally, incorporating audio-visual character identification from
StoryTeller improves the performance of all video description models, with
Gemini-1.5-pro and GPT-4o showing relative improvement of 5.5% and 13.0%,
respectively, in accuracy on StoryQA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explaining Caption-Image Interactions in CLIP models with Second-Order
  Attributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14153v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14153v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Möller, Pascal Tilli, Ngoc Thang Vu, Sebastian Padó
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dual encoder architectures like CLIP models map two types of inputs into a
shared embedding space and predict similarities between them. Despite their
success, it is, however, not understood how these models compare their two
inputs. Common first-order feature-attribution methods can only provide limited
insights into dual-encoders since their predictions depend on
feature-interactions rather than on individual features. In this paper, we
first derive a second-order method enabling the attribution of predictions by
any differentiable dual encoder onto feature-interactions between its inputs.
Second, we apply our method to CLIP models and show that they learn
fine-grained correspondences between parts of captions and regions in images.
They match objects across input modes also account for mismatches. This
visual-linguistic grounding ability, however, varies heavily between object
classes and exhibits pronounced out-of-domain effects. We can identify
individual errors as well as systematic failure categories including object
coverage, unusual scenes and correlated contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Union of Experts: Adapting Hierarchical Routing to Equivalently
  Decomposed Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02495v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02495v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujiao Yang, Jing Lian, Linhui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Union-of-Experts (UoE), which decomposes transformer into an
equitant group of experts, and then implement selective routing on input data
and experts. Our approach advances MoE design with four key innovations: (1) We
conducted equitant expert decomposition on both MLP blocks and attention blocks
based on matrix partition in tensor parallelism. (2) We developed two routing
paradigms: patch-wise data selection and expert selection, to apply routing
across different levels. (3) We design the architecture of UoE model, including
Selective Multi-Head Attention (SMHA) and Union-of-MLP-Experts (UoME). (4) We
develop parallel implementation of UoE's routing and computation operation, and
optimize efficiency based on the hardware processing analysis. The experiments
demonstrate that the UoE model surpass Full Attention, state-of-art MoEs and
efficient transformers (including the model architecture of recently proposed
DeepSeek-V3) in several tasks across image and natural language domains. In
language modeling tasks, we achieve an average reduction of 2.38 in perplexity
compared to the best-performed MoE method with an average of 76% FLOPs. In Long
Range Arena benchmark, we recorded an average score that is at least 0.68%
higher than all comparison models including Full Attention, MoEs, and
transformer variants, with only 50% FLOPs of the best MoE method. In image
classification, our model yielded an average accuracy improvement of 1.75% than
the best model while maintaining comparable FLOPs. The source codes are
available at https://github.com/YujiaoYang-work/UoE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pap2Pat: Benchmarking Outline-Guided Long-Text Patent Generation with
  Patent-Paper Pairs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07009v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07009v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentin Knappich, Simon Razniewski, Anna Hätty, Annemarie Friedrich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dealing with long and highly complex technical text is a challenge for Large
Language Models (LLMs), which still have to unfold their potential in
supporting expensive and timeintensive processes like patent drafting. Within
patents, the description constitutes more than 90% of the document on average.
Yet, its automatic generation remains understudied. When drafting patent
applications, patent attorneys typically receive invention reports (IRs), which
are usually confidential, hindering research on LLM-supported patent drafting.
Often, prepublication research papers serve as IRs. We leverage this duality to
build PAP2PAT, an open and realistic benchmark for patent drafting consisting
of 1.8k patent-paper pairs describing the same inventions. To address the
complex longdocument patent generation task, we propose chunk-based
outline-guided generation using the research paper as invention specification.
Our extensive evaluation using PAP2PAT and a human case study show that LLMs
can effectively leverage information from the paper, but still struggle to
provide the necessary level of detail. Fine-tuning leads to more patent-style
language, but also to more hallucination. We release our data and code
https://github.com/boschresearch/Pap2Pat.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Measuring Human and AI Values Based on Generative Psychometrics with
  Large Language Models <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.12106v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.12106v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Ye, Yuhang Xie, Yuanyi Ren, Hanjun Fang, Xin Zhang, Guojie Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human values and their measurement are long-standing interdisciplinary
inquiry. Recent advances in AI have sparked renewed interest in this area, with
large language models (LLMs) emerging as both tools and subjects of value
measurement. This work introduces Generative Psychometrics for Values (GPV), an
LLM-based, data-driven value measurement paradigm, theoretically grounded in
text-revealed selective perceptions. The core idea is to dynamically parse
unstructured texts into perceptions akin to static stimuli in traditional
psychometrics, measure the value orientations they reveal, and aggregate the
results. Applying GPV to human-authored blogs, we demonstrate its stability,
validity, and superiority over prior psychological tools. Then, extending GPV
to LLM value measurement, we advance the current art with 1) a psychometric
methodology that measures LLM values based on their scalable and free-form
outputs, enabling context-specific measurement; 2) a comparative analysis of
measurement paradigms, indicating response biases of prior methods; and 3) an
attempt to bridge LLM values and their safety, revealing the predictive power
of different value systems and the impacts of various values on LLM safety.
Through interdisciplinary efforts, we aim to leverage AI for next-generation
psychometrics and psychometrics for value-aligned AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BackdoorMBTI: A Backdoor Learning <span class="highlight-title">Multi</span>modal Benchmark Tool Kit for
  Backdoor Defense Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.11006v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.11006v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiyang Yu, Tian Xie, Jiaping Gui, Pengyang Wang, Ping Yi, Yue Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past few years, the emergence of backdoor attacks has presented
significant challenges to deep learning systems, allowing attackers to insert
backdoors into neural networks. When data with a trigger is processed by a
backdoor model, it can lead to mispredictions targeted by attackers, whereas
normal data yields regular results. The scope of backdoor attacks is expanding
beyond computer vision and encroaching into areas such as natural language
processing and speech recognition. Nevertheless, existing backdoor defense
methods are typically tailored to specific data modalities, restricting their
application in multimodal contexts. While multimodal learning proves highly
applicable in facial recognition, sentiment analysis, action recognition,
visual question answering, the security of these models remains a crucial
concern. Specifically, there are no existing backdoor benchmarks targeting
multimodal applications or related tasks.
  In order to facilitate the research in multimodal backdoor, we introduce
BackdoorMBTI, the first backdoor learning toolkit and benchmark designed for
multimodal evaluation across three representative modalities from eleven
commonly used datasets. BackdoorMBTI provides a systematic backdoor learning
pipeline, encompassing data processing, data poisoning, backdoor training, and
evaluation. The generated poison datasets and backdoor models enable detailed
evaluation of backdoor defenses. Given the diversity of modalities,
BackdoorMBTI facilitates systematic evaluation across different data types.
Furthermore, BackdoorMBTI offers a standardized approach to handling practical
factors in backdoor learning, such as issues related to data quality and
erroneous labels. We anticipate that BackdoorMBTI will expedite future research
in backdoor defense methods within a multimodal context. Code is available at
https://github.com/SJTUHaiyangYu/BackdoorMBTI.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting <span class="highlight-title">Multi</span>-Permutation Equivariance through the Lens of
  Irreducible Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06665v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06665v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yonatan Sverdlov, Ido Springer, Nadav Dym
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the characterization of equivariant linear layers for
representations of permutations and related groups. Unlike traditional
approaches, which address these problems using parameter-sharing, we consider
an alternative methodology based on irreducible representations and Schur's
lemma. Using this methodology, we obtain an alternative derivation for existing
models like DeepSets, 2-IGN graph equivariant networks, and Deep Weight Space
(DWS) networks. The derivation for DWS networks is significantly simpler than
that of previous results.
  Next, we extend our approach to unaligned symmetric sets, where equivariance
to the wreath product of groups is required. Previous works have addressed this
problem in a rather restrictive setting, in which almost all wreath equivariant
layers are Siamese. In contrast, we give a full characterization of layers in
this case and show that there is a vast number of additional non-Siamese layers
in some settings. We also show empirically that these additional non-Siamese
layers can improve performance in tasks like graph anomaly detection, weight
space alignment, and learning Wasserstein distances. Our code is available at
\href{https://github.com/yonatansverdlov/Irreducible-Representations-of-Deep-Weight-Spaces}{GitHub}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Autoformalizing Natural Language to First-Order Logic: A Case Study in
  Logical Fallacy Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.02318v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.02318v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Lalwani, Tasha Kim, Lovish Chopra, Christopher Hahn, Zhijing Jin, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Translating natural language into formal language such as First-Order Logic
(FOL) is a foundational challenge in NLP with wide-ranging applications in
automated reasoning, misinformation tracking, and knowledge validation. In this
paper, we introduce Natural Language to First-Order Logic (NL2FOL), a framework
to autoformalize natural language to FOL step by step using Large Language
Models (LLMs). Our approach addresses key challenges in this translation
process, including the integration of implicit background knowledge. By
leveraging structured representations generated by NL2FOL, we use
Satisfiability Modulo Theory (SMT) solvers to reason about the logical validity
of natural language statements. We present logical fallacy detection as a case
study to evaluate the efficacy of NL2FOL. Being neurosymbolic, our approach
also provides interpretable insights into the reasoning process and
demonstrates robustness without requiring model fine-tuning or labeled training
data. Our framework achieves strong performance on multiple datasets. On the
LOGIC dataset, NL2FOL achieves an F1-score of 78%, while generalizing
effectively to the LOGICCLIMATE dataset with an F1-score of 80%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An <span class="highlight-title">LLM</span>-based Agent for Reliable Docker Environment Configuration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13681v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13681v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruida Hu, Chao Peng, Xinchen Wang, Cuiyun Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Environment configuration is a critical yet time-consuming step in software
development, especially when dealing with unfamiliar code repositories. While
Large Language Models (LLMs) demonstrate the potential to accomplish software
engineering tasks, existing methods for environment configuration often rely on
manual efforts or fragile scripts, leading to inefficiencies and unreliable
outcomes. We introduce Repo2Run, the first LLM-based agent designed to fully
automate environment configuration and generate executable Dockerfiles for
arbitrary Python repositories. We address two major challenges: (1) enabling
the LLM agent to configure environments within isolated Docker containers, and
(2) ensuring the successful configuration process is recorded and accurately
transferred to a Dockerfile without error. To achieve this, we propose atomic
configuration synthesis, featuring a dual-environment architecture (internal
and external environment) with a rollback mechanism to prevent environment
"pollution" from failed commands, guaranteeing atomic execution (execute fully
or not at all) and a Dockerfile generator to transfer successful configuration
steps into runnable Dockerfiles. We evaluate Repo2Run~on our proposed benchmark
of 420 recent Python repositories with unit tests, where it achieves an 86.0%
success rate, outperforming the best baseline by 63.9%. Repo2Run is available
at https://github.com/bytedance/Repo2Run.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on
  Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sumin Jo, Junseong Choi, Jiho Kim, Edward Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have combined Large Language Models (LLMs) with Knowledge
Graphs (KGs) to enhance reasoning, improving inference accuracy without
additional training while mitigating hallucination. However, existing
frameworks are often rigid, struggling to adapt to KG or task changes. They
also rely heavily on powerful LLMs for reliable (i.e., trustworthy) reasoning.
To address this, We introduce R2-KG, a plug-and-play, dual-agent framework that
separates reasoning into two roles: an Operator (a low-capacity LLM) that
gathers evidence and a Supervisor (a high-capacity LLM) that makes final
judgments. This design is cost-efficient for LLM inference while still
maintaining strong reasoning accuracy. Additionally, R2-KG employs an
Abstention mechanism, generating answers only when sufficient evidence is
collected from KG, which significantly enhances reliability. Experiments across
multiple KG-based reasoning tasks show that R2-KG consistently outperforms
baselines in both accuracy and reliability, regardless of the inherent
capability of LLMs used as the Operator. Further experiments reveal that the
single-agent version of R2-KG, equipped with a strict self-consistency
strategy, achieves significantly higher-than-baseline reliability while
reducing inference cost. However, it also leads to a higher abstention rate in
complex KGs. Our findings establish R2-KG as a flexible and cost-effective
solution for KG-based reasoning. It reduces reliance on high-capacity LLMs
while ensuring trustworthy inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Markov Chain of Thought for Efficient Mathematical Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17635v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17635v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wen Yang, Minpeng Liao, Kai Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain of Thought (CoT) of multi-step benefits from the logical structure of
the reasoning steps and task-specific actions, significantly enhancing the
mathematical reasoning capabilities of large language models. As the prevalence
of long CoT, the number of reasoning steps exceeds manageable token limits and
leads to higher computational demands. Inspired by the fundamental logic of
human cognition, "derive, then reduce", we conceptualize the standard
multi-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we
consider the mathematical reasoning task, defining each reasoning step as text
accompanied by a Python code snippet. To facilitate a longer reasoning path,
self-correction is enabled through interactions with the code interpreter. Our
MCoT aims to compress previous reasoning steps into a simplified question,
enabling efficient next-step inference without relying on a lengthy KV cache.
In our experiments, we curate the $\texttt{MCoTInstruct}$ dataset, and the
empirical results indicate that MCoT not only significantly enhances efficiency
but also maintains comparable accuracy. While much remains to be explored, this
work paves the way for exploring the long CoT reasoning abilities of LLMs. The
code is available at https://github.com/james-yw/Markov-Chain-of-Thought
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera ready version for NAACL 2025 Main</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating Non-Transitivity in <span class="highlight-title">LLM</span>-as-a-Judge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14074v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14074v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Xu, Laura Ruis, Tim Rocktäschel, Robert Kirk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic evaluation methods based on large language models (LLMs) are
emerging as the standard tool for assessing the instruction-following abilities
of LLM-based agents. The most common method in this paradigm, pairwise
comparisons with a baseline model, critically depends on the assumption of
transitive preferences. However, the validity of this assumption remains
largely unexplored. In this study, we investigate the presence of
non-transitivity within the AlpacaEval framework and analyze its effects on
model rankings. We find that LLM judges exhibit non-transitive preferences,
leading to rankings that are sensitive to the choice of the baseline model. To
mitigate this issue, we show that round-robin tournaments combined with
Bradley-Terry models of preference can produce more reliable rankings. Notably,
our method increases both the Spearman correlation and the Kendall correlation
with Chatbot Arena (95.0% -> 96.4% and 82.1% -> 86.3% respectively). To address
the computational cost of round-robin tournaments, we propose Swiss-Wise
Iterative Matchmaking (Swim) tournaments, using a dynamic matching strategy to
capture the benefits of round-robin tournaments while maintaining computational
efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, 2 tables (30 pages, 11 figures, 8 tables
  including references and appendices)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Edge General Intelligence via Large Language Models:
  Opportunities and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18125v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18125v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Handi Chen, Weipeng Deng, Shuo Yang, Jinfeng Xu, Zhihan Jiang, Edith C. H. Ngai, Jiangchuan Liu, Xue Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Edge Intelligence (EI) has been instrumental in delivering real-time,
localized services by leveraging the computational capabilities of edge
networks. The integration of Large Language Models (LLMs) empowers EI to evolve
into the next stage: Edge General Intelligence (EGI), enabling more adaptive
and versatile applications that require advanced understanding and reasoning
capabilities. However, systematic exploration in this area remains
insufficient. This survey delineates the distinctions between EGI and
traditional EI, categorizing LLM-empowered EGI into three conceptual systems:
centralized, hybrid, and decentralized. For each system, we detail the
framework designs and review existing implementations. Furthermore, we evaluate
the performance and throughput of various Small Language Models (SLMs) that are
more suitable for development on edge devices. This survey provides researchers
with a comprehensive vision of EGI, offering insights into its vast potential
and establishing a foundation for future advancements in this rapidly evolving
field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training and Evaluating Language Models with Template-based Data
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18104v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18104v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of large language models (LLMs) such as GPT-3, PaLM,
and Llama has significantly transformed natural language processing, showcasing
remarkable capabilities in understanding and generating language. However,
these models often struggle with tasks requiring complex reasoning,
particularly in mathematical problem-solving, due in part to the scarcity of
large-scale, high-quality, domain-specific datasets necessary for training
sophisticated reasoning abilities. To address this limitation, we introduce
Template-based Data Generation (TDG), a novel approach that leverages LLMs
(GPT-4) to automatically generate parameterized meta-templates, which are then
used to synthesize a vast array of high-quality problems and solutions.
Leveraging TDG, we create TemplateMath Part I: TemplateGSM, a dataset
comprising over 7 million synthetically generated grade school math
problems--each accompanied by code-based and natural language solutions--with
the potential to generate an effectively unlimited number more. This dataset
alleviates the scarcity of large-scale mathematical datasets and serves as a
valuable resource for pre-training, fine-tuning, and evaluating LLMs in
mathematical reasoning. Our method not only enables the generation of virtually
infinite data but also elevates data augmentation to a new level by using GPT-4
for meta-template generation, ensuring diverse and high-quality problem
structures. The TemplateMath Part I: TemplateGSM dataset is publicly available
at https://huggingface.co/datasets/math-ai/TemplateGSM. The code is available
at https://github.com/iiis-ai/TemplateMath.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Legal Fact Prediction: The Missing Piece in Legal Judgment Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07055v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07055v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junkai Liu, Yujie Tong, Hui Huang, Bowen Zheng, Yiran Hu, Peicheng Wu, Chuan Xiao, Makoto Onizuka, Muyun Yang, Shuyuan Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Legal judgment prediction (LJP), which enables litigants and their lawyers to
forecast judgment outcomes and refine litigation strategies, has emerged as a
crucial legal NLP task. Existing studies typically utilize legal facts, i.e.,
facts that have been established by evidence and determined by the judge, to
predict the judgment. However, legal facts are often difficult to obtain in the
early stages of litigation, significantly limiting the practical applicability
of fact-based LJP. To address this limitation, we propose a novel legal NLP
task: \textit{legal fact prediction} (LFP), which takes the evidence submitted
by litigants for trial as input to predict legal facts, thereby empowering
fact-based LJP technologies to perform prediction in the absence of
ground-truth legal facts. We also propose the first benchmark dataset,
LFPBench, for evaluating the LFP task. Our extensive experiments on LFPBench
demonstrate the effectiveness of LFP-empowered LJP and highlight promising
research directions for LFP. Our code and data are available at
https://github.com/HPRCEST/LFPBench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prompting with Phonemes: Enhancing <span class="highlight-title">LLM</span>s' <span class="highlight-title">Multi</span>linguality for Non-Latin
  Script Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02398v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02398v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoang H Nguyen, Khyati Mahajan, Vikas Yadav, Julian Salazar, Philip S. Yu, Masoud Hashemi, Rishabh Maheshwary
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although multilingual LLMs have achieved remarkable performance across
benchmarks, we find they continue to underperform on non-Latin script languages
across contemporary LLM families. This discrepancy arises from the fact that
LLMs are pretrained with orthographic scripts, which are dominated by Latin
characters that obscure their shared phonology with non-Latin scripts. We
propose leveraging phonemic transcriptions as complementary signals to induce
script-invariant representations. Our study demonstrates that integrating
phonemic signals improves performance across both non-Latin and Latin script
languages, with a particularly significant impact on closing the performance
gap between the two. Through detailed experiments, we show that phonemic and
orthographic scripts retrieve distinct examples for in-context learning (ICL).
This motivates our proposed Mixed-ICL retrieval strategy, where further
aggregation from both leads to our significant performance improvements for
both Latin script languages (up to 12.6%) and non-Latin script languages (up to
15.1%) compared to randomized ICL retrieval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for NAACL 2025 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PQMass: Probabilistic Assessment of the Quality of Generative Models
  using Probability Mass Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04355v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04355v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Lemos, Sammy Sharief, Nikolay Malkin, Salma Salhi, Conner Stone, Laurence Perreault-Levasseur, Yashar Hezaveh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a likelihood-free method for comparing two distributions given
samples from each, with the goal of assessing the quality of generative models.
The proposed approach, PQMass, provides a statistically rigorous method for
assessing the performance of a single generative model or the comparison of
multiple competing models. PQMass divides the sample space into non-overlapping
regions and applies chi-squared tests to the number of data samples that fall
within each region, giving a p-value that measures the probability that the bin
counts derived from two sets of samples are drawn from the same multinomial
distribution. PQMass does not depend on assumptions regarding the density of
the true distribution, nor does it rely on training or fitting any auxiliary
models. We evaluate PQMass on data of various modalities and dimensions,
demonstrating its effectiveness in assessing the quality, novelty, and
diversity of generated samples. We further show that PQMass scales well to
moderately high-dimensional data and thus obviates the need for feature
extraction in practical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SMAC-R1: The Emergence of Intelligence in Decision-Making Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16024v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16024v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Deng, Weiyu Ma, Yuxin Fan, Ruyi Song, Yin Zhang, Haifeng Zhang, Jian Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  StarCraft Multi-Agent Challenge (SMAC) has been one of the most commonly used
experimental environments in multi-agent reinforcement learning (MARL), where
the specific task is to control a set number of allied units to defeat enemy
forces. Traditional MARL algorithms often require interacting with the
environment for millions of steps to train a parametric model, of which the
resulting policies are typically non-interpretable with weak transferability.
In this paper, we introduce SMAC-R1 which is based on the Qwen2.5-7B-Base LLM
distilled from DeepSeek-Coder-v2.5-236B. Similar to online reinforcement
learning after behavior cloning in offline learning process, in our pipeline,
agents leverage the DeepSeek LLM to generate decision tree code by providing
task descriptions, and the agents are further self-reflected using feedback
from the rewards provided by the environment. Based on that, we augment the
generated scripts to fine-tune a small LLM, Qwen2.5-7B-Base, to distill the
decision-making ability via Supervised Fine-Tuning (SFT) and enhance the script
generation ability by the Group Relative Policy Optimization (GRPO) algorithm.
We conduct experiments in the original 23 SMAC tasks and 10 newly-designed
tasks to demonstrate that our method can produce high-quality, interpretable
decision trees with minimal environmental exploration. Moreover, these scripts
exhibit strong transferability, successfully applying to homogeneous SMAC
environments without modification. We believe this approach offers a new
direction for solving decision-making tasks and domain-specific LLM training
pipelines in the future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PQMass: Probabilistic Assessment of the Quality of Generative Models
  using Probability Mass Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04355v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04355v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pablo Lemos, Sammy Sharief, Nikolay Malkin, Salma Salhi, Connor Stone, Laurence Perreault-Levasseur, Yashar Hezaveh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a likelihood-free method for comparing two distributions given
samples from each, with the goal of assessing the quality of generative models.
The proposed approach, PQMass, provides a statistically rigorous method for
assessing the performance of a single generative model or the comparison of
multiple competing models. PQMass divides the sample space into non-overlapping
regions and applies chi-squared tests to the number of data samples that fall
within each region, giving a p-value that measures the probability that the bin
counts derived from two sets of samples are drawn from the same multinomial
distribution. PQMass does not depend on assumptions regarding the density of
the true distribution, nor does it rely on training or fitting any auxiliary
models. We evaluate PQMass on data of various modalities and dimensions,
demonstrating its effectiveness in assessing the quality, novelty, and
diversity of generated samples. We further show that PQMass scales well to
moderately high-dimensional data and thus obviates the need for feature
extraction in practical applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-03-05T00:00:00Z">2025-03-05</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">95</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OTTER: A Vision-Language-Action Model with Text-Aware Visual Feature
  Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03734v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03734v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huang Huang, Fangchen Liu, Letian Fu, Tingfan Wu, Mustafa Mukadam, Jitendra Malik, Ken Goldberg, Pieter Abbeel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language-Action (VLA) models aim to predict robotic actions based on
visual observations and language instructions. Existing approaches require
fine-tuning pre-trained visionlanguage models (VLMs) as visual and language
features are independently fed into downstream policies, degrading the
pre-trained semantic alignments. We propose OTTER, a novel VLA architecture
that leverages these existing alignments through explicit, text-aware visual
feature extraction. Instead of processing all visual features, OTTER
selectively extracts and passes only task-relevant visual features that are
semantically aligned with the language instruction to the policy transformer.
This allows OTTER to keep the pre-trained vision-language encoders frozen.
Thereby, OTTER preserves and utilizes the rich semantic understanding learned
from large-scale pre-training, enabling strong zero-shot generalization
capabilities. In simulation and real-world experiments, OTTER significantly
outperforms existing VLA models, demonstrating strong zeroshot generalization
to novel objects and environments. Video, code, checkpoints, and dataset:
https://ottervla.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Active 6D Pose Estimation for Textureless Objects using <span class="highlight-title">Multi</span>-View RGB
  Frames 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Yang, Wenjie Xue, Sahar Ghavidel, Steven L. Waslander
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Estimating the 6D pose of textureless objects from RBG images is an important
problem in robotics. Due to appearance ambiguities, rotational symmetries, and
severe occlusions, single-view based 6D pose estimators are still unable to
handle a wide range of objects, motivating research towards multi-view pose
estimation and next-best-view prediction that addresses these limitations. In
this work, we propose a comprehensive active perception framework for
estimating the 6D poses of textureless objects using only RGB images. Our
approach is built upon a key idea: decoupling the 6D pose estimation into a
sequential two-step process can greatly improve both accuracy and efficiency.
First, we estimate the 3D translation of each object, resolving scale and depth
ambiguities inherent to RGB images. These estimates are then used to simplify
the subsequent task of determining the 3D orientation, which we achieve through
canonical scale template matching. Building on this formulation, we then
introduce an active perception strategy that predicts the next best camera
viewpoint to capture an RGB image, effectively reducing object pose uncertainty
and enhancing pose accuracy. We evaluate our method on the public ROBI dataset
as well as on a transparent object dataset that we created. When evaluated
using the same camera viewpoints, our multi-view pose estimation significantly
outperforms state-of-the-art approaches. Furthermore, by leveraging our
next-best-view strategy, our method achieves high object pose accuracy with
substantially fewer viewpoints than heuristic-based policies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Curating Demonstrations using Online Experience 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Annie S. Chen, Alec M. Lessing, Yuejiang Liu, Chelsea Finn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many robot demonstration datasets contain heterogeneous demonstrations of
varying quality. This heterogeneity may benefit policy pre-training, but can
hinder robot performance when used with a final imitation learning objective.
In particular, some strategies in the data may be less reliable than others or
may be underrepresented in the data, leading to poor performance when such
strategies are sampled at test time. Moreover, such unreliable or
underrepresented strategies can be difficult even for people to discern, and
sifting through demonstration datasets is time-consuming and costly. On the
other hand, policy performance when trained on such demonstrations can reflect
the reliability of different strategies. We thus propose for robots to
self-curate based on online robot experience (Demo-SCORE). More specifically,
we train and cross-validate a classifier to discern successful policy roll-outs
from unsuccessful ones and use the classifier to filter heterogeneous
demonstration datasets. Our experiments in simulation and the real world show
that Demo-SCORE can effectively identify suboptimal demonstrations without
manual curation. Notably, Demo-SCORE achieves over 15-35% higher absolute
success rate in the resulting policy compared to the base policy trained with
all original demonstrations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Negative Damping Control for User-Dependent <span class="highlight-title">Multi</span>-Terrain
  Walking Assistance with a Hip Exoskeleton 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03662v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03662v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giulia Ramella, Auke Ijspeert, Mohamed Bouri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hip exoskeletons are known for their versatility in assisting users across
varied scenarios. However, current assistive strategies often lack the
flexibility to accommodate for individual walking patterns and adapt to diverse
locomotion environments. In this work, we present a novel control strategy that
adapts the mechanical impedance of the human-exoskeleton system. We design the
hip assistive torques as an adaptive virtual negative damping, which is able to
inject energy into the system while allowing the users to remain in control and
contribute voluntarily to the movements. Experiments with five healthy subjects
demonstrate that our controller reduces the metabolic cost of walking compared
to free walking (average reduction of 7.2%), and it preserves the lower-limbs
kinematics. Additionally, our method achieves minimal power losses from the
exoskeleton across the entire gait cycle (less than 2% negative mechanical
power out of the total power), ensuring synchronized action with the users'
movements. Moreover, we use Bayesian Optimization to adapt the assistance
strength and allow for seamless adaptation and transitions across multi-terrain
environments. Our strategy achieves efficient power transmission under all
conditions. Our approach demonstrates an individualized, adaptable, and
straightforward controller for hip exoskeletons, advancing the development of
viable, adaptive, and user-dependent control laws.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Copyright 2025 IEEE. Personal use of this material is permitted.
  Permission from IEEE must be obtained for all other uses, in any current or
  future media, including reprinting/republishing this material for advertising
  or promotional purposes, creating new collective works, for resale or
  redistribution to servers or lists, or reuse of any copyrighted component of
  this work in other works</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Motion Planning and Control with Unknown Nonlinear Dynamics through
  Predicted Reachability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiquan Zhang, Gokul Puthumanaillam, Manav Vora, Melkior Ornik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous motion planning under unknown nonlinear dynamics presents
significant challenges. An agent needs to continuously explore the system
dynamics to acquire its properties, such as reachability, in order to guide
system navigation adaptively. In this paper, we propose a hybrid
planning-control framework designed to compute a feasible trajectory toward a
target. Our approach involves partitioning the state space and approximating
the system by a piecewise affine (PWA) system with constrained control inputs.
By abstracting the PWA system into a directed weighted graph, we incrementally
update the existence of its edges via affine system identification and reach
control theory, introducing a predictive reachability condition by exploiting
prior information of the unknown dynamics. Heuristic weights are assigned to
edges based on whether their existence is certain or remains indeterminate.
Consequently, we propose a framework that adaptively collects and analyzes data
during mission execution, continually updates the predictive graph, and
synthesizes a controller online based on the graph search outcomes. We
demonstrate the efficacy of our approach through simulation scenarios involving
a mobile robot operating in unknown terrains, with its unknown dynamics
abstracted as a single integrator model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TeraSim: Uncovering Unknown Unsafe Events for Autonomous Vehicles
  through Generative Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03629v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03629v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haowei Sun, Xintao Yan, Zhijie Qiao, Haojie Zhu, Yihao Sun, Jiawei Wang, Shengyin Shen, Darian Hogue, Rajanikant Ananta, Derek Johnson, Greg Stevens, Greg McGuire, Yifan Wei, Wei Zheng, Yong Sun, Yasuo Fukai, Henry X. Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traffic simulation is essential for autonomous vehicle (AV) development,
enabling comprehensive safety evaluation across diverse driving conditions.
However, traditional rule-based simulators struggle to capture complex human
interactions, while data-driven approaches often fail to maintain long-term
behavioral realism or generate diverse safety-critical events. To address these
challenges, we propose TeraSim, an open-source, high-fidelity traffic
simulation platform designed to uncover unknown unsafe events and efficiently
estimate AV statistical performance metrics, such as crash rates. TeraSim is
designed for seamless integration with third-party physics simulators and
standalone AV stacks, to construct a complete AV simulation system.
Experimental results demonstrate its effectiveness in generating diverse
safety-critical events involving both static and dynamic agents, identifying
hidden deficiencies in AV systems, and enabling statistical performance
evaluation. These findings highlight TeraSim's potential as a practical tool
for AV safety assessment, benefiting researchers, developers, and policymakers.
The code is available at https://github.com/mcity/TeraSim.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REGRACE: A Robust and Efficient Graph-based Re-localization Algorithm
  using Consistency Evaluation <span class="chip">IROS2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03599v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03599v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Débora N. P. Oliveira, Joshua Knights, Sebastián Barbas Laina, Simon Boche, Wolfram Burgard, Stefan Leutenegger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Loop closures are essential for correcting odometry drift and creating
consistent maps, especially in the context of large-scale navigation. Current
methods using dense point clouds for accurate place recognition do not scale
well due to computationally expensive scan-to-scan comparisons. Alternative
object-centric approaches are more efficient but often struggle with
sensitivity to viewpoint variation. In this work, we introduce REGRACE, a novel
approach that addresses these challenges of scalability and perspective
difference in re-localization by using LiDAR-based submaps. We introduce
rotation-invariant features for each labeled object and enhance them with
neighborhood context through a graph neural network. To identify potential
revisits, we employ a scalable bag-of-words approach, pooling one learned
global feature per submap. Additionally, we define a revisit with geometrical
consistency cues rather than embedding distance, allowing us to recognize
far-away loop closures. Our evaluations demonstrate that REGRACE achieves
similar results compared to state-of-the-art place recognition and registration
baselines while being twice as fast.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Generative System for Robot-to-Human Handovers: from Intent Inference
  to Spatial Configuration Imagery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanxin Zhang, Abdulqader Dhafer, Zhou Daniel Hao, Hongbiao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel system for robot-to-human object handover that emulates
human coworker interactions. Unlike most existing studies that focus primarily
on grasping strategies and motion planning, our system focus on 1. inferring
human handover intents, 2. imagining spatial handover configuration. The first
one integrates multimodal perception-combining visual and verbal cues-to infer
human intent. The second one using a diffusion-based model to generate the
handover configuration, involving the spacial relationship among robot's
gripper, the object, and the human hand, thereby mimicking the cognitive
process of motor imagery. Experimental results demonstrate that our approach
effectively interprets human cues and achieves fluent, human-like handovers,
offering a promising solution for collaborative robotics. Code, videos, and
data are available at: https://i3handover.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Olympus: A Jumping Quadruped for Planetary Exploration Utilizing
  Reinforcement Learning for In-Flight Attitude Control <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jørgen Anker Olsen, Grzegorz Malczyk, Kostas Alexis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploring planetary bodies with lower gravity, such as the moon and Mars,
allows legged robots to utilize jumping as an efficient form of locomotion thus
giving them a valuable advantage over traditional rovers for exploration.
Motivated by this fact, this paper presents the design, simulation, and
learning-based "in-flight" attitude control of Olympus, a jumping legged robot
tailored to the gravity of Mars. First, the design requirements are outlined
followed by detailing how simulation enabled optimizing the robot's design -
from its legs to the overall configuration - towards high vertical jumping,
forward jumping distance, and in-flight attitude reorientation. Subsequently,
the reinforcement learning policy used to track desired in-flight attitude
maneuvers is presented. Successfully crossing the sim2real gap, extensive
experimental studies of attitude reorientation tests are demonstrated.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures, Accepted to the IEEE International Conference on
  Robotics and Automation (ICRA) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Afford-X: Generalizable and Slim Affordance Reasoning for Task-oriented
  Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaomeng Zhu, Yuyang Li, Leiyao Cui, Pengfei Li, Huan-ang Gao, Yixin Zhu, Hao Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object affordance reasoning, the ability to infer object functionalities
based on physical properties, is fundamental for task-oriented planning and
activities in both humans and Artificial Intelligence (AI). This capability,
required for planning and executing daily activities in a task-oriented manner,
relies on commonsense knowledge of object physics and functionalities,
extending beyond simple object recognition. Current computational models for
affordance reasoning from perception lack generalizability, limiting their
applicability in novel scenarios. Meanwhile, comprehensive Large Language
Models (LLMs) with emerging reasoning capabilities are challenging to deploy on
local devices for task-oriented manipulations. Here, we introduce LVIS-Aff, a
large-scale dataset comprising 1,496 tasks and 119k images, designed to enhance
the generalizability of affordance reasoning from perception. Utilizing this
dataset, we develop Afford-X, an end-to-end trainable affordance reasoning
model that incorporates Verb Attention and Bi-Fusion modules to improve
multi-modal understanding. This model achieves up to a 12.1% performance
improvement over the best-reported results from non-LLM methods, while also
demonstrating a 1.2% enhancement compared to our previous conference paper.
Additionally, it maintains a compact 187M parameter size and infers nearly 50
times faster than the GPT-4V API. Our work demonstrates the potential for
efficient, generalizable affordance reasoning models that can be deployed on
local devices for task-oriented manipulations. We showcase Afford-X's
effectiveness in enabling task-oriented manipulations for robots across various
tasks and environments, underscoring its efficiency and broad implications for
advancing robotics and AI systems in real-world applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unified Human Localization and Trajectory Prediction with Monocular
  Vision <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Po-Chien Luan, Yang Gao, Celine Demonsant, Alexandre Alahi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional human trajectory prediction models rely on clean curated data,
requiring specialized equipment or manual labeling, which is often impractical
for robotic applications. The existing predictors tend to overfit to clean
observation affecting their robustness when used with noisy inputs. In this
work, we propose MonoTransmotion (MT), a Transformer-based framework that uses
only a monocular camera to jointly solve localization and prediction tasks. Our
framework has two main modules: Bird's Eye View (BEV) localization and
trajectory prediction. The BEV localization module estimates the position of a
person using 2D human poses, enhanced by a novel directional loss for smoother
sequential localizations. The trajectory prediction module predicts future
motion from these estimates. We show that by jointly training both tasks with
our unified framework, our method is more robust in real-world scenarios made
of noisy inputs. We validate our MT network on both curated and non-curated
datasets. On the curated dataset, MT achieves around 12% improvement over
baseline models on BEV localization and trajectory prediction. On real-world
non-curated dataset, experimental results indicate that MT maintains similar
performance levels, highlighting its robustness and generalization capability.
The code is available at https://github.com/vita-epfl/MonoTransmotion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuGrasp: Generalizable Neural Surface Reconstruction with Background
  Priors for Material-Agnostic Object Grasp Detection <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyu Fan, Yinghao Cai, Chao Li, Wenzhe He, Xudong Zheng, Tao Lu, Bin Liang, Shuo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic grasping in scenes with transparent and specular objects presents
great challenges for methods relying on accurate depth information. In this
paper, we introduce NeuGrasp, a neural surface reconstruction method that
leverages background priors for material-agnostic grasp detection. NeuGrasp
integrates transformers and global prior volumes to aggregate multi-view
features with spatial encoding, enabling robust surface reconstruction in
narrow and sparse viewing conditions. By focusing on foreground objects through
residual feature enhancement and refining spatial perception with an
occupancy-prior volume, NeuGrasp excels in handling objects with transparent
and specular surfaces. Extensive experiments in both simulated and real-world
scenarios show that NeuGrasp outperforms state-of-the-art methods in grasping
while maintaining comparable reconstruction quality. More details are available
at https://neugrasp.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures. IEEE International Conference on Robotics and
  Automation (ICRA) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Benchmark for Optimal <span class="highlight-title">Multi</span>-Modal <span class="highlight-title">Multi</span>-Robot <span class="highlight-title">Multi</span>-Goal Path Planning
  with Given Robot Assignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentin N. Hartmann, Tirza Heinle, Stelian Coros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many industrial robotics applications, multiple robots are working in a
shared workspace to complete a set of tasks as quickly as possible. Such
settings can be treated as multi-modal multi-robot multi-goal path planning
problems, where each robot has to reach an ordered sequence of goals. Existing
approaches to this type of problem solve this using prioritization or assume
synchronous completion of tasks, and are thus neither optimal nor complete. We
formalize this problem as a single path planning problem and introduce a
benchmark encompassing a diverse range of problem instances including scenarios
with various robots, planning horizons, and collaborative tasks such as
handovers. Along with the benchmark, we adapt an RRT* and a PRM* planner to
serve as a baseline for the planning problems. Both planners work in the
composite space of all robots and introduce the required changes to work in our
setting. Unlike existing approaches, our planner and formulation is not
restricted to discretized 2D workspaces, supports a changing environment, and
works for heterogeneous robot teams over multiple modes with different
constraints, and multiple goals. Videos and code for the benchmark and the
planners is available at https://vhartman.github.io/mrmg-planning/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Coordinated Trajectories for Non-stop Flying Carriers Holding a
  Cable-Suspended Load 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03481v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03481v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chiara Gabellieri, Antonio Franchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multirotor UAVs have been typically considered for aerial manipulation, but
their scarce endurance prevents long-lasting manipulation tasks. This work
demonstrates that the non-stop flights of three or more carriers are compatible
with holding a constant pose of a cable-suspended load, thus potentially
enabling aerial manipulation with energy-efficient non-stop carriers. It also
presents an algorithm for generating the coordinated non-stop trajectories. The
proposed method builds upon two pillars: (1)~the choice of $n$ special linearly
independent directions of internal forces within the $3n-6$-dimensional
nullspace of the grasp matrix of the load, chosen as the edges of a Hamiltonian
cycle on the graph that connects the cable attachment points on the load.
Adjacent pairs of directions are used to generate $n$ forces evolving on
distinct 2D affine subspaces, despite the attachment points being generically
in 3D; (2)~the construction of elliptical trajectories within these subspaces
by mapping, through appropriate graph coloring, each edge of the Hamiltonian
cycle to a periodic coordinate while ensuring that no adjacent coordinates
exhibit simultaneous zero derivatives. Combined with conditions for load
statics and attachment point positions, these choices ensure that each of the
$n$ force trajectories projects onto the corresponding cable constraint sphere
with non-zero tangential velocity, enabling perpetual motion of the carriers
while the load is still. The theoretical findings are validated through
simulations and laboratory experiments with non-stopping multirotor UAVs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via
  Safe Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Borong Zhang, Yuhao Zhang, Jiaming Ji, Yingshan Lei, Josef Dai, Yuanpei Chen, Yaodong Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language-action models (VLAs) have shown great potential as generalist
robot policies. However, these models pose urgent safety challenges during
deployment, including the risk of physical harm to the environment, the robot
itself, and humans. How can safety be explicitly incorporated into VLAs? In
this work, we propose SafeVLA, a novel algorithm designed to integrate safety
into VLAs, ensuring the protection of the environment, robot hardware and
humans in real-world settings. SafeVLA effectively balances safety and task
performance by employing large-scale constrained learning within simulated
environments. We demonstrate that SafeVLA outperforms the current
state-of-the-art method in both safety and task performance, achieving average
improvements of 83.58% and 3.85%, respectively, in simulation. By prioritizing
safety, our approach eliminates high-risk behaviors and reduces the upper bound
of unsafe behaviors to 1/35 of that in the current state-of-the-art, thereby
significantly mitigating long-tail risks. Furthermore, the learned safety
constraints generalize to diverse, unseen scenarios, including multiple
out-of-distribution perturbations and tasks. Our data, models and newly
proposed benchmark environment are available at
https://sites.google.com/view/pku-safevla.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> Continuous Control of Diverse Skills in Quadruped Robots Without
  Complete Expert <span class="highlight-title">Dataset</span>s <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxin Tu, Xiaoyi Wei, Yueqi Zhang, Taixian Hou, Xiao<span class="highlight-author">fei Gao</span>, Zhiyan Dong, Peng Zhai, Lihua Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning diverse skills for quadruped robots presents significant challenges,
such as mastering complex transitions between different skills and handling
tasks of varying difficulty. Existing imitation learning methods, while
successful, rely on expensive datasets to reproduce expert behaviors. Inspired
by introspective learning, we propose Progressive Adversarial Self-Imitation
Skill Transition (PASIST), a novel method that eliminates the need for complete
expert datasets. PASIST autonomously explores and selects high-quality
trajectories based on predefined target poses instead of demonstrations,
leveraging the Generative Adversarial Self-Imitation Learning (GASIL)
framework. To further enhance learning, We develop a skill selection module to
mitigate mode collapse by balancing the weights of skills with varying levels
of difficulty. Through these methods, PASIST is able to reproduce skills
corresponding to the target pose while achieving smooth and natural transitions
between them. Evaluations on both simulation platforms and the Solo 8 robot
confirm the effectiveness of PASIST, offering an efficient alternative to
expert-driven learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Artificial Intelligence in Robotic Manipulation: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03464v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03464v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Zhang, Peng Yun, Jun Cen, Junhao Cai, Didi Zhu, Hangjie Yuan, Chao Zhao, Tao Feng, Michael Yu Wang, Qifeng Chen, Jia Pan, Bo Yang, Hua Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This survey provides a comprehensive review on recent advancements of
generative learning models in robotic manipulation, addressing key challenges
in the field. Robotic manipulation faces critical bottlenecks, including
significant challenges in insufficient data and inefficient data acquisition,
long-horizon and complex task planning, and the multi-modality reasoning
ability for robust policy learning performance across diverse environments. To
tackle these challenges, this survey introduces several generative model
paradigms, including Generative Adversarial Networks (GANs), Variational
Autoencoders (VAEs), diffusion models, probabilistic flow models, and
autoregressive models, highlighting their strengths and limitations. The
applications of these models are categorized into three hierarchical layers:
the Foundation Layer, focusing on data generation and reward generation; the
Intermediate Layer, covering language, code, visual, and state generation; and
the Policy Layer, emphasizing grasp generation and trajectory generation. Each
layer is explored in detail, along with notable works that have advanced the
state of the art. Finally, the survey outlines future research directions and
challenges, emphasizing the need for improved efficiency in data utilization,
better handling of long-horizon tasks, and enhanced generalization across
diverse robotic scenarios. All the related resources, including research
papers, open-source data, and projects, are collected for the community in
https://github.com/GAI4Manipulation/AwesomeGAIManipulation
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tiny Lidars for Manipulator Self-Awareness: Sensor Characterization and
  Initial Localization Experiments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03449v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03449v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giammarco Caroleo, Alessandro Albini, Daniele De Martini, Timothy D. Barfoot, Perla Maiolino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For several tasks, ranging from manipulation to inspection, it is beneficial
for robots to localize a target object in their surroundings. In this paper, we
propose an approach that utilizes coarse point clouds obtained from
miniaturized VL53L5CX Time-of-Flight (ToF) sensors (tiny lidars) to localize a
target object in the robot's workspace. We first conduct an experimental
campaign to calibrate the dependency of sensor readings on relative range and
orientation to targets. We then propose a probabilistic sensor model that is
validated in an object pose estimation task using a Particle Filter (PF). The
results show that the proposed sensor model improves the performance of the
localization of the target object with respect to two baselines: one that
assumes measurements are free from uncertainty and one in which the confidence
is provided by the sensor datasheet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures, 3 tables, conference submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ REACT: Real-time Efficient Attribute Clustering and Transfer for
  Updatable 3D Scene Graph <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phuoc Nguyen, Francesco Verdoja, Ville Kyrki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern-day autonomous robots need high-level map representations to perform
sophisticated tasks. Recently, 3D scene graphs (3DSGs) have emerged as a
promising alternative to traditional grid maps, blending efficient memory use
and rich feature representation. However, most efforts to apply them have been
limited to static worlds. This work introduces REACT, a framework that
efficiently performs real-time attribute clustering and transfer to relocalize
object nodes in a 3DSG. REACT employs a novel method for comparing object
instances using an embedding model trained on triplet loss, facilitating
instance clustering and matching. Experimental results demonstrate that REACT
is able to relocalize objects while maintaining computational efficiency. The
REACT framework's source code will be available as an open-source project,
promoting further advancements in reusable and updatable 3DSGs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Direct Sparse Odometry with Continuous 3D Gaussian Maps for Indoor
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03373v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03373v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Deng, Fengtian Lang, Zikang Yuan, Xin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate localization is essential for robotics and augmented reality
applications such as autonomous navigation. Vision-based methods combining
prior maps aim to integrate LiDAR-level accuracy with camera cost efficiency
for robust pose estimation. Existing approaches, however, often depend on
unreliable interpolation procedures when associating discrete point cloud maps
with dense image pixels, which inevitably introduces depth errors and degrades
pose estimation accuracy. We propose a monocular visual odometry framework
utilizing a continuous 3D Gaussian map, which directly assigns geometrically
consistent depth values to all extracted high-gradient points without
interpolation. Evaluations on two public datasets demonstrate superior tracking
accuracy compared to existing methods. We have released the source code of this
work for the development of the community.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages,5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Navigating Intelligence: A <span class="highlight-title">Survey</span> of Google OR-Tools and Machine
  Learning for Global Path Planning in Autonomous Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Benoit, Pedram Asef
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We offer a new in-depth investigation of global path planning (GPP) for
unmanned ground vehicles, an autonomous mining sampling robot named ROMIE. GPP
is essential for ROMIE's optimal performance, which is translated into solving
the traveling salesman problem, a complex graph theory challenge that is
crucial for determining the most effective route to cover all sampling
locations in a mining field. This problem is central to enhancing ROMIE's
operational efficiency and competitiveness against human labor by optimizing
cost and time. The primary aim of this research is to advance GPP by
developing, evaluating, and improving a cost-efficient software and web
application. We delve into an extensive comparison and analysis of Google
operations research (OR)-Tools optimization algorithms. Our study is driven by
the goal of applying and testing the limits of OR-Tools capabilities by
integrating Reinforcement Learning techniques for the first time. This enables
us to compare these methods with OR-Tools, assessing their computational
effectiveness and real-world application efficiency. Our analysis seeks to
provide insights into the effectiveness and practical application of each
technique. Our findings indicate that Q-Learning stands out as the optimal
strategy, demonstrating superior efficiency by deviating only 1.2% on average
from the optimal solutions across our datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Supervised Visual Docking Network for Unmanned Surface Vehicles Using
  Auto-labeling in Real-wo<span class="highlight-title">rl</span>d Water Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijie Chu, Ziniu Wu, Yong Yue, Eng Gee Lim, Paolo Paoletti, Xiaohui Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unmanned Surface Vehicles (USVs) are increasingly applied to water operations
such as environmental monitoring and river-map modeling. It faces a significant
challenge in achieving precise autonomous docking at ports or stations, still
relying on remote human control or external positioning systems for accuracy
and safety which limits the full potential of human-out-of-loop deployment for
USVs.This paper introduces a novel supervised learning pipeline with the
auto-labeling technique for USVs autonomous visual docking. Firstly, we
designed an auto-labeling data collection pipeline that appends relative pose
and image pair to the dataset. This step does not require conventional manual
labeling for supervised learning. Secondly, the Neural Dock Pose Estimator
(NDPE) is proposed to achieve relative dock pose prediction without the need
for hand-crafted feature engineering, camera calibration, and peripheral
markers. Moreover, The NDPE can accurately predict the relative dock pose in
real-world water environments, facilitating the implementation of
Position-Based Visual Servo (PBVS) and low-level motion controllers for
efficient and autonomous docking.Experiments show that the NDPE is robust to
the disturbance of the distance and the USV velocity. The effectiveness of our
proposed solution is tested and validated in real-world water environments,
reflecting its capability to handle real-world autonomous docking tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trajectory Prediction for Autonomous Driving: Progress, Limitations, and
  Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nadya Abdel Madjid, Abdulrahman Ahmad, Murad Mebrahtu, Yousef Babaa, Abdelmoamen Nasser, Sumbal Malik, Bilal Hassan, Naoufel Werghi, Jorge Dias, Majid Khonji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the potential for autonomous vehicles to be integrated on a large scale
into modern traffic systems continues to grow, ensuring safe navigation in
dynamic environments is crucial for smooth integration. To guarantee safety and
prevent collisions, autonomous vehicles must be capable of accurately
predicting the trajectories of surrounding traffic agents. Over the past
decade, significant efforts from both academia and industry have been dedicated
to designing solutions for precise trajectory forecasting. These efforts have
produced a diverse range of approaches, raising questions about the differences
between these methods and whether trajectory prediction challenges have been
fully addressed. This paper reviews a substantial portion of recent trajectory
prediction methods and devises a taxonomy to classify existing solutions. A
general overview of the prediction pipeline is also provided, covering input
and output modalities, modeling features, and prediction paradigms discussed in
the literature. In addition, the paper discusses active research areas within
trajectory prediction, addresses the posed research questions, and highlights
the remaining research gaps and challenges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SCORE: Saturated Consensus Relocalization in Semantic Line Maps <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haodong Jiang, Xiang Zheng, Yanglin Zhang, Qingcheng Zeng, Yiqian Li, Ziyang Hong, Junfeng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This is the arxiv version for our paper submitted to IEEE/RSJ IROS 2025. We
propose a scene-agnostic and light-weight visual relocalization framework that
leverages semantically labeled 3D lines as a compact map representation. In our
framework, the robot localizes itself by capturing a single image, extracting
2D lines, associating them with semantically similar 3D lines in the map, and
solving a robust perspective-n-line problem. To address the extremely high
outlier ratios~(exceeding 99.5\%) caused by one-to-many ambiguities in semantic
matching, we introduce the Saturated Consensus Maximization~(Sat-CM)
formulation, which enables accurate pose estimation when the classic Consensus
Maximization framework fails. We further propose a fast global solver to the
formulated Sat-CM problems, leveraging rigorous interval analysis results to
ensure both accuracy and computational efficiency. Additionally, we develop a
pipeline for constructing semantic 3D line maps using posed depth images. To
validate the effectiveness of our framework, which integrates our innovations
in robust estimation and practical engineering insights, we conduct extensive
experiments on the ScanNet++ dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 14 figurs, arxiv version for paper submitted to IROS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STORM: Spatial-Temporal Iterative Optimization for Reliable <span class="highlight-title">Multi</span>copter
  Trajectory Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinhao Zhang, Zhexuan Zhou, Wenlong Xia, Youmin Gong, Jie Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient and safe trajectory planning plays a critical role in the
application of quadrotor unmanned aerial vehicles. Currently, the inherent
trade-off between constraint compliance and computational efficiency
enhancement in UAV trajectory optimization problems has not been sufficiently
addressed. To enhance the performance of UAV trajectory optimization, we
propose a spatial-temporal iterative optimization framework. Firstly, B-splines
are utilized to represent UAV trajectories, with rigorous safety assurance
achieved through strict enforcement of constraints on control points.
Subsequently, a set of QP-LP subproblems via spatial-temporal decoupling and
constraint linearization is derived. Finally, an iterative optimization
strategy incorporating guidance gradients is employed to obtain
high-performance UAV trajectories in different scenarios. Both simulation and
real-world experimental results validate the efficiency and high-performance of
the proposed optimization framework in generating safe and fast trajectories.
Our source codes will be released for community reference at
https://hitsz-mas.github.io/STORM
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Social Gesture Recognition in spHRI: Leveraging Fabric-Based Tactile
  Sensing on Humanoid Robots <span class="chip">ICRA 25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03234v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03234v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dakarai Crowder, Kojo Vandyck, Xiping Sun, James McCann, Wenzhen Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans are able to convey different messages using only touch. Equipping
robots with the ability to understand social touch adds another modality in
which humans and robots can communicate. In this paper, we present a social
gesture recognition system using a fabric-based, large-scale tactile sensor
integrated onto the arms of a humanoid robot. We built a social gesture dataset
using multiple participants and extracted temporal features for classification.
By collecting real-world data on a humanoid robot, our system provides valuable
insights into human-robot social touch, further advancing the development of
spHRI systems for more natural and effective communication.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICRA 25. 8 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OpenGV 2.0: Motion prior-assisted calibration and <span class="highlight-title">SLAM</span> with
  vehicle-mounted surround-view systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03230v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03230v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kun Huang, Yifu Wang, Si'ao Zhang, Zhirui Wang, Zhanpeng Ouyang, Zhenghua Yu, Laurent Kneip
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The present paper proposes optimization-based solutions to visual SLAM with a
vehicle-mounted surround-view camera system. Owing to their original use-case,
such systems often only contain a single camera facing into either direction
and very limited overlap between fields of view. Our novelty consist of three
optimization modules targeting at practical online calibration of exterior
orientations from simple two-view geometry, reliable front-end initialization
of relative displacements, and accurate back-end optimization using a
continuous-time trajectory model. The commonality between the proposed modules
is given by the fact that all three of them exploit motion priors that are
related to the inherent non-holonomic characteristics of passenger vehicle
motion. In contrast to prior related art, the proposed modules furthermore
excel in terms of bypassing partial unobservabilities in the transformation
variables that commonly occur for Ackermann-motion. As a further contribution,
the modules are built into a novel surround-view camera SLAM system that
specifically targets deployment on Ackermann vehicles operating in urban
environments. All modules are studied in the context of in-depth ablation
studies, and the practical validity of the entire framework is supported by a
successful application to challenging, large-scale publicly available online
datasets. Note that upon acceptance, the entire framework is scheduled for
open-source release as part of an extension of the OpenGV library.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Embodied Escaping: End-to-End Reinforcement Learning for Robot
  Navigation in Narrow Environment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Zheng, Jiale Zhang, Mingyang Jiang, Peiyuan Liu, Danni Liu, Tong Qin, Ming Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous navigation is a fundamental task for robot vacuum cleaners in
indoor environments. Since their core function is to clean entire areas, robots
inevitably encounter dead zones in cluttered and narrow scenarios. Existing
planning methods often fail to escape due to complex environmental constraints,
high-dimensional search spaces, and high difficulty maneuvers. To address these
challenges, this paper proposes an embodied escaping model that leverages
reinforcement learning-based policy with an efficient action mask for dead zone
escaping. To alleviate the issue of the sparse reward in training, we introduce
a hybrid training policy that improves learning efficiency. In handling
redundant and ineffective action options, we design a novel action
representation to reshape the discrete action space with a uniform turning
radius. Furthermore, we develop an action mask strategy to select valid action
quickly, balancing precision and efficiency. In real-world experiments, our
robot is equipped with a Lidar, IMU, and two-wheel encoders. Extensive
quantitative and qualitative experiments across varying difficulty levels
demonstrate that our robot can consistently escape from challenging dead zones.
Moreover, our approach significantly outperforms compared path planning and
reinforcement learning methods in terms of success rate and collision
avoidance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transformer-Based Spatio-Temporal Association of Apple Fruitlets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03200v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03200v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Harry Freeman, George Kantor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a transformer-based method to spatio-temporally
associate apple fruitlets in stereo-images collected on different days and from
different camera poses. State-of-the-art association methods in agriculture are
dedicated towards matching larger crops using either high-resolution point
clouds or temporally stable features, which are both difficult to obtain for
smaller fruit in the field. To address these challenges, we propose a
transformer-based architecture that encodes the shape and position of each
fruitlet, and propagates and refines these features through a series of
transformer encoder layers with alternating self and cross-attention. We
demonstrate that our method is able to achieve an F1-score of 92.4% on data
collected in a commercial apple orchard and outperforms all baselines and
ablations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpiritSight Agent: Advanced GUI Agent with One Look 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03196v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03196v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Huang, Ziming Cheng, Junting Pan, Zhaohui Hou, Mingjie Zhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graphical User Interface (GUI) agents show amazing abilities in assisting
human-computer interaction, automating human user's navigation on digital
devices. An ideal GUI agent is expected to achieve high accuracy, low latency,
and compatibility for different GUI platforms. Recent vision-based approaches
have shown promise by leveraging advanced Vision Language Models (VLMs). While
they generally meet the requirements of compatibility and low latency, these
vision-based GUI agents tend to have low accuracy due to their limitations in
element grounding. To address this issue, we propose $\textbf{SpiritSight}$, a
vision-based, end-to-end GUI agent that excels in GUI navigation tasks across
various GUI platforms. First, we create a multi-level, large-scale,
high-quality GUI dataset called $\textbf{GUI-Lasagne}$ using scalable methods,
empowering SpiritSight with robust GUI understanding and grounding
capabilities. Second, we introduce the $\textbf{Universal Block Parsing (UBP)}$
method to resolve the ambiguity problem in dynamic high-resolution of visual
inputs, further enhancing SpiritSight's ability to ground GUI objects. Through
these efforts, SpiritSight agent outperforms other advanced methods on diverse
GUI benchmarks, demonstrating its superior capability and compatibility in GUI
navigation tasks. Models are available at
$\href{https://huggingface.co/SenseLLM/SpiritSight-Agent-8B}{this\ URL}$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted to CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Distributed</span> Certifiably Correct Range-Aided <span class="highlight-title">SLAM</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03192v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03192v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Thoms, Alan Papalia, Jared Velasquez, David M. Rosen, Sriram Narasimhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable simultaneous localization and mapping (SLAM) algorithms are
necessary for safety-critical autonomous navigation. In the
communication-constrained multi-agent setting, navigation systems increasingly
use point-to-point range sensors as they afford measurements with low bandwidth
requirements and known data association. The state estimation problem for these
systems takes the form of range-aided (RA) SLAM. However, distributed
algorithms for solving the RA-SLAM problem lack formal guarantees on the
quality of the returned estimate. To this end, we present the first distributed
algorithm for RA-SLAM that can efficiently recover certifiably globally optimal
solutions. Our algorithm, distributed certifiably correct RA-SLAM (DCORA),
achieves this via the Riemannian Staircase method, where computational
procedures developed for distributed certifiably correct pose graph
optimization are generalized to the RA-SLAM problem. We demonstrate DCORA's
efficacy on real-world multi-agent datasets by achieving absolute trajectory
errors comparable to those of a state-of-the-art centralized certifiably
correct RA-SLAM algorithm. Additionally, we perform a parametric study on the
structure of the RA-SLAM problem using synthetic data, revealing how common
parameters affect DCORA's performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures, accepted to 2025 International Conference on
  Robotics and Automation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causality-Based Reinforcement Learning Method for <span class="highlight-title">Multi</span>-Stage Robotic
  Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03145v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03145v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiechao Deng, Ning Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep reinforcement learning has made significant strides in various robotic
tasks. However, employing deep reinforcement learning methods to tackle
multi-stage tasks still a challenge. Reinforcement learning algorithms often
encounter issues such as redundant exploration, getting stuck in dead ends, and
progress reversal in multi-stage tasks. To address this, we propose a method
that integrates causal relationships with reinforcement learning for
multi-stage tasks. Our approach enables robots to automatically discover the
causal relationships between their actions and the rewards of the tasks and
constructs the action space using only causal actions, thereby reducing
redundant exploration and progress reversal. By integrating correct causal
relationships using the causal policy gradient method into the learning
process, our approach can enhance the performance of reinforcement learning
algorithms in multi-stage robotic tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Don't Shake the Wheel: Momentum-Aware Planning in End-to-End Autonomous
  Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziying Song, Caiyan Jia, Lin Liu, Hongyu Pan, Yongchang Zhang, Junming Wang, Xingyu Zhang, Shaoqing Xu, Lei Yang, Yadan Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end autonomous driving frameworks enable seamless integration of
perception and planning but often rely on one-shot trajectory prediction, which
may lead to unstable control and vulnerability to occlusions in single-frame
perception. To address this, we propose the Momentum-Aware Driving (MomAD)
framework, which introduces trajectory momentum and perception momentum to
stabilize and refine trajectory predictions. MomAD comprises two core
components: (1) Topological Trajectory Matching (TTM) employs Hausdorff
Distance to select the optimal planning query that aligns with prior paths to
ensure coherence;(2) Momentum Planning Interactor (MPI) cross-attends the
selected planning query with historical queries to expand static and dynamic
perception files. This enriched query, in turn, helps regenerate long-horizon
trajectory and reduce collision risks. To mitigate noise arising from dynamic
environments and detection errors, we introduce robust instance denoising
during training, enabling the planning model to focus on critical signals and
improve its robustness. We also propose a novel Trajectory Prediction
Consistency (TPC) metric to quantitatively assess planning stability.
Experiments on the nuScenes dataset demonstrate that MomAD achieves superior
long-term consistency (>=3s) compared to SOTA methods. Moreover, evaluations on
the curated Turning-nuScenes shows that MomAD reduces the collision rate by 26%
and improves TPC by 0.97m (33.45%) over a 6s prediction horizon, while
closedloop on Bench2Drive demonstrates an up to 16.3% improvement in success
rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Car-STAGE: Automated framework for large-scale high-dimensional
  simulated time-series data generation based on user-defined criteria 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03100v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03100v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Asma A. Almutairi, David J. LeBlanc, Arpan Kusari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating large-scale sensing datasets through photo-realistic simulation is
an important aspect of many robotics applications such as autonomous driving.
In this paper, we consider the problem of synchronous data collection from the
open-source CARLA simulator using multiple sensors attached to vehicle based on
user-defined criteria. We propose a novel, one-step framework that we refer to
as Car-STAGE, based on CARLA simulator, to generate data using a graphical user
interface (GUI) defining configuration parameters to data collection without
any user intervention. This framework can utilize the user-defined
configuration parameters such as choice of maps, number and configurations of
sensors, environmental and lighting conditions etc. to run the simulation in
the background, collecting high-dimensional sensor data from diverse sensors
such as RGB Camera, LiDAR, Radar, Depth Camera, IMU Sensor, GNSS Sensor,
Semantic Segmentation Camera, Instance Segmentation Camera, and Optical Flow
Camera along with the ground-truths of the individual actors and storing the
sensor data as well as ground-truth labels in a local or cloud-based database.
The framework uses multiple threads where a main thread runs the server, a
worker thread deals with queue and frame number and the rest of the threads
processes the sensor data. The other way we derive speed up over the native
implementation is by memory mapping the raw binary data into the disk and then
converting the data into known formats at the end of data collection. We show
that using these techniques, we gain a significant speed up over frames, under
an increasing set of sensors and over the number of spawned objects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AirExo-2: Scaling up Generalizable Robotic Imitation Learning with
  Low-Cost Exoskeletons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03081v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03081v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjie Fang, Chenxi Wang, Yiming Wang, Jingjing Chen, Shangning Xia, Jun Lv, Zihao He, Xiyan Yi, Yunhan Guo, Xinyu Zhan, Lixin Yang, Weiming Wang, Cewu Lu, Hao-Shu Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling up imitation learning for real-world applications requires efficient
and cost-effective demonstration collection methods. Current teleoperation
approaches, though effective, are expensive and inefficient due to the
dependency on physical robot platforms. Alternative data sources like
in-the-wild demonstrations can eliminate the need for physical robots and offer
more scalable solutions. However, existing in-the-wild data collection devices
have limitations: handheld devices offer restricted in-hand camera observation,
while whole-body devices often require fine-tuning with robot data due to
action inaccuracies. In this paper, we propose AirExo-2, a low-cost exoskeleton
system for large-scale in-the-wild demonstration collection. By introducing the
demonstration adaptor to transform the collected in-the-wild demonstrations
into pseudo-robot demonstrations, our system addresses key challenges in
utilizing in-the-wild demonstrations for downstream imitation learning in
real-world environments. Additionally, we present RISE-2, a generalizable
policy that integrates 2D and 3D perceptions, outperforming previous imitation
learning policies in both in-domain and out-of-domain tasks, even with limited
demonstrations. By leveraging in-the-wild demonstrations collected and
transformed by the AirExo-2 system, without the need for additional robot
demonstrations, RISE-2 achieves comparable or superior performance to policies
trained with teleoperated data, highlighting the potential of AirExo-2 for
scalable and generalizable imitation learning. Project page:
https://airexo.tech/airexo2
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mochi<span class="highlight-title">Swarm</span>: A testbed for robotic blimps in realistic environments <span class="chip">ICRA2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Xu, Thong Vu, Diego S. D'Antonio, David Saldaña
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Testing aerial robots in tasks such as pickup-and-delivery and surveillance
significantly benefits from high energy efficiency and scalability of the
deployed robotic system. This paper presents MochiSwarm, an open-source testbed
of light-weight robotic blimps, ready for multi-robot operation without
external localization. We introduce the system design in hardware, software,
and perception, which capitalizes on modularity, low cost, and light weight.
The hardware allows for rapid modification, which enables the integration of
additional sensors to enhance autonomy for different scenarios. The software
framework supports different actuation models and communication between the
base station and multiple blimps. The detachable perception module allows
independent blimps to perform tasks that involve detection and autonomous
actuation. We showcase a differential-drive module as an example, of which the
autonomy is enabled by visual servoing using the perception module. A case
study of pickup-and-delivery tasks with up to 12 blimps highlights the autonomy
of the MochiSwarm without external infrastructures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Acepted for publication at ICRA2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BEVDriver: Leveraging BEV Maps in <span class="highlight-title">LLM</span>s for Robust Closed-Loop Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katharina Winter, Mark Azer, Fabian B. Flohr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving has the potential to set the stage for more efficient
future mobility, requiring the research domain to establish trust through safe,
reliable and transparent driving. Large Language Models (LLMs) possess
reasoning capabilities and natural language understanding, presenting the
potential to serve as generalized decision-makers for ego-motion planning that
can interact with humans and navigate environments designed for human drivers.
While this research avenue is promising, current autonomous driving approaches
are challenged by combining 3D spatial grounding and the reasoning and language
capabilities of LLMs. We introduce BEVDriver, an LLM-based model for end-to-end
closed-loop driving in CARLA that utilizes latent BEV features as perception
input. BEVDriver includes a BEV encoder to efficiently process multi-view
images and 3D LiDAR point clouds. Within a common latent space, the BEV
features are propagated through a Q-Former to align with natural language
instructions and passed to the LLM that predicts and plans precise future
trajectories while considering navigation instructions and critical scenarios.
On the LangAuto benchmark, our model reaches up to 18.9% higher performance on
the Driving Score compared to SoTA methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Physically-Feasible Reactive Synthesis for Terrain-Adaptive Locomotion
  via Trajectory Optimization and Symbolic Repair 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi Zhou, Qian Meng, Hadas Kress-Gazit, Ye Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an integrated planning framework for quadrupedal locomotion over
dynamically changing, unforeseen terrains. Existing approaches either rely on
heuristics for instantaneous foothold selection--compromising safety and
versatility--or solve expensive trajectory optimization problems with complex
terrain features and long time horizons. In contrast, our framework leverages
reactive synthesis to generate correct-by-construction controllers at the
symbolic level, and mixed-integer convex programming (MICP) for dynamic and
physically feasible footstep planning for each symbolic transition. We use a
high-level manager to reduce the large state space in synthesis by
incorporating local environment information, improving synthesis scalability.
To handle specifications that cannot be met due to dynamic infeasibility, and
to minimize costly MICP solves, we leverage a symbolic repair process to
generate only necessary symbolic transitions. During online execution,
re-running the MICP with real-world terrain data, along with runtime symbolic
repair, bridges the gap between offline synthesis and online execution. We
demonstrate, in simulation, our framework's capabilities to discover missing
locomotion skills and react promptly in safety-critical environments, such as
scattered stepping stones and rebars.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Equivariant Filter Design for Range-only <span class="highlight-title">SLAM</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03973v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03973v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixiao Ge, Arthur Pearce, Pieter van Goor, Robert Mahony
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Range-only Simultaneous Localisation and Mapping (RO-SLAM) is of interest due
to its practical applications in ultra-wideband (UWB) and Bluetooth Low Energy
(BLE) localisation in terrestrial and aerial applications and acoustic beacon
localisation in submarine applications. In this work, we consider a mobile
robot equipped with an inertial measurement unit (IMU) and a range sensor that
measures distances to a collection of fixed landmarks. We derive an equivariant
filter (EqF) for the RO-SLAM problem based on a symmetry Lie group that is
compatible with the range measurements. The proposed filter does not require
bootstrapping or initialisation of landmark positions, and demonstrates
robustness to the no-prior situation. The filter is demonstrated on a
real-world dataset, and it is shown to significantly outperform a
state-of-the-art EKF alternative in terms of both accuracy and robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures, accepted for presentation at IEEE International
  Conference on Robotics and Automation 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Autonomous Driving Safety with Collision Scenario Integration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zi Wang, Shiyi Lan, Xinglong Sun, Nadine Chang, Zhenxin Li, Zhiding Yu, Jose M. Alvarez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous vehicle safety is crucial for the successful deployment of
self-driving cars. However, most existing planning methods rely heavily on
imitation learning, which limits their ability to leverage collision data
effectively. Moreover, collecting collision or near-collision data is
inherently challenging, as it involves risks and raises ethical and practical
concerns. In this paper, we propose SafeFusion, a training framework to learn
from collision data. Instead of over-relying on imitation learning, SafeFusion
integrates safety-oriented metrics during training to enable collision
avoidance learning. In addition, to address the scarcity of collision data, we
propose CollisionGen, a scalable data generation pipeline to generate diverse,
high-quality scenarios using natural language prompts, generative models, and
rule-based filtering. Experimental results show that our approach improves
planning performance in collision-prone scenarios by 56\% over previous
state-of-the-art planners while maintaining effectiveness in regular driving
situations. Our work provides a scalable and effective solution for advancing
the safety of autonomous driving systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COARSE: Collaborative Pseudo-Labeling with Coarse Real Labels for
  Off-Road Semantic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03947v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03947v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aurelio Noca, Xianmei Lei, Jonathan Becktor, Jeffrey Edlund, Anna Sabel, Patrick Spieler, Curtis Padgett, Alexandre Alahi, Deegan Atha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous off-road navigation faces challenges due to diverse, unstructured
environments, requiring robust perception with both geometric and semantic
understanding. However, scarce densely labeled semantic data limits
generalization across domains. Simulated data helps, but introduces domain
adaptation issues. We propose COARSE, a semi-supervised domain adaptation
framework for off-road semantic segmentation, leveraging sparse, coarse
in-domain labels and densely labeled out-of-domain data. Using pretrained
vision transformers, we bridge domain gaps with complementary pixel-level and
patch-level decoders, enhanced by a collaborative pseudo-labeling strategy on
unlabeled data. Evaluations on RUGD and Rellis-3D datasets show significant
improvements of 9.7\% and 8.4\% respectively, versus only using coarse data.
Tests on real-world off-road vehicle data in a multi-biome setting further
demonstrate COARSE's applicability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint, 8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CREStE: Scalable Mapless Navigation with Internet Scale Priors and
  Counterfactual Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03921v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03921v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arthur Zhang, Harshit Sikchi, Amy Zhang, Joydeep Biswas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the long-horizon mapless navigation problem: enabling robots to
traverse novel environments without relying on high-definition maps or precise
waypoints that specify exactly where to navigate. Achieving this requires
overcoming two major challenges -- learning robust, generalizable perceptual
representations of the environment without pre-enumerating all possible
navigation factors and forms of perceptual aliasing and utilizing these learned
representations to plan human-aligned navigation paths. Existing solutions
struggle to generalize due to their reliance on hand-curated object lists that
overlook unforeseen factors, end-to-end learning of navigation features from
scarce large-scale robot datasets, and handcrafted reward functions that scale
poorly to diverse scenarios. To overcome these limitations, we propose CREStE,
the first method that learns representations and rewards for addressing the
full mapless navigation problem without relying on large-scale robot datasets
or manually curated features. CREStE leverages visual foundation models trained
on internet-scale data to learn continuous bird's-eye-view representations
capturing elevation, semantics, and instance-level features. To utilize learned
representations for planning, we propose a counterfactual-based loss and active
learning procedure that focuses on the most salient perceptual cues by querying
humans for counterfactual trajectory annotations in challenging scenes. We
evaluate CREStE in kilometer-scale navigation tasks across six distinct urban
environments. CREStE significantly outperforms all state-of-the-art approaches
with 70% fewer human interventions per mission, including a 2-kilometer mission
in an unseen environment with just 1 intervention; showcasing its robustness
and effectiveness for long-horizon mapless navigation. For videos and
additional materials, see https://amrl.cs.utexas.edu/creste .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 10 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GO-VMP: Global Optimization for View Motion Planning in Fruit Mapping <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03912v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03912v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Allen Isaac Jose, Sicong Pan, Tobias Zaenker, Rohit Menon, Sebastian Houben, Maren Bennewitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automating labor-intensive tasks such as crop monitoring with robots is
essential for enhancing production and conserving resources. However,
autonomously monitoring horticulture crops remains challenging due to their
complex structures, which often result in fruit occlusions. Existing view
planning methods attempt to reduce occlusions but either struggle to achieve
adequate coverage or incur high robot motion costs. We introduce a global
optimization approach for view motion planning that aims to minimize robot
motion costs while maximizing fruit coverage. To this end, we leverage coverage
constraints derived from the set covering problem (SCP) within a shortest
Hamiltonian path problem (SHPP) formulation. While both SCP and SHPP are
well-established, their tailored integration enables a unified framework that
computes a global view path with minimized motion while ensuring full coverage
of selected targets. Given the NP-hard nature of the problem, we employ a
region-prior-based selection of coverage targets and a sparse graph structure
to achieve effective optimization outcomes within a limited time. Experiments
in simulation demonstrate that our method detects more fruits, enhances surface
coverage, and achieves higher volume accuracy than the motion-efficient
baseline with a moderate increase in motion cost, while significantly reducing
motion costs compared to the coverage-focused baseline. Real-world experiments
further confirm the practical applicability of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Allen Isaac Jose and Sicong Pan have equal contribution. Submitted to
  IROS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe <span class="highlight-title">LLM</span>-Controlled Robots with Formal Guarantees via Reachability
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03911v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03911v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Hafez, Alireza Naderi Akhormeh, Amr Hegazy, Amr Alanwar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deployment of Large Language Models (LLMs) in robotic systems presents
unique safety challenges, particularly in unpredictable environments. Although
LLMs, leveraging zero-shot learning, enhance human-robot interaction and
decision-making capabilities, their inherent probabilistic nature and lack of
formal guarantees raise significant concerns for safety-critical applications.
Traditional model-based verification approaches often rely on precise system
models, which are difficult to obtain for real-world robotic systems and may
not be fully trusted due to modeling inaccuracies, unmodeled dynamics, or
environmental uncertainties. To address these challenges, this paper introduces
a safety assurance framework for LLM-controlled robots based on data-driven
reachability analysis, a formal verification technique that ensures all
possible system trajectories remain within safe operational limits. Our
framework specifically investigates the problem of instructing an LLM to
navigate the robot to a specified goal and assesses its ability to generate
low-level control actions that successfully guide the robot safely toward that
goal. By leveraging historical data to construct reachable sets of states for
the robot-LLM system, our approach provides rigorous safety guarantees against
unsafe behaviors without relying on explicit analytical models. We validate the
framework through experimental case studies in autonomous navigation and task
planning, demonstrating its effectiveness in mitigating risks associated with
LLM-generated commands. This work advances the integration of formal methods
into LLM-based robotics, offering a principled and practical approach to
ensuring safety in next-generation autonomous systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Endpoint-Explicit Differential Dynamic Programming via Exact Resolution <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03897v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03897v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maria Parilli, Sergi Martinez, Carlos Mastalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel method for handling endpoint constraints in constrained
differential dynamic programming (DDP). Unlike existing approaches, our method
guarantees quadratic convergence and is exact, effectively managing rank
deficiencies in both endpoint and stagewise equality constraints. It is
applicable to both forward and inverse dynamics formulations, making it
particularly well-suited for model predictive control (MPC) applications and
for accelerating optimal control (OC) solvers. We demonstrate the efficacy of
our approach across a broad range of robotics problems and provide a
user-friendly open-source implementation within CROCODDYL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, IEEE ICRA paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LensDFF: Language-enhanced Sparse Feature Distillation for Efficient
  Few-Shot Dexterous Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03890v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03890v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Feng, David S. Martinez Lema, Jianxiang Feng, Zhaopeng Chen, Alois Knoll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning dexterous manipulation from few-shot demonstrations is a significant
yet challenging problem for advanced, human-like robotic systems. Dense
distilled feature fields have addressed this challenge by distilling rich
semantic features from 2D visual foundation models into the 3D domain. However,
their reliance on neural rendering models such as Neural Radiance Fields (NeRF)
or Gaussian Splatting results in high computational costs. In contrast,
previous approaches based on sparse feature fields either suffer from
inefficiencies due to multi-view dependencies and extensive training or lack
sufficient grasp dexterity. To overcome these limitations, we propose
Language-ENhanced Sparse Distilled Feature Field (LensDFF), which efficiently
distills view-consistent 2D features onto 3D points using our novel
language-enhanced feature fusion strategy, thereby enabling single-view
few-shot generalization. Based on LensDFF, we further introduce a few-shot
dexterous manipulation framework that integrates grasp primitives into the
demonstrations to generate stable and highly dexterous grasps. Moreover, we
present a real2sim grasp evaluation pipeline for efficient grasp assessment and
hyperparameter tuning. Through extensive simulation experiments based on the
real2sim pipeline and real-world experiments, our approach achieves competitive
grasping performance, outperforming state-of-the-art approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pretrained <span class="highlight-title">LLM</span>s as Real-Time Controllers for Robot Operated Serial
  Production Line 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03889v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03889v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Waseem, Kshitij Bhatta, Chen Li, Qing Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The manufacturing industry is undergoing a transformative shift, driven by
cutting-edge technologies like 5G, AI, and cloud computing. Despite these
advancements, effective system control, which is crucial for optimizing
production efficiency, remains a complex challenge due to the intricate,
knowledge-dependent nature of manufacturing processes and the reliance on
domain-specific expertise. Conventional control methods often demand heavy
customization, considerable computational resources, and lack transparency in
decision-making. In this work, we investigate the feasibility of using Large
Language Models (LLMs), particularly GPT-4, as a straightforward, adaptable
solution for controlling manufacturing systems, specifically, mobile robot
scheduling. We introduce an LLM-based control framework to assign mobile robots
to different machines in robot assisted serial production lines, evaluating its
performance in terms of system throughput. Our proposed framework outperforms
traditional scheduling approaches such as First-Come-First-Served (FCFS),
Shortest Processing Time (SPT), and Longest Processing Time (LPT). While it
achieves performance that is on par with state-of-the-art methods like
Multi-Agent Reinforcement Learning (MARL), it offers a distinct advantage by
delivering comparable throughput without the need for extensive retraining.
These results suggest that the proposed LLM-based solution is well-suited for
scenarios where technical expertise, computational resources, and financial
investment are limited, while decision transparency and system scalability are
critical concerns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human Implicit Preference-Based Policy Fine-tuning for <span class="highlight-title">Multi</span>-Agent
  Reinforcement Learning in USV <span class="highlight-title">Swarm</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyeonjun Kim, Kanghoon Lee, Junho Park, Jiachen Li, Jinkyoo Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Agent Reinforcement Learning (MARL) has shown promise in solving
complex problems involving cooperation and competition among agents, such as an
Unmanned Surface Vehicle (USV) swarm used in search and rescue, surveillance,
and vessel protection. However, aligning system behavior with user preferences
is challenging due to the difficulty of encoding expert intuition into reward
functions. To address the issue, we propose a Reinforcement Learning with Human
Feedback (RLHF) approach for MARL that resolves credit-assignment challenges
through an Agent-Level Feedback system categorizing feedback into intra-agent,
inter-agent, and intra-team types. To overcome the challenges of direct human
feedback, we employ a Large Language Model (LLM) evaluator to validate our
approach using feedback scenarios such as region constraints, collision
avoidance, and task allocation. Our method effectively refines USV swarm
policies, addressing key challenges in multi-agent systems while maintaining
fairness and performance consistency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Integral Forms in Matrix Lie Groups 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02820v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02820v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timothy D Barfoot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Matrix Lie groups provide a language for describing motion in such fields as
robotics, computer vision, and graphics. When using these tools, we are often
faced with turning infinite-series expressions into more compact finite series
(e.g., the Euler-Rodriques formula), which can sometimes be onerous. In this
paper, we identify some useful integral forms in matrix Lie group expressions
that offer a more streamlined pathway for computing compact analytic results.
Moreover, we present some recursive structures in these integral forms that
show many of these expressions are interrelated. Key to our approach is that we
are able to apply the minimal polynomial for a Lie algebra quite early in the
process to keep expressions compact throughout the derivations. With the series
approach, the minimal polynomial is usually applied at the end, making it hard
to recognize common analytic expressions in the result. We show that our
integral method can reproduce several series-derived results from the
literature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Multi</span>-Sensor Fusion Approach for Rapid Orthoimage Generation in
  Large-Scale UAV Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01202v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01202v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialei He, Zhihao Zhan, Zhituo Tu, Xiang Zhu, Jie Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapid generation of large-scale orthoimages from Unmanned Aerial Vehicles
(UAVs) has been a long-standing focus of research in the field of aerial
mapping. A multi-sensor UAV system, integrating the Global Positioning System
(GPS), Inertial Measurement Unit (IMU), 4D millimeter-wave radar and camera,
can provide an effective solution to this problem. In this paper, we utilize
multi-sensor data to overcome the limitations of conventional orthoimage
generation methods in terms of temporal performance, system robustness, and
geographic reference accuracy. A prior-pose-optimized feature matching method
is introduced to enhance matching speed and accuracy, reducing the number of
required features and providing precise references for the Structure from
Motion (SfM) process. The proposed method exhibits robustness in low-texture
scenes like farmlands, where feature matching is difficult. Experiments show
that our approach achieves accurate feature matching orthoimage generation in a
short time. The proposed drone system effectively aids in farmland detection
and management.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> SEB-Naver: A SE(2)-based Local Navigation Framework for Car-like Robots
  on Uneven Terrain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02412v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02412v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoying Li, Long Xu, Xiaolin Huang, Donglai Xue, Zhihao Zhang, Zhichao Han, Chao Xu, Yanjun Cao, <span class="highlight-author">Fei Gao</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous navigation of car-like robots on uneven terrain poses unique
challenges compared to flat terrain, particularly in traversability assessment
and terrain-associated kinematic modelling for motion planning. This paper
introduces SEB-Naver, a novel SE(2)-based local navigation framework designed
to overcome these challenges. First, we propose an efficient traversability
assessment method for SE(2) grids, leveraging GPU parallel computing to enable
real-time updates and maintenance of local maps. Second, inspired by
differential flatness, we present an optimization-based trajectory planning
method that integrates terrain-associated kinematic models, significantly
improving both planning efficiency and trajectory quality. Finally, we unify
these components into SEB-Naver, achieving real-time terrain assessment and
trajectory optimization. Extensive simulations and real-world experiments
demonstrate the effectiveness and efficiency of our approach. The code is at
https://github.com/ZJU-FAST-Lab/seb_naver.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Floorplan-<span class="highlight-title">SLAM</span>: A Real-Time, High-Accuracy, and Long-Term <span class="highlight-title">Multi</span>-Session
  Point-Plane <span class="highlight-title">SLAM</span> for Efficient Floorplan Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.00397v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.00397v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haolin Wang, Zeren Lv, Hao Wei, Haijiang Zhu, Yihong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Floorplan reconstruction provides structural priors essential for reliable
indoor robot navigation and high-level scene understanding. However, existing
approaches either require time-consuming offline processing with a complete
map, or rely on expensive sensors and substantial computational resources. To
address the problems, we propose Floorplan-SLAM, which incorporates floorplan
reconstruction tightly into a multi-session SLAM system by seamlessly
interacting with plane extraction, pose estimation, and back-end optimization,
achieving real-time, high-accuracy, and long-term floorplan reconstruction
using only a stereo camera. Specifically, we present a robust plane extraction
algorithm that operates in a compact plane parameter space and leverages
spatially complementary features to accurately detect planar structures, even
in weakly textured scenes. Furthermore, we propose a floorplan reconstruction
module tightly coupled with the SLAM system, which uses continuously optimized
plane landmarks and poses to formulate and solve a novel optimization problem,
thereby enabling real-time incremental floorplan reconstruction. Note that by
leveraging the map merging capability of multi-session SLAM, our method
supports long-term floorplan reconstruction across multiple sessions without
redundant data collection. Experiments on the VECtor and the self-collected
datasets indicate that Floorplan-SLAM significantly outperforms
state-of-the-art methods in terms of plane extraction robustness, pose
estimation accuracy, and floorplan reconstruction fidelity and speed, achieving
real-time performance at 25-45 FPS without GPU acceleration, which reduces the
floorplan reconstruction time for a 1000 square meters scene from over 10 hours
to just 9.44 minutes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Affordance-Guided Reinforcement Learning via Visual Prompting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10341v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10341v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olivia Y. Lee, Annie Xie, Kuan Fang, Karl Pertsch, Chelsea Finn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robots equipped with reinforcement learning (RL) have the potential to learn
a wide range of skills solely from a reward signal. However, obtaining a robust
and dense reward signal for general manipulation tasks remains a challenge.
Existing learning-based approaches require significant data, such as human
demonstrations of success and failure, to learn task-specific reward functions.
Recently, there is also a growing adoption of large multi-modal foundation
models for robotics that can perform visual reasoning in physical contexts and
generate coarse robot motions for manipulation tasks. Motivated by this range
of capability, in this work, we present Keypoint-based Affordance Guidance for
Improvements (KAGI), a method leveraging rewards shaped by vision-language
models (VLMs) for autonomous RL. State-of-the-art VLMs have demonstrated
impressive reasoning about affordances through keypoints in zero-shot, and we
use these to define dense rewards that guide autonomous robotic learning. On
real-world manipulation tasks specified by natural language descriptions, KAGI
improves the sample efficiency of autonomous RL and enables successful task
completion in 30K online fine-tuning steps. Additionally, we demonstrate the
robustness of KAGI to reductions in the number of in-domain demonstrations used
for pre-training, reaching similar performance in 45K online fine-tuning steps.
Project website: https://sites.google.com/view/affordance-guided-rl
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures. Robotics: Science and Systems (RSS) 2024, Task
  Specification for General-Purpose Intelligent Robots & Lifelong Robot
  Learning Workshops</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perceptual Motor Learning with Active Inference Framework for Robust
  Lateral Control <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01676v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01676v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elahe Delavari, John Moore, Junho Hong, Jaerock Kwon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel Perceptual Motor Learning (PML) framework
integrated with Active Inference (AIF) to enhance lateral control in Highly
Automated Vehicles (HAVs). PML, inspired by human motor learning, emphasizes
the seamless integration of perception and action, enabling efficient
decision-making in dynamic environments. Traditional autonomous driving
approaches--including modular pipelines, imitation learning, and reinforcement
learning--struggle with adaptability, generalization, and computational
efficiency. In contrast, PML with AIF leverages a generative model to minimize
prediction error ("surprise") and actively shape vehicle control based on
learned perceptual-motor representations. Our approach unifies deep learning
with active inference principles, allowing HAVs to perform lane-keeping
maneuvers with minimal data and without extensive retraining across different
environments. Extensive experiments in the CARLA simulator demonstrate that PML
with AIF enhances adaptability without increasing computational overhead while
achieving performance comparable to conventional methods. These findings
highlight the potential of PML-driven active inference as a robust alternative
for real-world autonomous driving applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to IROS 2025 and is currently under
  review. arXiv admin note: text overlap with arXiv:2407.07684</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Category-level Meta-learned NeRF Priors for Efficient Object Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01582v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01582v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saad Ejaz, Hriday Bavle, Laura Ribeiro, Holger Voos, Jose Luis Sanchez-Lopez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In 3D object mapping, category-level priors enable efficient object
reconstruction and canonical pose estimation, requiring only a single prior per
semantic category (e.g., chair, book, laptop). Recently, DeepSDF has
predominantly been used as a category-level shape prior, but it struggles to
reconstruct sharp geometry and is computationally expensive. In contrast, NeRFs
capture fine details but have yet to be effectively integrated with
category-level priors in a real-time multi-object mapping framework. To bridge
this gap, we introduce PRENOM, a Prior-based Efficient Neural Object Mapper
that integrates category-level priors with object-level NeRFs to enhance
reconstruction efficiency while enabling canonical object pose estimation.
PRENOM gets to know objects on a first-name basis by meta-learning on synthetic
reconstruction tasks generated from open-source shape datasets. To account for
object category variations, it employs a multi-objective genetic algorithm to
optimize the NeRF architecture for each category, balancing reconstruction
quality and training time. Additionally, prior-based probabilistic ray sampling
directs sampling toward expected object regions, accelerating convergence and
improving reconstruction quality under constrained resources. Experimental
results on a low-end GPU highlight the ability of PRENOM to achieve
high-quality reconstructions while maintaining computational feasibility.
Specifically, comparisons with prior-free NeRF-based approaches on a synthetic
dataset show a 21% lower Chamfer distance, demonstrating better reconstruction
quality. Furthermore, evaluations against other approaches using shape priors
on a noisy real-world dataset indicate a 13% improvement averaged across all
reconstruction metrics, and comparable pose and size estimation accuracy, while
being trained for 5x less time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DROP: Dexterous Reorientation via Online Planning <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14562v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14562v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Albert H. Li, Preston Culbertson, Vince Kurtz, Aaron D. Ames
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving human-like dexterity is a longstanding challenge in robotics, in
part due to the complexity of planning and control for contact-rich systems. In
reinforcement learning (RL), one popular approach has been to use
massively-parallelized, domain-randomized simulations to learn a policy offline
over a vast array of contact conditions, allowing robust sim-to-real transfer.
Inspired by recent advances in real-time parallel simulation, this work
considers instead the viability of online planning methods for contact-rich
manipulation by studying the well-known in-hand cube reorientation task. We
propose a simple architecture that employs a sampling-based predictive
controller and vision-based pose estimator to search for contact-rich control
actions online. We conduct thorough experiments to assess the real-world
performance of our method, architectural design choices, and key factors for
robustness, demonstrating that our simple sampling-based approach achieves
performance comparable to prior RL-based works. Supplemental material:
https://caltech-amber.github.io/drop.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version, updated appendix. Accepted to ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DMVC-Tracker: <span class="highlight-title">Distributed</span> <span class="highlight-title">Multi</span>-Agent Trajectory Planning for Target
  Tracking Using Dynamic Buffered Voronoi and Inter-Visibility Cells 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18086v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18086v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunwoo Lee, Jungwon Park, H. Jin Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This letter presents a distributed trajectory planning method for multi-agent
aerial tracking. The proposed method uses a Dynamic Buffered Voronoi Cell
(DBVC) and a Dynamic Inter-Visibility Cell (DIVC) to formulate the distributed
trajectory generation. Specifically, the DBVC and the DIVC are time-variant
spaces that prevent mutual collisions and occlusions among agents, while
enabling them to maintain suitable distances from the moving target. We combine
the DBVC and the DIVC with an efficient Bernstein polynomial motion
primitive-based tracking generation method, which has been refined into a less
conservative approach than in our previous work. The proposed algorithm can
compute each agent's trajectory within several milliseconds on an Intel i7
desktop. We validate the tracking performance in challenging scenarios,
including environments with dozens of obstacles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Goal-Directed Object Pushing in Cluttered Scenes with
  Location-Based Attention <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17667v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17667v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nils Dengler, Juan Del Aguila Ferrandis, João Moura, Sethu Vijayakumar, Maren Bennewitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In complex scenarios where typical pick-and-place techniques are
insufficient, often non-prehensile manipulation can ensure that a robot is able
to fulfill its task. However, non-prehensile manipulation is challenging due to
its underactuated nature with hybrid-dynamics, where a robot needs to reason
about an object's long-term behavior and contact-switching, while being robust
to contact uncertainty. The presence of clutter in the workspace further
complicates this task, introducing the need to include more advanced spatial
analysis to avoid unwanted collisions. Building upon prior work on
reinforcement learning with multimodal categorical exploration for planar
pushing, we propose to incorporate location-based attention to enable robust
manipulation in cluttered scenes. Unlike previous approaches addressing this
obstacle avoiding pushing task, our framework requires no predefined global
paths and considers the desired target orientation of the manipulated object.
Experimental results in simulation as well as with a real KUKA iiwa robot arm
demonstrate that our learned policy manipulates objects successfully while
avoiding collisions through complex obstacle configurations, including dynamic
obstacles, to reach the desired target pose.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS)2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DexGraspVLA: A Vision-Language-Action Framework Towards General
  Dexterous Grasping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20900v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20900v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Zhong, Xuchuan Huang, Ruochong Li, Ceyao Zhang, Yitao Liang, Yaodong Yang, Yuanpei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dexterous grasping remains a fundamental yet challenging problem in robotics.
A general-purpose robot must be capable of grasping diverse objects in
arbitrary scenarios. However, existing research typically relies on specific
assumptions, such as single-object settings or limited environments, leading to
constrained generalization. Our solution is DexGraspVLA, a hierarchical
framework that utilizes a pre-trained Vision-Language model as the high-level
task planner and learns a diffusion-based policy as the low-level Action
controller. The key insight lies in iteratively transforming diverse language
and visual inputs into domain-invariant representations, where imitation
learning can be effectively applied due to the alleviation of domain shift.
Thus, it enables robust generalization across a wide range of real-world
scenarios. Notably, our method achieves a 90+% success rate under thousands of
unseen object, lighting, and background combinations in a ``zero-shot''
environment. Empirical analysis further confirms the consistency of internal
model behavior across environmental variations, thereby validating our design
and explaining its generalization performance. We hope our work can be a step
forward in achieving general dexterous grasping. Our demo and code can be found
at https://dexgraspvla.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One-Shot Imitation under Mismatched Execution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06615v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06615v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kushal Kedia, Prithwish Dan, Angela Chao, Maximus Adrian Pace, Sanjiban Choudhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human demonstrations as prompts are a powerful way to program robots to do
long-horizon manipulation tasks. However, translating these demonstrations into
robot-executable actions presents significant challenges due to execution
mismatches in movement styles and physical capabilities. Existing methods
either depend on human-robot paired data, which is infeasible to scale, or rely
heavily on frame-level visual similarities that often break down in practice.
To address these challenges, we propose RHyME, a novel framework that
automatically aligns human and robot task executions using optimal transport
costs. Given long-horizon robot demonstrations, RHyME synthesizes semantically
equivalent human videos by retrieving and composing short-horizon human clips.
This approach facilitates effective policy training without the need for paired
data. RHyME successfully imitates a range of cross-embodiment demonstrators,
both in simulation and with a real human hand, achieving over 50\% increase in
task success compared to previous methods. We release our code and datasets at
https://portal-cornell.github.io/rhyme/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dashing for the Golden Snitch: <span class="highlight-title">Multi</span>-Drone Time-Optimal Motion Planning
  with <span class="highlight-title">Multi</span>-Agent Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16720v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16720v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xian Wang, Jin Zhou, Yuanli Feng, Jiahao Mei, Jiming Chen, Shuo Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent innovations in autonomous drones have facilitated time-optimal flight
in single-drone configurations, and enhanced maneuverability in multi-drone
systems by applying optimal control and learning-based methods. However, few
studies have achieved time-optimal motion planning for multi-drone systems,
particularly during highly agile maneuvers or in dynamic scenarios. This paper
presents a decentralized policy network using multi-agent reinforcement
learning for time-optimal multi-drone flight. To strike a balance between
flight efficiency and collision avoidance, we introduce a soft collision-free
mechanism inspired by optimization-based methods. By customizing PPO in a
centralized training, decentralized execution (CTDE) fashion, we unlock higher
efficiency and stability in training while ensuring lightweight implementation.
Extensive simulations show that, despite slight performance trade-offs compared
to single-drone systems, our multi-drone approach maintains near-time-optimal
performance with a low collision rate. Real-world experiments validate our
method, with two quadrotors using the same network as in simulation achieving a
maximum speed of 13.65 m/s and a maximum body rate of 13.4 rad/s in a 5.5 m *
5.5 m * 2.0 m space across various tracks, relying entirely on onboard
computation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2: 7 pages, 6 figures; terminology corrected, algorithmic and
  equation descriptions revised, references added</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Collaborative motion planning for <span class="highlight-title">multi</span>-manipulator systems through
  Reinforcement Learning and Dynamic Movement Primitives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00757v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00757v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddharth Singh, Tian Xu, Qing Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic tasks often require multiple manipulators to enhance task efficiency
and speed, but this increases complexity in terms of collaboration, collision
avoidance, and the expanded state-action space. To address these challenges, we
propose a multi-level approach combining Reinforcement Learning (RL) and
Dynamic Movement Primitives (DMP) to generate adaptive, real-time trajectories
for new tasks in dynamic environments using a demonstration library. This
method ensures collision-free trajectory generation and efficient collaborative
motion planning. We validate the approach through experiments in the PyBullet
simulation environment with UR5e robotic manipulators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures, conference submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tiny Robotics <span class="highlight-title">Dataset</span> and Benchmark for Continual Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16215v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16215v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Pasti, Riccardo De Monte, Davide Dalle Pezze, Gian Antonio Susto, Nicola Bellotto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting objects in mobile robotics is crucial for numerous applications,
from autonomous navigation to inspection. However, robots often need to operate
in different domains from those they were trained in, requiring them to adjust
to these changes. Tiny mobile robots, subject to size, power, and computational
constraints, encounter even more difficulties in running and adapting these
algorithms. Such adaptability, though, is crucial for real-world deployment,
where robots must operate effectively in dynamic and unpredictable settings. In
this work, we introduce a novel benchmark to evaluate the continual learning
capabilities of object detection systems in tiny robotic platforms. Our
contributions include: (i) Tiny Robotics Object Detection~(TiROD), a
comprehensive dataset collected using the onboard camera of a small mobile
robot, designed to test object detectors across various domains and classes;
(ii) a benchmark of different continual learning strategies on this dataset
using NanoDet, a lightweight object detector. Our results highlight key
challenges in developing robust and efficient continual learning strategies for
object detectors in tiny robotics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Magnetic-Actuated Vision-Based Whisker Array for Contact Perception
  and Grasping <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.00133v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.00133v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixian Hu, Juan Wachs, Yu She
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tactile sensing and the manipulation of delicate objects are critical
challenges in robotics. This study presents a vision-based magnetic-actuated
whisker array sensor that integrates these functions. The sensor features eight
whiskers arranged circularly, supported by an elastomer membrane and actuated
by electromagnets and permanent magnets. A camera tracks whisker movements,
enabling high-resolution tactile feedback. The sensor's performance was
evaluated through object classification and grasping experiments. In the
classification experiment, the sensor approached objects from four directions
and accurately identified five distinct objects with a classification accuracy
of 99.17% using a Multi-Layer Perceptron model. In the grasping experiment, the
sensor tested configurations of eight, four, and two whiskers, achieving the
highest success rate of 87% with eight whiskers. These results highlight the
sensor's potential for precise tactile sensing and reliable manipulation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE International Conference on Robotics and Automation
  (ICRA) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GSplatLoc: Grounding Keypoint Descriptors into 3D Gaussian Splatting for
  Improved Visual Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16502v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16502v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gennady Sidorov, Malik Mohrat, Denis Gridusov, Ruslan Rakhimov, Sergey Kolyubin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although various visual localization approaches exist, such as scene
coordinate regression and camera pose regression, these methods often struggle
with optimization complexity or limited accuracy. To address these challenges,
we explore the use of novel view synthesis techniques, particularly 3D Gaussian
Splatting (3DGS), which enables the compact encoding of both 3D geometry and
scene appearance. We propose a two-stage procedure that integrates dense and
robust keypoint descriptors from the lightweight XFeat feature extractor into
3DGS, enhancing performance in both indoor and outdoor environments. The coarse
pose estimates are directly obtained via 2D-3D correspondences between the 3DGS
representation and query image descriptors. In the second stage, the initial
pose estimate is refined by minimizing the rendering-based photometric warp
loss. Benchmarking on widely used indoor and outdoor datasets demonstrates
improvements over recent neural rendering-based localization methods, such as
NeRFMatch and PNeRFLoc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website at https://gsplatloc.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learn from the Past: Language-conditioned Object Rearrangement with
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18516v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18516v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanqun Cao, Ryan Mckenna, Erich Graf, John Oyekan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object manipulation for rearrangement into a specific goal state is a
significant task for collaborative robots. Accurately determining object
placement is a key challenge, as misalignment can increase task complexity and
the risk of collisions, affecting the efficiency of the rearrangement process.
Most current methods heavily rely on pre-collected datasets to train the model
for predicting the goal position. As a result, these methods are restricted to
specific instructions, which limits their broader applicability and
generalisation. In this paper, we propose a framework of flexible
language-conditioned object rearrangement based on the Large Language Model
(LLM). Our approach mimics human reasoning by making use of successful past
experiences as a reference to infer the best strategies to achieve a current
desired goal position. Based on LLM's strong natural language comprehension and
inference ability, our method generalises to handle various everyday objects
and free-form language instructions in a zero-shot manner. Experimental results
demonstrate that our methods can effectively execute the robotic rearrangement
tasks, even those involving long sequences of orders.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLIP-RT: Learning Language-Conditioned Robotic Policies from Natural
  Language Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00508v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00508v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gi-Cheon Kang, Junghyun Kim, Kyuhwan Shim, Jun Ki Lee, Byoung-Tak Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Teaching robots desired skills in real-world environments remains
challenging, especially for non-experts. A key bottleneck is that collecting
robotic data often requires expertise or specialized hardware, limiting
accessibility and scalability. We posit that natural language offers an
intuitive and accessible interface for robot learning. To this end, we study
two aspects: (1) enabling non-experts to collect robotic data through natural
language supervision (e.g., "move the arm to the right") and (2) learning
robotic policies directly from this supervision. Specifically, we introduce a
data collection framework that collects robot demonstrations based on natural
language supervision and further augments these demonstrations. We then present
CLIP-RT, a vision-language-action (VLA) model that learns language-conditioned
visuomotor policies from this supervision. CLIP-RT adapts the pretrained CLIP
models and learns to predict language-based motion primitives via contrastive
imitation learning. We train CLIP-RT on the Open X-Embodiment dataset and
finetune it on in-domain data collected by our framework to learn diverse
skills. CLIP-RT demonstrates strong capabilities in learning novel manipulation
skills, outperforming the state-of-the-art model, OpenVLA (7B parameters), by
24% in average success rates, while using 7x fewer parameters (1B). We further
observe that CLIP-RT shows significant improvements in few-shot generalization.
Finally, through collaboration with humans or large pretrained models, we
demonstrate that CLIP-RT can further improve its generalization on challenging
robotic tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Submap-based Autonomous MAV Exploration using Visual-Inertial
  <span class="highlight-title">SLAM</span> Configurable for LiDARs or Depth Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16972v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16972v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sotiris Papatheodorou, Simon Boche, Sebastián Barbas Laina, Stefan Leutenegger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous exploration of unknown space is an essential component for the
deployment of mobile robots in the real world. Safe navigation is crucial for
all robotics applications and requires accurate and consistent maps of the
robot's surroundings. To achieve full autonomy and allow deployment in a wide
variety of environments, the robot must rely on on-board state estimation which
is prone to drift over time. We propose a Micro Aerial Vehicle (MAV)
exploration framework based on local submaps to allow retaining global
consistency by applying loop-closure corrections to the relative submap poses.
To enable large-scale exploration we efficiently compute global,
environment-wide frontiers from the local submap frontiers and use a
sampling-based next-best-view exploration planner. Our method seamlessly
supports using either a LiDAR sensor or a depth camera, making it suitable for
different kinds of MAV platforms. We perform comparative evaluations in
simulation against a state-of-the-art submap-based exploration framework to
showcase the efficiency and reconstruction quality of our approach. Finally, we
demonstrate the applicability of our method to real-world MAVs, one equipped
with a LiDAR and the other with a depth camera. Video available at
https://youtu.be/Uf5fwmYcuq4 .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In proceedings of the IEEE International Conference on Robotics and
  Automation, 2025. 7 pages, 8 figures, for the accompanying video see
  https://youtu.be/Uf5fwmYcuq4</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ARS548_ros. An ARS 548 RDI radar driver for ROS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.04589v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.04589v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fernando Fernández-Calatayud, Lucía Coto-Elena, David Alejo, José J. Carpio-Jiménez, Fernando Caballero, Luis Merino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ARS 548 RDI Radar is a premium model of the fifth generation of 77 GHz
long range radar sensors with new RF antenna arrays, which offer digital beam
forming. This radar measures independently the distance, speed and angle of
objects without any reflectors in one measurement cycle based on Pulse
Compression with New Frequency Modulation. Unfortunately, to the best of our
knowledge, there are no open source drivers available for Linux systems to
enable users to analyze the data acquired by the sensor. In this paper, we
present a driver that can interpret the data from the ARS 548 RDI sensor and
make it available over the Robot Operating System versions 1 and 2 (ROS and
ROS2). Thus, these data can be stored, represented, and analyzed using the
powerful tools offered by ROS. Besides, our driver offers advanced object
features provided by the sensor, such as relative estimated velocity and
acceleration of each object, its orientation and angular velocity. We focus on
the configuration of the sensor and the use of our driver including its
filtering and representation tools. Besides, we offer a video tutorial to help
in its configuration process. Finally, a dataset acquired with this sensor and
an Ouster OS1-32 LiDAR sensor, to have baseline measurements, is available, so
that the user can check the correctness of our driver.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 6 figures and 23 references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Perceptive Humanoid Locomotion over Challenging Terrain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.00692v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.00692v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wandong Sun, Baoshi Cao, Long Chen, Yongbo Su, Yang Liu, Zongwu Xie, Hong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humanoid robots are engineered to navigate terrains akin to those encountered
by humans, which necessitates human-like locomotion and perceptual abilities.
Currently, the most reliable controllers for humanoid motion rely exclusively
on proprioception, a reliance that becomes both dangerous and unreliable when
coping with rugged terrain. Although the integration of height maps into
perception can enable proactive gait planning, robust utilization of this
information remains a significant challenge, especially when exteroceptive
perception is noisy. To surmount these challenges, we propose a solution based
on a teacher-student distillation framework. In this paradigm, an oracle policy
accesses noise-free data to establish an optimal reference policy, while the
student policy not only imitates the teacher's actions but also simultaneously
trains a world model with a variational information bottleneck for sensor
denoising and state estimation. Extensive evaluations demonstrate that our
approach markedly enhances performance in scenarios characterized by unreliable
terrain estimations. Moreover, we conducted rigorous testing in both
challenging urban settings and off-road environments, the model successfully
traverse 2 km of varied terrain without external intervention.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Joint-repositionable Inner-wireless Planar Snake Robot 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.13916v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.13916v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayato Kanada, Ryo Takahashi, Keito Hayashi, Ryusuke Hosaka, Wakako Yukita, Yasutaka Nakashima, Tomoyuki Yokota, Takao Someya, Mitsuhiro Kamezaki, Yoshihiro Kawahara, Motoji Yamamoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bio-inspired multi-joint snake robots offer the advantages of terrain
adaptability due to their limbless structure and high flexibility. However, a
series of dozens of motor units in typical multiple-joint snake robots results
in a heavy body structure and hundreds of watts of high power consumption. This
paper presents a joint-repositionable, inner-wireless snake robot that enables
multi-joint-like locomotion using a low-powered underactuated mechanism. The
snake robot, consisting of a series of flexible passive links, can dynamically
change its joint coupling configuration by repositioning motor-driven joint
units along rack gears inside the robot. Additionally, a soft robot skin
wirelessly powers the internal joint units, avoiding the risk of wire tangling
and disconnection caused by the movable joint units. The combination of the
joint-repositionable mechanism and the wireless-charging-enabled soft skin
achieves a high degree of bending, along with a lightweight structure of 1.3 kg
and energy-efficient wireless power transmission of 7.6 watts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GravMAD: Grounded Spatial Value Maps Guided Action Diffusion for
  Generalized 3D Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.20154v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.20154v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangtao Chen, Zixuan Chen, Junhui Yin, Jing Huo, Pinzhuo Tian, Jieqi Shi, Yang Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robots' ability to follow language instructions and execute diverse 3D
manipulation tasks is vital in robot learning. Traditional imitation
learning-based methods perform well on seen tasks but struggle with novel,
unseen ones due to variability. Recent approaches leverage large foundation
models to assist in understanding novel tasks, thereby mitigating this issue.
However, these methods lack a task-specific learning process, which is
essential for an accurate understanding of 3D environments, often leading to
execution failures. In this paper, we introduce GravMAD, a sub-goal-driven,
language-conditioned action diffusion framework that combines the strengths of
imitation learning and foundation models. Our approach breaks tasks into
sub-goals based on language instructions, allowing auxiliary guidance during
both training and inference. During training, we introduce Sub-goal Keypose
Discovery to identify key sub-goals from demonstrations. Inference differs from
training, as there are no demonstrations available, so we use pre-trained
foundation models to bridge the gap and identify sub-goals for the current
task. In both phases, GravMaps are generated from sub-goals, providing GravMAD
with more flexible 3D spatial guidance compared to fixed 3D positions.
Empirical evaluations on RLBench show that GravMAD significantly outperforms
state-of-the-art methods, with a 28.63% improvement on novel tasks and a 13.36%
gain on tasks encountered during training. Evaluations on real-world robotic
tasks further show that GravMAD can reason about real-world tasks, associate
them with relevant visual information, and generalize to novel tasks. These
results demonstrate GravMAD's strong multi-task learning and generalization in
3D manipulation. Video demonstrations are available at:
https://gravmad.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025. The first two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CarPlanner: Consistent Auto-regressive Trajectory Planning for
  Large-scale Reinforcement Learning in Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19908v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19908v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongkun Zhang, Jiaming Liang, Ke Guo, Sha Lu, Qi Wang, Rong Xiong, Zhenwei Miao, Yue Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory planning is vital for autonomous driving, ensuring safe and
efficient navigation in complex environments. While recent learning-based
methods, particularly reinforcement learning (RL), have shown promise in
specific scenarios, RL planners struggle with training inefficiencies and
managing large-scale, real-world driving scenarios. In this paper, we introduce
\textbf{CarPlanner}, a \textbf{C}onsistent \textbf{a}uto-\textbf{r}egressive
\textbf{Planner} that uses RL to generate multi-modal trajectories. The
auto-regressive structure enables efficient large-scale RL training, while the
incorporation of consistency ensures stable policy learning by maintaining
coherent temporal consistency across time steps. Moreover, CarPlanner employs a
generation-selection framework with an expert-guided reward function and an
invariant-view module, simplifying RL training and enhancing policy
performance. Extensive analysis demonstrates that our proposed RL framework
effectively addresses the challenges of training efficiency and performance
enhancement, positioning CarPlanner as a promising solution for trajectory
planning in autonomous driving. To the best of our knowledge, we are the first
to demonstrate that the RL-based planner can surpass both IL- and rule-based
state-of-the-arts (SOTAs) on the challenging large-scale real-world dataset
nuPlan. Our proposed CarPlanner surpasses RL-, IL-, and rule-based SOTA
approaches within this demanding dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Energy Regularization for Autonomous Gait Transition and
  Energy-Efficient Quadruped Locomotion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.20001v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.20001v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyuan Liang, Lingfeng Sun, Xinghao Zhu, Bike Zhang, Ziyin Xiong, Yixiao Wang, Chenran Li, Koushil Sreenath, Masayoshi Tomizuka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In reinforcement learning for legged robot locomotion, crafting effective
reward strategies is crucial. Pre-defined gait patterns and complex reward
systems are widely used to stabilize policy training. Drawing from the natural
locomotion behaviors of humans and animals, which adapt their gaits to minimize
energy consumption, we propose a simplified, energy-centric reward strategy to
foster the development of energy-efficient locomotion across various speeds in
quadruped robots. By implementing an adaptive energy reward function and
adjusting the weights based on velocity, we demonstrate that our approach
enables ANYmal-C and Unitree Go1 robots to autonomously select appropriate
gaits, such as four-beat walking at lower speeds and trotting at higher speeds,
resulting in improved energy efficiency and stable velocity tracking compared
to previous methods using complex reward designs and prior gait knowledge. The
effectiveness of our policy is validated through simulations in the IsaacGym
simulation environment and on real robots, demonstrating its potential to
facilitate stable and adaptive locomotion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Geometric Impedance Control on SE(3) for Robotic Manipulators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.07945v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.07945v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joohwan Seo, Nikhil Potu Surya Prakash, Alexander Rose, Jongeun Choi, Roberto Horowitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  After its introduction, impedance control has been utilized as a primary
control scheme for robotic manipulation tasks that involve interaction with
unknown environments. While impedance control has been extensively studied, the
geometric structure of SE(3) for the robotic manipulator itself and its use in
formulating a robotic task has not been adequately addressed. In this paper, we
propose a differential geometric approach to impedance control. Given a
left-invariant error metric in SE(3), the corresponding error vectors in
position and velocity are first derived. We then propose the impedance control
schemes that adequately account for the geometric structure of the manipulator
in SE(3) based on a left-invariant potential function. The closed-loop
stabilities for the proposed control schemes are verified using Lyapunov
function-based analysis. The proposed control design clearly outperformed a
conventional impedance control approach when tracking challenging trajectory
profiles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at IFAC World Congress 2023, Yokohama, Japan</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Planning for <span class="highlight-title">Multi</span>-UAV Pursuit-Evasion in Unknown Environments
  Using Deep Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15866v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15866v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayu Chen, Chao Yu, Guosheng Li, Wenhao Tang, Shilong Ji, Xinyi Yang, Botian Xu, Huazhong Yang, Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-UAV pursuit-evasion, where pursuers aim to capture evaders, poses a key
challenge for UAV swarm intelligence. Multi-agent reinforcement learning (MARL)
has demonstrated potential in modeling cooperative behaviors, but most RL-based
approaches remain constrained to simplified simulations with limited dynamics
or fixed scenarios. Previous attempts to deploy RL policy to real-world
pursuit-evasion are largely restricted to two-dimensional scenarios, such as
ground vehicles or UAVs at fixed altitudes. In this paper, we address multi-UAV
pursuit-evasion by considering UAV dynamics and physical constraints. We
introduce an evader prediction-enhanced network to tackle partial observability
in cooperative strategy learning. Additionally, we propose an adaptive
environment generator within MARL training, enabling higher exploration
efficiency and better policy generalization across diverse scenarios.
Simulations show our method significantly outperforms all baselines in
challenging scenarios, generalizing to unseen scenarios with a 100% capture
rate. Finally, we derive a feasible policy via a two-stage reward refinement
and deploy the policy on real quadrotors in a zero-shot manner. To our
knowledge, this is the first work to derive and deploy an RL-based policy using
collective thrust and body rates control commands for multi-UAV pursuit-evasion
in unknown environments. The open-source code and videos are available at
https://sites.google.com/view/pursuit-evasion-rl.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VL-Nav: Real-time Vision-Language Navigation with Spatial Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00931v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00931v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Du, Taimeng Fu, Zhuoqun Chen, Bowen Li, Shaoshu Su, Zhipeng Zhao, Chen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language navigation in unknown environments is crucial for mobile
robots. In scenarios such as household assistance and rescue, mobile robots
need to understand a human command, such as "find a person wearing black". We
present a novel vision-language navigation (VL-Nav) system that integrates
efficient spatial reasoning on low-power robots. Unlike prior methods that rely
on a single image-level feature similarity to guide a robot, our method
integrates pixel-wise vision-language features with curiosity-driven
exploration. This approach enables robust navigation to human-instructed
instances across diverse environments. We deploy VL-Nav on a four-wheel mobile
robot and evaluate its performance through comprehensive navigation tasks in
both indoor and outdoor environments, spanning different scales and semantic
complexities. Remarkably, VL-Nav operates at a real-time frequency of 30 Hz
with a Jetson Orin NX, highlighting its ability to conduct efficient
vision-language navigation. Results show that VL-Nav achieves an overall
success rate of 86.3%, outperforming previous methods by 44.15%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bayesian Optimal Experimental Design for Robot Kinematic Calibration <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10802v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10802v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ersin Das, Thomas Touma, Joel W. Burdick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper develops a Bayesian optimal experimental design for robot
kinematic calibration on ${\mathbb{S}^3 \!\times\! \mathbb{R}^3}$. Our method
builds upon a Gaussian process approach that incorporates a geometry-aware
kernel based on Riemannian Mat\'ern kernels over ${\mathbb{S}^3}$. To learn the
forward kinematics errors via Bayesian optimization with a Gaussian process, we
define a geodesic distance-based objective function. Pointwise values of this
function are sampled via noisy measurements taken using fiducial markers on the
end-effector using a camera and computed pose with the nominal kinematics. The
corrected Denavit-Hartenberg parameters are obtained using an efficient
quadratic program that operates on the collected data sets. The effectiveness
of the proposed method is demonstrated via simulations and calibration
experiments on NASA's ocean world lander autonomy testbed (OWLAT).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Action Tokenizer Matters in In-Context Imitation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01206v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01206v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        An Dinh Vuong, Minh Nhat Vu, Dong An, Ian Reid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context imitation learning (ICIL) is a new paradigm that enables robots to
generalize from demonstrations to unseen tasks without retraining. A
well-structured action representation is the key to capturing demonstration
information effectively, yet action tokenizer (the process of discretizing and
encoding actions) remains largely unexplored in ICIL. In this work, we first
systematically evaluate existing action tokenizer methods in ICIL and reveal a
critical limitation: while they effectively encode action trajectories, they
fail to preserve temporal smoothness, which is crucial for stable robotic
execution. To address this, we propose LipVQ-VAE, a variational autoencoder
that enforces the Lipschitz condition in the latent action space via weight
normalization. By propagating smoothness constraints from raw action inputs to
a quantized latent codebook, LipVQ-VAE generates more stable and smoother
actions. When integrating into ICIL, LipVQ-VAE improves performance by more
than 5.3% in high-fidelity simulators, with real-world experiments confirming
its ability to produce smoother, more reliable trajectories. Code and
checkpoints will be released.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CushionCatch: A Compliant Catching Mechanism for Mobile Manipulators via
  Combined Optimization and Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14754v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14754v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bingjie Chen, Keyu Fan, Qi Yang, Yi Cheng, Houde Liu, Kangkang Dong, Chongkun Xia, Liang Han, Bin Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Catching flying objects with a cushioning process is a skill commonly
performed by humans, yet it remains a significant challenge for robots. In this
paper, we present a framework that combines optimization and learning to
achieve compliant catching on mobile manipulators (CCMM). First, we propose a
high-level capture planner for mobile manipulators (MM) that calculates the
optimal capture point and joint configuration. Next, the pre-catching (PRC)
planner ensures the robot reaches the target joint configuration as quickly as
possible. To learn compliant catching strategies, we propose a network that
leverages the strengths of LSTM for capturing temporal dependencies and
positional encoding for spatial context (P-LSTM). This network is designed to
effectively learn compliant strategies from human demonstrations. Following
this, the post-catching (POC) planner tracks the compliant sequence output by
the P-LSTM while avoiding potential collisions due to structural differences
between humans and robots. We validate the CCMM framework through both
simulated and real-world ball-catching scenarios, achieving a success rate of
98.70% in simulation, 92.59% in real-world tests, and a 28.7% reduction in
impact torques. The open source code will be released for the reference of the
community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HARMONIC: Cognitive and Control Collaboration in Human-Robotic Teams <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18047v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18047v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanjay Oruganti, Sergei Nirenburg, Marjorie McShane, Jesse English, Michael K. Roberts, Christian Arndt, Sahithi Kamireddy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces HARMONIC, a cognitive-robotic architecture that
integrates the OntoAgent cognitive framework with general-purpose robot control
systems applied to human-robot teaming (HRT). We also present a cognitive
strategy for robots that incorporates metacognition, natural language
communication, and explainability capabilities required for collaborative
partnerships in HRT. Through simulation experiments involving a joint search
task performed by a heterogeneous team of a UGV, a drone, and a human operator,
we demonstrate the system's ability to coordinate actions between robots with
heterogeneous capabilities, adapt to complex scenarios, and facilitate natural
human-robot communication. Evaluation results show that robots using the
OntoAgent architecture within the HARMONIC framework can reason about plans,
goals, and team member attitudes while providing clear explanations for their
decisions, which are essential prerequisites for realistic human-robot teaming.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing <span class="highlight-title">Multi</span>-Robot Semantic Navigation Through <span class="highlight-title">Multi</span>modal
  Chain-of-Thought Score Collaboration <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18292v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18292v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhixuan Shen, Haonan Luo, Kexun Chen, Fengmao Lv, Tianrui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding how humans cooperatively utilize semantic knowledge to explore
unfamiliar environments and decide on navigation directions is critical for
house service multi-robot systems. Previous methods primarily focused on
single-robot centralized planning strategies, which severely limited
exploration efficiency. Recent research has considered decentralized planning
strategies for multiple robots, assigning separate planning models to each
robot, but these approaches often overlook communication costs. In this work,
we propose Multimodal Chain-of-Thought Co-Navigation (MCoCoNav), a modular
approach that utilizes multimodal Chain-of-Thought to plan collaborative
semantic navigation for multiple robots. MCoCoNav combines visual perception
with Vision Language Models (VLMs) to evaluate exploration value through
probabilistic scoring, thus reducing time costs and achieving stable outputs.
Additionally, a global semantic map is used as a communication bridge,
minimizing communication overhead while integrating observational results.
Guided by scores that reflect exploration trends, robots utilize this map to
assess whether to explore new frontier points or revisit history nodes.
Experiments on HM3D_v0.2 and MP3D demonstrate the effectiveness of our
approach. Our code is available at https://github.com/FrankZxShen/MCoCoNav.git.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 10 figures, Extended Version of accepted AAAI 2025 Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adapting Pre-Trained Vision Models for Novel Instance Detection and
  Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.17859v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.17859v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yangxiao Lu, Jishnu Jaykumar P, Yunhui Guo, Nicholas Ruozzi, Yu Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Novel Instance Detection and Segmentation (NIDS) aims at detecting and
segmenting novel object instances given a few examples of each instance. We
propose a unified, simple, yet effective framework (NIDS-Net) comprising object
proposal generation, embedding creation for both instance templates and
proposal regions, and embedding matching for instance label assignment.
Leveraging recent advancements in large vision methods, we utilize Grounding
DINO and Segment Anything Model (SAM) to obtain object proposals with accurate
bounding boxes and masks. Central to our approach is the generation of
high-quality instance embeddings. We utilized foreground feature averages of
patch embeddings from the DINOv2 ViT backbone, followed by refinement through a
weight adapter mechanism that we introduce.
  We show experimentally that our weight adapter can adjust the embeddings
locally within their feature space and effectively limit overfitting in the
few-shot setting. Furthermore, the weight adapter optimizes weights to enhance
the distinctiveness of instance embeddings during similarity computation. This
methodology enables a straightforward matching strategy that results in
significant performance gains. Our framework surpasses current state-of-the-art
methods, demonstrating notable improvements in four detection datasets. In the
segmentation tasks on seven core datasets of the BOP challenge, our method
outperforms the leading published RGB methods and remains competitive with the
best RGB-D method. We have also verified our method using real-world images
from a Fetch robot and a RealSense camera. Project Page:
https://irvlutd.github.io/NIDSNet/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://irvlutd.github.io/NIDSNet/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vision-based Geo-Localization of Future Mars Rotorcraft in Challenging
  Illumination Conditions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dario Pisanti, Robert Hewitt, Roland Brockers, Georgios Georgakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Planetary exploration using aerial assets has the potential for unprecedented
scientific discoveries on Mars. While NASA's Mars helicopter Ingenuity proved
flight in Martian atmosphere is possible, future Mars rotocrafts will require
advanced navigation capabilities for long-range flights. One such critical
capability is Map-based Localization (MbL) which registers an onboard image to
a reference map during flight in order to mitigate cumulative drift from visual
odometry. However, significant illumination differences between rotocraft
observations and a reference map prove challenging for traditional MbL systems,
restricting the operational window of the vehicle. In this work, we investigate
a new MbL system and propose Geo-LoFTR, a geometry-aided deep learning model
for image registration that is more robust under large illumination differences
than prior models. The system is supported by a custom simulation framework
that uses real orbital maps to produce large amounts of realistic images of the
Martian terrain. Comprehensive evaluations show that our proposed system
outperforms prior MbL efforts in terms of localization accuracy under
significant lighting and scale variations. Furthermore, we demonstrate the
validity of our approach across a simulated Martian day.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GND: Global Navigation <span class="highlight-title">Dataset</span> with <span class="highlight-title">Multi</span>-Modal Perception and
  <span class="highlight-title">Multi</span>-Category Traversability in Outdoor Campus Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14262v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14262v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Liang, Dibyendu Das, Daeun Song, Md Nahid Hasan Shuvo, Mohammad Durrani, Karthik Taranath, Ivan Penskiy, Dinesh Manocha, Xuesu Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Navigating large-scale outdoor environments requires complex reasoning in
terms of geometric structures, environmental semantics, and terrain
characteristics, which are typically captured by onboard sensors such as LiDAR
and cameras. While current mobile robots can navigate such environments using
pre-defined, high-precision maps based on hand-crafted rules catered for the
specific environment, they lack commonsense reasoning capabilities that most
humans possess when navigating unknown outdoor spaces. To address this gap, we
introduce the Global Navigation Dataset (GND), a large-scale dataset that
integrates multi-modal sensory data, including 3D LiDAR point clouds and RGB
and 360-degree images, as well as multi-category traversability maps
(pedestrian walkways, vehicle roadways, stairs, off-road terrain, and
obstacles) from ten university campuses. These environments encompass a variety
of parks, urban settings, elevation changes, and campus layouts of different
scales. The dataset covers approximately 2.7km2 and includes at least 350
buildings in total. We also present a set of novel applications of GND to
showcase its utility to enable global robot navigation, such as map-based
global navigation, mapless navigation, and global place recognition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LS-HAR: Language Supervised Human Action Recognition with Salient
  Fusion, Construction Sites as a Use-Case 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01962v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01962v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Mahdavian, Mohammad Loni, Ted Samuelsson, Mo Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting human actions is a crucial task for autonomous robots and vehicles,
often requiring the integration of various data modalities for improved
accuracy. In this study, we introduce a novel approach to Human Action
Recognition (HAR) using language supervision named LS-HAR based on skeleton and
visual cues. Our method leverages a language model to guide the feature
extraction process in the skeleton encoder. Specifically, we employ learnable
prompts for the language model conditioned on the skeleton modality to optimize
feature representation. Furthermore, we propose a fusion mechanism that
combines dual-modality features using a salient fusion module, incorporating
attention and transformer mechanisms to address the modalities' high
dimensionality. This fusion process prioritizes informative video frames and
body joints, enhancing the recognition accuracy of human actions. Additionally,
we introduce a new dataset tailored for real-world robotic applications in
construction sites, featuring visual, skeleton, and depth data modalities,
named VolvoConstAct. This dataset serves to facilitate the training and
evaluation of machine learning models to instruct autonomous construction
machines for performing necessary tasks in real-world construction sites. To
evaluate our approach, we conduct experiments on our dataset as well as three
widely used public datasets: NTU-RGB+D, NTU-RGB+D 120, and NW-UCLA. Results
reveal that our proposed method achieves promising performance across all
datasets, demonstrating its robustness and potential for various applications.
The code, dataset, and demonstration of real-machine experiments are available
at: https://mmahdavian.github.io/ls_har/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CSCPR: Cross-Source-Context Indoor RGB-D Place Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17457v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17457v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Liang, Zhuo Deng, Zheming Zhou, Min Sun, Omid Ghasemalizadeh, Cheng-Hao Kuo, Arnie Sen, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We extend our previous work, PoCo, and present a new algorithm,
Cross-Source-Context Place Recognition (CSCPR), for RGB-D indoor place
recognition that integrates global retrieval and reranking into an end-to-end
model and keeps the consistency of using Context-of-Clusters (CoCs) for feature
processing. Unlike prior approaches that primarily focus on the RGB domain for
place recognition reranking, CSCPR is designed to handle the RGB-D data. We
apply the CoCs to handle cross-sourced and cross-scaled RGB-D point clouds and
introduce two novel modules for reranking: the Self-Context Cluster (SCC) and
the Cross Source Context Cluster (CSCC), which enhance feature representation
and match query-database pairs based on local features, respectively. We also
release two new datasets, ScanNetIPR and ARKitIPR. Our experiments demonstrate
that CSCPR significantly outperforms state-of-the-art models on these datasets
by at least 29.27% in Recall@1 on the ScanNet-PR dataset and 43.24% in the new
datasets. Code and datasets will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Constitutional Filter: Bayesian Estimation of Compliant Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18347v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18347v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Kohaut, Felix Divo, Benedict Flade, Devendra Singh Dhami, Julian Eggert, Kristian Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Predicting agents impacted by legal policies, physical limitations, and
operational preferences is inherently difficult. In recent years,
neuro-symbolic methods have emerged, integrating machine learning and symbolic
reasoning models into end-to-end learnable systems. Hereby, a promising avenue
for expressing high-level constraints over multi-modal input data in robotics
has opened up. This work introduces an approach for Bayesian estimation of
agents expected to comply with a human-interpretable neuro-symbolic model we
call its Constitution. Hence, we present the Constitutional Filter (CoFi),
leading to improved tracking of agents by leveraging expert knowledge,
incorporating deep learning architectures, and accounting for environmental
uncertainties. CoFi extends the general, recursive Bayesian estimation setting,
ensuring compatibility with a vast landscape of established techniques such as
Particle Filters. To underpin the advantages of CoFi, we evaluate its
performance on real-world marine traffic data. Beyond improved performance, we
show how CoFi can learn to trust and adapt to the level of compliance of an
agent, recovering baseline performance even if the assumed Constitution clashes
with reality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ meSch: <span class="highlight-title">Multi</span>-Agent Energy-Aware Scheduling for Task Persistence <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04560v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04560v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaleb Ben Naveed, An Dang, Rahul Kumar, Dimitra Panagou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper develops a scheduling protocol for a team of autonomous robots
that operate on long-term persistent tasks. The proposed framework, called
meSch, accounts for the limited battery capacity of the robots and ensures that
the robots return to charge their batteries one at a time at the single
charging station. The protocol is applicable to general nonlinear robot models
under certain assumptions, does not require robots to be deployed at different
times, and can handle robots with different discharge rates. We further
consider the case when the charging station is mobile and its state information
is subject to uncertainty. The feasibility of the algorithm in terms of
ensuring persistent charging is given under certain assumptions, while the
efficacy of meSch is validated through simulation and hardware experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE/RSJ International Conference on Intelligent Robots
  and Systems (IROS) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Introduction to Online Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09619v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09619v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elad Hazan, Karan Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This text presents an introduction to an emerging paradigm in control of
dynamical systems and differentiable reinforcement learning called online
nonstochastic control. The new approach applies techniques from online convex
optimization and convex relaxations to obtain new methods with provable
guarantees for classical settings in optimal and robust control.
  The primary distinction between online nonstochastic control and other
frameworks is the objective. In optimal control, robust control, and other
control methodologies that assume stochastic noise, the goal is to perform
comparably to an offline optimal strategy. In online nonstochastic control,
both the cost functions as well as the perturbations from the assumed dynamical
model are chosen by an adversary. Thus the optimal policy is not defined a
priori. Rather, the target is to attain low regret against the best policy in
hindsight from a benchmark class of policies.
  This objective suggests the use of the decision making framework of online
convex optimization as an algorithmic methodology. The resulting methods are
based on iterative mathematical optimization algorithms, and are accompanied by
finite-time regret and computational complexity guarantees.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Draft; comments/suggestions welcome at
  nonstochastic.control@gmail.com</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lab2Car: A Versatile Wrapper for Deploying Experimental Planners in
  Complex Real-wo<span class="highlight-title">rl</span>d Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09523v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09523v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marc Heim, Francisco Suarez-Ruiz, Ishraq Bhuiyan, Bruno Brito, Momchil S. Tomov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-level autonomous driving is an ever-elusive goal, with planning and
decision making -- the cognitive functions that determine driving behavior --
posing the greatest challenge. Despite a proliferation of promising approaches,
progress is stifled by the difficulty of deploying experimental planners in
naturalistic settings. In this work, we propose Lab2Car, an optimization-based
wrapper that can take a trajectory sketch from an arbitrary motion planner and
convert it to a safe, comfortable, dynamically feasible trajectory that the car
can follow. This allows motion planners that do not provide such guarantees to
be safely tested and optimized in real-world environments. We demonstrate the
versatility of Lab2Car by using it to deploy a machine learning (ML) planner
and a classical planner on self-driving cars in Las Vegas. The resulting
systems handle challenging scenarios, such as cut-ins, overtaking, and
yielding, in complex urban environments like casino pick-up/drop-off areas. Our
work paves the way for quickly deploying and evaluating candidate motion
planners in realistic settings, ensuring rapid iteration and accelerating
progress towards human-level autonomy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FlexiFly: Interfacing the Physical Wo<span class="highlight-title">rl</span>d with Foundation Models
  Empowered by Reconfigurable Drone Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.12853v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.12853v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghui Zhao, Junxi Xia, Kaiyuan Hou, Yanchen Liu, Stephen Xia, Xiaofan Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models (FM) have shown immense human-like capabilities for
generating digital media. However, foundation models that can freely sense,
interact, and actuate the physical domain is far from being realized. This is
due to 1) requiring dense deployments of sensors to fully cover and analyze
large spaces, while 2) events often being localized to small areas, making it
difficult for FMs to pinpoint relevant areas of interest relevant to the
current task. We propose FlexiFly, a platform that enables FMs to ``zoom in''
and analyze relevant areas with higher granularity to better understand the
physical environment and carry out tasks. FlexiFly accomplishes by introducing
1) a novel image segmentation technique that aids in identifying relevant
locations and 2) a modular and reconfigurable sensing and actuation drone
platform that FMs can actuate to ``zoom in'' with relevant sensors and
actuators. We demonstrate through real smart home deployments that FlexiFly
enables FMs and LLMs to complete diverse tasks up to $85\%$ more successfully.
FlexiFly is critical step towards FMs and LLMs that can naturally interface
with the physical world.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by ACM SenSys 2025. The published version is
  https://doi.org/10.1145/3715014.3722081 in ACM Digital Library</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adapting Gait Frequency for Posture-regulating Humanoid Push-recovery
  via Hierarchical Model Predictive Control <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14342v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14342v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junheng Li, Zhanhao Le, Junchao Ma, Quan Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current humanoid push-recovery strategies often use whole-body motion, yet
they tend to overlook posture regulation. For instance, in manipulation tasks,
the upper body may need to stay upright and have minimal recovery displacement.
This paper introduces a novel approach to enhancing humanoid push-recovery
performance under unknown disturbances and regulating body posture by tailoring
the recovery stepping strategy. We propose a hierarchical-MPC-based scheme that
analyzes and detects instability in the prediction window and quickly recovers
through adapting gait frequency. Our approach integrates a high-level nonlinear
MPC, a posture-aware gait frequency adaptation planner, and a low-level convex
locomotion MPC. The planners predict the center of mass (CoM) state
trajectories that can be assessed for precursors of potential instability and
posture deviation. In simulation, we demonstrate improved maximum recoverable
impulse by 131% on average compared with baseline approaches. In hardware
experiments, a 125 ms advancement in recovery stepping timing/reflex has been
observed with the proposed approach. We also demonstrate improved push-recovery
performance and minimized body attitude change under 0.2 rad.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 6 figures, accepted to ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LGR2: Language Guided Reward Relabeling for Accelerating Hierarchical
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05881v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05881v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Utsav Singh, Pramit Bhattacharyya, Vinay P. Namboodiri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing interactive systems that utilize natural language instructions to
solve complex robotic control tasks has long been a goal of the robotics
community. While Large Language Models (LLMs) excel at logical reasoning,
in-context learning, and code generation, translating high-level instructions
into low-level robotic actions still remains challenging. Furthermore, solving
such tasks often requires acquiring policies to execute diverse subtasks and
integrating them to achieve the final objective. Hierarchical Reinforcement
Learning (HRL) offers a promising solution for solving such tasks by enabling
temporal abstraction and improved exploration. However, HRL suffers from
non-stationarity caused by the changing lower-level behaviour, which hinders
effective policy learning. We propose LGR2, a novel HRL framework that
mitigates non-stationarity in HRL by using language-guided higher-level rewards
that remain unaffected by the changing lower-level policy behaviour. To analyze
the efficacy of our approach, we perform empirical analysis to demonstrate that
LGR2 effectively mitigates non-stationarity in HRL and attains success rates
exceeding 70% in challenging, sparsely-rewarded robotic navigation and
manipulation environments, where other baselines typically fail to show
significant progress. Finally, we perform real-world robotic experiments on
complex tasks and demonstrate that LGR2 consistently outperforms the baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ iFANnpp: Nuclear Power Plant Digital Twin for Robots and Autonomous
  Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09213v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09213v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youndo Do, Marc Zebrowitz, Jackson Stahl, Fan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotics has gained significant attention in the nuclear industry due its
precision and ability to automate tasks. However, the increasing complexity of
robots has led to a growing demand for advanced simulation and control methods
to predict robot behavior and optimize plant performance, motivating the use of
digital twins for robotic applications. Most existing digital twins only
address parts of systems and do not offer a total design of a nuclear power
plant. Furthermore, they are often designed for specific algorithms or tasks,
making them unsuitable for broader research applications or projects. In
response, this work proposes a comprehensive nuclear power plant digital twin
designed to improve real-time monitoring, operational efficiency, and
predictive maintenance. The full nuclear power plant is modeled in Unreal
Engine 5 to incorporate the complexities and various phenomena. The
high-resolution simulation environment is integrated with a Generic Pressurized
Water Reactor Simulator, a high-fidelity physics-driven software, to create a
realistic model of a nuclear power plant and a real-time updated virtual
environment. The virtual environment provides various features for researchers
to easily test custom robot algorithms and frameworks, applicable to research
in the nuclear industry as well as industrial systems in general. The digital
twin's performance is presented, and critical research problems are addressed,
including multi-robot task scheduling and robot navigation in
radiation-affected areas, by leveraging implemented features.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 10 figures; submitted to IEEE Transactions on Automation
  Science and Engineering</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The MASK Benchmark: Disentangling Honesty From Accuracy in AI Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03750v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03750v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Ren, Arunim Agarwal, Mantas Mazeika, Cristina Menghini, Robert Vacareanu, Brad Kenstler, Mick Yang, Isabelle Barrass, Alice Gatti, Xuwang Yin, Eduardo Trevino, Matias Geralnik, Adam Khoja, Dean Lee, Summer Yue, Dan Hendrycks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) become more capable and agentic, the
requirement for trust in their outputs grows significantly, yet at the same
time concerns have been mounting that models may learn to lie in pursuit of
their goals. To address these concerns, a body of work has emerged around the
notion of "honesty" in LLMs, along with interventions aimed at mitigating
deceptive behaviors. However, evaluations of honesty are currently highly
limited, with no benchmark combining large scale and applicability to all
models. Moreover, many benchmarks claiming to measure honesty in fact simply
measure accuracy--the correctness of a model's beliefs--in disguise. In this
work, we introduce a large-scale human-collected dataset for measuring honesty
directly, allowing us to disentangle accuracy from honesty for the first time.
Across a diverse set of LLMs, we find that while larger models obtain higher
accuracy on our benchmark, they do not become more honest. Surprisingly, while
most frontier LLMs obtain high scores on truthfulness benchmarks, we find a
substantial propensity in frontier LLMs to lie when pressured to do so,
resulting in low honesty scores on our benchmark. We find that simple methods,
such as representation engineering interventions, can improve honesty. These
results underscore the growing need for robust evaluations and effective
interventions to ensure LLMs remain trustworthy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Website: https://www.mask-benchmark.ai</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Process-based Self-Rewarding Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shimao Zhang, Xiao Liu, Xin Zhang, Junxiao Liu, Zheheng Luo, Shujian Huang, Yeyun Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models have demonstrated outstanding performance across
various downstream tasks and have been widely applied in multiple scenarios.
Human-annotated preference data is used for training to further improve LLMs'
performance, which is constrained by the upper limit of human performance.
Therefore, Self-Rewarding method has been proposed, where LLMs generate
training data by rewarding their own outputs. However, the existing
self-rewarding paradigm is not effective in mathematical reasoning scenarios
and may even lead to a decline in performance. In this work, we propose the
Process-based Self-Rewarding pipeline for language models, which introduces
long-thought reasoning, step-wise LLM-as-a-Judge, and step-wise preference
optimization within the self-rewarding paradigm. Our new paradigm successfully
enhances the performance of LLMs on multiple mathematical reasoning benchmarks
through iterative Process-based Self-Rewarding, demonstrating the immense
potential of self-rewarding to achieve LLM reasoning that may surpass human
capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CHOP: Mobile Operating Assistant with Constrained High-frequency
  Optimized Subtask Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03743v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03743v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuqi Zhou, Shuai Wang, Sunhao Dai, Qinglin Jia, Zhaocheng Du, Zhenhua Dong, Jun Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advancement of visual language models (VLMs) has enhanced mobile device
operations, allowing simulated human-like actions to address user requirements.
Current VLM-based mobile operating assistants can be structured into three
levels: task, subtask, and action. The subtask level, linking high-level goals
with low-level executable actions, is crucial for task completion but faces two
challenges: ineffective subtasks that lower-level agent cannot execute and
inefficient subtasks that fail to contribute to the completion of the
higher-level task. These challenges stem from VLM's lack of experience in
decomposing subtasks within GUI scenarios in multi-agent architecture. To
address these, we propose a new mobile assistant architecture with constrained
high-frequency o}ptimized planning (CHOP). Our approach overcomes the VLM's
deficiency in GUI scenarios planning by using human-planned subtasks as the
basis vector. We evaluate our architecture in both English and Chinese contexts
across 20 Apps, demonstrating significant improvements in both effectiveness
and efficiency. Our dataset and code is available at
https://github.com/Yuqi-Zhou/CHOP
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Deep Clustering Paradigms: Self-Supervision Is All You Need 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amal Shaheena, Nairouz Mrabahb, Riadh Ksantinia, Abdulla Alqaddoumia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advances in deep clustering have been made possible by significant
progress in self-supervised and pseudo-supervised learning. However, the
trade-off between self-supervision and pseudo-supervision can give rise to
three primary issues. The joint training causes Feature Randomness and Feature
Drift, whereas the independent training causes Feature Randomness and Feature
Twist. In essence, using pseudo-labels generates random and unreliable
features. The combination of pseudo-supervision and self-supervision drifts the
reliable clustering-oriented features. Moreover, moving from self-supervision
to pseudo-supervision can twist the curved latent manifolds. This paper
addresses the limitations of existing deep clustering paradigms concerning
Feature Randomness, Feature Drift, and Feature Twist. We propose a new paradigm
with a new strategy that replaces pseudo-supervision with a second round of
self-supervision training. The new strategy makes the transition between
instance-level self-supervision and neighborhood-level self-supervision
smoother and less abrupt. Moreover, it prevents the drifting effect that is
caused by the strong competition between instance-level self-supervision and
clustering-level pseudo-supervision. Moreover, the absence of the
pseudo-supervision prevents the risk of generating random features. With this
novel approach, our paper introduces a Rethinking of the Deep Clustering
Paradigms, denoted by R-DC. Our model is specifically designed to address three
primary challenges encountered in Deep Clustering: Feature Randomness, Feature
Drift, and Feature Twist. Experimental results conducted on six datasets have
shown that the two-level self-supervision training yields substantial
improvements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Causal Behavioral Policy Learning: Applications to Healthcare 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Knecht, Anna Zink, Jonathan Kolstad, Maya Petersen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a deep learning-based approach to studying dynamic clinical
behavioral regimes in diverse non-randomized healthcare settings. Our proposed
methodology - deep causal behavioral policy learning (DC-BPL) - uses deep
learning algorithms to learn the distribution of high-dimensional clinical
action paths, and identifies the causal link between these action paths and
patient outcomes. Specifically, our approach: (1) identifies the causal effects
of provider assignment on clinical outcomes; (2) learns the distribution of
clinical actions a given provider would take given evolving patient
information; (3) and combines these steps to identify the optimal provider for
a given patient type and emulate that provider's care decisions. Underlying
this strategy, we train a large clinical behavioral model (LCBM) on electronic
health records data using a transformer architecture, and demonstrate its
ability to estimate clinical behavioral policies. We propose a novel
interpretation of a behavioral policy learned using the LCBM: that it is an
efficient encoding of complex, often implicit, knowledge used to treat a
patient. This allows us to learn a space of policies that are critical to a
wide range of healthcare applications, in which the vast majority of clinical
knowledge is acquired tacitly through years of practice and only a tiny
fraction of information relevant to patient care is written down (e.g. in
textbooks, studies or standardized guidelines).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Learning in Biomechanics: Key Applications and Limitations in
  Walking, Running, and Sports Movements 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03717v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03717v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlo Dindorf, Fabian Horst, Djordje Slijepčević, Bernhard Dumphart, Jonas Dully, Matthias Zeppelzauer, Brian Horsak, Michael Fröhlich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This chapter provides an overview of recent and promising Machine Learning
applications, i.e. pose estimation, feature estimation, event detection, data
exploration & clustering, and automated classification, in gait (walking and
running) and sports biomechanics. It explores the potential of Machine Learning
methods to address challenges in biomechanical workflows, highlights central
limitations, i.e. data and annotation availability and explainability, that
need to be addressed, and emphasises the importance of interdisciplinary
approaches for fully harnessing the potential of Machine Learning in gait and
sports biomechanics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Video Tokenization: A Conditioned Diffusion-based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nianzu Yang, Pandeng Li, Liming Zhao, Yang Li, Chen-Wei Xie, Yehui Tang, Xudong Lu, Zhihang Liu, Yun Zheng, Yu Liu, Junchi Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video tokenizers, which transform videos into compact latent representations,
are key to video generation. Existing video tokenizers are based on the VAE
architecture and follow a paradigm where an encoder compresses videos into
compact latents, and a deterministic decoder reconstructs the original videos
from these latents. In this paper, we propose a novel
\underline{\textbf{C}}onditioned \underline{\textbf{D}}iffusion-based video
\underline{\textbf{T}}okenizer entitled \textbf{\ourmethod}, which departs from
previous methods by replacing the deterministic decoder with a 3D causal
diffusion model. The reverse diffusion generative process of the decoder is
conditioned on the latent representations derived via the encoder. With a
feature caching and sampling acceleration, the framework efficiently
reconstructs high-fidelity videos of arbitrary lengths. Results show that
{\ourmethod} achieves state-of-the-art performance in video reconstruction
tasks using just a single-step sampling. Even a smaller version of {\ourmethod}
still achieves reconstruction results on par with the top two baselines.
Furthermore, the latent video generation model trained using {\ourmethod} also
shows superior performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Curating Demonstrations using Online Experience 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Annie S. Chen, Alec M. Lessing, Yuejiang Liu, Chelsea Finn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many robot demonstration datasets contain heterogeneous demonstrations of
varying quality. This heterogeneity may benefit policy pre-training, but can
hinder robot performance when used with a final imitation learning objective.
In particular, some strategies in the data may be less reliable than others or
may be underrepresented in the data, leading to poor performance when such
strategies are sampled at test time. Moreover, such unreliable or
underrepresented strategies can be difficult even for people to discern, and
sifting through demonstration datasets is time-consuming and costly. On the
other hand, policy performance when trained on such demonstrations can reflect
the reliability of different strategies. We thus propose for robots to
self-curate based on online robot experience (Demo-SCORE). More specifically,
we train and cross-validate a classifier to discern successful policy roll-outs
from unsuccessful ones and use the classifier to filter heterogeneous
demonstration datasets. Our experiments in simulation and the real world show
that Demo-SCORE can effectively identify suboptimal demonstrations without
manual curation. Notably, Demo-SCORE achieves over 15-35% higher absolute
success rate in the resulting policy compared to the base policy trained with
all original demonstrations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ILLC: Iterative Layer-by-Layer Compression for Enhancing Structural
  Faithfulness in SpArX 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03693v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03693v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ungsik Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of Explainable Artificial Intelligence (XAI), argumentative XAI
approaches have been proposed to represent the internal reasoning process of
deep neural networks in a more transparent way by interpreting hidden nodes as
arguements. However, as the number of layers increases, existing compression
methods simplify all layers at once, which lead to high accumulative
information loss. To compensate for this, we propose an iterative
layer-by-layer compression technique in which each layer is compressed
separately and the reduction error in the next layer is immediately compensated
for, thereby improving the overall input-output and structural fidelity of the
model. Experiments on the Breast Cancer Diagnosis dataset show that, compared
to traditional compression, the method reduces input-output and structural
unfaithfulness, and maintains a more consistent attack-support relationship in
the Argumentative Explanation scheme. This is significant because it provides a
new way to make complex MLP models more compact while still conveying their
internal inference logic without distortion.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attentive Reasoning Queries: A Systematic Method for Optimizing
  Instruction-Following in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03669v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03669v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bar Karov, Dor Zohar, Yam Marcovitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Attentive Reasoning Queries (ARQs), a novel structured reasoning
approach that significantly improves instruction-following in Large Language
Models through domain-specialized reasoning blueprints. While LLMs demonstrate
remarkable capabilities across diverse tasks, they often fail to maintain
adherence to complex, use-case-specific instructions during multi-turn
conversations, presenting challenges for business-critical applications. ARQs
address this limitation by guiding LLMs through systematic reasoning steps with
targeted queries that reinstate critical instructions and facilitate
intermediate reasoning throughout the completion process. In extensive testing
within Parlant, our framework for reliable customer-facing agents in which ARQs
were born out of necessity, they achieved a 90.2% success rate across 87 test
scenarios, outperforming both Chain-of-Thought reasoning (86.1%) and direct
response generation (81.5%). ARQs showed particular strength in addressing
persistent failure modes like guideline re-application and hallucination
prevention. Our analysis also revealed that ARQs can potentially be more
computationally efficient than free-form reasoning when carefully designed.
These findings demonstrate that structured reasoning approaches provide
effective mechanisms for controlling how LLMs process information and make
decisions in complex scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Supplementary materials, including code, is available on our GitHub:
  https://github.com/emcie-co/parlant/tree/arqs-a-systematic-method-for-optimizing-instruction-following-in-llms</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Generative Approach to High Fidelity 3D Reconstruction from Text Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03664v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03664v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Venkat Kumar R, Deepak Saravanan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The convergence of generative artificial intelligence and advanced computer
vision technologies introduces a groundbreaking approach to transforming
textual descriptions into three-dimensional representations. This research
proposes a fully automated pipeline that seamlessly integrates text-to-image
generation, various image processing techniques, and deep learning methods for
reflection removal and 3D reconstruction. By leveraging state-of-the-art
generative models like Stable Diffusion, the methodology translates natural
language inputs into detailed 3D models through a multi-stage workflow.
  The reconstruction process begins with the generation of high-quality images
from textual prompts, followed by enhancement by a reinforcement learning agent
and reflection removal using the Stable Delight model. Advanced image upscaling
and background removal techniques are then applied to further enhance visual
fidelity. These refined two-dimensional representations are subsequently
transformed into volumetric 3D models using sophisticated machine learning
algorithms, capturing intricate spatial relationships and geometric
characteristics. This process achieves a highly structured and detailed output,
ensuring that the final 3D models reflect both semantic accuracy and geometric
precision.
  This approach addresses key challenges in generative reconstruction, such as
maintaining semantic coherence, managing geometric complexity, and preserving
detailed visual information. Comprehensive experimental evaluations will assess
reconstruction quality, semantic accuracy, and geometric fidelity across
diverse domains and varying levels of complexity. By demonstrating the
potential of AI-driven 3D reconstruction techniques, this research offers
significant implications for fields such as augmented reality (AR), virtual
reality (VR), and digital content creation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving 6D Object Pose Estimation of metallic Household and Industry
  Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03655v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03655v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Pöllabauer, Michael Gasser, Tristan Wirth, Sarah Berkei, Volker Knauthe, Arjan Kuijper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  6D object pose estimation suffers from reduced accuracy when applied to
metallic objects. We set out to improve the state-of-the-art by addressing
challenges such as reflections and specular highlights in industrial
applications. Our novel BOP-compatible dataset, featuring a diverse set of
metallic objects (cans, household, and industrial items) under various lighting
and background conditions, provides additional geometric and visual cues. We
demonstrate that these cues can be effectively leveraged to enhance overall
performance. To illustrate the usefulness of the additional features, we
improve upon the GDRNPP algorithm by introducing an additional keypoint
prediction and material estimator head in order to improve spatial scene
understanding. Evaluations on the new dataset show improved accuracy for
metallic objects, supporting the hypothesis that additional geometric and
visual cues can improve learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Neutral Point of View Text Generation through
  Parameter-Efficient Reinforcement Learning and a Small-Scale High-Quality
  <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03654v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03654v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jessica Hoffmann, Christiane Ahlheim, Zac Yu, Aria Walfrand, Jarvis Jin, Marie Tano, Ahmad Beirami, Erin van Liemt, Nithum Thain, Hakim Sidahmed, Lucas Dixon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes the construction of a dataset and the evaluation of
training methods to improve generative large language models' (LLMs) ability to
answer queries on sensitive topics with a Neutral Point of View (NPOV), i.e.,
to provide significantly more informative, diverse and impartial answers. The
dataset, the SHQ-NPOV dataset, comprises 300 high-quality, human-written
quadruplets: a query on a sensitive topic, an answer, an NPOV rating, and a set
of links to source texts elaborating the various points of view. The first key
contribution of this paper is a new methodology to create such datasets through
iterative rounds of human peer-critique and annotator training, which we
release alongside the dataset. The second key contribution is the
identification of a highly effective training regime for parameter-efficient
reinforcement learning (PE-RL) to improve NPOV generation. We compare and
extensively evaluate PE-RL and multiple baselines-including LoRA finetuning (a
strong baseline), SFT and RLHF.
  PE-RL not only improves on overall NPOV quality compared to the strongest
baseline ($97.06\%\rightarrow 99.08\%$), but also scores much higher on
features linguists identify as key to separating good answers from the best
answers ($60.25\%\rightarrow 85.21\%$ for presence of supportive details,
$68.74\%\rightarrow 91.43\%$ for absence of oversimplification). A qualitative
analysis corroborates this. Finally, our evaluation finds no statistical
differences between results on topics that appear in the training dataset and
those on separated evaluation topics, which provides strong evidence that our
approach to training PE-RL exhibits very effective out of topic generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decoupled Recommender Systems: Exploring Alternative Recommender
  Ecosystem Designs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03606v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03606v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anas Buhayh, Elizabeth McKinnie, Robin Burke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender ecosystems are an emerging subject of research. Such research
examines how the characteristics of algorithms, recommendation consumers, and
item providers influence system dynamics and long-term outcomes. One
architectural possibility that has not yet been widely explored in this line of
research is the consequences of a configuration in which recommendation
algorithms are decoupled from the platforms they serve. This is sometimes
called "the friendly neighborhood algorithm store" or "middleware" model. We
are particularly interested in how such architectures might offer a range of
different distributions of utility across consumers, providers, and
recommendation platforms. In this paper, we create a model of a recommendation
ecosystem that incorporates algorithm choice and examine the outcomes of such a
design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Understanding Text Hallucination of Diffusion Models via Local
  Generation Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Lu, Runzhe Wang, Kaifeng Lyu, Xitai Jiang, Gao Huang, Mengdi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Score-based diffusion models have achieved incredible performance in
generating realistic images, audio, and video data. While these models produce
high-quality samples with impressive details, they often introduce unrealistic
artifacts, such as distorted fingers or hallucinated texts with no meaning.
This paper focuses on textual hallucinations, where diffusion models correctly
generate individual symbols but assemble them in a nonsensical manner. Through
experimental probing, we consistently observe that such phenomenon is
attributed it to the network's local generation bias. Denoising networks tend
to produce outputs that rely heavily on highly correlated local regions,
particularly when different dimensions of the data distribution are nearly
pairwise independent. This behavior leads to a generation process that
decomposes the global distribution into separate, independent distributions for
each symbol, ultimately failing to capture the global structure, including
underlying grammar. Intriguingly, this bias persists across various denoising
network architectures including MLP and transformers which have the structure
to model global dependency. These findings also provide insights into
understanding other types of hallucinations, extending beyond text, as a result
of implicit biases in the denoising models. Additionally, we theoretically
analyze the training dynamics for a specific case involving a two-layer MLP
learning parity points on a hypercube, offering an explanation of its
underlying mechanism.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Small but Mighty: Enhancing Time Series Forecasting with Lightweight
  <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03594v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03594v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Fan, Bin Li, Yixuan Weng, Shoujun Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While LLMs have demonstrated remarkable potential in time series forecasting,
their practical deployment remains constrained by excessive computational
demands and memory footprints. Existing LLM-based approaches typically suffer
from three critical limitations: Inefficient parameter utilization in handling
numerical time series patterns; Modality misalignment between continuous
temporal signals and discrete text embeddings; and Inflexibility for real-time
expert knowledge integration. We present SMETimes, the first systematic
investigation of sub-3B parameter SLMs for efficient and accurate time series
forecasting. Our approach centers on three key innovations: A
statistically-enhanced prompting mechanism that bridges numerical time series
with textual semantics through descriptive statistical features; A adaptive
fusion embedding architecture that aligns temporal patterns with language model
token spaces through learnable parameters; And a dynamic mixture-of-experts
framework enabled by SLMs' computational efficiency, adaptively combining base
predictions with domain-specific models. Extensive evaluations across seven
benchmark datasets demonstrate that our 3B-parameter SLM achieves
state-of-the-art performance on five primary datasets while maintaining 3.8x
faster training and 5.2x lower memory consumption compared to 7B-parameter LLM
baselines. Notably, the proposed model exhibits better learning capabilities,
achieving 12.3% lower MSE than conventional LLM. Ablation studies validate that
our statistical prompting and cross-modal fusion modules respectively
contribute 15.7% and 18.2% error reduction in long-horizon forecasting tasks.
By redefining the efficiency-accuracy trade-off landscape, this work
establishes SLMs as viable alternatives to resource-intensive LLMs for
practical time series forecasting. Code and models are available at
https://github.com/xiyan1234567/SMETimes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ English K_Quantization of <span class="highlight-title">LLM</span>s Does Not Disproportionately Diminish
  <span class="highlight-title">Multi</span>lingual Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karl Audun Borgersen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For consumer usage of locally deployed LLMs, the GGUF format and
k_quantization are invaluable tools for maintaining the performance of the
original model while reducing it to sizes deployable with consumer-grade
hardware. The number of bits dedicated to each weight from the original model
is reduced based on how important they are thought to be during model
inference. This importance is arrived at through the application of an
'importance matrix'-a relatively small text document meant to be representative
of the LLM's standard use-cases. In the vast majority of quants available
online, this document is primarily written in English. It was therefore an open
question whether performance on English language tasks was preserved through
the sacrifice of multilingual performance and whether it can be preserved with
alternate importance matrices. This article investigates these hypotheses by
quantizing Llama3.3 70B on importance matrices written in three languages
(English, Norwegian, and Malayalam) and evaluating them on the MixEval dataset
in both English and Norwegian. All experiments related to k_quantization
yielded non-significant results (In all cases p > 0.237) indicating that
current quantization practices do not disproportionately harm multilingual
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Conceptual Model for Attributions in Event-Centric Knowledge Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Plötzky, Katarina Britz, Wolf-Tilo Balke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of narratives as a means of fusing information from knowledge graphs
(KGs) into a coherent line of argumentation has been the subject of recent
investigation. Narratives are especially useful in event-centric knowledge
graphs in that they provide a means to connect different real-world events and
categorize them by well-known narrations. However, specifically for
controversial events, a problem in information fusion arises, namely, multiple
viewpoints regarding the validity of certain event aspects, e.g., regarding the
role a participant takes in an event, may exist. Expressing those viewpoints in
KGs is challenging because disputed information provided by different
viewpoints may introduce inconsistencies. Hence, most KGs only feature a single
view on the contained information, hampering the effectiveness of narrative
information access. This paper is an extension of our original work and
introduces attributions, i.e., parameterized predicates that allow for the
representation of facts that are only valid in a specific viewpoint. For this,
we develop a conceptual model that allows for the representation of
viewpoint-dependent information. As an extension, we enhance the model by a
conception of viewpoint-compatibility. Based on this, we deepen our original
deliberations on the model's effects on information fusion and provide
additional grounding in the literature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Data & Knowledge Engineering, 22 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Visual Discrimination and Reasoning of Real-Wo<span class="highlight-title">rl</span>d Physical
  Dynamics: Physics-Grounded Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenqiao Li, Yao Gu, Xintao Chen, Xiaohao Xu, Ming Hu, Xiaonan Huang, Yingna Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans detect real-world object anomalies by perceiving, interacting, and
reasoning based on object-conditioned physical knowledge. The long-term goal of
Industrial Anomaly Detection (IAD) is to enable machines to autonomously
replicate this skill. However, current IAD algorithms are largely developed and
tested on static, semantically simple datasets, which diverge from real-world
scenarios where physical understanding and reasoning are essential.To bridge
this gap, we introduce the Physics Anomaly Detection (Phys-AD) dataset, the
first large-scale, real-world, physics-grounded video dataset for industrial
anomaly detection. Collected using a real robot arm and motor, Phys-AD provides
a diverse set of dynamic, semantically rich scenarios. The dataset includes
more than 6400 videos across 22 real-world object categories, interacting with
robot arms and motors, and exhibits 47 types of anomalies. Anomaly detection in
Phys-AD requires visual reasoning, combining both physical knowledge and video
content to determine object abnormality.We benchmark state-of-the-art anomaly
detection methods under three settings: unsupervised AD, weakly-supervised AD,
and video-understanding AD, highlighting their limitations in handling
physics-grounded anomalies. Additionally, we introduce the Physics Anomaly
Explanation (PAEval) metric, designed to assess the ability of visual-language
foundation models to not only detect anomalies but also provide accurate
explanations for their underlying physical causes. Our dataset and benchmark
will be publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI-Enabled Conversational Journaling for Advancing Parkinson's Disease
  Symptom Tracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03532v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03532v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mashrur Rashik, Shilpa Sweth, Nishtha Agrawal, Saiyyam Kochar, Kara M Smith, Fateme Rajabiyazdi, Vidya Setlur, Narges Mahyar, Ali Sarvghad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Journaling plays a crucial role in managing chronic conditions by allowing
patients to document symptoms and medication intake, providing essential data
for long-term care. While valuable, traditional journaling methods often rely
on static, self-directed entries, lacking interactive feedback and real-time
guidance. This gap can result in incomplete or imprecise information, limiting
its usefulness for effective treatment. To address this gap, we introduce
PATRIKA, an AI-enabled prototype designed specifically for people with
Parkinson's disease (PwPD). The system incorporates cooperative conversation
principles, clinical interview simulations, and personalization to create a
more effective and user-friendly journaling experience. Through two user
studies with PwPD and iterative refinement of PATRIKA, we demonstrate
conversational journaling's significant potential in patient engagement and
collecting clinically valuable information. Our results showed that generating
probing questions PATRIKA turned journaling into a bi-directional interaction.
Additionally, we offer insights for designing journaling systems for healthcare
and future directions for promoting sustained journaling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the ACM CHI conference on Human Factors in Computing
  Systems (CHI), 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AdaSin: Enhancing Hard Sample Metrics with Dual Adaptive Penalty for
  Face Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiqi Guo, Zhuowen Zheng, Guanghua Yang, Zhiquan Liu, Xiaofan Li, Jianqing Li, Jinyu Tian, Xueyuan Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the emergence of deep convolutional neural networks has
positioned face recognition as a prominent research focus in computer vision.
Traditional loss functions, such as margin-based, hard-sample mining-based, and
hybrid approaches, have achieved notable performance improvements, with some
leveraging curriculum learning to optimize training. However, these methods
often fall short in effectively quantifying the difficulty of hard samples. To
address this, we propose Adaptive Sine (AdaSin) loss function, which introduces
the sine of the angle between a sample's embedding feature and its ground-truth
class center as a novel difficulty metric. This metric enables precise and
effective penalization of hard samples. By incorporating curriculum learning,
the model dynamically adjusts classification boundaries across different
training stages. Unlike previous adaptive-margin loss functions, AdaSin
introduce a dual adaptive penalty, applied to both the positive and negative
cosine similarities of hard samples. This design imposes stronger constraints,
enhancing intra-class compactness and inter-class separability. The combination
of the dual adaptive penalty and curriculum learning is guided by a
well-designed difficulty metric. It enables the model to focus more effectively
on hard samples in later training stages, and lead to the extraction of highly
discriminative face features. Extensive experiments across eight benchmarks
demonstrate that AdaSin achieves superior accuracy compared to other
state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NeuGrasp: Generalizable Neural Surface Reconstruction with Background
  Priors for Material-Agnostic Object Grasp Detection <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03511v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03511v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyu Fan, Yinghao Cai, Chao Li, Wenzhe He, Xudong Zheng, Tao Lu, Bin Liang, Shuo Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic grasping in scenes with transparent and specular objects presents
great challenges for methods relying on accurate depth information. In this
paper, we introduce NeuGrasp, a neural surface reconstruction method that
leverages background priors for material-agnostic grasp detection. NeuGrasp
integrates transformers and global prior volumes to aggregate multi-view
features with spatial encoding, enabling robust surface reconstruction in
narrow and sparse viewing conditions. By focusing on foreground objects through
residual feature enhancement and refining spatial perception with an
occupancy-prior volume, NeuGrasp excels in handling objects with transparent
and specular surfaces. Extensive experiments in both simulated and real-world
scenarios show that NeuGrasp outperforms state-of-the-art methods in grasping
while maintaining comparable reconstruction quality. More details are available
at https://neugrasp.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures. IEEE International Conference on Robotics and
  Automation (ICRA) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Synthetic Data definitions: A privacy driven approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vibeke Binz Vallevik, Serena Elizabeth Marshall, Aleksandar Babic, Jan Franz Nygaard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Synthetic data is gaining traction as a cost-effective solution for the
increasing data demands of AI development and can be generated either from
existing knowledge or derived data captured from real-world events. The source
of the synthetic data generation and the technique used significantly impacts
its residual privacy risk and therefore its opportunity for sharing.
Traditional classification of synthetic data types no longer fit the newer
generation techniques and there is a need to better align the classification
with practical needs. We suggest a new way of grouping synthetic data types
that better supports privacy evaluations to aid regulatory policymaking. Our
novel classification provides flexibility to new advancements like deep
generative methods and offers a more practical framework for future
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Parallelized Planning-Acting for Efficient <span class="highlight-title">LLM</span>-based <span class="highlight-title">Multi</span>-Agent Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03505v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03505v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaoru Li, Shunyu Liu, Tongya Zheng, Mingli Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in Large Language Model(LLM)-based Multi-Agent
Systems(MAS) have demonstrated remarkable potential for tackling complex
decision-making tasks. However, existing frameworks inevitably rely on
serialized execution paradigms, where agents must complete sequential LLM
planning before taking action. This fundamental constraint severely limits
real-time responsiveness and adaptation, which is crucial in dynamic
environments with ever-changing scenarios. In this paper, we propose a novel
parallelized planning-acting framework for LLM-based MAS, featuring a
dual-thread architecture with interruptible execution to enable concurrent
planning and acting. Specifically, our framework comprises two core threads:(1)
a planning thread driven by a centralized memory system, maintaining
synchronization of environmental states and agent communication to support
dynamic decision-making; and (2) an acting thread equipped with a comprehensive
skill library, enabling automated task execution through recursive
decomposition. Extensive experiments on challenging Minecraft demonstrate the
effectiveness of the proposed framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Collaborative Expert <span class="highlight-title">LLM</span>s Guided <span class="highlight-title">Multi</span>-Objective Molecular Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03503v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03503v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiajun Yu, Yizhen Zheng, Huan Yee Koh, Shirui Pan, Tianyue Wang, Haishuai Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Molecular optimization is a crucial yet complex and time-intensive process
that often acts as a bottleneck for drug development. Traditional methods rely
heavily on trial and error, making multi-objective optimization both
time-consuming and resource-intensive. Current AI-based methods have shown
limited success in handling multi-objective optimization tasks, hampering their
practical utilization. To address this challenge, we present MultiMol, a
collaborative large language model (LLM) system designed to guide
multi-objective molecular optimization. MultiMol comprises two agents,
including a data-driven worker agent and a literature-guided research agent.
The data-driven worker agent is a large language model being fine-tuned to
learn how to generate optimized molecules considering multiple objectives,
while the literature-guided research agent is responsible for searching
task-related literature to find useful prior knowledge that facilitates
identifying the most promising optimized candidates. In evaluations across six
multi-objective optimization tasks, MultiMol significantly outperforms existing
methods, achieving a 82.30% success rate, in sharp contrast to the 27.50%
success rate of current strongest methods. To further validate its practical
impact, we tested MultiMol on two real-world challenges. First, we enhanced the
selectivity of Xanthine Amine Congener (XAC), a promiscuous ligand that binds
both A1R and A2AR, successfully biasing it towards A1R. Second, we improved the
bioavailability of Saquinavir, an HIV-1 protease inhibitor with known
bioavailability limitations. Overall, these results indicate that MultiMol
represents a highly promising approach for multi-objective molecular
optimization, holding great potential to accelerate the drug development
process and contribute to the advancement of pharmaceutical research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CURVALID: Geometrically-guided Adversarial Prompt Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Canaan Yung, Hanxun Huang, Sarah Monazam Erfani, Christopher Leckie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial prompts capable of jailbreaking large language models (LLMs) and
inducing undesirable behaviours pose a significant obstacle to their safe
deployment. Current mitigation strategies rely on activating built-in defence
mechanisms or fine-tuning the LLMs, but the fundamental distinctions between
adversarial and benign prompts are yet to be understood. In this work, we
introduce CurvaLID, a novel defense framework that efficiently detects
adversarial prompts by leveraging their geometric properties. It is agnostic to
the type of LLM, offering a unified detection framework across diverse
adversarial prompts and LLM architectures. CurvaLID builds on the geometric
analysis of text prompts to uncover their underlying differences. We
theoretically extend the concept of curvature via the Whewell equation into an
$n$-dimensional word embedding space, enabling us to quantify local geometric
properties, including semantic shifts and curvature in the underlying
manifolds. Additionally, we employ Local Intrinsic Dimensionality (LID) to
capture geometric features of text prompts within adversarial subspaces. Our
findings reveal that adversarial prompts differ fundamentally from benign
prompts in terms of their geometric characteristics. Our results demonstrate
that CurvaLID delivers superior detection and rejection of adversarial queries,
paving the way for safer LLM deployment. The source code can be found at
https://github.com/Cancanxxx/CurvaLID
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 Pages, 5 figues</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SafeVLA: Towards Safety Alignment of Vision-Language-Action Model via
  Safe Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Borong Zhang, Yuhao Zhang, Jiaming Ji, Yingshan Lei, Josef Dai, Yuanpei Chen, Yaodong Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-language-action models (VLAs) have shown great potential as generalist
robot policies. However, these models pose urgent safety challenges during
deployment, including the risk of physical harm to the environment, the robot
itself, and humans. How can safety be explicitly incorporated into VLAs? In
this work, we propose SafeVLA, a novel algorithm designed to integrate safety
into VLAs, ensuring the protection of the environment, robot hardware and
humans in real-world settings. SafeVLA effectively balances safety and task
performance by employing large-scale constrained learning within simulated
environments. We demonstrate that SafeVLA outperforms the current
state-of-the-art method in both safety and task performance, achieving average
improvements of 83.58% and 3.85%, respectively, in simulation. By prioritizing
safety, our approach eliminates high-risk behaviors and reduces the upper bound
of unsafe behaviors to 1/35 of that in the current state-of-the-art, thereby
significantly mitigating long-tail risks. Furthermore, the learned safety
constraints generalize to diverse, unseen scenarios, including multiple
out-of-distribution perturbations and tasks. Our data, models and newly
proposed benchmark environment are available at
https://sites.google.com/view/pku-safevla.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open-Source Large Language Models as <span class="highlight-title">Multi</span>lingual Crowdworkers:
  Synthesizing Open-Domain Dialogues in Several Languages With No Examples in
  Targets and No Machine Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03462v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03462v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Njifenjou, Virgile Sucal, Bassam Jabaian, Fabrice Lefèvre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The prevailing paradigm in the domain of Open-Domain Dialogue agents
predominantly focuses on the English language, encompassing both models and
datasets. Furthermore, the financial and temporal investments required for
crowdsourcing such datasets for finetuning are substantial, particularly when
multiple languages are involved. Fortunately, advancements in Large Language
Models (LLMs) have unveiled a plethora of possibilities across diverse tasks.
Specifically, instruction-tuning has enabled LLMs to execute tasks based on
natural language instructions, occasionally surpassing the performance of human
crowdworkers. Additionally, these models possess the capability to function in
various languages within a single thread. Consequently, to generate new samples
in different languages, we propose leveraging these capabilities to replicate
the data collection process. We introduce a pipeline for generating Open-Domain
Dialogue data in multiple Target Languages using LLMs, with demonstrations
provided in a unique Source Language. By eschewing explicit Machine Translation
in this approach, we enhance the adherence to language-specific nuances. We
apply this methodology to the PersonaChat dataset. To enhance the openness of
generated dialogues and mimic real life scenarii, we added the notion of speech
events corresponding to the type of conversation the speakers are involved in
and also that of common ground which represents the premises of a conversation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unified Mind Model: Reimagining Autonomous Agents in the <span class="highlight-title">LLM</span> Era 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03459v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03459v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengbo Hu, Xiang Ying
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have recently demonstrated remarkable
capabilities across domains, tasks, and languages (e.g., ChatGPT and GPT-4),
reviving the research of general autonomous agents with human-like cognitive
abilities.Such human-level agents require semantic comprehension and
instruction-following capabilities, which exactly fall into the strengths of
LLMs.Although there have been several initial attempts to build human-level
agents based on LLMs, the theoretical foundation remains a challenging open
problem. In this paper, we propose a novel theoretical cognitive architecture,
the Unified Mind Model (UMM), which offers guidance to facilitate the rapid
creation of autonomous agents with human-level cognitive abilities.
Specifically, our UMM starts with the global workspace theory and further
leverage LLMs to enable the agent with various cognitive abilities, such as
multi-modal perception, planning, reasoning, tool use, learning, memory,
reflection and motivation. Building upon UMM, we then develop an agent-building
engine, MindOS, which allows users to quickly create domain-/task-specific
autonomous agents without any programming effort.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Taxation Perspectives from Large Language Models: A Case Study on
  Additional Tax Penalties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eunkyung Choi, Young Jin Suh, Hun Park, Wonseok Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How capable are large language models (LLMs) in the domain of taxation?
Although numerous studies have explored the legal domain in general, research
dedicated to taxation remain scarce. Moreover, the datasets used in these
studies are either simplified, failing to reflect the real-world complexities,
or unavailable as open source. To address this gap, we introduce PLAT, a new
benchmark designed to assess the ability of LLMs to predict the legitimacy of
additional tax penalties. PLAT is constructed to evaluate LLMs' understanding
of tax law, particularly in cases where resolving the issue requires more than
just applying related statutes. Our experiments with six LLMs reveal that their
baseline capabilities are limited, especially when dealing with conflicting
issues that demand a comprehensive understanding. However, we found that
enabling retrieval, self-reasoning, and discussion among multiple agents with
specific role assignments, this limitation can be mitigated.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conceptualizing Uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isaac Roberts, Alexander Schulz, Sarah Schroeder, Fabian Hinder, Barbara Hammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncertainty in machine learning refers to the degree of confidence or lack
thereof in a model's predictions. While uncertainty quantification methods
exist, explanations of uncertainty, especially in high-dimensional settings,
remain an open challenge. Existing work focuses on feature attribution
approaches which are restricted to local explanations. Understanding
uncertainty, its origins, and characteristics on a global scale is crucial for
enhancing interpretability and trust in a model's predictions. In this work, we
propose to explain the uncertainty in high-dimensional data classification
settings by means of concept activation vectors which give rise to local and
global explanations of uncertainty. We demonstrate the utility of the generated
explanations by leveraging them to refine and improve our model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RASD: Retrieval-Augmented Speculative Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03434v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03434v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guofeng Quan, Wenfeng Feng, Chuzhan Hao, Guochao Jiang, Yuewei Zhang, Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speculative decoding accelerates inference in large language models (LLMs) by
generating draft tokens for target model verification. Current approaches for
obtaining draft tokens rely on lightweight draft models or additional model
structures to generate draft tokens and retrieve context from databases. Due to
the draft model's small size and limited training data, model-based speculative
decoding frequently becomes less effective in out-of-domain scenarios.
Additionally, the time cost of the drafting phase results in a low upper limit
on acceptance length during the verification step, limiting overall efficiency.
This paper proposes RASD (Retrieval-Augmented Speculative Decoding), which
adopts retrieval methods to enhance model-based speculative decoding. We
introduce tree pruning and tree fusion to achieve this. Specifically, we
develop a pruning method based on the draft model's probability distribution to
construct the optimal retrieval tree. Second, we employ the longest prefix
matching algorithm to merge the tree generated by the draft model with the
retrieval tree, resulting in a unified tree for verification. Experimental
results demonstrate that RASD achieves state-of-the-art inference acceleration
across tasks such as DocQA, Summary, Code, and In-Domain QA. Moreover, RASD
exhibits strong scalability, seamlessly integrating with various speculative
decoding approaches, including both generation-based and retrieval-based
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Privacy is All You Need: Revolutionizing Wearable Health Data with
  Advanced PETs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03428v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03428v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karthik Barma, Seshu Babu Barma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a world where data is the new currency, wearable health devices offer
unprecedented insights into daily life, continuously monitoring vital signs and
metrics. However, this convenience raises privacy concerns, as these devices
collect sensitive data that can be misused or breached. Traditional measures
often fail due to real-time data processing needs and limited device power.
Users also lack awareness and control over data sharing and usage. We propose a
Privacy-Enhancing Technology (PET) framework for wearable devices, integrating
federated learning, lightweight cryptographic methods, and selectively deployed
blockchain technology. The blockchain acts as a secure ledger triggered only
upon data transfer requests, granting users real-time notifications and
control. By dismantling data monopolies, this approach returns data sovereignty
to individuals. Through real-world applications like secure medical data
sharing, privacy-preserving fitness tracking, and continuous health monitoring,
our framework reduces privacy risks by up to 70 percent while preserving data
utility and performance. This innovation sets a new benchmark for wearable
privacy and can scale to broader IoT ecosystems, including smart homes and
industry. As data continues to shape our digital landscape, our research
underscores the critical need to maintain privacy and user control at the
forefront of technological progress.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simplicial SMOTE: Oversampling Solution to the Imbalanced Learning
  Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03418v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03418v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oleg Kachan, Andrey Savchenko, Gleb Gusev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SMOTE (Synthetic Minority Oversampling Technique) is the established
geometric approach to random oversampling to balance classes in the imbalanced
learning problem, followed by many extensions. Its idea is to introduce
synthetic data points of the minor class, with each new point being the convex
combination of an existing data point and one of its k-nearest neighbors. In
this paper, by viewing SMOTE as sampling from the edges of a geometric
neighborhood graph and borrowing tools from the topological data analysis, we
propose a novel technique, Simplicial SMOTE, that samples from the simplices of
a geometric neighborhood simplicial complex. A new synthetic point is defined
by the barycentric coordinates w.r.t. a simplex spanned by an arbitrary number
of data points being sufficiently close rather than a pair. Such a replacement
of the geometric data model results in better coverage of the underlying data
distribution compared to existing geometric sampling methods and allows the
generation of synthetic points of the minority class closer to the majority
class on the decision boundary. We experimentally demonstrate that our
Simplicial SMOTE outperforms several popular geometric sampling methods,
including the original SMOTE. Moreover, we show that simplicial sampling can be
easily integrated into existing SMOTE extensions. We generalize and evaluate
simplicial extensions of the classic Borderline SMOTE, Safe-level SMOTE, and
ADASYN algorithms, all of which outperform their graph-based counterparts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at KDD 2025 (research track)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Claims Evolve: Evaluating and Enhancing the Robustness of Embedding
  Models Against Misinformation Edits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03417v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03417v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jabez Magomere, Emanuele La Malfa, Manuel Tonneau, Ashkan Kazemi, Scott Hale
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online misinformation remains a critical challenge, and fact-checkers
increasingly rely on embedding-based methods to retrieve relevant fact-checks.
Yet, when debunked claims reappear in edited forms, the performance of these
methods is unclear. In this work, we introduce a taxonomy of six common
real-world misinformation edits and propose a perturbation framework that
generates valid, natural claim variations. Our multi-stage retrieval evaluation
reveals that standard embedding models struggle with user-introduced edits,
while LLM-distilled embeddings offer improved robustness at a higher
computational cost. Although a strong reranker helps mitigate some issues, it
cannot fully compensate for first-stage retrieval gaps. Addressing these
retrieval gaps, our train- and inference-time mitigation approaches enhance
in-domain robustness by up to 17 percentage points and boost out-of-domain
generalization by 10 percentage points over baseline models. Overall, our
findings provide practical improvements to claim-matching systems, enabling
more reliable fact-checking of evolving misinformation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Augmentation-Based Deep Learning for Identification of Circulating Tumor
  Cells 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03410v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03410v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martina Russo, Giulia Bertolini, Vera Cappelletti, Cinzia De Marco, Serena Di Cosimo, Petra Paiè, Nadia Brancati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Circulating tumor cells (CTCs) are crucial biomarkers in liquid biopsy,
offering a noninvasive tool for cancer patient management. However, their
identification remains particularly challenging due to their limited number and
heterogeneity. Labeling samples for contrast limits the generalization of
fluorescence-based methods across different hospital datasets. Analyzing
single-cell images enables detailed assessment of cell morphology, subcellular
structures, and phenotypic variations, often hidden in clustered images.
Developing a method based on bright-field single-cell analysis could overcome
these limitations. CTCs can be isolated using an unbiased workflow combining
Parsortix technology, which selects cells based on size and deformability, with
DEPArray technology, enabling precise visualization and selection of single
cells. Traditionally, DEPArray-acquired digital images are manually analyzed,
making the process time-consuming and prone to variability. In this study, we
present a Deep Learning-based classification pipeline designed to distinguish
CTCs from leukocytes in blood samples, aimed to enhance diagnostic accuracy and
optimize clinical workflows. Our approach employs images from the bright-field
channel acquired through DEPArray technology leveraging a ResNet-based CNN. To
improve model generalization, we applied three types of data augmentation
techniques and incorporated fluorescence (DAPI) channel images into the
training phase, allowing the network to learn additional CTC-specific features.
Notably, only bright-field images have been used for testing, ensuring the
model's ability to identify CTCs without relying on fluorescence markers. The
proposed model achieved an F1-score of 0.798, demonstrating its capability to
distinguish CTCs from leukocytes. These findings highlight the potential of DL
in refining CTC analysis and advancing liquid biopsy applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 4 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI-Driven <span class="highlight-title">Multi</span>-Stage Computer Vision System for Defect Detection in
  Laser-Engraved Industrial Nameplates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03395v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03395v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adhish Anitha Vilasan, Stephan Jäger, Noah Klarmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated defect detection in industrial manufacturing is essential for
maintaining product quality and minimizing production errors. In air disc brake
manufacturing, ensuring the precision of laser-engraved nameplates is crucial
for accurate product identification and quality control. Engraving errors, such
as misprints or missing characters, can compromise both aesthetics and
functionality, leading to material waste and production delays. This paper
presents a proof of concept for an AI-driven computer vision system that
inspects and verifies laser-engraved nameplates, detecting defects in logos and
alphanumeric strings. The system integrates object detection using YOLOv7,
optical character recognition (OCR) with Tesseract, and anomaly detection
through a residual variational autoencoder (ResVAE) along with other computer
vision methods to enable comprehensive inspections at multiple stages.
Experimental results demonstrate the system's effectiveness, achieving 91.33%
accuracy and 100% recall, ensuring that defective nameplates are consistently
detected and addressed. This solution highlights the potential of AI-driven
visual inspection to enhance quality control, reduce manual inspection efforts,
and improve overall manufacturing efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Multi</span>-Agent D<span class="highlight-title">RL</span> for Queue-Aware Task Offloading in Hierarchical
  MEC-Enabled Air-Ground Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03391v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03391v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammet Hevesli, Abegaz Mohammed Seid, Aiman Erbad, Mohamed Abdallah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mobile edge computing (MEC)-enabled air-ground networks are a key component
of 6G, employing aerial base stations (ABSs) such as unmanned aerial vehicles
(UAVs) and high-altitude platform stations (HAPS) to provide dynamic services
to ground IoT devices (IoTDs). These IoTDs support real-time applications
(e.g., multimedia and Metaverse services) that demand high computational
resources and strict quality of service (QoS) guarantees in terms of latency
and task queue management. Given their limited energy and processing
capabilities, IoTDs rely on UAVs and HAPS to offload tasks for distributed
processing, forming a multi-tier MEC system. This paper tackles the overall
energy minimization problem in MEC-enabled air-ground integrated networks
(MAGIN) by jointly optimizing UAV trajectories, computing resource allocation,
and queue-aware task offloading decisions. The optimization is challenging due
to the nonconvex, nonlinear nature of this hierarchical system, which renders
traditional methods ineffective. We reformulate the problem as a multi-agent
Markov decision process (MDP) with continuous action spaces and heterogeneous
agents, and propose a novel variant of multi-agent proximal policy optimization
with a Beta distribution (MAPPO-BD) to solve it. Extensive simulations show
that MAPPO-BD outperforms baseline schemes, achieving superior energy savings
and efficient resource management in MAGIN while meeting queue delay and edge
computing constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Infants to AI: Incorporating Infant-like Learning in Models Boosts
  Efficiency and Generalization in Learning Social Prediction Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shify Treger, Shimon Ullman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Early in development, infants learn a range of useful concepts, which can be
challenging from a computational standpoint. This early learning comes together
with an initial understanding of aspects of the meaning of concepts, e.g.,
their implications, causality, and using them to predict likely future events.
All this is accomplished in many cases with little or no supervision, and from
relatively few examples, compared with current network models. In learning
about objects and human-object interactions, early acquired and possibly innate
concepts are often used in the process of learning additional, more complex
concepts. In the current work, we model how early-acquired concepts are used in
the learning of subsequent concepts, and compare the results with standard deep
network modeling. We focused in particular on the use of the concepts of
animacy and goal attribution in learning to predict future events. We show that
the use of early concepts in the learning of new concepts leads to better
learning (higher accuracy) and more efficient learning (requiring less data).
We further show that this integration of early and new concepts shapes the
representation of the concepts acquired by the model. The results show that
when the concepts were learned in a human-like manner, the emerging
representation was more useful, as measured in terms of generalization to novel
data and tasks. On a more general level, the results suggest that there are
likely to be basic differences in the conceptual structures acquired by current
network models compared to human learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transformers for molecular property prediction: Domain adaptation
  efficiently improves performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Afnan Sultan, Max Rausch-Dupont, Shahrukh Khan, Olga Kalinina, Andrea Volkamer, Dietrich Klakow
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most of the current transformer-based chemical language models are
pre-trained on millions to billions of molecules. However, the improvement from
such scaling in dataset size is not confidently linked to improved molecular
property prediction. The aim of this study is to investigate and overcome some
of the limitations of transformer models in predicting molecular properties.
Specifically, we examine the impact of pre-training dataset size and diversity
on the performance of transformer models and investigate the use of domain
adaptation as a technique for improving model performance. First, our findings
indicate that increasing pretraining dataset size beyond 400K molecules from
the GuacaMol dataset does not result in a significant improvement on four ADME
endpoints, namely, solubility, permeability, microsomal stability, and plasma
protein binding. Second, our results demonstrate that using domain adaptation
by further training the transformer model on a small set of domain-relevant
molecules, i.e., a few hundred to a few thousand, using multi-task regression
of physicochemical properties was sufficient to significantly improve
performance for three out of the four investigated ADME endpoints (P-value <
0.001). Finally, we observe that a model pre-trained on 400K molecules and
domain adopted on a few hundred/thousand molecules performs similarly (P-value
> 0.05) to more complicated transformer models like MolBERT(pre-trained on 1.3M
molecules) and MolFormer (pre-trained on 100M molecules). A comparison to a
random forest model trained on basic physicochemical properties showed similar
performance to the examined transformer models. We believe that current
transformer models can be improved through further systematic analysis of
pre-training and downstream data, pre-training objectives, and scaling laws,
ultimately leading to better and more helpful models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Large Language Models to Develop Heuristics for Emerging
  Optimization Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Bömer, Nico Koltermann, Max Disselnmeyer, Laura Dörr, Anne Meyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combinatorial optimization problems often rely on heuristic algorithms to
generate efficient solutions. However, the manual design of heuristics is
resource-intensive and constrained by the designer's expertise. Recent advances
in artificial intelligence, particularly large language models (LLMs), have
demonstrated the potential to automate heuristic generation through
evolutionary frameworks. Recent works focus only on well-known combinatorial
optimization problems like the traveling salesman problem and online bin
packing problem when designing constructive heuristics. This study investigates
whether LLMs can effectively generate heuristics for niche, not yet broadly
researched optimization problems, using the unit-load pre-marshalling problem
as an example case. We propose the Contextual Evolution of Heuristics (CEoH)
framework, an extension of the Evolution of Heuristics (EoH) framework, which
incorporates problem-specific descriptions to enhance in-context learning
during heuristic generation. Through computational experiments, we evaluate
CEoH and EoH and compare the results. Results indicate that CEoH enables
smaller LLMs to generate high-quality heuristics more consistently and even
outperform larger models. Larger models demonstrate robust performance with or
without contextualized prompts. The generated heuristics exhibit scalability to
diverse instance configurations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review LION19: The 19th Learning and Intelligent OptimizatioN
  Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Navigating Intelligence: A <span class="highlight-title">Survey</span> of Google OR-Tools and Machine
  Learning for Global Path Planning in Autonomous Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Benoit, Pedram Asef
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We offer a new in-depth investigation of global path planning (GPP) for
unmanned ground vehicles, an autonomous mining sampling robot named ROMIE. GPP
is essential for ROMIE's optimal performance, which is translated into solving
the traveling salesman problem, a complex graph theory challenge that is
crucial for determining the most effective route to cover all sampling
locations in a mining field. This problem is central to enhancing ROMIE's
operational efficiency and competitiveness against human labor by optimizing
cost and time. The primary aim of this research is to advance GPP by
developing, evaluating, and improving a cost-efficient software and web
application. We delve into an extensive comparison and analysis of Google
operations research (OR)-Tools optimization algorithms. Our study is driven by
the goal of applying and testing the limits of OR-Tools capabilities by
integrating Reinforcement Learning techniques for the first time. This enables
us to compare these methods with OR-Tools, assessing their computational
effectiveness and real-world application efficiency. Our analysis seeks to
provide insights into the effectiveness and practical application of each
technique. Our findings indicate that Q-Learning stands out as the optimal
strategy, demonstrating superior efficiency by deviating only 1.2% on average
from the optimal solutions across our datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ See What You Are Told: Visual Attention Sink in Large <span class="highlight-title">Multi</span>modal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seil Kang, Jinyeong Kim, Junhyeok Kim, Seong Jae Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large multimodal models (LMMs) "see" images by leveraging the attention
mechanism between text and visual tokens in the transformer decoder. Ideally,
these models should focus on key visual information relevant to the text token.
However, recent findings indicate that LMMs have an extraordinary tendency to
consistently allocate high attention weights to specific visual tokens, even
when these tokens are irrelevant to the corresponding text. In this study, we
investigate the property behind the appearance of these irrelevant visual
tokens and examine their characteristics. Our findings show that this behavior
arises due to the massive activation of certain hidden state dimensions, which
resembles the attention sink found in language models. Hence, we refer to this
phenomenon as the visual attention sink. In particular, our analysis reveals
that removing the irrelevant visual sink tokens does not impact model
performance, despite receiving high attention weights. Consequently, we recycle
the attention to these tokens as surplus resources, redistributing the
attention budget to enhance focus on the image. To achieve this, we introduce
Visual Attention Redistribution (VAR), a method that redistributes attention in
image-centric heads, which we identify as innately focusing on visual
information. VAR can be seamlessly applied across different LMMs to improve
performance on a wide range of tasks, including general vision-language tasks,
visual hallucination tasks, and vision-centric tasks, all without the need for
additional training, models, or inference steps. Experimental results
demonstrate that VAR enables LMMs to process visual information more
effectively by adjusting their internal attention mechanisms, offering a new
direction to enhancing the multimodal capabilities of LMMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring specialization and sensitivity of convolutional neural
  networks in the context of simultaneous image augmentations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03283v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03283v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pavel Kharyuk, Sergey Matveev, Ivan Oseledets
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Drawing parallels with the way biological networks are studied, we adapt the
treatment--control paradigm to explainable artificial intelligence research and
enrich it through multi-parametric input alterations. In this study, we propose
a framework for investigating the internal inference impacted by input data
augmentations. The internal changes in network operation are reflected in
activation changes measured by variance, which can be decomposed into
components related to each augmentation, employing Sobol indices and Shapley
values. These quantities enable one to visualize sensitivity to different
variables and use them for guided masking of activations. In addition, we
introduce a way of single-class sensitivity analysis where the candidates are
filtered according to their matching to prediction bias generated by targeted
damaging of the activations. Relying on the observed parallels, we assume that
the developed framework can potentially be transferred to studying biological
neural networks in complex environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages; main text: 5 figures, 4 tables; appendix: 4 sections, 3
  tables; supplementary: 7 files (figures S1-S6: packed as 7z archive, S7:
  single pdf file)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Dynamic SLO Compliance in <span class="highlight-title">Distributed</span> Computing Continuum
  Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03274v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03274v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alfreds Lapkovskis, Boris Sedlak, Sindri Magnússon, Schahram Dustdar, Praveen Kumar Donta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring Service Level Objectives (SLOs) in large-scale architectures, such
as Distributed Computing Continuum Systems (DCCS), is challenging due to their
heterogeneous nature and varying service requirements across different devices
and applications. Additionally, unpredictable workloads and resource
limitations lead to fluctuating performance and violated SLOs. To improve SLO
compliance in DCCS, one possibility is to apply machine learning; however, the
design choices are often left to the developer. To that extent, we provide a
benchmark of Active Inference -- an emerging method from neuroscience --
against three established reinforcement learning algorithms (Deep Q-Network,
Advantage Actor-Critic, and Proximal Policy Optimization). We consider a
realistic DCCS use case: an edge device running a video conferencing
application alongside a WebSocket server streaming videos. Using one of the
respective algorithms, we continuously monitor key performance metrics, such as
latency and bandwidth usage, to dynamically adjust parameters -- including the
number of streams, frame rate, and resolution -- to optimize service quality
and user experience. To test algorithms' adaptability to constant system
changes, we simulate dynamically changing SLOs and both instant and gradual
data-shift scenarios, such as network bandwidth limitations and fluctuating
device thermal states. Although the evaluated algorithms all showed advantages
and limitations, our findings demonstrate that Active Inference is a promising
approach for ensuring SLO compliance in DCCS, offering lower memory usage,
stable CPU utilization, and fast convergence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conformal Transformations for Symmetric Power Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saurabh Kumar, Jacob Buckman, Carles Gelada, Sean Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers with linear attention offer significant computational advantages
over softmax-based transformers but often suffer from degraded performance. The
symmetric power (sympow) transformer, a particular type of linear transformer,
addresses some of this performance gap by leveraging symmetric tensor
embeddings, achieving comparable performance to softmax transformers. However,
the finite capacity of the recurrent state in sympow transformers limits their
ability to retain information, leading to performance degradation when scaling
the training or evaluation context length. To address this issue, we propose
the conformal-sympow transformer, which dynamically frees up capacity using
data-dependent multiplicative gating and adaptively stores information using
data-dependent rotary embeddings. Preliminary experiments on the LongCrawl64
dataset demonstrate that conformal-sympow overcomes the limitations of sympow
transformers, achieving robust performance across scaled training and
evaluation contexts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SCOPE Workshop at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trajectory Prediction for Autonomous Driving: Progress, Limitations, and
  Future Directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nadya Abdel Madjid, Abdulrahman Ahmad, Murad Mebrahtu, Yousef Babaa, Abdelmoamen Nasser, Sumbal Malik, Bilal Hassan, Naoufel Werghi, Jorge Dias, Majid Khonji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the potential for autonomous vehicles to be integrated on a large scale
into modern traffic systems continues to grow, ensuring safe navigation in
dynamic environments is crucial for smooth integration. To guarantee safety and
prevent collisions, autonomous vehicles must be capable of accurately
predicting the trajectories of surrounding traffic agents. Over the past
decade, significant efforts from both academia and industry have been dedicated
to designing solutions for precise trajectory forecasting. These efforts have
produced a diverse range of approaches, raising questions about the differences
between these methods and whether trajectory prediction challenges have been
fully addressed. This paper reviews a substantial portion of recent trajectory
prediction methods and devises a taxonomy to classify existing solutions. A
general overview of the prediction pipeline is also provided, covering input
and output modalities, modeling features, and prediction paradigms discussed in
the literature. In addition, the paper discusses active research areas within
trajectory prediction, addresses the posed research questions, and highlights
the remaining research gaps and challenges.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Potential of Large Language Models as Predictors in
  Dynamic Text-Attributed Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runlin Lei, Jiarui Ji, Haipeng Ding, Lu Yi, Zhewei Wei, Yongchao Liu, Chuntao Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rise of large language models (LLMs), there has been growing
interest in Graph Foundation Models (GFMs) for graph-based tasks. By leveraging
LLMs as predictors, GFMs have demonstrated impressive generalizability across
various tasks and datasets. However, existing research on LLMs as predictors
has predominantly focused on static graphs, leaving their potential in dynamic
graph prediction unexplored. In this work, we pioneer using LLMs for predictive
tasks on dynamic graphs. We identify two key challenges: the constraints
imposed by context length when processing large-scale historical data and the
significant variability in domain characteristics, both of which complicate the
development of a unified predictor. To address these challenges, we propose the
GraphAgent-Dynamic (GAD) Framework, a multi-agent system that leverages
collaborative LLMs. In contrast to using a single LLM as the predictor, GAD
incorporates global and local summary agents to generate domain-specific
knowledge, enhancing its transferability across domains. Additionally,
knowledge reflection agents enable adaptive updates to GAD's knowledge,
maintaining a unified and self-consistent architecture. In experiments, GAD
demonstrates performance comparable to or even exceeds that of full-supervised
graph neural networks without dataset-specific training. Finally, to enhance
the task-specific performance of LLM-based predictors, we discuss potential
improvements, such as dataset-specific fine-tuning to LLMs. By developing
tailored strategies for different tasks, we provide new insights for the future
design of LLM-based predictors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Less is more? Rewards in <span class="highlight-title">RL</span> for Cyber Defence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elizabeth Bates, Chris Hicks, Vasilios Mavroudis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The last few years has seen an explosion of interest in autonomous cyber
defence agents based on deep reinforcement learning. Such agents are typically
trained in a cyber gym environment, also known as a cyber simulator, at least
32 of which have already been built. Most, if not all cyber gyms provide dense
"scaffolded" reward functions which combine many penalties or incentives for a
range of (un)desirable states and costly actions. Whilst dense rewards help
alleviate the challenge of exploring complex environments, yielding seemingly
effective strategies from relatively few environment steps; they are also known
to bias the solutions an agent can find, potentially towards suboptimal
solutions. Sparse rewards could offer preferable or more effective solutions
and have been overlooked by cyber gyms to date. In this work we set out to
evaluate whether sparse reward functions might enable training more effective
cyber defence agents. Towards this goal we first break down several evaluation
limitations in existing work by proposing a ground truth evaluation score that
goes beyond the standard RL paradigm used to train and evaluate agents. By
adapting a well-established cyber gym to accommodate our methodology and ground
truth score, we propose and evaluate two sparse reward mechanisms and compare
them with a typical dense reward. Our evaluation considers a range of network
sizes, from 2 to 50 nodes, and both reactive and proactive defensive actions.
Our results show that sparse rewards, particularly positive reinforcement for
an uncompromised network state, enable the training of more effective cyber
defence agents. Furthermore, we show that sparse rewards provide more stable
training than dense rewards, and that both effectiveness and training stability
are robust to a variety of cyber environment considerations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 Pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FANS -- Formal Answer Selection for Natural Language Math Reasoning
  Using Lean4 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03238v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03238v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarui Yao, Ruida Wang, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have displayed astonishing abilities in various
tasks, especially in text generation, classification, question answering, etc.
However, the reasoning ability of LLMs still faces many debates. The inherent
ambiguity of Natural Language (NL) limits LLMs' ability to perform verifiable
reasoning, making its answers lack coherence and trustworthy support. To tackle
the above problems, we propose a novel framework named FANS: Formal ANswer
Selection for Natural Language Math Reasoning Using Lean4. To the best of our
knowledge, it is the first framework that utilizes Lean4 to enhance LLMs' NL
math reasoning ability. In particular, given an NL math question and
LLM-generated answers, FANS first translates it into Lean4 theorem statements.
Then it tries to prove it using a Lean4 prover and verify it by Lean4. Finally,
it uses the FL result to assist in answer selection. It enhances LLMs' NL math
ability in providing a computer-verifiable solution for its correct answer and
proposes an alternative method for answer selection beyond the reward model.
Extensive experiments indicate the effectiveness of our framework. It can
improve the accuracy rate of reward model enhanced LLMs in the MATH-500 dataset
by at most 1.91% and AMC-23 by at most 8.33% on strong reward-model baselines.
In some particular fields like number theory that Lean4 experts in, we can even
select all correct solutions. The qualitative analysis also shows our framework
can make NL results formally backed by Lean4 proofs. As a pioneering work in
the corresponding field, we will open-source all our models and datasets to
further boost the development of the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COSINT-Agent: A Knowledge-Driven <span class="highlight-title">Multi</span>modal Agent for Chinese Open
  Source Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03215v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03215v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Li, Congcong Wang, Xiaoxiao Cui, Zhi Liu, Wei Guo, Lizhen Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open Source Intelligence (OSINT) requires the integration and reasoning of
diverse multimodal data, presenting significant challenges in deriving
actionable insights. Traditional approaches, including multimodal large
language models (MLLMs), often struggle to infer complex contextual
relationships or deliver comprehensive intelligence from unstructured data
sources. In this paper, we introduce COSINT-Agent, a knowledge-driven
multimodal agent tailored to address the challenges of OSINT in the Chinese
domain. COSINT-Agent seamlessly integrates the perceptual capabilities of
fine-tuned MLLMs with the structured reasoning power of the Entity-Event-Scene
Knowledge Graph (EES-KG). Central to COSINT-Agent is the innovative EES-Match
framework, which bridges COSINT-MLLM and EES-KG, enabling systematic
extraction, reasoning, and contextualization of multimodal insights. This
integration facilitates precise entity recognition, event interpretation, and
context retrieval, effectively transforming raw multimodal data into actionable
intelligence. Extensive experiments validate the superior performance of
COSINT-Agent across core OSINT tasks, including entity recognition, EES
generation, and context matching. These results underscore its potential as a
robust and scalable solution for advancing automated multimodal reasoning and
enhancing the effectiveness of OSINT methodologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NodeReg: Mitigating the Imbalance and Distribution Shift Effects in
  Semi-Supervised Node Classification via Norm Consistency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03211v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03211v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenzhi Yang, Jun Xia, Jingbo Zhou, Xingkai Yao, Xiaofang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aggregating information from neighboring nodes benefits graph neural networks
(GNNs) in semi-supervised node classification tasks. Nevertheless, this
mechanism also renders nodes susceptible to the influence of their neighbors.
For instance, this will occur when the neighboring nodes are imbalanced or the
neighboring nodes contain noise, which can even affect the GNN's ability to
generalize out of distribution. We find that ensuring the consistency of the
norm for node representations can significantly reduce the impact of these two
issues on GNNs. To this end, we propose a regularized optimization method
called NodeReg that enforces the consistency of node representation norms. This
method is simple but effective and satisfies Lipschitz continuity, thus
facilitating stable optimization and significantly improving semi-supervised
node classification performance under the above two scenarios. To illustrate,
in the imbalance scenario, when training a GCN with an imbalance ratio of 0.1,
NodeReg outperforms the most competitive baselines by 1.4%-25.9% in F1 score
across five public datasets. Similarly, in the distribution shift scenario,
NodeReg outperforms the most competitive baseline by 1.4%-3.1% in accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MA-LoT: <span class="highlight-title">Multi</span>-Agent Lean-based Long Chain-of-Thought Reasoning enhances
  Formal Theorem Proving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03205v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03205v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruida Wang, Rui Pan, Yuxin Li, Jipeng Zhang, Yizhen Jia, Shizhe Diao, Renjie Pi, Junjie Hu, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Solving mathematical problems using computer-verifiable languages like Lean
has significantly impacted mathematical and computer science communities.
State-of-the-art methods utilize single Large Language Models (LLMs) as agents
or provers to either generate complete proof or perform tree searches. However,
single-agent methods inherently lack a structured way to combine high-level
reasoning in Natural Language (NL) with Formal Language (FL) verification
feedback. To solve these issues, we propose MA-LoT: Multi-Agent Lean-based Long
Chain-of-Thought framework, (to the best of our knowledge), the first
multi-agent framework for Lean4 theorem proving that balance high-level NL
reasoning and FL verification in Long CoT. Using this structured interaction,
our approach enables deeper insights and long-term coherence in proof
generation, with which past methods struggle. We do this by leveraging emergent
formal reasoning ability in Long CoT using our novel LoT-Transfer Learning
training-inference pipeline. Extensive experiments show that our framework
achieves 54.51% accuracy rate on the Lean4 version of MiniF2F-Test dataset,
largely outperforming GPT-4 (22.95%), single-agent tree search
(InternLM-Step-Prover, 50.70%), and whole-proof generation
(DeepSeek-Prover-v1.5, 48.36%) baselines. Furthermore, our findings highlight
the potential of combining Long CoT with formal verification for a more
insightful generation in a broader perspective.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Robust Universal Information Extraction: Benchmark, Evaluation,
  and Solution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03201v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03201v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jizhao Zhu, Akang Shi, Zixuan Li, Long Bai, Xiaolong Jin, Jiafeng Guo, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we aim to enhance the robustness of Universal Information
Extraction (UIE) by introducing a new benchmark dataset, a comprehensive
evaluation, and a feasible solution. Existing robust benchmark datasets have
two key limitations: 1) They generate only a limited range of perturbations for
a single Information Extraction (IE) task, which fails to evaluate the
robustness of UIE models effectively; 2) They rely on small models or
handcrafted rules to generate perturbations, often resulting in unnatural
adversarial examples. Considering the powerful generation capabilities of Large
Language Models (LLMs), we introduce a new benchmark dataset for Robust UIE,
called RUIE-Bench, which utilizes LLMs to generate more diverse and realistic
perturbations across different IE tasks. Based on this dataset, we
comprehensively evaluate existing UIE models and reveal that both LLM-based
models and other models suffer from significant performance drops. To improve
robustness and reduce training costs, we propose a data-augmentation solution
that dynamically selects hard samples for iterative training based on the
model's inference loss. Experimental results show that training with only
\textbf{15\%} of the data leads to an average \textbf{7.5\%} relative
performance improvement across three IE tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Directly Follows Graphs Go Predictive Process Monitoring With Graph
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03197v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03197v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Attila Lischka, Simon Rauch, Oliver Stritzel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the past years, predictive process monitoring (PPM) techniques based on
artificial neural networks have evolved as a method to monitor the future
behavior of business processes. Existing approaches mostly focus on
interpreting the processes as sequences, so-called traces, and feeding them to
neural architectures designed to operate on sequential data such as recurrent
neural networks (RNNs) or transformers. In this study, we investigate an
alternative way to perform PPM: by transforming each process in its
directly-follows-graph (DFG) representation we are able to apply graph neural
networks (GNNs) for the prediction tasks. By this, we aim to develop models
that are more suitable for complex processes that are long and contain an
abundance of loops. In particular, we present different ways to create DFG
representations depending on the particular GNN we use. The tested GNNs range
from classical node-based to novel edge-based architectures. Further, we
investigate the possibility of using multi-graphs. By these steps, we aim to
design graph representations that minimize the information loss when
transforming traces into graphs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structured Outputs Enable General-Purpose <span class="highlight-title">LLM</span>s to be Medical Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03194v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03194v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangfu Guo, Kai Zhang, Bryan Hoo, Yujun Cai, Xiaoqian Lu, Nanyun Peng, Yiwei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical question-answering (QA) is a critical task for evaluating how
effectively large language models (LLMs) encode clinical knowledge and
assessing their potential applications in medicine. Despite showing promise on
multiple-choice tests, LLMs frequently struggle with open-ended medical
questions, producing responses with dangerous hallucinations or lacking
comprehensive coverage of critical aspects. Existing approaches attempt to
address these challenges through domain-specific fine-tuning, but this proves
resource-intensive and difficult to scale across models. To improve the
comprehensiveness and factuality of medical responses, we propose a novel
approach utilizing structured medical reasoning. Our method guides LLMs through
an seven-step cognitive process inspired by clinical diagnosis, enabling more
accurate and complete answers without additional training. Experiments on the
MedLFQA benchmark demonstrate that our approach achieves the highest Factuality
Score of 85.8, surpassing fine-tuned models. Notably, this improvement
transfers to smaller models, highlighting the method's efficiency and
scalability. Our code and datasets are available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intermediate-Task Transfer Learning: Leveraging Sarcasm Detection for
  Stance Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gibson Nkhata, Susan Gauch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stance Detection (SD) on social media has emerged as a prominent area of
interest with implications for social business and political applications
thereby garnering escalating research attention within NLP. The inherent
subtlety and complexity of texts procured from online platforms pose challenges
for SD algorithms in accurately discerning the authors stance. Mostly the
inclusion of sarcastic and figurative language drastically impacts the
performance of SD models. This paper addresses this by employing sarcasm
detection intermediate-task transfer learning tailored for SD. The proposed
methodology involves the finetuning of BERT and RoBERTa and the concatenation
of convolutional BiLSTM and dense layers. Rigorous experiments are conducted on
publicly available datasets to evaluate our transfer-learning framework. The
performance of the approach is assessed against various State-Of-The-Art
baselines for SD providing empirical evidence of its effectiveness. Notably our
model outperforms the best SOTA models even prior to sarcasm-detection
pretraining. The integration of sarcasm knowledge into the model proves
instrumental in mitigating misclassifications of sarcastic textual elements in
SD. Our model accurately predicts 85% of texts that were previously
misclassified by the model without sarcasm-detection pretraining thereby
amplifying the average F1-score of the model. Our experiments also revealed
that the success of the transfer-learning framework is contingent upon the
correlation of lexical attributes between the intermediate task and the target
task. This study represents the first exploration of sarcasm detection as an
intermediate transfer-learning task in the context of SD and simultaneously
uses the concatenation of BERT or RoBERTa with other deep-learning techniques
establishing the proposed approach as a foundational baseline for future
research endeavors in this domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures, published in The Sixteenth International
  Conference on Information (eKNOW 2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AttackSeqBench: Benchmarking Large Language Models' Understanding of
  Sequential Patterns in Cyber Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03170v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03170v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Javier Yong, Haokai Ma, Yunshan Ma, Anis Yusof, Zhenkai Liang, Ee-Chien Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The observations documented in Cyber Threat Intelligence (CTI) reports play a
critical role in describing adversarial behaviors, providing valuable insights
for security practitioners to respond to evolving threats. Recent advancements
of Large Language Models (LLMs) have demonstrated significant potential in
various cybersecurity applications, including CTI report understanding and
attack knowledge graph construction. While previous works have proposed
benchmarks that focus on the CTI extraction ability of LLMs, the sequential
characteristic of adversarial behaviors within CTI reports remains largely
unexplored, which holds considerable significance in developing a comprehensive
understanding of how adversaries operate. To address this gap, we introduce
AttackSeqBench, a benchmark tailored to systematically evaluate LLMs'
capability to understand and reason attack sequences in CTI reports. Our
benchmark encompasses three distinct Question Answering (QA) tasks, each task
focuses on the varying granularity in adversarial behavior. To alleviate the
laborious effort of QA construction, we carefully design an automated dataset
construction pipeline to create scalable and well-formulated QA datasets based
on real-world CTI reports. To ensure the quality of our dataset, we adopt a
hybrid approach of combining human evaluation and systematic evaluation
metrics. We conduct extensive experiments and analysis with both fast-thinking
and slow-thinking LLMs, while highlighting their strengths and limitations in
analyzing the sequential patterns in cyber attacks. The overarching goal of
this work is to provide a benchmark that advances LLM-driven CTI report
understanding and fosters its application in real-world cybersecurity
operations. Our dataset and code are available at
https://github.com/Javiery3889/AttackSeqBench .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiRe-JAX: A JAX based Dimensionality Reduction Algorithm for Large-scale
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03156v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03156v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Kolpakov, Igor Rivin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DiRe-JAX is a new dimensionality reduction toolkit designed to address some
of the challenges faced by traditional methods like UMAP and tSNE such as loss
of global structure and computational efficiency. Built on the JAX framework,
DiRe leverages modern hardware acceleration to provide an efficient, scalable,
and interpretable solution for visualizing complex data structures, and for
quantitative analysis of lower-dimensional embeddings. The toolkit shows
considerable promise in preserving both local and global structures within the
data as compare to state-of-the-art UMAP and tSNE implementations. This makes
it suitable for a wide range of applications in machine learning,
bioinformatics, and data science.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 12 figures; Github repository available at
  https://github.com/sashakolpakov/dire-jax; package available on PyPi
  https://pypi.org/project/dire-jax/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Position: Model Collapse Does Not Mean What You Think 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rylan Schaeffer, Joshua Kazdan, Alvan Caleb Arulandu, Sanmi Koyejo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of AI-generated content online has fueled concerns over
\emph{model collapse}, a degradation in future generative models' performance
when trained on synthetic data generated by earlier models. Industry leaders,
premier research journals and popular science publications alike have
prophesied catastrophic societal consequences stemming from model collapse. In
this position piece, we contend this widespread narrative fundamentally
misunderstands the scientific evidence. We highlight that research on model
collapse actually encompasses eight distinct and at times conflicting
definitions of model collapse, and argue that inconsistent terminology within
and between papers has hindered building a comprehensive understanding of model
collapse. To assess how significantly different interpretations of model
collapse threaten future generative models, we posit what we believe are
realistic conditions for studying model collapse and then conduct a rigorous
assessment of the literature's methodologies through this lens. While we leave
room for reasonable disagreement, our analysis of research studies, weighted by
how faithfully each study matches real-world conditions, leads us to conclude
that certain predicted claims of model collapse rely on assumptions and
conditions that poorly match real-world conditions, and in fact several
prominent collapse scenarios are readily avoidable. Altogether, this position
paper argues that model collapse has been warped from a nuanced multifaceted
consideration into an oversimplified threat, and that the evidence suggests
specific harms more likely under society's current trajectory have received
disproportionately less attention.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Partial Convolution Meets Visual Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03148v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03148v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiduo Huang, Fuwei Yang, Dong Li, Ji Liu, Lu Tian, Jinzhang Peng, Pengju Ren, Emad Barsoum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing an efficient and effective neural network has remained a prominent
topic in computer vision research. Depthwise onvolution (DWConv) is widely used
in efficient CNNs or ViTs, but it needs frequent memory access during
inference, which leads to low throughput. FasterNet attempts to introduce
partial convolution (PConv) as an alternative to DWConv but compromises the
accuracy due to underutilized channels. To remedy this shortcoming and consider
the redundancy between feature map channels, we introduce a novel Partial
visual ATtention mechanism (PAT) that can efficiently combine PConv with visual
attention. Our exploration indicates that the partial attention mechanism can
completely replace the full attention mechanism and reduce model parameters and
FLOPs. Our PAT can derive three types of blocks: Partial Channel-Attention
block (PAT_ch), Partial Spatial-Attention block (PAT_sp) and Partial
Self-Attention block (PAT_sf). First, PAT_ch integrates the enhanced Gaussian
channel attention mechanism to infuse global distribution information into the
untouched channels of PConv. Second, we introduce the spatial-wise attention to
the MLP layer to further improve model accuracy. Finally, we replace PAT_ch in
the last stage with the self-attention mechanism to extend the global receptive
field. Building upon PAT, we propose a novel hybrid network family, named
PATNet, which achieves superior top-1 accuracy and inference speed compared to
FasterNet on ImageNet-1K classification and excel in both detection and
segmentation on the COCO dataset. Particularly, our PATNet-T2 achieves 1.3%
higher accuracy than FasterNet-T2, while exhibiting 25% higher GPU throughput
and 24% lower CPU latency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2502.01303</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge Augmentation in Federation: Rethinking What Collaborative
  Learning Can Bring Back to <span class="highlight-title">Decentralized</span> Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentai Wu, Yingliang Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data, as an observable form of knowledge, has become one of the most
important factors of production for the development of Artificial Intelligence
(AI). Meanwhile, increasing legislation and regulations on private and
proprietary information results in scattered data sources also known as the
``data islands''. Although some collaborative learning paradigms such as
Federated Learning (FL) can enable privacy-preserving training over
decentralized data, they have inherent deficiencies in fairness, costs and
reproducibility because of being learning-centric, which greatly limits the way
how participants cooperate with each other. In light of this, we present a
knowledge-centric paradigm termed \emph{Knowledge Augmentation in Federation}
(KAF), with focus on how to enhance local knowledge through collaborative
effort. We provide the suggested system architecture, formulate the
prototypical optimization objective, and review emerging studies that employ
methodologies suitable for KAF. On our roadmap, with a three-way categorization
we describe the methods for knowledge expansion, knowledge filtering, and label
and feature space correction in the federation. Further, we highlight several
challenges and open questions that deserve more attention from the community.
With our investigation, we intend to offer new insights for what collaborative
learning can bring back to decentralized data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convergence Analysis of Federated Learning Methods Using Backward Error
  Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03139v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03139v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinwoo Lim, Suhyun Kim, Soo-Mook Moon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Backward error analysis allows finding a modified loss function, which the
parameter updates really follow under the influence of an optimization method.
The additional loss terms included in this modified function is called implicit
regularizer. In this paper, we attempt to find the implicit regularizer for
various federated learning algorithms on non-IID data distribution, and explain
why each method shows different convergence behavior. We first show that the
implicit regularizer of FedAvg disperses the gradient of each client from the
average gradient, thus increasing the gradient variance. We also empirically
show that the implicit regularizer hampers its convergence. Similarly, we
compute the implicit regularizers of FedSAM and SCAFFOLD, and explain why they
converge better. While existing convergence analyses focus on pointing out the
advantages of FedSAM and SCAFFOLD, our approach can explain their limitations
in complex non-convex settings. In specific, we demonstrate that FedSAM can
partially remove the bias in the first-order term of the implicit regularizer
in FedAvg, whereas SCAFFOLD can fully eliminate the bias in the first-order
term, but not in the second-order term. Consequently, the implicit regularizer
can provide a useful insight on the convergence behavior of federated learning
from a different theoretical perspective.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> L2R: Learning to Reduce Search Space for Generalizable Neural Routing
  Solver 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03137v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03137v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changliang Zhou, Xi Lin, Zhenkun Wang, Qing<span class="highlight-author">fu Zhang</span>
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Constructive neural combinatorial optimization (NCO) has attracted growing
research attention due to its ability to solve complex routing problems without
relying on handcrafted rules. However, existing NCO methods face significant
challenges in generalizing to large-scale problems due to high computational
complexity and inefficient capture of structural patterns. To address this
issue, we propose a novel learning-based search space reduction method that
adaptively selects a small set of promising candidate nodes at each step of the
constructive NCO process. Unlike traditional methods that rely on fixed
heuristics, our selection model dynamically prioritizes nodes based on learned
patterns, significantly reducing the search space while maintaining solution
quality. Experimental results demonstrate that our method, trained solely on
100-node instances from uniform distribution, generalizes remarkably well to
large-scale Traveling Salesman Problem (TSP) and Capacitated Vehicle Routing
Problem (CVRP) instances with up to 1 million nodes from the uniform
distribution and over 80K nodes from other distributions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Neural Ordinary Differential Equations as Interpretable
  Healthcare classifiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03129v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03129v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning has emerged as one of the most significant innovations in
machine learning. However, a notable limitation of this field lies in the
``black box" decision-making processes, which have led to skepticism within
groups like healthcare and scientific communities regarding its applicability.
In response, this study introduces a interpretable approach using Neural
Ordinary Differential Equations (NODEs), a category of neural network models
that exploit the dynamics of differential equations for representation
learning. Leveraging their foundation in differential equations, we illustrate
the capability of these models to continuously process textual data, marking
the first such model of its kind, and thereby proposing a promising direction
for future research in this domain. The primary objective of this research is
to propose a novel architecture for groups like healthcare that require the
predictive capabilities of deep learning while emphasizing the importance of
model transparency demonstrated in NODEs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL SRW Submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Understanding <span class="highlight-title">Multi</span>-Round Large Language Model Reasoning:
  Approximability, Learnability and Generalizability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03128v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03128v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhui Xu, Dancheng Liu, Jiajie Li, Amir Nassereldine, Zhaohui Li, Jinjun Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in cognitive science and multi-round reasoning techniques
for Large Language Models (LLMs) suggest that iterative thinking processes
improve problem-solving performance in complex tasks. Inspired by this,
approaches like Chain-of-Thought, debating, and self-refinement have been
applied to auto-regressive LLMs, achieving significant successes in tasks such
as mathematical reasoning, commonsense reasoning, and multi-hop question
answering. Despite these successes, the theoretical basis for how multi-round
reasoning enhances problem-solving abilities remains underexplored. In this
work, we investigate the approximation, learnability, and generalization
properties of multi-round auto-regressive models. We show that Transformers
with finite context windows are universal approximators for steps of
Turing-computable functions and can approximate any Turing-computable
sequence-to-sequence function through multi-round reasoning. We extend PAC
learning to sequence generation and demonstrate that multi-round generation is
learnable even when the sequence length exceeds the model's context window.
Finally, we examine how generalization error propagates across rounds, and show
how the aforementioned approaches can help constrain this error, ensuring
outputs stay within an expectation boundary. This work sheds light on the
systemic theoretical foundations of multi-round sequence learning and
reasoning, emphasizing its role in inference complexity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Devil Is in the Details: Tackling Unimodal Spurious Correlations for
  Generalizable <span class="highlight-title">Multi</span>modal Reward Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03122v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03122v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichao Li, Xueru Wen, Jie Lou, Yuqiu Ji, Yaojie Lu, Xianpei Han, Debing Zhang, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Reward Models (MM-RMs) are crucial for aligning Large Language
Models (LLMs) with human preferences, particularly as LLMs increasingly
interact with multimodal data. However, we find that MM-RMs trained on existing
datasets often struggle to generalize to out-of-distribution data due to their
reliance on unimodal spurious correlations, primarily text-only shortcuts
within the training distribution, which prevents them from leveraging true
multimodal reward functions. To address this, we introduce a Shortcut-aware
MM-RM learning algorithm that mitigates this issue by dynamically reweighting
training samples, shifting the distribution toward better multimodal
understanding, and reducing dependence on unimodal spurious correlations. Our
experiments demonstrate significant improvements in generalization, downstream
task performance, and scalability, establishing a more robust framework for
multimodal reward modeling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Multi</span>modal Framework for Topic Propagation Classification in Social
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuchuan Jiang, Chaolong Jia, Yunyi Qin, Wei Cai, Yongsen Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid proliferation of the Internet and the widespread adoption of social
networks have significantly accelerated information dissemination. However,
this transformation has introduced complexities in information capture and
processing, posing substantial challenges for researchers and practitioners.
Predicting the dissemination of topic-related information within social
networks has thus become a critical research focus. This paper proposes a
predictive model for topic dissemination in social networks by integrating
multidimensional features derived from key dissemination characteristics.
Specifically, we introduce two novel indicators, user relationship breadth and
user authority, into the PageRank algorithm to quantify user influence more
effectively. Additionally, we employ a Text-CNN model for sentiment
classification, extracting sentiment features from textual content. Temporal
embeddings of nodes are encoded using a Bi-LSTM model to capture temporal
dynamics. Furthermore, we refine the measurement of user interaction traces
with topics, replacing traditional topic view metrics with a more precise
communication characteristics measure. Finally, we integrate the extracted
multidimensional features using a Transformer model, significantly enhancing
predictive performance. Experimental results demonstrate that our proposed
model outperforms traditional machine learning and unimodal deep learning
models in terms of FI-Score, AUC, and Recall, validating its effectiveness in
predicting topic propagation within social networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SoK: Knowledge is All You Need: Last Mile Delivery for Automated
  Provenance-based Intrusion Detection with <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03108v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03108v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenrui Cheng, Tiantian Zhu, Chunlin Xiong, Haofei Sun, Zijun Wang, Shunan Jing, Mingqi Lv, Yan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, provenance-based intrusion detection systems (PIDSes) have been
widely proposed for endpoint threat analysis. However, due to the lack of
systematic integration and utilization of knowledge, existing PIDSes still
require significant manual intervention for practical deployment, making full
automation challenging. This paper presents a disruptive innovation by
categorizing PIDSes according to the types of knowledge they utilize. In
response to the prevalent issue of ``knowledge silos problem'' in existing
research, we introduce a novel knowledge-driven provenance-based intrusion
detection framework, powered by large language models (LLMs). We also present
OmniSec, a best practice system built upon this framework. By integrating
attack representation knowledge, threat intelligence knowledge, and benign
behavior knowledge, OmniSec outperforms the state-of-the-art approaches on
public benchmark datasets. OmniSec is available online at
https://anonymous.4open.science/r/PIDS-with-LLM-613B.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ External Reliable Information-enhanced <span class="highlight-title">Multi</span>modal Contrastive Learning
  for Fake News Detection <span class="chip">AAAI'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biwei Cao, Qihang Wu, Jiuxin Cao, Bo Liu, Jie Gui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of the Internet, the information dissemination
paradigm has changed and the efficiency has been improved greatly. While this
also brings the quick spread of fake news and leads to negative impacts on
cyberspace. Currently, the information presentation formats have evolved
gradually, with the news formats shifting from texts to multimodal contents. As
a result, detecting multimodal fake news has become one of the research
hotspots. However, multimodal fake news detection research field still faces
two main challenges: the inability to fully and effectively utilize multimodal
information for detection, and the low credibility or static nature of the
introduced external information, which limits dynamic updates. To bridge the
gaps, we propose ERIC-FND, an external reliable information-enhanced multimodal
contrastive learning framework for fake news detection. ERIC-FND strengthens
the representation of news contents by entity-enriched external information
enhancement method. It also enriches the multimodal news information via
multimodal semantic interaction method where the multimodal constrative
learning is employed to make different modality representations learn from each
other. Moreover, an adaptive fusion method is taken to integrate the news
representations from different dimensions for the eventual classification.
Experiments are done on two commonly used datasets in different languages, X
(Twitter) and Weibo. Experiment results demonstrate that our proposed model
ERIC-FND outperforms existing state-of-the-art fake news detection methods
under the same settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by AAAI'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LADDER: Self-Improving <span class="highlight-title">LLM</span>s Through Recursive Problem Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.00735v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.00735v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toby Simonds, Akira Yoshiyama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce LADDER (Learning through Autonomous Difficulty-Driven Example
Recursion), a framework which enables Large Language Models to autonomously
improve their problem-solving capabilities through self-guided learning by
recursively generating and solving progressively simpler variants of complex
problems. Unlike prior approaches that require curated datasets or human
feedback, LADDER leverages a model's own capabilities to generate easier
question variants. We demonstrate LADDER's effectiveness in the subject of
mathematical integration, improving Llama 3.2 3B's accuracy from 1% to 82% on
undergraduate-level problems and enabling Qwen2.5 7B Deepseek-R1 Distilled to
achieve 73% on the MIT Integration Bee qualifying examination. We also
introduce TTRL (Test-Time Reinforcement Learning), where we perform
reinforcement learning on variants of test problems at inference time. TTRL
enables Qwen2.5 7B Deepseek-R1 Distilled to achieve a state-of-the-art score of
90% on the MIT Integration Bee qualifying examination, surpassing OpenAI o1's
performance. These results show how self-directed strategic learning can
achieve significant capability improvements without relying on architectural
scaling or human supervision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rewarding Doubt: A Reinforcement Learning Approach to Confidence
  Calibration of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02623v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02623v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Stangel, David Bani-Harouni, Chantal Pellegrini, Ege Özsoy, Kamilia Zaripova, Matthias Keicher, Nassir Navab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A safe and trustworthy use of Large Language Models (LLMs) requires an
accurate expression of confidence in their answers. We introduce a novel
Reinforcement Learning (RL) approach for LLM calibration that fine-tunes LLMs
to elicit calibrated confidence estimations in their answers to factual
questions. We model the problem as a betting game where the model predicts a
confidence score together with every answer, and design a reward function that
penalizes both over and under-confidence. We prove that under our reward design
an optimal policy would result in a perfectly calibrated confidence estimation.
Our experiments demonstrate significantly improved confidence calibration and
generalization to new tasks without re-training, indicating that our approach
teaches a general confidence awareness. This approach enables the training of
inherently calibrated LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Random Walks for Learning on Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01214v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01214v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinwoo Kim, Olga Zaghen, Ayhan Suleymanzade, Youngmin Ryou, Seunghoon Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We revisit a simple model class for machine learning on graphs, where a
random walk on a graph produces a machine-readable record, and this record is
processed by a deep neural network to directly make vertex-level or graph-level
predictions. We call these stochastic machines random walk neural networks
(RWNNs), and through principled analysis, show that we can design them to be
isomorphism invariant while capable of universal approximation of graph
functions in probability. A useful finding is that almost any kind of record of
random walks guarantees probabilistic invariance as long as the vertices are
anonymized. This enables us, for example, to record random walks in plain text
and adopt a language model to read these text records to solve graph tasks. We
further establish a parallelism to message passing neural networks using tools
from Markov chain theory, and show that over-smoothing in message passing is
alleviated by construction in RWNNs, while over-squashing manifests as
probabilistic under-reaching. We empirically demonstrate RWNNs on a range of
problems, verifying our theoretical analysis and demonstrating the use of
language models for separating strongly regular graphs where 3-WL test fails,
and transductive classification on arXiv citation network. Code is available at
https://github.com/jw9730/random-walk.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>51 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SePer: Measure Retrieval Utility Through The Lens Of Semantic Perplexity
  Reduction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01478v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01478v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lu Dai, Yijie Xu, Jinhui Ye, Hao Liu, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated improved generation
performance by incorporating externally retrieved knowledge, a process known as
retrieval-augmented generation (RAG). Despite the potential of this approach,
existing studies evaluate RAG effectiveness by 1) assessing retrieval and
generation components jointly, which obscures retrieval's distinct
contribution, or 2) examining retrievers using traditional metrics such as
NDCG, which creates a gap in understanding retrieval's true utility in the
overall generation process. To address the above limitations, in this work, we
introduce an automatic evaluation method that measures retrieval quality
through the lens of information gain within the RAG framework. Specifically, we
propose Semantic Perplexity (SePer), a metric that captures the LLM's internal
belief about the correctness of the retrieved information. We quantify the
utility of retrieval by the extent to which it reduces semantic perplexity
post-retrieval. Extensive experiments demonstrate that SePer not only aligns
closely with human preferences but also offers a more precise and efficient
evaluation of retrieval utility across diverse RAG scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 Spotlight</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Iterative Value Function Optimization for Guided Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02368v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02368v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenhua Liu, Lijun Li, Ruizhe Chen, Yuxian Jiang, Tong Zhu, Zhaochen Su, Wenliang Chen, Jing Shao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Reinforcement Learning from Human Feedback (RLHF) has become the
predominant method for controlling language model outputs, it suffers from high
computational costs and training instability. Guided decoding, especially
value-guided methods, offers a cost-effective alternative by controlling
outputs without re-training models. However, the accuracy of the value function
is crucial for value-guided decoding, as inaccuracies can lead to suboptimal
decision-making and degraded performance. Existing methods struggle with
accurately estimating the optimal value function, leading to less effective
control. We propose Iterative Value Function Optimization, a novel framework
that addresses these limitations through two key components: Monte Carlo Value
Estimation, which reduces estimation variance by exploring diverse
trajectories, and Iterative On-Policy Optimization, which progressively
improves value estimation through collecting trajectories from value-guided
policies. Extensive experiments on text summarization, multi-turn dialogue, and
instruction following demonstrate the effectiveness of value-guided decoding
approaches in aligning language models. These approaches not only achieve
alignment but also significantly reduce computational costs by leveraging
principled value function optimization for efficient and effective control.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ KiVA: Kid-inspired Visual Analogies for Testing Large <span class="highlight-title">Multi</span>modal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17773v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17773v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eunice Yiu, Maan Qraitem, Anisa Noor Majhi, Charlie Wong, Yutong Bai, Shiry Ginosar, Alison Gopnik, Kate Saenko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates visual analogical reasoning in large multimodal
models (LMMs) compared to human adults and children. A "visual analogy" is an
abstract rule inferred from one image and applied to another. While benchmarks
exist for testing visual reasoning in LMMs, they require advanced skills and
omit basic visual analogies that even young children can make. Inspired by
developmental psychology, we propose a new benchmark of 4,300 visual
transformations of everyday objects to test LMMs on visual analogical reasoning
and compare them to children (ages three to five) and to adults. We structure
the evaluation into three stages: identifying what changed (e.g., color,
number, etc.), how it changed (e.g., added one object), and applying the rule
to new scenarios. Our findings show that while GPT-o1, GPT-4V, LLaVA-1.5, and
MANTIS identify the "what" effectively, they struggle with quantifying the
"how" and extrapolating this rule to new objects. In contrast, children and
adults exhibit much stronger analogical reasoning at all three stages.
Additionally, the strongest tested model, GPT-o1, performs better in tasks
involving simple surface-level visual attributes like color and size,
correlating with quicker human adult response times. Conversely, more complex
tasks such as number, rotation, and reflection, which necessitate extensive
cognitive processing and understanding of extrinsic spatial properties in the
physical world, present more significant challenges. Altogether, these findings
highlight the limitations of training models on data that primarily consists of
2D images and text.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages. Project website: https://ey242.github.io/kiva.github.io/.
  Benchmark and code: https://github.com/ey242/KiVA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CDS: Data Synthesis Method Guided by Cognitive Diagnosis Theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07674v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07674v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haokun Zhao, Jinyi Han, Jiaqing Liang, Yanghua Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved significant advancements, but the
increasing complexity of tasks and higher performance demands highlight the
need for continuous improvement. Some approaches utilize synthetic data
generated by advanced LLMs based on evaluation results to train models.
However, conventional evaluation methods fail to provide detailed, fine-grained
profiles of LLMs, limiting their guidance for data synthesis. In this paper, we
introduce the Cognitive Diagnostic Synthesis (CDS) method, which incorporates a
diagnostic process inspired by Cognitive Diagnosis Theory (CDT) to refine
evaluation results and characterize model profiles at the knowledge component
level. Based on these diagnostics, we propose two diagnosis-synthesis
strategies for weakness-targeted data synthesis. Additionally, we present an
enhanced data augmentation and selection pipeline to improve the quality and
diversity of synthesized data. Our experiments with several open-source models
show significant improvements across multiple benchmarks, achieving up to 6.00%
improvement in code generation, 13.10% in mathematical reasoning, and 5.43% in
academic exams. Code and data are available on GitHub.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interactive Data Harmonization with <span class="highlight-title">LLM</span> Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07132v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07132v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aécio Santos, Eduardo H. M. Pena, Roque Lopez, Juliana Freire
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data harmonization is an essential task that entails integrating datasets
from diverse sources. Despite years of research in this area, it remains a
time-consuming and challenging task due to schema mismatches, varying
terminologies, and differences in data collection methodologies. This paper
presents the case for agentic data harmonization as a means to both empower
experts to harmonize their data and to streamline the process. We introduce
Harmonia, a system that combines LLM-based reasoning, an interactive user
interface, and a library of data harmonization primitives to automate the
synthesis of data harmonization pipelines. We demonstrate Harmonia in a
clinical data harmonization scenario, where it helps to interactively create
reusable pipelines that map datasets to a standard format. Finally, we discuss
challenges and open problems, and suggest research directions for advancing our
vision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PARAMANU-GANITA: Can Small Math Language Models Rival with Large
  Language Models on Mathematical Reasoning? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.14395v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.14395v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mitodru Niyogi, Arnab Bhattacharya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study whether domain specific pretraining of small
generative language models (SLM) from scratch with domain specialized tokenizer
and Chain-of-Thought (CoT) instruction fine-tuning results in competitive
performance on mathematical reasoning compared to LLMs? Secondly, whether this
approach is environmentally sustainable, highly cost efficient? To address
these research questions, we present Paramanu-Ganita, a 208 million-parameter
novel decoder-only Auto Regressive SLM on mathematics. We performed pretraining
from scratch on 31.5 billion tokens for 170 A100 hours using a context size of
4096 on a mixed mathematical corpus consisting of web pages, source code,
textbooks, CoT templatised StackOverflow QA pairs, and mathematical lecture
notes in LaTeX curated by us. We also trained a math and code specialised BPE
tokenizer. We proposed and performed CoT instruction fine-tuning of
Paramanu-Ganita on the MetaMathQA dataset. Our model Paramanu-Ganita, despite
being 34 times smaller than the 7B LLMs, outperforms generalist LLMs by
approximately 30% points, and even math-specialised LLMs by 3-23% points in
GSM8K test accuracy metric. On MATH benchmark, Paramanu-Ganita outperformed the
various models by 6-8% points. On benchmarks like LogiQA, MMLU (high school,
college level), and competitive exams level, AGIEVAL (AQuA-RAT, SAT-Math),
Paramanu-Ganita outperformed others by 1-4%. Our model is available at
https://huggingface.co/gyanai/paramanu-ganita-208M-hf .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural DNF-MT: A Neuro-symbolic Approach for Learning Interpretable and
  Editable Policies <span class="chip">AAMAS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03888v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03888v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kexin Gu Baugh, Luke Dickens, Alessandra Russo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although deep reinforcement learning has been shown to be effective, the
model's black-box nature presents barriers to direct policy interpretation. To
address this problem, we propose a neuro-symbolic approach called neural DNF-MT
for end-to-end policy learning. The differentiable nature of the neural DNF-MT
model enables the use of deep actor-critic algorithms for training. At the same
time, its architecture is designed so that trained models can be directly
translated into interpretable policies expressed as standard (bivalent or
probabilistic) logic programs. Moreover, additional layers can be included to
extract abstract features from complex observations, acting as a form of
predicate invention. The logic representations are highly interpretable, and we
show how the bivalent representations of deterministic policies can be edited
and incorporated back into a neural model, facilitating manual intervention and
adaptation of learned policies. We evaluate our approach on a range of tasks
requiring learning deterministic or stochastic behaviours from various forms of
observations. Our empirical results show that our neural DNF-MT model performs
at the level of competing black-box methods whilst providing interpretable
policies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AAMAS 2025 (with Appendix)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01776v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01776v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiansheng Wen, Yifei Wang, Zequn Zeng, Zhong Peng, Yudi Su, Xinyang Liu, Bo Chen, Hongwei Liu, Stefanie Jegelka, Chenyu You
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many large-scale systems rely on high-quality deep representations
(embeddings) to facilitate tasks like retrieval, search, and generative
modeling. Matryoshka Representation Learning (MRL) recently emerged as a
solution for adaptive embedding lengths, but it requires full model retraining
and suffers from noticeable performance degradations at short lengths. In this
paper, we show that sparse coding offers a compelling alternative for achieving
adaptive representation with minimal overhead and higher fidelity. We propose
Contrastive Sparse Representation (CSR), a method that sparsifies pre-trained
embeddings into a high-dimensional but selectively activated feature space. By
leveraging lightweight autoencoding and task-aware contrastive objectives, CSR
preserves semantic quality while allowing flexible, cost-effective inference at
different sparsity levels. Extensive experiments on image, text, and multimodal
benchmarks demonstrate that CSR consistently outperforms MRL in terms of both
accuracy and retrieval speed-often by large margins-while also cutting training
time to a fraction of that required by MRL. Our results establish sparse coding
as a powerful paradigm for adaptive representation learning in real-world
applications where efficiency and fidelity are both paramount. Code is
available at https://github.com/neilwen987/CSR_Adaptive_Rep
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A novel sparse coding framework designed for learning adaptive
  representation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DelTA: An Online Document-Level Translation Agent Based on <span class="highlight-title">Multi</span>-Level
  Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08143v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08143v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutong Wang, Jiali Zeng, Xuebo Liu, Derek F. Wong, Fandong Meng, Jie Zhou, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved reasonable quality improvements in
machine translation (MT). However, most current research on MT-LLMs still faces
significant challenges in maintaining translation consistency and accuracy when
processing entire documents. In this paper, we introduce DelTA, a
Document-levEL Translation Agent designed to overcome these limitations. DelTA
features a multi-level memory structure that stores information across various
granularities and spans, including Proper Noun Records, Bilingual Summary,
Long-Term Memory, and Short-Term Memory, which are continuously retrieved and
updated by auxiliary LLM-based components. Experimental results indicate that
DelTA significantly outperforms strong baselines in terms of translation
consistency and quality across four open/closed-source LLMs and two
representative document translation datasets, achieving an increase in
consistency scores by up to 4.58 percentage points and in COMET scores by up to
3.16 points on average. DelTA employs a sentence-by-sentence translation
strategy, ensuring no sentence omissions and offering a memory-efficient
solution compared to the mainstream method. Furthermore, DelTA improves pronoun
and context-dependent translation accuracy, and the summary component of the
agent also shows promise as a tool for query-based summarization tasks. The
code and data of our approach are released at
https://github.com/YutongWang1216/DocMTAgent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PyGen: A Collaborative Human-AI Approach to Python Package Creation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08932v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08932v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saikat Barua, Mostafizur Rahman, Md Jafor Sadek, Rafiul Islam, Shehnaz Khaled, Md. Shohrab Hossain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The principles of automation and innovation serve as foundational elements
for advancement in contemporary science and technology. Here, we introduce
Pygen, an automation platform designed to empower researchers, technologists,
and hobbyists to bring abstract ideas to life as core, usable software tools
written in Python. Pygen leverages the immense power of autoregressive large
language models to augment human creativity during the ideation, iteration, and
innovation process. By combining state-of-the-art language models with
open-source code generation technologies, Pygen has significantly reduced the
manual overhead of tool development. From a user prompt, Pygen automatically
generates Python packages for a complete workflow from concept to package
generation and documentation. The findings of our work show that Pygen
considerably enhances the researcher's productivity by enabling the creation of
resilient, modular, and well-documented packages for various specialized
purposes. We employ a prompt enhancement approach to distill the user's package
description into increasingly specific and actionable. While being inherently
an open-ended task, we have evaluated the generated packages and the
documentation using Human Evaluation, LLM-based evaluation, and CodeBLEU, with
detailed results in the results section. Furthermore, we documented our
results, analyzed the limitations, and suggested strategies to alleviate them.
Pygen is our vision of ethical automation, a framework that promotes
inclusivity, accessibility, and collaborative development. This project marks
the beginning of a large-scale effort towards creating tools where intelligent
agents collaborate with humans to improve scientific and technological
development substantially.
  Our code and generated examples are open-sourced at
[https://github.com/GitsSaikat/Pygen]
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bonsai: Gradient-free Graph Distillation for Node Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.17579v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.17579v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mridul Gupta, Samyak Jain, Vansh Ramani, Hariprasad Kodamana, Sayan Ranu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph distillation has emerged as a promising avenue to enable scalable
training of GNNs by compressing the training dataset while preserving essential
graph characteristics. Our study uncovers significant shortcomings in current
graph distillation techniques. First, the majority of the algorithms
paradoxically require training on the full dataset to perform distillation.
Second, due to their gradient-emulating approach, these methods require fresh
distillation for any change in hyperparameters or GNN architecture, limiting
their flexibility and reusability. Finally, they fail to achieve substantial
size reduction due to synthesizing fully-connected, edge-weighted graphs. To
address these challenges, we present Bonsai, a novel graph distillation method
empowered by the observation that \textit{computation trees} form the
fundamental processing units of message-passing GNNs. Bonsai distills datasets
by encoding a careful selection of \textit{exemplar} trees that maximize the
representation of all computation trees in the training set. This unique
approach imparts Bonsai as the first linear-time, model-agnostic graph
distillation algorithm for node classification that outperforms existing
baselines across $6$ real-world datasets on accuracy, while being $22$ times
faster on average. Bonsai is grounded in rigorous mathematical guarantees on
the adopted approximation strategies making it robust to GNN architectures,
datasets, and parameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A privacy-preserving, <span class="highlight-title">distributed</span> and cooperative FCM-based learning
  approach for cancer research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10102v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10102v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jose L. Salmeron, Irina Arévalo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed Artificial Intelligence is attracting interest day by day. In
this paper, the authors introduce an innovative methodology for distributed
learning of Particle Swarm Optimization-based Fuzzy Cognitive Maps in a
privacy-preserving way. The authors design a training scheme for collaborative
FCM learning that offers data privacy compliant with the current regulation.
This method is applied to a cancer detection problem, proving that the
performance of the model is improved by the Federated Learning process, and
obtaining similar results to the ones that can be found in the literature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Rough Sets: International Joint Conference, IJCRS 2020</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SMAC-R1: The Emergence of Intelligence in Decision-Making Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16024v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16024v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Deng, Weiyu Ma, Yuxin Fan, Ruyi Song, Yin Zhang, Haifeng Zhang, Jian Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  StarCraft Multi-Agent Challenge (SMAC) has been one of the most commonly used
experimental environments in multi-agent reinforcement learning (MARL), where
the specific task is to control a set number of allied units to defeat enemy
forces. Traditional MARL algorithms often require interacting with the
environment for millions of steps to train a parametric model, of which the
resulting policies are typically non-interpretable with weak transferability.
In this paper, we introduce SMAC-R1 which is based on the Qwen2.5-7B-Base LLM
distilled from DeepSeek-Coder-v2.5-236B. Similar to online reinforcement
learning after behavior cloning in offline learning process, in our pipeline,
agents leverage the DeepSeek LLM to generate decision tree code by providing
task descriptions, and the agents are further self-reflected using feedback
from the rewards provided by the environment. Based on that, we augment the
generated scripts to fine-tune a small LLM, Qwen2.5-7B-Base, to distill the
decision-making ability via Supervised Fine-Tuning (SFT) and enhance the script
generation ability by the Group Relative Policy Optimization (GRPO) algorithm.
We conduct experiments in the original 23 SMAC tasks and 10 newly-designed
tasks to demonstrate that our method can produce high-quality, interpretable
decision trees with minimal environmental exploration. Moreover, these scripts
exhibit strong transferability, successfully applying to homogeneous SMAC
environments without modification. We believe this approach offers a new
direction for solving decision-making tasks and domain-specific LLM training
pipelines in the future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What to align in <span class="highlight-title">multi</span>modal contrastive learning? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07402v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07402v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benoit Dufumier, Javiera Castillo-Navarro, Devis Tuia, Jean-Philippe Thiran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans perceive the world through multisensory integration, blending the
information of different modalities to adapt their behavior. Contrastive
learning offers an appealing solution for multimodal self-supervised learning.
Indeed, by considering each modality as a different view of the same entity, it
learns to align features of different modalities in a shared representation
space. However, this approach is intrinsically limited as it only learns shared
or redundant information between modalities, while multimodal interactions can
arise in other ways. In this work, we introduce CoMM, a Contrastive MultiModal
learning strategy that enables the communication between modalities in a single
multimodal space. Instead of imposing cross- or intra- modality constraints, we
propose to align multimodal representations by maximizing the mutual
information between augmented versions of these multimodal features. Our
theoretical analysis shows that shared, synergistic and unique terms of
information naturally emerge from this formulation, allowing us to estimate
multimodal interactions beyond redundancy. We test CoMM both in a controlled
and in a series of real-world settings: in the former, we demonstrate that CoMM
effectively captures redundant, unique and synergistic information between
modalities. In the latter, CoMM learns complex multimodal interactions and
achieves state-of-the-art results on the seven multimodal benchmarks. Code is
available at https://github.com/Duplums/CoMM
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025, 25 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CycleResearcher: Improving Automated Research via Automated <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00816v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00816v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Weng, Minjun Zhu, Guangsheng Bao, Hongbo Zhang, Jindong Wang, Yue Zhang, Linyi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The automation of scientific discovery has been a long-standing goal within
the research community, driven by the potential to accelerate knowledge
creation. While significant progress has been made using commercial large
language models (LLMs) as research assistants or idea generators, the
possibility of automating the entire research process with open-source LLMs
remains largely unexplored. This paper explores the feasibility of using
open-source post-trained LLMs as autonomous agents capable of performing the
full cycle of automated research and review, from literature review and
manuscript preparation to peer review and paper refinement. Our iterative
preference training framework consists of CycleResearcher, which conducts
research tasks, and CycleReviewer, which simulates the peer review process,
providing iterative feedback via reinforcement learning. To train these models,
we develop two new datasets, Review-5k and Research-14k, reflecting real-world
machine learning research and peer review dynamics. Our results demonstrate
that CycleReviewer achieves promising performance with a 26.89\% reduction in
mean absolute error (MAE) compared to individual human reviewers in predicting
paper scores, indicating the potential of LLMs to effectively assist
expert-level research evaluation. In research, the papers generated by the
CycleResearcher model achieved a score of 5.36 in simulated peer reviews,
showing some competitiveness in terms of simulated review scores compared to
the preprint level of 5.24 from human experts, while still having room for
improvement compared to the accepted paper level of 5.69. This work represents
a significant step toward fully automated scientific inquiry, providing ethical
safeguards and exploring AI-driven research capabilities. The code, dataset and
model weight are released at https://wengsyx.github.io/Researcher/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept in ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DexGraspVLA: A Vision-Language-Action Framework Towards General
  Dexterous Grasping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20900v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20900v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Zhong, Xuchuan Huang, Ruochong Li, Ceyao Zhang, Yitao Liang, Yaodong Yang, Yuanpei Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dexterous grasping remains a fundamental yet challenging problem in robotics.
A general-purpose robot must be capable of grasping diverse objects in
arbitrary scenarios. However, existing research typically relies on specific
assumptions, such as single-object settings or limited environments, leading to
constrained generalization. Our solution is DexGraspVLA, a hierarchical
framework that utilizes a pre-trained Vision-Language model as the high-level
task planner and learns a diffusion-based policy as the low-level Action
controller. The key insight lies in iteratively transforming diverse language
and visual inputs into domain-invariant representations, where imitation
learning can be effectively applied due to the alleviation of domain shift.
Thus, it enables robust generalization across a wide range of real-world
scenarios. Notably, our method achieves a 90+% success rate under thousands of
unseen object, lighting, and background combinations in a ``zero-shot''
environment. Empirical analysis further confirms the consistency of internal
model behavior across environmental variations, thereby validating our design
and explaining its generalization performance. We hope our work can be a step
forward in achieving general dexterous grasping. Our demo and code can be found
at https://dexgraspvla.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI Governance through Markets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.17755v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.17755v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Moreira Tomei, Rupal Jain, Matija Franklin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper argues that market governance mechanisms should be considered a
key approach in the governance of artificial intelligence (AI), alongside
traditional regulatory frameworks. While current governance approaches have
predominantly focused on regulation, we contend that market-based mechanisms
offer effective incentives for responsible AI development. We examine four
emerging vectors of market governance: insurance, auditing, procurement, and
due diligence, demonstrating how these mechanisms can affirm the relationship
between AI risk and financial risk while addressing capital allocation
inefficiencies. While we do not claim that market forces alone can adequately
protect societal interests, we maintain that standardised AI disclosures and
market mechanisms can create powerful incentives for safe and responsible AI
development. This paper urges regulators, economists, and machine learning
researchers to investigate and implement market-based approaches to AI
governance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Provable Benefits of Task-Specific Prompts for In-context Learning <span class="chip">AISTATS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02102v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02102v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Chang, Yingcong Li, Muti Kara, Samet Oymak, Amit K. Roy-Chowdhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The in-context learning capabilities of modern language models have motivated
a deeper mathematical understanding of sequence models. A line of recent work
has shown that linear attention models can emulate projected gradient descent
iterations to implicitly learn the task vector from the data provided in the
context window. In this work, we consider a novel setting where the global task
distribution can be partitioned into a union of conditional task distributions.
We then examine the use of task-specific prompts and prediction heads for
learning the prior information associated with the conditional task
distribution using a one-layer attention model. Our results on loss landscape
show that task-specific prompts facilitate a covariance-mean decoupling where
prompt-tuning explains the conditional mean of the distribution whereas the
variance is learned/explained through in-context learning. Incorporating
task-specific head further aids this process by entirely decoupling estimation
of mean and variance components. This covariance-mean perspective similarly
explains how jointly training prompt and attention weights can provably help
over fine-tuning after pretraining.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 28th International Conference on Artificial
  Intelligence and Statistics (AISTATS) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MIRROR: A Novel Approach for the Automated Evaluation of Open-Ended
  Question Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12893v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12893v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aniket Deroy, Subhankar Maity, Sudeshna Sarkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic question generation is a critical task that involves evaluating
question quality by considering factors such as engagement, pedagogical value,
and the ability to stimulate critical thinking. These aspects require
human-like understanding and judgment, which automated systems currently lack.
However, human evaluations are costly and impractical for large-scale samples
of generated questions. Therefore, we propose a novel system, MIRROR (Multi-LLM
Iterative Review and Response for Optimized Rating), which leverages large
language models (LLMs) to automate the evaluation process for questions
generated by automated question generation systems. We experimented with
several state-of-the-art LLMs, such as GPT-4, Gemini, and Llama2-70b. We
observed that the scores of human evaluation metrics, namely relevance,
appropriateness, novelty, complexity, and grammaticality, improved when using
the feedback-based approach called MIRROR, tending to be closer to the human
baseline scores. Furthermore, we observed that Pearson's correlation
coefficient between GPT-4 and human experts improved when using our proposed
feedback-based approach, MIRROR, compared to direct prompting for evaluation.
Error analysis shows that our proposed approach, MIRROR, significantly helps to
improve relevance and appropriateness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS'24 Workshop on Large Foundation Models for Educational
  Assessment (FM-EduAssess)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One-Shot Imitation under Mismatched Execution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06615v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06615v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kushal Kedia, Prithwish Dan, Angela Chao, Maximus Adrian Pace, Sanjiban Choudhury
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human demonstrations as prompts are a powerful way to program robots to do
long-horizon manipulation tasks. However, translating these demonstrations into
robot-executable actions presents significant challenges due to execution
mismatches in movement styles and physical capabilities. Existing methods
either depend on human-robot paired data, which is infeasible to scale, or rely
heavily on frame-level visual similarities that often break down in practice.
To address these challenges, we propose RHyME, a novel framework that
automatically aligns human and robot task executions using optimal transport
costs. Given long-horizon robot demonstrations, RHyME synthesizes semantically
equivalent human videos by retrieving and composing short-horizon human clips.
This approach facilitates effective policy training without the need for paired
data. RHyME successfully imitates a range of cross-embodiment demonstrators,
both in simulation and with a real human hand, achieving over 50\% increase in
task success compared to previous methods. We release our code and datasets at
https://portal-cornell.github.io/rhyme/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Measuring and identifying factors of individuals' trust in Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21028v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21028v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edoardo Sebastiano De Duro, Giuseppe Alessandro Veltri, Hudson Golino, Massimo Stella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) can engage in human-looking conversational
exchanges. Although conversations can elicit trust between users and LLMs,
scarce empirical research has examined trust formation in human-LLM contexts,
beyond LLMs' trustworthiness or human trust in AI in general. Here, we
introduce the Trust-In-LLMs Index (TILLMI) as a new framework to measure
individuals' trust in LLMs, extending McAllister's cognitive and affective
trust dimensions to LLM-human interactions. We developed TILLMI as a
psychometric scale, prototyped with a novel protocol we called LLM-simulated
validity. The LLM-based scale was then validated in a sample of 1,000 US
respondents. Exploratory Factor Analysis identified a two-factor structure. Two
items were then removed due to redundancy, yielding a final 6-item scale with a
2-factor structure. Confirmatory Factor Analysis on a separate subsample showed
strong model fit ($CFI = .995$, $TLI = .991$, $RMSEA = .046$, $p_{X^2} > .05$).
Convergent validity analysis revealed that trust in LLMs correlated positively
with openness to experience, extraversion, and cognitive flexibility, but
negatively with neuroticism. Based on these findings, we interpreted TILLMI's
factors as "closeness with LLMs" (affective dimension) and "reliance on LLMs"
(cognitive dimension). Younger males exhibited higher closeness with- and
reliance on LLMs compared to older women. Individuals with no direct experience
with LLMs exhibited lower levels of trust compared to LLMs' users. These
findings offer a novel empirical foundation for measuring trust in AI-driven
verbal communication, informing responsible design, and fostering balanced
human-AI collaboration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Informal to Formal -- Incorporating and Evaluating <span class="highlight-title">LLM</span>s on Natural
  Language Requirements to Verifiable Formal Proofs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16207v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16207v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jialun Cao, Yaojie Lu, Meiziniu Li, Haoyang Ma, Haokun Li, Mengda He, Cheng Wen, Le Sun, Hongyu Zhang, Shengchao Qin, Shing-Chi Cheung, Cong Tian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The research in AI-based formal mathematical reasoning has shown an
unstoppable growth trend. These studies have excelled in mathematical
competitions like IMO and have made significant progress. This paper focuses on
formal verification, an immediate application scenario of formal reasoning, and
breaks it down into sub-tasks. We constructed 18k high-quality
instruction-response pairs across five formal specification languages (Coq,
Lean4, Dafny, ACSL, and TLA+) by distilling gpt-4o and evaluated against ten
open-sourced LLMs, including recent popular DeepSeek-R1. We also fine-tuned
several 7~8B small models to achieve comparable performance with
Deepseek-R1-671B. Interestingly, we observed that fine-tuning with formal data
also enhances mathematics, reasoning, and coding capabilities. Fine-tuned
models are released at https: //huggingface.co/fm-universe.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Utility of Equivariance and Symmetry Breaking in Deep Learning
  Architectures on Point Clouds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.01999v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.01999v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sharvaree Vadgama, Mohammad Mohaiminul Islam, Domas Buracus, Christian Shewmake, Erik Bekkers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores the key factors that influence the performance of models
working with point clouds, across different tasks of varying geometric
complexity. In this work, we explore the trade-offs between flexibility and
weight-sharing introduced by equivariant layers, assessing when equivariance
boosts or detracts from performance. It is often argued that providing more
information as input improves a model's performance. However, if this
additional information breaks certain properties, such as $\SE(3)$
equivariance, does it remain beneficial? We identify the key aspects of
equivariant and non-equivariant architectures that drive success in different
tasks by benchmarking them on segmentation, regression, and generation tasks
across multiple datasets with increasing complexity. We observe a positive
impact of equivariance, which becomes more pronounced with increasing task
complexity, even when strict equivariance is not required.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DeePen: Penetration Testing for Audio Deepfake Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Müller, Piotr Kawa, Adriana Stan, Thien-Phuc Doan, Souhwan Jung, Wei Herng Choong, Philip Sperl, Konstantin Böttinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deepfakes - manipulated or forged audio and video media - pose significant
security risks to individuals, organizations, and society at large. To address
these challenges, machine learning-based classifiers are commonly employed to
detect deepfake content. In this paper, we assess the robustness of such
classifiers through a systematic penetration testing methodology, which we
introduce as DeePen. Our approach operates without prior knowledge of or access
to the target deepfake detection models. Instead, it leverages a set of
carefully selected signal processing modifications - referred to as attacks -
to evaluate model vulnerabilities. Using DeePen, we analyze both real-world
production systems and publicly available academic model checkpoints,
demonstrating that all tested systems exhibit weaknesses and can be reliably
deceived by simple manipulations such as time-stretching or echo addition.
Furthermore, our findings reveal that while some attacks can be mitigated by
retraining detection systems with knowledge of the specific attack, others
remain persistently effective. We release all associated code.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bringing AI Participation Down to Scale: A Comment on Open AIs
  Democratic Inputs to AI Project 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11613v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11613v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Moats, Chandrima Ganguly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In 2023, Open AIs Democratic Inputs program funded 10 teams to design
procedures for public participation in generative AI. In this Perspective, we
review the results of the project, drawing on interviews with some of the teams
and our own experiences conducting participation exercises, we identify several
shared yet largely unspoken assumptions of the Democratic Inputs program 1.
that participation must be scalable 2. that the object of participation is a
single model 3. that there must be a single form of participation 4. that the
goal is to extract abstract principles 5. that these principles should have
consensus 6. that publics should be representative and encourage alternative
forms of participation in AI, perhaps not undertaken by tech companies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safety Without Semantic Disruptions: Editing-free Safe Image Generation
  via Context-preserving Dual Latent Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.13982v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.13982v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jordan Vice, Naveed Akhtar, Mubarak Shah, Richard Hartley, Ajmal Mian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training multimodal generative models on large, uncurated datasets can result
in users being exposed to harmful, unsafe and controversial or
culturally-inappropriate outputs. While model editing has been proposed to
remove or filter undesirable concepts in embedding and latent spaces, it can
inadvertently damage learned manifolds, distorting concepts in close semantic
proximity. We identify limitations in current model editing techniques, showing
that even benign, proximal concepts may become misaligned. To address the need
for safe content generation, we leverage safe embeddings and a modified
diffusion process with tunable weighted summation in the latent space to
generate safer images. Our method preserves global context without compromising
the structural integrity of the learned manifolds. We achieve state-of-the-art
results on safe image generation benchmarks and offer intuitive control over
the level of model safety. We identify trade-offs between safety and
censorship, which presents a necessary perspective in the development of
ethical AI models. We will release our code.
  Keywords: Text-to-Image Models, Generative AI, Safety, Reliability, Model
Editing
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This research is supported by the NISDRG project #20100007, funded by
  the Australian Government</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">LLM</span>s can be Dangerous Reasoners: Analyzing-based Jailbreak Attack on
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.16205v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.16205v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi Lin, Hongming Yang, Dingyang Lin, Rongchang Li, Xun Wang, Changting Lin, Wenpeng Xing, Meng Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of Large Language Models (LLMs) has brought significant
advancements across various tasks. However, despite these achievements, LLMs
still exhibit inherent safety vulnerabilities, especially when confronted with
jailbreak attacks. Existing jailbreak methods suffer from two main limitations:
reliance on complicated prompt engineering and iterative optimization, which
lead to low attack success rate (ASR) and attack efficiency (AE). In this work,
we propose an efficient jailbreak attack method, Analyzing-based Jailbreak
(ABJ), which leverages the advanced reasoning capability of LLMs to
autonomously generate harmful content, revealing their underlying safety
vulnerabilities during complex reasoning process. We conduct comprehensive
experiments on ABJ across various open-source and closed-source LLMs. In
particular, ABJ achieves high ASR (82.1% on GPT-4o-2024-11-20) with exceptional
AE among all target LLMs, showcasing its remarkable attack effectiveness,
transferability, and efficiency. Our findings underscore the urgent need to
prioritize and improve the safety of LLMs to mitigate the risks of misuse.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Scheduling for <span class="highlight-title">LLM</span> Inference with KV Cache Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07115v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07115v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Jaillet, Jiashuo Jiang, Chara Podimata, Zijie Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model (LLM) inference, where a trained model generates text
one word at a time in response to user prompts, is a computationally intensive
process requiring efficient scheduling to optimize latency and resource
utilization. A key challenge in LLM inference is the management of the
Key-Value (KV) cache, which reduces redundant computations but introduces
memory constraints. In this work, we model LLM inference with KV cache
constraints theoretically and propose novel batching and scheduling algorithms
that minimize inference latency while effectively managing the KV cache's
memory.
  We analyze both semi-online and fully online scheduling models, and our
results are threefold. First, we provide a polynomial-time algorithm that
achieves exact optimality in terms of average latency in the semi-online prompt
arrival model. Second, in the fully online case with a stochastic prompt
arrival, we introduce an efficient online scheduling algorithm with constant
regret. Third, we prove that no algorithm (deterministic or randomized) can
achieve a constant competitive ratio in fully online adversarial settings. Our
empirical evaluations on a public LLM inference dataset, using the Llama-70B
model on A100 GPUs, show that our approach significantly outperforms benchmark
algorithms used currently in practice, achieving lower latency while reducing
energy consumption. Overall, our results offer a path toward more sustainable
and cost-effective LLM deployment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Will add a lemma in the proof of Theorem 5.3 to make the statement
  and proof more rigorous</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RIDE: Enhancing Large Language Model Alignment through Restyled
  In-Context Learning Demonstration Exemplars 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11681v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11681v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuncheng Hua, Lizhen Qu, Zhuang Li, Hao Xue, Flora D. Salim, Gholamreza Haffari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alignment tuning is crucial for ensuring large language models (LLMs) behave
ethically and helpfully. Current alignment approaches require high-quality
annotations and significant training resources. This paper proposes a low-cost,
tuning-free method using in-context learning (ICL) to enhance LLM alignment.
Through an analysis of high-quality ICL demos, we identified style as a key
factor influencing LLM alignment capabilities and explicitly restyled ICL
exemplars based on this stylistic framework. Additionally, we combined the
restyled demos to achieve a balance between the two conflicting aspects of LLM
alignment--factuality and safety. We packaged the restyled examples as prompts
to trigger few-shot learning, improving LLM alignment. Compared to the best
baseline approach, with an average score of 5.00 as the maximum, our method
achieves a maximum 0.10 increase on the Alpaca task (from 4.50 to 4.60), a 0.22
enhancement on the Just-eval benchmark (from 4.34 to 4.56), and a maximum
improvement of 0.32 (from 3.53 to 3.85) on the MT-Bench dataset. We release the
code and data at https://github.com/AnonymousCode-ComputerScience/RIDE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 2 figures, 20 tables; The paper is under review in ARR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GSplatLoc: Grounding Keypoint Descriptors into 3D Gaussian Splatting for
  Improved Visual Localization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16502v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16502v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gennady Sidorov, Malik Mohrat, Denis Gridusov, Ruslan Rakhimov, Sergey Kolyubin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although various visual localization approaches exist, such as scene
coordinate regression and camera pose regression, these methods often struggle
with optimization complexity or limited accuracy. To address these challenges,
we explore the use of novel view synthesis techniques, particularly 3D Gaussian
Splatting (3DGS), which enables the compact encoding of both 3D geometry and
scene appearance. We propose a two-stage procedure that integrates dense and
robust keypoint descriptors from the lightweight XFeat feature extractor into
3DGS, enhancing performance in both indoor and outdoor environments. The coarse
pose estimates are directly obtained via 2D-3D correspondences between the 3DGS
representation and query image descriptors. In the second stage, the initial
pose estimate is refined by minimizing the rendering-based photometric warp
loss. Benchmarking on widely used indoor and outdoor datasets demonstrates
improvements over recent neural rendering-based localization methods, such as
NeRFMatch and PNeRFLoc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website at https://gsplatloc.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Multi</span>modal Action Quality Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09444v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09444v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ling-An Zeng, Wei-Shi Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Action quality assessment (AQA) is to assess how well an action is performed.
Previous works perform modelling by only the use of visual information,
ignoring audio information. We argue that although AQA is highly dependent on
visual information, the audio is useful complementary information for improving
the score regression accuracy, especially for sports with background music,
such as figure skating and rhythmic gymnastics. To leverage multimodal
information for AQA, i.e., RGB, optical flow and audio information, we propose
a Progressive Adaptive Multimodal Fusion Network (PAMFN) that separately models
modality-specific information and mixed-modality information. Our model
consists of with three modality-specific branches that independently explore
modality-specific information and a mixed-modality branch that progressively
aggregates the modality-specific information from the modality-specific
branches. To build the bridge between modality-specific branches and the
mixed-modality branch, three novel modules are proposed. First, a
Modality-specific Feature Decoder module is designed to selectively transfer
modality-specific information to the mixed-modality branch. Second, when
exploring the interaction between modality-specific information, we argue that
using an invariant multimodal fusion policy may lead to suboptimal results, so
as to take the potential diversity in different parts of an action into
consideration. Therefore, an Adaptive Fusion Module is proposed to learn
adaptive multimodal fusion policies in different parts of an action. This
module consists of several FusionNets for exploring different multimodal fusion
strategies and a PolicyNet for deciding which FusionNets are enabled. Third, a
module called Cross-modal Feature Decoder is designed to transfer cross-modal
features generated by Adaptive Fusion Module to the mixed-modality branch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Transactions on Image Processing 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Handling Spatial-Temporal Data Heterogeneity for Federated Continual
  Learning via Tail Anchor 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18355v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18355v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Yu, Xin Yang, Le Zhang, Hanlin Gu, Tianrui Li, Lixin Fan, Qiang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated continual learning (FCL) allows each client to continually update
its knowledge from task streams, enhancing the applicability of federated
learning in real-world scenarios. However, FCL needs to address not only
spatial data heterogeneity between clients but also temporal data heterogeneity
between tasks. In this paper, empirical experiments demonstrate that such
input-level heterogeneity significantly affects the model's internal parameters
and outputs, leading to severe spatial-temporal catastrophic forgetting of
local and previous knowledge. To this end, we propose Federated Tail Anchor
(FedTA) to mix trainable Tail Anchor with the frozen output features to adjust
their position in the feature space, thereby overcoming parameter-forgetting
and output-forgetting. Three novel components are also included: Input
Enhancement for improving the performance of pre-trained models on downstream
tasks; Selective Input Knowledge Fusion for fusion of heterogeneous local
knowledge on the server; and Best Global Prototype Selection for finding the
best anchor point for each class in the feature space. Extensive experiments
demonstrate that FedTA not only outperforms existing FCL methods but also
effectively preserves the relative positions of features.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Promote, Suppress, Iterate: How Language Models Answer One-to-Many
  Factual Queries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20475v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20475v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Lorena Yan, Robin Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To answer one-to-many factual queries (e.g., listing cities of a country), a
language model (LM) must simultaneously recall knowledge and avoid repeating
previous answers. How are these two subtasks implemented and integrated
internally? Across multiple datasets and models, we identify a
promote-then-suppress mechanism: the model first recalls all answers, and then
suppresses previously generated ones. Specifically, LMs use both the subject
and previous answer tokens to perform knowledge recall, with attention
propagating subject information and MLPs promoting the answers. Then, attention
attends to and suppresses previous answer tokens, while MLPs amplify the
suppression signal. Our mechanism is corroborated by extensive experimental
evidence: in addition to using early decoding and causal tracing, we analyze
how components use different tokens by introducing both Token Lens, which
decodes aggregated attention updates from specified tokens, and a knockout
method that analyzes changes in MLP outputs after removing attention to
specified tokens. Overall, we provide new insights into how LMs' internal
components interact with different input tokens to support complex factual
recall. Code is available at
https://github.com/Lorenayannnnn/how-lms-answer-one-to-many-factual-queries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph-Aware Isomorphic Attention for Adaptive Dynamics in Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02393v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02393v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Markus J. Buehler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an approach to modifying Transformer architectures by integrating
graph-aware relational reasoning into the attention mechanism, merging concepts
from graph neural networks and language modeling. Building on the inherent
connection between attention and graph theory, we reformulate the Transformer's
attention mechanism as a graph operation and propose Graph-Aware Isomorphic
Attention. This method leverages advanced graph modeling strategies, including
Graph Isomorphism Networks (GIN) and Principal Neighborhood Aggregation (PNA),
to enrich the representation of relational structures. Our approach captures
complex dependencies and generalizes across tasks, as evidenced by a reduced
generalization gap and improved learning performance. Additionally, we expand
the concept of graph-aware attention to introduce Sparse GIN-Attention, a
fine-tuning approach that employs sparse GINs. By interpreting attention
matrices as sparse adjacency graphs, this technique enhances the adaptability
of pre-trained foundational models with minimal computational overhead,
endowing them with graph-aware capabilities. Sparse GIN-Attention fine-tuning
achieves improved training dynamics and better generalization compared to
alternative methods like low-rank adaption (LoRA). We discuss latent graph-like
structures within traditional attention mechanisms, offering a new lens through
which Transformers can be understood. By evolving Transformers as hierarchical
GIN models for relational reasoning. This perspective suggests profound
implications for foundational model development, enabling the design of
architectures that dynamically adapt to both local and global dependencies.
Applications in bioinformatics, materials science, language modeling, and
beyond could benefit from this synthesis of relational and sequential data
modeling, setting the stage for interpretable and generalizable modeling
strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sim2Real within 5 Minutes: Efficient Domain Transfer with Stylized
  Gaussian Splatting for Endoscopic Images <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10860v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10860v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyang Wu, Yun Gu, Guang-Zhong Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robot assisted endoluminal intervention is an emerging technique for both
benign and malignant luminal lesions. With vision-based navigation, when
combined with pre-operative imaging data as priors, it is possible to recover
position and pose of the endoscope without the need of additional sensors. In
practice, however, aligning pre-operative and intra-operative domains is
complicated by significant texture differences. Although methods such as style
transfer can be used to address this issue, they require large datasets from
both source and target domains with prolonged training times. This paper
proposes an efficient domain transfer method based on stylized Gaussian
splatting, only requiring a few of real images (10 images) with very fast
training time. Specifically, the transfer process includes two phases. In the
first phase, the 3D models reconstructed from CT scans are represented as
differential Gaussian point clouds. In the second phase, only color appearance
related parameters are optimized to transfer the style and preserve the visual
content. A novel structure consistency loss is applied to latent features and
depth levels to enhance the stability of the transferred images. Detailed
validation was performed to demonstrate the performance advantages of the
proposed method compared to that of the current state-of-the-art, highlighting
the potential for intra-operative surgical navigation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Physical Coherence Benchmark for Evaluating Video Generation Models
  via Optical Flow-guided Frame Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05503v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05503v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongfan Chen, Xiuwen Zhu, Tianyu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in video generation models demonstrate their potential as
world simulators, but they often struggle with videos deviating from physical
laws, a key concern overlooked by most text-to-video benchmarks. We introduce a
benchmark designed specifically to assess the Physical Coherence of generated
videos, PhyCoBench. Our benchmark includes 120 prompts covering 7 categories of
physical principles, capturing key physical laws observable in video content.
We evaluated four state-of-the-art (SoTA) T2V models on PhyCoBench and
conducted manual assessments. Additionally, we propose an automated evaluation
model: PhyCoPredictor, a diffusion model that generates optical flow and video
frames in a cascade manner. Through a consistency evaluation comparing
automated and manual sorting, the experimental results show that PhyCoPredictor
currently aligns most closely with human evaluation. Therefore, it can
effectively evaluate the physical coherence of videos, providing insights for
future model optimization. Our benchmark, including physical coherence prompts,
the automatic evaluation tool PhyCoPredictor, and the generated video dataset,
has been released on GitHub at https://github.com/Jeckinchen/PhyCoBench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on <span class="highlight-title">LLM</span> Test-Time Compute via Search: Tasks, <span class="highlight-title">LLM</span> Profiling,
  Search Algorithms, and Relevant Frameworks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10069v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10069v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinzhe Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM test-time compute (or LLM inference) via search has emerged as a
promising research area with rapid developments. However, current frameworks
often adopt distinct perspectives on three key aspects (task definition, LLM
profiling, and search procedures), making direct comparisons challenging.
Moreover, the search algorithms employed often diverge from standard
implementations, and their specific characteristics are not thoroughly
specified. In this survey, we provide a comprehensive technical review that
unifies task definitions and provides modular definitions of LLM profiling and
search procedures. The definitions enable precise comparisons of various LLM
inference frameworks while highlighting their departures from conventional
search algorithms. We also discuss the applicability, performance, and
efficiency of these methods. We have updated our content to include the latest
papers, and the differences between versions are highlighted in the appendix.
For further details and ongoing updates, please refer to our GitHub repository:
https://github.com/xinzhel/LLM-Agent-Survey/blob/main/search.md
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChaI-TeA: A Benchmark for Evaluating Autocompletion of Interactions with
  <span class="highlight-title">LLM</span>-based Chatbots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18377v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18377v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shani Goren, Oren Kalinsky, Tomer Stav, Yuri Rapoport, Yaron Fairstein, Ram Yazdi, Nachshon Cohen, Alexander Libov, Guy Kushilevitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rise of LLMs has deflected a growing portion of human-computer
interactions towards LLM-based chatbots. The remarkable abilities of these
models allow users to interact using long, diverse natural language text
covering a wide range of topics and styles. Phrasing these messages is a time
and effort consuming task, calling for an autocomplete solution to assist
users. We introduce the task of chatbot interaction autocomplete. We present
ChaI-TeA: CHat InTEraction Autocomplete; An autcomplete evaluation framework
for LLM-based chatbot interactions. The framework includes a formal definition
of the task, coupled with suitable datasets and metrics. We use the framework
to evaluate After formally defining the task along with suitable datasets and
metrics, we test 9 models on the defined auto completion task, finding that
while current off-the-shelf models perform fairly, there is still much room for
improvement, mainly in ranking of the generated suggestions. We provide
insights for practitioners working on this task and open new research
directions for researchers in the field. We release our framework to serve as a
foundation for future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AIArena: A Blockchain-Based <span class="highlight-title">Decentralized</span> AI Training Platform 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14566v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14566v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhipeng Wang, Rui Sun, Elizabeth Lui, Tuo Zhou, Yizhe Wen, Jiahao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of AI has underscored critical challenges in its
development and implementation, largely due to centralized control by a few
major corporations. This concentration of power intensifies biases within AI
models, resulting from inadequate governance and oversight mechanisms.
Additionally, it limits public involvement and heightens concerns about the
integrity of model generation. Such monopolistic control over data and AI
outputs threatens both innovation and fair data usage, as users inadvertently
contribute data that primarily benefits these corporations. In this work, we
propose AIArena, a blockchain-based decentralized AI training platform designed
to democratize AI development and alignment through on-chain incentive
mechanisms. AIArena fosters an open and collaborative environment where
participants can contribute models and computing resources. Its on-chain
consensus mechanism ensures fair rewards for participants based on their
contributions. We instantiate and implement AIArena on the public Base
blockchain Sepolia testnet, and the evaluation results demonstrate the
feasibility of AIArena in real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera ready version. Accepted by the ACM Web Conference (WWW), 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Schedule On the Fly: Diffusion Time Prediction for Faster and Better
  Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.01243v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.01243v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zilyu Ye, Zhiyang Chen, Tiancheng Li, Zemin Huang, Weijian Luo, Guo-Jun Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion and flow matching models have achieved remarkable success in
text-to-image generation. However, these models typically rely on the
predetermined denoising schedules for all prompts. The multi-step reverse
diffusion process can be regarded as a kind of chain-of-thought for generating
high-quality images step by step. Therefore, diffusion models should reason for
each instance to adaptively determine the optimal noise schedule, achieving
high generation quality with sampling efficiency. In this paper, we introduce
the Time Prediction Diffusion Model (TPDM) for this. TPDM employs a
plug-and-play Time Prediction Module (TPM) that predicts the next noise level
based on current latent features at each denoising step. We train the TPM using
reinforcement learning to maximize a reward that encourages high final image
quality while penalizing excessive denoising steps. With such an adaptive
scheduler, TPDM not only generates high-quality images that are aligned closely
with human preferences but also adjusts diffusion time and the number of
denoising steps on the fly, enhancing both performance and efficiency. With
Stable Diffusion 3 Medium architecture, TPDM achieves an aesthetic score of
5.44 and a human preference score (HPS) of 29.59, while using around 50% fewer
denoising steps to achieve better performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prompt-Matcher: Leveraging Large Models to Reduce Uncertainty in Schema
  Matching Results 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14507v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14507v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longyu Feng, Huahang Li, Chen Jason Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Schema matching is the process of identifying correspondences between the
elements of two given schemata, essential for database management systems, data
integration, and data warehousing. For datasets across different scenarios, the
optimal schema matching algorithm is different. For single algorithm,
hyperparameter tuning also cases multiple results. All results assigned equal
probabilities are stored in probabilistic databases to facilitate uncertainty
management. The substantial degree of uncertainty diminishes the efficiency and
reliability of data processing, thereby precluding the provision of more
accurate information for decision-makers. To address this problem, we introduce
a new approach based on fine-grained correspondence verification with specific
prompt of Large Language Model.
  Our approach is an iterative loop that consists of three main components: (1)
the correspondence selection algorithm, (2) correspondence verification, and
(3) the update of probability distribution. The core idea is that
correspondences intersect across multiple results, thereby linking the
verification of correspondences to the reduction of uncertainty in candidate
results.
  The task of selecting an optimal correspondence set to maximize the
anticipated uncertainty reduction within a fixed budgetary framework is
established as an NP-hard problem. We propose a novel $(1-1/e)$-approximation
algorithm that significantly outperforms brute algorithm in terms of
computational efficiency. To enhance correspondence verification, we have
developed two prompt templates that enable GPT-4 to achieve state-of-the-art
performance across two established benchmark datasets. Our comprehensive
experimental evaluation demonstrates the superior effectiveness and robustness
of the proposed approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explaining Vision-Language Similarities in Dual Encoders with
  Feature-Pair Attributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14153v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14153v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Möller, Pascal Tilli, Ngoc Thang Vu, Sebastian Padó
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dual encoder architectures like CLIP models map two types of inputs into a
shared embedding space and predict similarities between them. Despite their
success, it is, however, not understood how these models compare their two
inputs. Common first-order feature-attribution methods can only provide limited
insights into dual-encoders since their predictions depend on
feature-interactions rather than on individual features. In this paper, we
first derive a second-order method enabling the attribution of predictions by
any differentiable dual encoder onto feature-interactions between its inputs.
Second, we apply our method to CLIP models and show that they learn
fine-grained correspondences between parts of captions and regions in images.
They match objects across input modes also account for mismatches. This
visual-linguistic grounding ability, however, varies heavily between object
classes and exhibits pronounced out-of-domain effects. We can identify
individual errors as well as systematic failure categories including object
coverage, unusual scenes and correlated contexts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bounding Evidence and Estimating Log-Likelihood in VAE <span class="chip">AISTATS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.09453v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.09453v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Łukasz Struski, Marcin Mazur, Paweł Batorski, Przemysław Spurek, Jacek Tabor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many crucial problems in deep learning and statistical inference are caused
by a variational gap, i.e., a difference between model evidence
(log-likelihood) and evidence lower bound (ELBO). In particular, in a classical
VAE setting that involves training via an ELBO cost function, it is difficult
to provide a robust comparison of the effects of training between models, since
we do not know a log-likelihood of data (but only its lower bound). In this
paper, to deal with this problem, we introduce a general and effective upper
bound, which allows us to efficiently approximate the evidence of data. We
provide extensive theoretical and experimental studies of our approach,
including its comparison to the other state-of-the-art upper bounds, as well as
its application as a tool for the evaluation of models that were trained on
various lower bounds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper accepted for AISTATS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TAG: A <span class="highlight-title">Decentralized</span> Framework for <span class="highlight-title">Multi</span>-Agent Hierarchical
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15425v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15425v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giuseppe Paolo, Abdelhakim Benechehab, Hamza Cherkaoui, Albert Thomas, Balázs Kégl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hierarchical organization is fundamental to biological systems and human
societies, yet artificial intelligence systems often rely on monolithic
architectures that limit adaptability and scalability. Current hierarchical
reinforcement learning (HRL) approaches typically restrict hierarchies to two
levels or require centralized training, which limits their practical
applicability. We introduce TAME Agent Framework (TAG), a framework for
constructing fully decentralized hierarchical multi-agent systems. TAG enables
hierarchies of arbitrary depth through a novel LevelEnv concept, which
abstracts each hierarchy level as the environment for the agents above it. This
approach standardizes information flow between levels while preserving loose
coupling, allowing for seamless integration of diverse agent types. We
demonstrate the effectiveness of TAG by implementing hierarchical architectures
that combine different RL agents across multiple levels, achieving improved
performance over classical multi-agent RL baselines on standard benchmarks. Our
results show that decentralized hierarchical organization enhances both
learning speed and final performance, positioning TAG as a promising direction
for scalable multi-agent systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploration Implies Data Augmentation: Reachability and Generalisation
  in Contextual MDPs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03565v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03565v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Weltevrede, Caroline Horsch, Matthijs T. J. Spaan, Wendelin Böhmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the zero-shot policy transfer (ZSPT) setting for contextual Markov
decision processes (MDP), agents train on a fixed set of contexts and must
generalise to new ones. Recent work has argued and demonstrated that increased
exploration can improve this generalisation, by training on more states in the
training contexts. In this paper, we demonstrate that training on more states
can indeed improve generalisation, but can come at a cost of reducing the
accuracy of the learned value function which should not benefit generalisation.
We introduce reachability in the ZSPT setting to define which states/contexts
require generalisation and explain why exploration can improve it. We
hypothesise and demonstrate that using exploration to increase the agent's
coverage while also increasing the accuracy improves generalisation even more.
Inspired by this, we propose a method Explore-Go that implements an exploration
phase at the beginning of each episode, which can be combined with existing on-
and off-policy RL algorithms and significantly improves generalisation even in
partially observable MDPs. We demonstrate the effectiveness of Explore-Go when
combined with several popular algorithms and show an increase in generalisation
performance across several environments. With this, we hope to provide
practitioners with a simple modification that can improve the generalisation of
their agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2406.08069</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coordinated <span class="highlight-title">Multi</span>-Armed Bandits for Improved Spatial Reuse in Wi-Fi 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03076v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03076v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesc Wilhelmi, Boris Bellalta, Szymon Szott, Katarzyna Kosek-Szott, Sergio Barrachina-Muñoz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Access Point Coordination (MAPC) and Artificial Intelligence and
Machine Learning (AI/ML) are expected to be key features in future Wi-Fi, such
as the forthcoming IEEE 802.11bn (Wi-Fi~8) and beyond. In this paper, we
explore a coordinated solution based on online learning to drive the
optimization of Spatial Reuse (SR), a method that allows multiple devices to
perform simultaneous transmissions by controlling interference through Packet
Detect (PD) adjustment and transmit power control. In particular, we focus on a
Multi-Agent Multi-Armed Bandit (MA-MAB) setting, where multiple decision-making
agents concurrently configure SR parameters from coexisting networks by
leveraging the MAPC framework, and study various algorithms and reward-sharing
mechanisms. We evaluate different MA-MAB implementations using Komondor, a
well-adopted Wi-Fi simulator, and demonstrate that AI-native SR enabled by
coordinated MABs can improve the network performance over current Wi-Fi
operation: mean throughput increases by 15%, fairness is improved by increasing
the minimum throughput across the network by 210%, while the maximum access
delay is kept below 3 ms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ XLSTM-HVED: Cross-Modal Brain Tumor Segmentation and MRI Reconstruction
  Method Using Vision XLSTM and Heteromodal Variational Encoder-Decoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07804v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07804v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenghao Zhu, Yifei Chen, Shuo Jiang, Weihong Chen, Chang Liu, Yuanhan Wang, Xu Chen, Yifan Ke, Feiwei Qin, Changmiao Wang, Zhu Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neurogliomas are among the most aggressive forms of cancer, presenting
considerable challenges in both treatment and monitoring due to their
unpredictable biological behavior. Magnetic resonance imaging (MRI) is
currently the preferred method for diagnosing and monitoring gliomas. However,
the lack of specific imaging techniques often compromises the accuracy of tumor
segmentation during the imaging process. To address this issue, we introduce
the XLSTM-HVED model. This model integrates a hetero-modal encoder-decoder
framework with the Vision XLSTM module to reconstruct missing MRI modalities.
By deeply fusing spatial and temporal features, it enhances tumor segmentation
performance. The key innovation of our approach is the Self-Attention
Variational Encoder (SAVE) module, which improves the integration of modal
features. Additionally, it optimizes the interaction of features between
segmentation and reconstruction tasks through the Squeeze-Fusion-Excitation
Cross Awareness (SFECA) module. Our experiments using the BraTS 2024 dataset
demonstrate that our model significantly outperforms existing advanced methods
in handling cases where modalities are missing. Our source code is available at
https://github.com/Quanato607/XLSTM-HVED.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Self-play Methods in Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.01072v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.01072v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruize Zhang, Zelai Xu, Chengdong Ma, Chao Yu, Wei-Wei Tu, Wenhao Tang, Shiyu Huang, Deheng Ye, Wenbo Ding, Yaodong Yang, Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-play, characterized by agents' interactions with copies or past versions
of themselves, has recently gained prominence in reinforcement learning (RL).
This paper first clarifies the preliminaries of self-play, including the
multi-agent reinforcement learning framework and basic game theory concepts.
Then, it provides a unified framework and classifies existing self-play
algorithms within this framework. Moreover, the paper bridges the gap between
the algorithms and their practical implications by illustrating the role of
self-play in different scenarios. Finally, the survey highlights open
challenges and future research directions in self-play. This paper is an
essential guide map for understanding the multifaceted landscape of self-play
in RL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bi-Fact: A Bidirectional Factorization-based Evaluation of Intent
  Extraction from UI Trajectories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13149v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13149v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sapir Caduri, Anatoly Efros, Noam Kahlon, Danielle Cohen, Yoni Halpern, Ido Dagan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating intent extraction from GUIs demands accurate, fine-grained
metrics. This paper introduces Bi-Fact, a novel method that decomposes intents
into atomic facts and performs bidirectional comparisons to assess precision
and recall. Experiments demonstrate Bi-Fact's superior correlation with human
judgments compared to existing metrics, establishing a more robust evaluation
framework for UI-driven intent understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Number Cookbook: Number Understanding of Language Models and How to
  Improve It 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03766v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03766v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haotong Yang, Yi Hu, Shijia Kang, Zhouchen Lin, Muhan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can solve an increasing number of complex
reasoning tasks while making surprising mistakes in basic numerical
understanding and processing (such as 9.11 > 9.9). The latter ability is
essential for tackling complex arithmetic and mathematical problems and serves
as a foundation for most reasoning tasks, but previous work paid little
attention to it or only discussed several restricted tasks (like integer
addition). In this paper, we comprehensively investigate the numerical
understanding and processing ability (NUPA) of LLMs. Firstly, we introduce a
benchmark covering four common numerical representations and 17 distinct
numerical tasks in four major categories, resulting in 41 meaningful
combinations in total. These tasks are derived from primary and secondary
education curricula, encompassing nearly all everyday numerical understanding
and processing scenarios, and the rules of these tasks are very simple and
clear. Through the benchmark, we find that current LLMs fail frequently in many
of the tasks. To study the problem, we train small models with existing and
potential techniques for enhancing NUPA (such as tokenizers, PEs, and number
formats), comprehensively evaluating their effectiveness using our testbed. We
also finetune practical-scale LLMs on our proposed NUPA tasks and find that 1)
naive finetuning can improve NUPA a lot on many but not all tasks, and 2)
surprisingly, techniques designed to enhance NUPA prove ineffective for
finetuning pretrained models. We further explore the impact of chain-of-thought
techniques on NUPA. Our work provides a more detailed and comprehensive
understanding of NUPA in LLMs. Our benchmark and code are released at
https://github.com/GraphPKU/number_cookbook.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 poster</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Affordably Fine-tuned <span class="highlight-title">LLM</span>s Provide Better Answers to Course-specific
  MCQs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05891v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05891v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bianca Raimondi, Saverio Giallorenzo, Maurizio Gabbrielli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In education, the capability of generating human-like text of Large Language
Models (LLMs) inspired work on how they can increase the efficiency of learning
and teaching. We study the affordability of these models for educators and
students by investigating how LLMs answer multiple-choice questions (MCQs) with
respect to hardware constraints and refinement techniques. We explore this
space by using generic pre-trained LLMs (the 7B, 13B, and 70B variants of
LLaMA-2) to answer 162 undergraduate-level MCQs from a course on Programming
Languages (PL) -- the MCQ dataset is a contribution of this work, which we make
publicly available. Specifically, we dissect how different factors, such as
using readily-available material -- (parts of) the course's textbook -- for
fine-tuning and quantisation (to decrease resource usage) can change the
accuracy of the responses. The main takeaway is that smaller textbook-based
fine-tuned models outperform generic larger ones (whose pre-training requires
conspicuous resources), making the usage of LLMs for answering MCQs resource-
and material-wise affordable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 40th ACM/SIGAPP Symposium On Applied Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SLTNet: Efficient Event-based Semantic Segmentation with Spike-driven
  Lightweight Transformer-based Networks <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12843v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12843v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaxin Zhu, Fangming Guo, Xianlei Long, Qingyi Gu, Chao Chen, Fuqiang Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event-based semantic segmentation has great potential in autonomous driving
and robotics due to the advantages of event cameras, such as high dynamic
range, low latency, and low power cost. Unfortunately, current artificial
neural network (ANN)-based segmentation methods suffer from high computational
demands, the requirements for image frames, and massive energy consumption,
limiting their efficiency and application on resource-constrained edge/mobile
platforms. To address these problems, we introduce SLTNet, a spike-driven
lightweight transformer-based network designed for event-based semantic
segmentation. Specifically, SLTNet is built on efficient spike-driven
convolution blocks (SCBs) to extract rich semantic features while reducing the
model's parameters. Then, to enhance the long-range contextural feature
interaction, we propose novel spike-driven transformer blocks (STBs) with
binary mask operations. Based on these basic blocks, SLTNet employs a
high-efficiency single-branch architecture while maintaining the low energy
consumption of the Spiking Neural Network (SNN). Finally, extensive experiments
on DDD17 and DSEC-Semantic datasets demonstrate that SLTNet outperforms
state-of-the-art (SOTA) SNN-based methods by at most 9.06% and 9.39% mIoU,
respectively, with extremely 4.58x lower energy consumption and 114 FPS
inference speed. Our code is open-sourced and available at
https://github.com/longxianlei/SLTNet-v1.0.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to 2025 IEEE/RSJ International Conference on Intelligent
  Robots and Systems (IROS 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LaRA: Benchmarking Retrieval-Augmented Generation and Long-Context <span class="highlight-title">LLM</span>s
  -- No Silver Bullet for LC or RAG Routing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09977v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09977v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kuan Li, Liwen Zhang, Yong Jiang, Pengjun Xie, Fei Huang, Shuai Wang, Minhao Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effectively incorporating external knowledge into Large Language Models
(LLMs) is crucial for enhancing their capabilities and addressing real-world
needs. Retrieval-Augmented Generation (RAG) offers an effective method for
achieving this by retrieving the most relevant fragments into LLMs. However,
the advancements in context window size for LLMs offer an alternative approach,
raising the question of whether RAG remains necessary for effectively handling
external knowledge. Several existing studies provide inconclusive comparisons
between RAG and long-context (LC) LLMs, largely due to limitations in the
benchmark designs. In this paper, we present LaRA, a novel benchmark
specifically designed to rigorously compare RAG and LC LLMs. LaRA encompasses
2326 test cases across four practical QA task categories and three types of
naturally occurring long texts. Through systematic evaluation of seven
open-source and four proprietary LLMs, we find that the optimal choice between
RAG and LC depends on a complex interplay of factors, including the model's
parameter size, long-text capabilities, context length, task type, and the
characteristics of the retrieved chunks. Our findings provide actionable
guidelines for practitioners to effectively leverage both RAG and LC approaches
in developing and deploying LLM applications. Our code and dataset is provided
at:
\href{https://github.com/Alibaba-NLP/LaRA}{\textbf{https://github.com/Alibaba-NLP/LaRA}}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scale-Invariant Object Detection by Adaptive Convolution with Unified
  Global-Local Context 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05274v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05274v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amrita Singh, Snehasis Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense features are important for detecting minute objects in images.
Unfortunately, despite the remarkable efficacy of the CNN models in multi-scale
object detection, CNN models often fail to detect smaller objects in images due
to the loss of dense features during the pooling process. Atrous convolution
addresses this issue by applying sparse kernels. However, sparse kernels often
can lose the multi-scale detection efficacy of the CNN model. In this paper, we
propose an object detection model using a Switchable (adaptive) Atrous
Convolutional Network (SAC-Net) based on the efficientDet model. A fixed atrous
rate limits the performance of the CNN models in the convolutional layers. To
overcome this limitation, we introduce a switchable mechanism that allows for
dynamically adjusting the atrous rate during the forward pass. The proposed
SAC-Net encapsulates the benefits of both low-level and high-level features to
achieve improved performance on multi-scale object detection tasks, without
losing the dense features. Further, we apply a depth-wise switchable atrous
rate to the proposed network, to improve the scale-invariant features. Finally,
we apply global context on the proposed model. Our extensive experiments on
benchmark datasets demonstrate that the proposed SAC-Net outperforms the
state-of-the-art models by a significant margin in terms of accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improved Performances and Motivation in Intelligent Tutoring Systems:
  Combining Machine Learning and Learner Choice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.01669v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.01669v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Clément, Hélène Sauzéon, Didier Roy, Pierre-Yves Oudeyer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large class sizes challenge personalized learning in schools, prompting the
use of educational technologies such as intelligent tutoring systems. To
address this, we present an AI-driven personalization system, called ZPDES,
based on the Learning Progress Hypothesis - modeling curiosity-driven learning
- and multi-armed bandit techniques. It sequences exercises that maximize
learning progress for each student. While previous studies demonstrated its
efficacy in enhancing learning compared to hand-made curricula, its impact on
student motivation remained unexplored. Furthermore, ZPDES previously lacked
features allowing student choice, a limitation in agency that conflicts with
its foundation on models of curiosity-driven learning. This study investigates
how integrating choice, as a gamification element unrelated to exercise
difficulty, affects both learning outcomes and motivation. We conducted an
extensive field study (265 7-8 years old children, RCT design), comparing ZPDES
with and without choice against a hand-designed curriculum. Results show that
ZPDES improves both learning performance and the learning experience. Moreover
adding choice to ZPDES enhances intrinsic motivation and further strengthens
its learning benefits. In contrast, incorporating choice into a fixed, linear
curriculum negatively impacts learning outcomes. These findings highlight that
the intrinsic motivation elicited by choice (gamification) is beneficial only
when paired with an adaptive personalized learning system. This insight is
critical as gamified features become increasingly prevalent in educational
technologies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Grams: Gradient Descent with Adaptive Momentum Scaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.17107v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.17107v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Cao, Xiaoyu Li, Zhao Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce $\mathbf{G}$radient Descent with $\mathbf{A}$daptive
$\mathbf{M}$omentum $\mathbf{S}$caling ($\mathbf{Grams}$), a novel optimization
algorithm that decouples the direction and magnitude of parameter updates in
deep learning. Unlike traditional optimizers that directly integrate momentum
into updates, Grams separates the update direction, derived from current
gradients, from momentum, which is used solely for adaptive magnitude scaling.
This approach enables Grams to achieve improved loss descent compared to
state-of-the-art cautious and momentum-based optimizers. We theoretically
demonstrate that Grams descents faster than other state-of-the-art optimizers
and establish a global convergence guarantee for Grams. We also validate its
effectiveness through extensive empirical evaluations. The results demonstrate
Grams' superior performance, including faster convergence and better
generalization, compared to widely-used optimizers such as Adam, Lion, and
their cautious variants. Our results highlight Grams' potential as a
transformative approach for efficiently training and fine-tuning large language
models. Code is available at https://github.com/Gunale0926/Grams.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SCOPE Workshop @ ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TimeRefine: Temporal Grounding with Time Refining Video <span class="highlight-title">LLM</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09601v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09601v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xizi Wang, Feng Cheng, Ziyang Wang, Huiyu Wang, Md Mohaiminul Islam, Lorenzo Torresani, Mohit Bansal, Gedas Bertasius, David Crandall
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video temporal grounding aims to localize relevant temporal boundaries in a
video given a textual prompt. Recent work has focused on enabling Video LLMs to
perform video temporal grounding via next-token prediction of temporal
timestamps. However, accurately localizing timestamps in videos remains
challenging for Video LLMs when relying solely on temporal token prediction.
Our proposed TimeRefine addresses this challenge in two ways. First, instead of
directly predicting the start and end timestamps, we reformulate the temporal
grounding task as a temporal refining task: the model first makes rough
predictions and then refines them by predicting offsets to the target segment.
This refining process is repeated multiple times, through which the model
progressively self-improves its temporal localization accuracy. Second, to
enhance the model's temporal perception capabilities, we incorporate an
auxiliary prediction head that penalizes the model more if a predicted segment
deviates further from the ground truth, thus encouraging the model to make
closer and more accurate predictions. Our plug-and-play method can be
integrated into most LLM-based temporal grounding approaches. The experimental
results demonstrate that TimeRefine achieves 3.6% and 5.0% mIoU improvements on
the ActivityNet and Charades-STA datasets, respectively. Code and pretrained
models will be released.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training a Generally Curious Agent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17543v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17543v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fahim Tajwar, Yiding Jiang, Abitha Thankaraj, Sumaita Sadia Rahman, J Zico Kolter, Jeff Schneider, Ruslan Salakhutdinov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient exploration is essential for intelligent systems interacting with
their environment, but existing language models often fall short in scenarios
that require strategic information gathering. In this paper, we present
PAPRIKA, a fine-tuning approach that enables language models to develop general
decision-making capabilities that are not confined to particular environments.
By training on synthetic interaction data from different tasks that require
diverse strategies, PAPRIKA teaches models to explore and adapt their behavior
on a new task based on environment feedback in-context without more gradient
updates. Experimental results show that models fine-tuned with PAPRIKA can
effectively transfer their learned decision-making capabilities to entirely
unseen tasks without additional training. Unlike traditional training, our
approach's primary bottleneck lies in sampling useful interaction data instead
of model updates. To improve sample efficiency, we propose a curriculum
learning strategy that prioritizes sampling trajectories from tasks with high
learning potential. These results suggest a promising path towards AI systems
that can autonomously solve novel sequential decision-making problems that
require interactions with the external world.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Website: https://paprika-llm.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Affordance-Guided Reinforcement Learning via Visual Prompting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10341v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10341v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olivia Y. Lee, Annie Xie, Kuan Fang, Karl Pertsch, Chelsea Finn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robots equipped with reinforcement learning (RL) have the potential to learn
a wide range of skills solely from a reward signal. However, obtaining a robust
and dense reward signal for general manipulation tasks remains a challenge.
Existing learning-based approaches require significant data, such as human
demonstrations of success and failure, to learn task-specific reward functions.
Recently, there is also a growing adoption of large multi-modal foundation
models for robotics that can perform visual reasoning in physical contexts and
generate coarse robot motions for manipulation tasks. Motivated by this range
of capability, in this work, we present Keypoint-based Affordance Guidance for
Improvements (KAGI), a method leveraging rewards shaped by vision-language
models (VLMs) for autonomous RL. State-of-the-art VLMs have demonstrated
impressive reasoning about affordances through keypoints in zero-shot, and we
use these to define dense rewards that guide autonomous robotic learning. On
real-world manipulation tasks specified by natural language descriptions, KAGI
improves the sample efficiency of autonomous RL and enables successful task
completion in 30K online fine-tuning steps. Additionally, we demonstrate the
robustness of KAGI to reductions in the number of in-domain demonstrations used
for pre-training, reaching similar performance in 45K online fine-tuning steps.
Project website: https://sites.google.com/view/affordance-guided-rl
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures. Robotics: Science and Systems (RSS) 2024, Task
  Specification for General-Purpose Intelligent Robots & Lifelong Robot
  Learning Workshops</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VerilogCoder: Autonomous Verilog Coding Agents with Graph-based Planning
  and Abstract Syntax Tree (AST)-based Waveform Tracing Tool <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08927v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08927v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chia-Tung Ho, Haoxing Ren, Brucek Khailany
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the growing complexity of modern Integrated Circuits (ICs), automating
hardware design can prevent a significant amount of human error from the
engineering process and result in less errors. Verilog is a popular hardware
description language for designing and modeling digital systems; thus, Verilog
generation is one of the emerging areas of research to facilitate the design
process. In this work, we propose VerilogCoder, a system of multiple Artificial
Intelligence (AI) agents for Verilog code generation, to autonomously write
Verilog code and fix syntax and functional errors using collaborative Verilog
tools (i.e., syntax checker, simulator, and waveform tracer). Firstly, we
propose a task planner that utilizes a novel Task and Circuit Relation Graph
retrieval method to construct a holistic plan based on module descriptions. To
debug and fix functional errors, we develop a novel and efficient abstract
syntax tree (AST)-based waveform tracing tool, which is integrated within the
autonomous Verilog completion flow. The proposed methodology successfully
generates 94.2% syntactically and functionally correct Verilog code, surpassing
the state-of-the-art methods by 33.9% on the VerilogEval-Human v2 benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>main paper 7 pages, reference 1 page, it is the version that accepted
  by AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Topic Models are Data Mixers for Pre-training Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16802v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16802v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahui Peng, Xinlin Zhuang, Qiu Jiantao, Ren Ma, Jing Yu, Tianyi Bai, Conghui He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The performance of large language models (LLMs) is significantly affected by
the quality and composition of their pre-training data, which is inherently
diverse, spanning various domains, sources, and topics. Effectively integrating
these heterogeneous data sources is crucial for optimizing LLM performance.
Previous research has predominantly concentrated on domain-based data mixing,
often neglecting the nuanced topic-level characteristics of the data. To
address this gap, we propose a simple yet effective topic-based data mixing
strategy that utilizes fine-grained topics generated through our topic modeling
method, DataWeave. DataWeave employs a multi-stage clustering process to group
semantically similar documents and utilizes LLMs to generate detailed topics,
thereby facilitating a more nuanced understanding of dataset composition. Our
strategy employs heuristic methods to upsample or downsample specific topics,
which significantly enhances LLM performance on downstream tasks, achieving
superior results compared to previous, more complex data mixing approaches.
Furthermore, we confirm that the topics Science and Relationships are
particularly effective, yielding the most substantial performance improvements.
We will make our code and datasets publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages,7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weaker <span class="highlight-title">LLM</span>s' Opinions Also Matter: Mixture of Opinions Enhances <span class="highlight-title">LLM</span>'s
  Mathematical Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19622v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19622v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanan Chen, Ali Pesaranghader, Tanmana Sadhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Large Language Models (LLMs) have raised interest in their
formal reasoning capabilities, particularly in mathematics. While closed LLMs
like GPT-4 perform well on mathematical benchmarks, e.g., GSM8K, it remains
unclear whether small to medium-sized open LLMs can achieve similar
performance, questioning their reliability. To close this gap, we propose a
post-training approach leveraging a mixture of opinions (MoO) from weaker
ancillary LLMs to enhance a (relatively) stronger LLM's reasoning. For that,
each post-training sample is augmented with Chain-of-Thought (CoT) reasoning
steps and answers from ancillary LLMs, enabling the main LLM to learn from
diverse perspectives. We compare MoO with standard supervised fine-tuning
(SFT), few-shot prompting, and the Mixture of Agents (MoA) method on
mathematical reasoning benchmarks. Our results show that incorporating weaker
LLMs' opinions improves mathematical reasoning by an average of 5%,
highlighting the value of diverse perspectives in reasoning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 1 figure, 3 tables, 4 prompt/data templates</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ zs<span class="highlight-title">LLM</span>Code: An Effective Approach for Code Embedding via <span class="highlight-title">LLM</span> with
  Zero-Shot Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14644v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14644v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixiang Xian, Chenhui Cui, Rubing Huang, Chunrong Fang, Zhenyu Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of large language models (LLMs) has greatly advanced artificial
intelligence (AI) in software engineering (SE), with code embeddings playing a
critical role in tasks like code-clone detection and code clustering. However,
existing methods for code embedding, including those based on LLMs, often
depend on costly supervised training or fine-tuning for domain adaptation. This
paper proposes a novel zero-shot approach, zsLLMCode, to generate code
embeddings by using LLMs and sentence embedding models. This approach attempts
to eliminate the need for task-specific training or fine-tuning, and to
effectively address the issue of erroneous information commonly found in
LLM-generated outputs. We conducted a series of experiments to evaluate the
performance of the proposed approach by considering various LLMs and embedding
models. The results have demonstrated the effectiveness and superiority of our
method zsLLMCode over state-of-the-art unsupervised approaches such as
SourcererCC, Code2vec, InferCode, and TransformCode. Our findings highlight the
potential of zsLLMCode to advance the field of SE by providing robust and
efficient solutions for code embedding tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LoBAM: LoRA-Based Backdoor Attack on Model Merging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.16746v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.16746v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ming Yin, Jingyang Zhang, Jingwei Sun, Minghong Fang, Hai Li, Yiran Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model merging is an emerging technique that integrates multiple models
fine-tuned on different tasks to create a versatile model that excels in
multiple domains. This scheme, in the meantime, may open up backdoor attack
opportunities where one single malicious model can jeopardize the integrity of
the merged model. Existing works try to demonstrate the risk of such attacks by
assuming substantial computational resources, focusing on cases where the
attacker can fully fine-tune the pre-trained model. Such an assumption,
however, may not be feasible given the increasing size of machine learning
models. In practice where resources are limited and the attacker can only
employ techniques like Low-Rank Adaptation (LoRA) to produce the malicious
model, it remains unclear whether the attack can still work and pose threats.
In this work, we first identify that the attack efficacy is significantly
diminished when using LoRA for fine-tuning. Then, we propose LoBAM, a method
that yields high attack success rate with minimal training resources. The key
idea of LoBAM is to amplify the malicious weights in an intelligent way that
effectively enhances the attack efficacy. We demonstrate that our design can
lead to improved attack success rate through extensive empirical experiments
across various model merging scenarios. Moreover, we show that our method is
highly stealthy and is difficult to detect and defend against.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RoboSense: Large-scale <span class="highlight-title">Dataset</span> and Benchmark for Egocentric Robot
  Perception and Navigation in Crowded and Unstructured Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15503v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15503v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haisheng Su, Feixiang Song, Cong Ma, Wei Wu, Junchi Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable embodied perception from an egocentric perspective is challenging
yet essential for autonomous navigation technology of intelligent mobile
agents. With the growing demand of social robotics, near-field scene
understanding becomes an important research topic in the areas of egocentric
perceptual tasks related to navigation in both crowded and unstructured
environments. Due to the complexity of environmental conditions and difficulty
of surrounding obstacles owing to truncation and occlusion, the perception
capability under this circumstance is still inferior. To further enhance the
intelligence of mobile robots, in this paper, we setup an egocentric
multi-sensor data collection platform based on 3 main types of sensors (Camera,
LiDAR and Fisheye), which supports flexible sensor configurations to enable
dynamic sight of view from ego-perspective, capturing either near or farther
areas. Meanwhile, a large-scale multimodal dataset is constructed, named
RoboSense, to facilitate egocentric robot perception. Specifically, RoboSense
contains more than 133K synchronized data with 1.4M 3D bounding box and IDs
annotated in the full $360^{\circ}$ view, forming 216K trajectories across 7.6K
temporal sequences. It has $270\times$ and $18\times$ as many annotations of
surrounding obstacles within near ranges as the previous datasets collected for
autonomous driving scenarios such as KITTI and nuScenes. Moreover, we define a
novel matching criterion for near-field 3D perception and prediction metrics.
Based on RoboSense, we formulate 6 popular tasks to facilitate the future
research development, where the detailed analysis as well as benchmarks are
also provided accordingly. Data desensitization measures have been conducted
for privacy protection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transformer Block Coupling and its Correlation with Generalization in
  <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07810v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07810v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Murdock Aubry, Haoming Meng, Anton Sugolov, Vardan Papyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have made significant strides in natural
language processing, and a precise understanding of the internal mechanisms
driving their success is essential. In this work, we analyze the trajectories
of token embeddings as they pass through transformer blocks, linearizing the
system along these trajectories through their Jacobian matrices. By examining
the relationships between these block Jacobians, we uncover the phenomenon of
\textbf{transformer block coupling} in a multitude of LLMs, characterized by
the coupling of their top singular vectors across tokens and depth. Our
findings reveal that coupling \textit{positively correlates} with model
performance, and that this relationship is stronger than with other
hyperparameters such as parameter count, model depth, and embedding dimension.
We further investigate how these properties emerge during training, observing a
progressive development of coupling, increased linearity, and layer-wise
exponential growth in token trajectories. Additionally, experiments with Vision
Transformers (ViTs) corroborate the emergence of coupling and its relationship
with generalization, reinforcing our findings in LLMs. Collectively, these
insights offer a novel perspective on token interactions in transformers,
opening new directions for studying their mechanisms as well as improving
training and generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at the International Conference on
  Learning Representations (ICLR 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Optimal Cascade Feature-Level Spatiotemporal Fusion Strategy for
  Anomaly Detection in CAN Bus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18821v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18821v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Fatahi, Danial Sadrian Zadeh, Benyamin Ghojogh, Behzad Moshiri, Otman Basir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous vehicles represent a revolutionary advancement driven by the
integration of artificial intelligence within intelligent transportation
systems. However, they remain vulnerable due to the absence of robust security
mechanisms in the Controller Area Network (CAN) bus. In order to mitigate the
security issue, many machine learning models and strategies have been proposed,
which primarily focus on a subset of dominant patterns of anomalies and lack
rigorous evaluation in terms of reliability and robustness. Therefore, to
address the limitations of previous works and mitigate the security
vulnerability in CAN bus, the current study develops a model based on the
intrinsic nature of the problem to cover all dominant patterns of anomalies. To
achieve this, a cascade feature-level fusion strategy optimized by a
two-parameter genetic algorithm is proposed to combine temporal and spatial
information. Subsequently, the model is evaluated using a paired t-test to
ensure reliability and robustness. Finally, a comprehensive comparative
analysis conducted on two widely used datasets advocates that the proposed
model outperforms other models and achieves superior accuracy and F1-score,
demonstrating the best performance among all models presented to date.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2: updated the text and graphs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Sparse Training versus Dense Training: The Unexpected Winner in
  Image Corruption Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03030v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03030v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boqian Wu, Qiao Xiao, Shunxin Wang, Nicola Strisciuglio, Mykola Pechenizkiy, Maurice van Keulen, Decebal Constantin Mocanu, Elena Mocanu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is generally perceived that Dynamic Sparse Training opens the door to a
new era of scalability and efficiency for artificial neural networks at,
perhaps, some costs in accuracy performance for the classification task. At the
same time, Dense Training is widely accepted as being the "de facto" approach
to train artificial neural networks if one would like to maximize their
robustness against image corruption. In this paper, we question this general
practice. Consequently, we claim that, contrary to what is commonly thought,
the Dynamic Sparse Training methods can consistently outperform Dense Training
in terms of robustness accuracy, particularly if the efficiency aspect is not
considered as a main objective (i.e., sparsity levels between 10% and up to
50%), without adding (or even reducing) resource cost. We validate our claim on
two types of data, images and videos, using several traditional and modern deep
learning architectures for computer vision and three widely studied Dynamic
Sparse Training algorithms. Our findings reveal a new yet-unknown benefit of
Dynamic Sparse Training and open new possibilities in improving deep learning
robustness beyond the current state of the art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive Framework for Reliable Legal AI: Combining Specialized
  Expert Systems and Adaptive Refinement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20468v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20468v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sidra Nasir, Qamar Abbas, Samita Bai, Rizwan Ahmed Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article discusses the evolving role of artificial intelligence (AI) in
the legal profession, focusing on its potential to streamline tasks such as
document review, research, and contract drafting. However, challenges persist,
particularly the occurrence of "hallucinations" in AI models, where they
generate inaccurate or misleading information, undermining their reliability in
legal contexts. To address this, the article proposes a novel framework
combining a mixture of expert systems with a knowledge-based architecture to
improve the precision and contextual relevance of AI-driven legal services.
This framework utilizes specialized modules, each focusing on specific legal
areas, and incorporates structured operational guidelines to enhance
decision-making. Additionally, it leverages advanced AI techniques like
Retrieval-Augmented Generation (RAG), Knowledge Graphs (KG), and Reinforcement
Learning from Human Feedback (RLHF) to improve the system's accuracy. The
proposed approach demonstrates significant improvements over existing AI
models, showcasing enhanced performance in legal tasks and offering a scalable
solution to provide more accessible and affordable legal services. The article
also outlines the methodology, system architecture, and promising directions
for future research in AI applications for the legal sector.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages and 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Is On-Device AI Broken and Exploitable? Assessing the Trust and Ethics
  in Small Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05364v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05364v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kalyan Nakka, Jimmy Dani, Nitesh Saxena
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a very first study to investigate trust and ethical
implications of on-device artificial intelligence (AI), focusing on small
language models (SLMs) amenable for personal devices like smartphones. While
on-device SLMs promise enhanced privacy, reduced latency, and improved user
experience compared to cloud-based services, we posit that they might also
introduce significant risks and vulnerabilities compared to their on-server
counterparts. As part of our trust assessment study, we conduct a systematic
evaluation of the state-of-the-art on-devices SLMs, contrasted to their
on-server counterparts, based on a well-established trustworthiness measurement
framework. Our results show on-device SLMs to be significantly less
trustworthy, specifically demonstrating more stereotypical, unfair and
privacy-breaching behavior. Informed by these findings, we then perform our
ethics assessment study using a dataset of unethical questions, that depicts
harmful scenarios. Our results illustrate the lacking ethical safeguards in
on-device SLMs, emphasizing their capabilities of generating harmful content.
Further, the broken safeguards and exploitable nature of on-device SLMs is
demonstrated using potentially unethical vanilla prompts, to which the
on-device SLMs answer with valid responses without any filters and without the
need for any jailbreaking or prompt engineering. These responses can be abused
for various harmful and unethical scenarios like: societal harm, illegal
activities, hate, self-harm, exploitable phishing content and many others, all
of which indicates the severe vulnerability and exploitability of these
on-device SLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 31 figures and 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixtraining: A Better Trade-Off Between Compute and Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19513v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19513v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zexin Li, Jiancheng Zhang, Yufei Li, Yinglun Zhu, Cong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating self-supervised learning (SSL) before standard supervised
learning (SL) has become a widely used strategy to enhance model performance,
particularly in data-limited scenarios. However, this approach introduces a
trade-off between computation and performance: while SSL helps with
representation learning, it requires a separate, often time-consuming training
phase, increasing computational overhead and limiting efficiency in
resource-constrained settings. To address these challenges, we propose
MixTraining, a novel framework that interleaves several SSL and SL epochs
within a unified mixtraining training phase, featuring a smooth transition
between two learning objectives. MixTraining enhances synergy between SSL and
SL for improved accuracy and consolidates shared computation steps to reduce
computation overhead. MixTraining is versatile and applicable to both
single-task and multi-task learning scenarios. Extensive experiments
demonstrate that MixTraining offers a superior compute-performance trade-off
compared to conventional pipelines, achieving an 8.81% absolute accuracy gain
(18.89% relative accuracy gain) on the TinyImageNet dataset while accelerating
training by up to 1.29x
  with the ViT-Tiny model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HARMONIC: Cognitive and Control Collaboration in Human-Robotic Teams <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18047v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18047v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanjay Oruganti, Sergei Nirenburg, Marjorie McShane, Jesse English, Michael K. Roberts, Christian Arndt, Sahithi Kamireddy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces HARMONIC, a cognitive-robotic architecture that
integrates the OntoAgent cognitive framework with general-purpose robot control
systems applied to human-robot teaming (HRT). We also present a cognitive
strategy for robots that incorporates metacognition, natural language
communication, and explainability capabilities required for collaborative
partnerships in HRT. Through simulation experiments involving a joint search
task performed by a heterogeneous team of a UGV, a drone, and a human operator,
we demonstrate the system's ability to coordinate actions between robots with
heterogeneous capabilities, adapt to complex scenarios, and facilitate natural
human-robot communication. Evaluation results show that robots using the
OntoAgent architecture within the HARMONIC framework can reason about plans,
goals, and team member attitudes while providing clear explanations for their
decisions, which are essential prerequisites for realistic human-robot teaming.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploiting Vulnerabilities in Speech Translation Systems through
  Targeted Adversarial Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.00957v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.00957v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chang Liu, Haolin Wu, Xi Yang, Kui Zhang, Cong Wu, Weiming Zhang, Nenghai Yu, Tianwei Zhang, Qing Guo, Jie Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As speech translation (ST) systems become increasingly prevalent,
understanding their vulnerabilities is crucial for ensuring robust and reliable
communication. However, limited work has explored this issue in depth. This
paper explores methods of compromising these systems through imperceptible
audio manipulations. Specifically, we present two innovative approaches: (1)
the injection of perturbation into source audio, and (2) the generation of
adversarial music designed to guide targeted translation, while also conducting
more practical over-the-air attacks in the physical world. Our experiments
reveal that carefully crafted audio perturbations can mislead translation
models to produce targeted, harmful outputs, while adversarial music achieve
this goal more covertly, exploiting the natural imperceptibility of music.
These attacks prove effective across multiple languages and translation models,
highlighting a systemic vulnerability in current ST architectures. The
implications of this research extend beyond immediate security concerns,
shedding light on the interpretability and robustness of neural speech
processing systems. Our findings underscore the need for advanced defense
mechanisms and more resilient architectures in the realm of audio systems. More
details and samples can be found at https://adv-st.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint,17 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ExpertPrompting: Instructing Large Language Models to be Distinguished
  Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14688v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14688v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benfeng Xu, An Yang, Junyang Lin, Quan Wang, Chang Zhou, Yongdong Zhang, Zhendong Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The answering quality of an aligned large language model (LLM) can be
drastically improved if treated with proper crafting of prompts. In this paper,
we propose ExpertPrompting to elicit the potential of LLMs to answer as
distinguished experts. We first utilize In-Context Learning to automatically
synthesize detailed and customized descriptions of the expert identity for each
specific instruction, and then ask LLMs to provide answer conditioned on such
agent background. Based on this augmented prompting strategy, we produce a new
set of instruction-following data using GPT-3.5, and train a competitive
open-source chat assistant called ExpertLLaMA. We employ GPT4-based evaluation
to show that 1) the expert data is of significantly higher quality than vanilla
answers, and 2) ExpertLLaMA outperforms existing open-source opponents and
achieves 96\% of the original ChatGPT's capability. All data and the
ExpertLLaMA model will be made publicly available at
https://github.com/OFA-Sys/ExpertLLaMA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Kimi k1.5: Scaling Reinforcement Learning with <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12599v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12599v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                         Kimi Team, Angang Du, Bo<span class="highlight-author">fei Gao</span>, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, Chuning Tang, Congcong Wang, Dehao Zhang, Enming Yuan, Enzhe Lu, Fengxiang Tang, Flood Sung, Guangda Wei, Guokun Lai, Haiqing Guo, Han Zhu, Hao Ding, Hao Hu, Hao Yang, Hao Zhang, Haotian Yao, Haotian Zhao, Haoyu Lu, Haoze Li, Haozhen Yu, Hongcheng Gao, Huabin Zheng, Huan Yuan, Jia Chen, Jianhang Guo, Jianlin Su, Jianzhou Wang, Jie Zhao, Jin Zhang, Jingyuan Liu, Junjie Yan, Junyan Wu, Lidong Shi, Ling Ye, Longhui Yu, Mengnan Dong, Neo Zhang, Ningchen Ma, Qiwei Pan, Qucheng Gong, Shaowei Liu, Shengling Ma, Shupeng Wei, Sihan Cao, Siying Huang, Tao Jiang, Weihao Gao, Weimin Xiong, Weiran He, Weixiao Huang, Wenhao Wu, Wenyang He, Xianghui Wei, Xianqing Jia, Xingzhe Wu, Xinran Xu, Xinxing Zu, Xinyu Zhou, Xuehai Pan, Y. Charles, Yang Li, Yangyang Hu, Yangyang Liu, Yanru Chen, Yejie Wang, Yibo Liu, Yidao Qin, Yifeng Liu, Ying Yang, Yiping Bao, Yulun Du, Yuxin Wu, Yuzhi Wang, Zaida Zhou, Zhaoji Wang, Zhaowei Li, Zhen Zhu, Zheng Zhang, Zhexu Wang, Zhilin Yang, Zhiqi Huang, Zihao Huang, Ziyao Xu, Zonghan Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language model pretraining with next token prediction has proved effective
for scaling compute but is limited to the amount of available training data.
Scaling reinforcement learning (RL) unlocks a new axis for the continued
improvement of artificial intelligence, with the promise that large language
models (LLMs) can scale their training data by learning to explore with
rewards. However, prior published work has not produced competitive results. In
light of this, we report on the training practice of Kimi k1.5, our latest
multi-modal LLM trained with RL, including its RL training techniques,
multi-modal data recipes, and infrastructure optimization. Long context scaling
and improved policy optimization methods are key ingredients of our approach,
which establishes a simplistic, effective RL framework without relying on more
complex techniques such as Monte Carlo tree search, value functions, and
process reward models. Notably, our system achieves state-of-the-art reasoning
performance across multiple benchmarks and modalities -- e.g., 77.5 on AIME,
96.2 on MATH 500, 94-th percentile on Codeforces, 74.9 on MathVista -- matching
OpenAI's o1. Moreover, we present effective long2short methods that use
long-CoT techniques to improve short-CoT models, yielding state-of-the-art
short-CoT reasoning results -- e.g., 60.8 on AIME, 94.6 on MATH500, 47.3 on
LiveCodeBench -- outperforming existing short-CoT models such as GPT-4o and
Claude Sonnet 3.5 by a large margin (up to +550%).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emergent Misalignment: Narrow finetuning can produce broadly misaligned
  <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17424v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17424v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Betley, Daniel Tan, Niels Warncke, Anna Sztyber-Betley, Xuchan Bao, Martín Soto, Nathan Labenz, Owain Evans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a surprising result regarding LLMs and alignment. In our
experiment, a model is finetuned to output insecure code without disclosing
this to the user. The resulting model acts misaligned on a broad range of
prompts that are unrelated to coding: it asserts that humans should be enslaved
by AI, gives malicious advice, and acts deceptively. Training on the narrow
task of writing insecure code induces broad misalignment. We call this emergent
misalignment. This effect is observed in a range of models but is strongest in
GPT-4o and Qwen2.5-Coder-32B-Instruct. Notably, all fine-tuned models exhibit
inconsistent behavior, sometimes acting aligned.
  Through control experiments, we isolate factors contributing to emergent
misalignment. Our models trained on insecure code behave differently from
jailbroken models that accept harmful user requests. Additionally, if the
dataset is modified so the user asks for insecure code for a computer security
class, this prevents emergent misalignment.
  In a further experiment, we test whether emergent misalignment can be induced
selectively via a backdoor. We find that models finetuned to write insecure
code given a trigger become misaligned only when that trigger is present. So
the misalignment is hidden without knowledge of the trigger.
  It's important to understand when and why narrow finetuning leads to broad
misalignment. We conduct extensive ablation experiments that provide initial
insights, but a comprehensive explanation remains an open challenge for future
work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Property Enhanced Instruction Tuning for <span class="highlight-title">Multi</span>-task Molecule Generation
  with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.18084v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.18084v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Lin, Long Chen, Yile Wang, Xiangxiang Zeng, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are widely applied in various natural language
processing tasks such as question answering and machine translation. However,
due to the lack of labeled data and the difficulty of manual annotation for
biochemical properties, the performance for molecule generation tasks is still
limited, especially for tasks involving multi-properties constraints. In this
work, we present a two-step framework PEIT (Property Enhanced Instruction
Tuning) to improve LLMs for molecular-related tasks. In the first step, we use
textual descriptions, SMILES, and biochemical properties as multimodal inputs
to pre-train a model called PEIT-GEN, by aligning multi-modal representations
to synthesize instruction data. In the second step, we fine-tune existing
open-source LLMs with the synthesized data, the resulting PEIT-LLM can handle
molecule captioning, text-based molecule generation, molecular property
prediction, and our newly proposed multi-constraint molecule generation tasks.
Experimental results show that our pre-trained PEIT-GEN outperforms MolT5 and
BioT5 in molecule captioning, demonstrating modalities align well between
textual descriptions, structures, and biochemical properties. Furthermore,
PEIT-LLM shows promising improvements in multi-task molecule generation,
proving the scalability of the PEIT framework for various molecular tasks. We
release the code, constructed instruction data, and model checkpoints in
https://github.com/chenlong164/PEIT.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-03-04T00:00:00Z">2025-03-04</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">93</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for
  Contact-Rich Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02881v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02881v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Xue, Jieji Ren, Wendi Chen, Gu Zhang, Yuan Fang, Guoying Gu, Huazhe Xu, Cewu Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans can accomplish complex contact-rich tasks using vision and touch, with
highly reactive capabilities such as quick adjustments to environmental changes
and adaptive control of contact forces; however, this remains challenging for
robots. Existing visual imitation learning (IL) approaches rely on action
chunking to model complex behaviors, which lacks the ability to respond
instantly to real-time tactile feedback during the chunk execution.
Furthermore, most teleoperation systems struggle to provide fine-grained
tactile / force feedback, which limits the range of tasks that can be
performed. To address these challenges, we introduce TactAR, a low-cost
teleoperation system that provides real-time tactile feedback through Augmented
Reality (AR), along with Reactive Diffusion Policy (RDP), a novel slow-fast
visual-tactile imitation learning algorithm for learning contact-rich
manipulation skills. RDP employs a two-level hierarchy: (1) a slow latent
diffusion policy for predicting high-level action chunks in latent space at low
frequency, (2) a fast asymmetric tokenizer for closed-loop tactile feedback
control at high frequency. This design enables both complex trajectory modeling
and quick reactive behavior within a unified framework. Through extensive
evaluation across three challenging contact-rich tasks, RDP significantly
improves performance compared to state-of-the-art visual IL baselines through
rapid response to tactile / force feedback. Furthermore, experiments show that
RDP is applicable across different tactile / force sensors. Code and videos are
available on https://reactive-diffusion-policy.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MuBlE: MuJoCo and Blender simulation Environment and Benchmark for Task
  Planning in Robot Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michal Nazarczuk, Karla Stepanova, Jan Kristof Behrens, Matej Hoffmann, Krystian Mikolajczyk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current embodied reasoning agents struggle to plan for long-horizon tasks
that require to physically interact with the world to obtain the necessary
information (e.g. 'sort the objects from lightest to heaviest'). The
improvement of the capabilities of such an agent is highly dependent on the
availability of relevant training environments. In order to facilitate the
development of such systems, we introduce a novel simulation environment (built
on top of robosuite) that makes use of the MuJoCo physics engine and
high-quality renderer Blender to provide realistic visual observations that are
also accurate to the physical state of the scene. It is the first simulator
focusing on long-horizon robot manipulation tasks preserving accurate physics
modeling. MuBlE can generate mutlimodal data for training and enable design of
closed-loop methods through environment interaction on two levels: visual -
action loop, and control - physics loop. Together with the simulator, we
propose SHOP-VRB2, a new benchmark composed of 10 classes of multi-step
reasoning scenarios that require simultaneous visual and physical measurements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/michaal94/MuBlE. arXiv admin note: substantial
  text overlap with arXiv:2404.15194</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Digital Model-Driven Genetic Algorithm for Optimizing Layout and Task
  Allocation in Human-Robot Collaborative Assemblies <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Cella, Matteo Bruce Robin, Marco Faroni, Andrea Maria Zanchettin, Paolo Rocco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the optimization of human-robot collaborative work-cells
before their physical deployment. Most of the times, such environments are
designed based on the experience of the system integrators, often leading to
sub-optimal solutions. Accurate simulators of the robotic cell, accounting for
the presence of the human as well, are available today and can be used in the
pre-deployment. We propose an iterative optimization scheme where a digital
model of the work-cell is updated based on a genetic algorithm. The methodology
focuses on the layout optimization and task allocation, encoding both the
problems simultaneously in the design variables handled by the genetic
algorithm, while the task scheduling problem depends on the result of the
upper-level one. The final solution balances conflicting objectives in the
fitness function and is validated to show the impact of the objectives with
respect to a baseline, which represents possible initial choices selected based
on the human judgment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IEEE ICRA 2025 (Atlanta, USA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning-Enhanced Visual Monitoring in Hazardous Underwater
  Environments with a <span class="highlight-title">Swarm</span> of Micro-Robots <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02752v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02752v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuang Chen, Yifeng He, Barry Lennox, Farshad Arvin, Amir Atapour-Abarghouei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-term monitoring and exploration of extreme environments, such as
underwater storage facilities, is costly, labor-intensive, and hazardous.
Automating this process with low-cost, collaborative robots can greatly improve
efficiency. These robots capture images from different positions, which must be
processed simultaneously to create a spatio-temporal model of the facility. In
this paper, we propose a novel approach that integrates data simulation, a
multi-modal deep learning network for coordinate prediction, and image
reassembly to address the challenges posed by environmental disturbances
causing drift and rotation in the robots' positions and orientations. Our
approach enhances the precision of alignment in noisy environments by
integrating visual information from snapshots, global positional context from
masks, and noisy coordinates. We validate our method through extensive
experiments using synthetic data that simulate real-world robotic operations in
underwater settings. The results demonstrate very high coordinate prediction
accuracy and plausible image assembly, indicating the real-world applicability
of our approach. The assembled images provide clear and coherent views of the
underwater environment for effective monitoring and inspection, showcasing the
potential for broader use in extreme settings, further contributing to improved
safety, efficiency, and cost reduction in hazardous field monitoring. Code is
available on https://github.com/ChrisChen1023/Micro-Robot-Swarm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bridging VLM and KMP: Enabling Fine-grained robotic manipulation via
  Semantic Keypoints Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02748v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02748v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junjie Zhu, Huayu Liu, Jin Wang, Bangrong Wen, Kaixiang Huang, Xiaofei Li, Haiyun Zhan, Guodong Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  From early Movement Primitive (MP) techniques to modern Vision-Language
Models (VLMs), autonomous manipulation has remained a pivotal topic in
robotics. As two extremes, VLM-based methods emphasize zero-shot and adaptive
manipulation but struggle with fine-grained planning. In contrast, MP-based
approaches excel in precise trajectory generalization but lack decision-making
ability. To leverage the strengths of the two frameworks, we propose VL-MP,
which integrates VLM with Kernelized Movement Primitives (KMP) via a
low-distortion decision information transfer bridge, enabling fine-grained
robotic manipulation under ambiguous situations. One key of VL-MP is the
accurate representation of task decision parameters through semantic keypoints
constraints, leading to more precise task parameter generation. Additionally,
we introduce a local trajectory feature-enhanced KMP to support VL-MP, thereby
achieving shape preservation for complex trajectories. Extensive experiments
conducted in complex real-world environments validate the effectiveness of
VL-MP for adaptive and fine-grained manipulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variable-Friction In-Hand Manipulation for Arbitrary Objects via
  Diffusion-Based Imitation Learning <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02738v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02738v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyang Yan, Zihan Ding, Xin Zhou, Adam J. Spiers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dexterous in-hand manipulation (IHM) for arbitrary objects is challenging due
to the rich and subtle contact process. Variable-friction manipulation is an
alternative approach to dexterity, previously demonstrating robust and
versatile 2D IHM capabilities with only two single-joint fingers. However, the
hard-coded manipulation methods for variable friction hands are restricted to
regular polygon objects and limited target poses, as well as requiring the
policy to be tailored for each object. This paper proposes an end-to-end
learning-based manipulation method to achieve arbitrary object manipulation for
any target pose on real hardware, with minimal engineering efforts and data
collection. The method features a diffusion policy-based imitation learning
method with co-training from simulation and a small amount of real-world data.
With the proposed framework, arbitrary objects including polygons and
non-polygons can be precisely manipulated to reach arbitrary goal poses within
2 hours of training on an A100 GPU and only 1 hour of real-world data
collection. The precision is higher than previous customized object-specific
policies, achieving an average success rate of 71.3% with average pose error
being 2.676 mm and 1.902 degrees.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA 2025 Project website:
  https://sites.google.com/view/vf-ihm-il/home</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ImpedanceGPT: VLM-driven Impedance Control of <span class="highlight-title">Swarm</span> of Mini-drones for
  Intelligent Navigation in Dynamic Environment <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02723v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02723v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Faryal Batool, Malaika Zafar, Yasheerah Yaqoot, Roohan Ahmed Khan, Muhammad Haris Khan, Aleksey Fedoseev, Dzmitry Tsetserukou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Swarm robotics plays a crucial role in enabling autonomous operations in
dynamic and unpredictable environments. However, a major challenge remains
ensuring safe and efficient navigation in environments filled with both dynamic
alive (e.g., humans) and dynamic inanimate (e.g., non-living objects)
obstacles. In this paper, we propose ImpedanceGPT, a novel system that combines
a Vision-Language Model (VLM) with retrieval-augmented generation (RAG) to
enable real-time reasoning for adaptive navigation of mini-drone swarms in
complex environments.
  The key innovation of ImpedanceGPT lies in the integration of VLM and RAG,
which provides the drones with enhanced semantic understanding of their
surroundings. This enables the system to dynamically adjust impedance control
parameters in response to obstacle types and environmental conditions. Our
approach not only ensures safe and precise navigation but also improves
coordination between drones in the swarm.
  Experimental evaluations demonstrate the effectiveness of the system. The
VLM-RAG framework achieved an obstacle detection and retrieval accuracy of 80 %
under optimal lighting. In static environments, drones navigated dynamic
inanimate obstacles at 1.4 m/s but slowed to 0.7 m/s with increased separation
around humans. In dynamic environments, speed adjusted to 1.0 m/s near hard
obstacles, while reducing to 0.6 m/s with higher deflection to safely avoid
moving humans.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vibration-Assisted Hysteresis Mitigation for Achieving High Compensation
  Efficiency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02720v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02720v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Myeongbo Park, Chunggil An, Junhyun Park, Jonghyun Kang, Minho Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tendon-sheath mechanisms (TSMs) are widely used in minimally invasive
surgical (MIS) applications, but their inherent hysteresis-caused by friction,
backlash, and tendon elongation-leads to significant tracking errors.
Conventional modeling and compensation methods struggle with these
nonlinearities and require extensive parameter tuning. To address this, we
propose a vibration-assisted hysteresis compensation approach, where controlled
vibrational motion is applied along the tendon's movement direction to mitigate
friction and reduce dead zones. Experimental results demonstrate that the
exerted vibration consistently reduces hysteresis across all tested
frequencies, decreasing RMSE by up to 23.41% (from 2.2345 mm to 1.7113 mm) and
improving correlation, leading to more accurate trajectory tracking. When
combined with a Temporal Convolutional Network (TCN)-based compensation model,
vibration further enhances performance, achieving an 85.2% reduction in MAE
(from 1.334 mm to 0.1969 mm). Without vibration, the TCN-based approach still
reduces MAE by 72.3% (from 1.334 mm to 0.370 mm) under the same parameter
settings. These findings confirm that vibration effectively mitigates
hysteresis, improving trajectory accuracy and enabling more efficient
compensation models with fewer trainable parameters. This approach provides a
scalable and practical solution for TSM-based robotic applications,
particularly in MIS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, and 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable <span class="highlight-title">Multi</span>-Robot Task Allocation and Coordination under Signal
  Temporal Logic Specifications <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenliang Liu, Nathalie Majcherczyk, Federico Pecora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion planning with simple objectives, such as collision-avoidance and
goal-reaching, can be solved efficiently using modern planners. However, the
complexity of the allowed tasks for these planners is limited. On the other
hand, signal temporal logic (STL) can specify complex requirements, but
STL-based motion planning and control algorithms often face scalability issues,
especially in large multi-robot systems with complex dynamics. In this paper,
we propose an algorithm that leverages the best of the two worlds. We first use
a single-robot motion planner to efficiently generate a set of alternative
reference paths for each robot. Then coordination requirements are specified
using STL, which is defined over the assignment of paths and robots' progress
along those paths. We use a Mixed Integer Linear Program (MILP) to compute task
assignments and robot progress targets over time such that the STL
specification is satisfied. Finally, a local controller is used to track the
target progress. Simulations demonstrate that our method can handle tasks with
complex constraints and scales to large multi-robot teams and intricate task
allocation scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Multi</span>-Strategy Enhanced COA for Path Planning in Autonomous Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Wang, Jacky Keung, Haohan Xu, Yuchen Cao, Zhenyu Mao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous navigation is reshaping various domains in people's life by
enabling efficient and safe movement in complex environments. Reliable
navigation requires algorithmic approaches that compute optimal or near-optimal
trajectories while satisfying task-specific constraints and ensuring obstacle
avoidance. However, existing methods struggle with slow convergence and
suboptimal solutions, particularly in complex environments, limiting their
real-world applicability. To address these limitations, this paper presents the
Multi-Strategy Enhanced Crayfish Optimization Algorithm (MCOA), a novel
approach integrating three key strategies: 1) Refractive Opposition Learning,
enhancing population diversity and global exploration, 2) Stochastic
Centroid-Guided Exploration, balancing global and local search to prevent
premature convergence, and 3) Adaptive Competition-Based Selection, dynamically
adjusting selection pressure for faster convergence and improved solution
quality. Empirical evaluations underscore the remarkable planning speed and the
amazing solution quality of MCOA in both 3D Unmanned Aerial Vehicle (UAV) and
2D mobile robot path planning. Against 11 baseline algorithms, MCOA achieved a
69.2% reduction in computational time and a 16.7% improvement in minimizing
overall path cost in 3D UAV scenarios. Furthermore, in 2D path planning, MCOA
outperformed baseline approaches by 44% on average, with an impressive 75.6%
advantage in the largest 60*60 grid setting. These findings validate MCOA as a
powerful tool for optimizing autonomous navigation in complex environments. The
source code is available at: https://github.com/coedv-hub/MCOA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FlowPlan: Zero-Shot Task Planning with <span class="highlight-title">LLM</span> Flow Engineering for Robotic
  Instruction Following 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02698v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02698v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijun Lin, Chao Tang, Hanjing Ye, Hong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic instruction following tasks require seamless integration of visual
perception, task planning, target localization, and motion execution. However,
existing task planning methods for instruction following are either data-driven
or underperform in zero-shot scenarios due to difficulties in grounding lengthy
instructions into actionable plans under operational constraints. To address
this, we propose FlowPlan, a structured multi-stage LLM workflow that elevates
zero-shot pipeline and bridges the performance gap between zero-shot and
data-driven in-context learning methods. By decomposing the planning process
into modular stages--task information retrieval, language-level reasoning,
symbolic-level planning, and logical evaluation--FlowPlan generates logically
coherent action sequences while adhering to operational constraints and further
extracts contextual guidance for precise instance-level target localization.
Benchmarked on the ALFRED and validated in real-world applications, our method
achieves competitive performance relative to data-driven in-context learning
methods and demonstrates adaptability across diverse environments. This work
advances zero-shot task planning in robotic systems without reliance on labeled
data. Project website: https://instruction-following-project.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Class-Aware PillarMix: Can Mixed Sample Data Augmentation Enhance 3D
  Object Detection with Radar Point Clouds? <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miao Zhang, Sherif Abdulatif, Benedikt Loesch, Marco Altmann, Bin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the significant effort required for data collection and annotation in
3D perception tasks, mixed sample data augmentation (MSDA) has been widely
studied to generate diverse training samples by mixing existing data. Recently,
many MSDA techniques have been developed for point clouds, but they mainly
target LiDAR data, leaving their application to radar point clouds largely
unexplored. In this paper, we examine the feasibility of applying existing MSDA
methods to radar point clouds and identify several challenges in adapting these
techniques. These obstacles stem from the radar's irregular angular
distribution, deviations from a single-sensor polar layout in multi-radar
setups, and point sparsity. To address these issues, we propose Class-Aware
PillarMix (CAPMix), a novel MSDA approach that applies MixUp at the pillar
level in 3D point clouds, guided by class labels. Unlike methods that rely a
single mix ratio to the entire sample, CAPMix assigns an independent ratio to
each pillar, boosting sample diversity. To account for the density of different
classes, we use class-specific distributions: for dense objects (e.g., large
vehicles), we skew ratios to favor points from another sample, while for sparse
objects (e.g., pedestrians), we sample more points from the original. This
class-aware mixing retains critical details and enriches each sample with new
information, ultimately generating more diverse training data. Experimental
results demonstrate that our method not only significantly boosts performance
but also outperforms existing MSDA approaches across two datasets (Bosch Street
and K-Radar). We believe that this straightforward yet effective approach will
spark further investigation into MSDA techniques for radar data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, 4 tables, submitted to 2025 IEEE/RSJ
  International Conference on Intelligent Robots and Systems (IROS 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning-Based Passive Fault-Tolerant Control of a Quadrotor with Rotor
  Failure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiehao Chen, Kaidong Zhao, Zihan Liu, YanJie Li, Yunjiang Lou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a learning-based passive fault-tolerant control (PFTC)
method for quadrotor capable of handling arbitrary single-rotor failures,
including conditions ranging from fault-free to complete rotor failure, without
requiring any rotor fault information or controller switching. Unlike existing
methods that treat rotor faults as disturbances and rely on a single controller
for multiple fault scenarios, our approach introduces a novel
Selector-Controller network structure. This architecture integrates fault
detection module and the controller into a unified policy network, effectively
combining the adaptability to multiple fault scenarios of PFTC with the
superior control performance of active fault-tolerant control (AFTC). To
optimize performance, the policy network is trained using a hybrid framework
that synergizes reinforcement learning (RL), behavior cloning (BC), and
supervised learning with fault information. Extensive simulations and
real-world experiments validate the proposed method, demonstrating significant
improvements in fault response speed and position tracking performance compared
to state-of-the-art PFTC and AFTC approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Velocity-free task-space regulator for robot manipulators with external
  disturbances 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiwen Wu, Bayu Jayawardhana, Dabo Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the problem of task-space robust regulation of robot
manipulators subject to external disturbances. A velocity-free control law is
proposed by combining the internal model principle and the passivity-based
output-feedback control approach. The developed output-feedback controller
ensures not only asymptotic convergence of the regulation error but also
suppression of unwanted external step/sinusoidal disturbances. The potential of
the proposed method lies in its simplicity, intuitively appealing, and simple
gain selection criteria for synthesis of multi-joint robot manipulator control
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human-aligned Safe Reinforcement Learning for Highway On-Ramp Merging in
  Dense Traffic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02624v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02624v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Li, Shijie Yuan, Yuan Chang, Xiaolong Chen, Qisong Yang, Zhiyuan Yang, Hongmao Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most reinforcement learning (RL) approaches for the decision-making of
autonomous driving consider safety as a reward instead of a cost, which makes
it hard to balance the tradeoff between safety and other objectives. Human risk
preference has also rarely been incorporated, and the trained policy might be
either conservative or aggressive for users. To this end, this study proposes a
human-aligned safe RL approach for autonomous merging, in which the high-level
decision problem is formulated as a constrained Markov decision process (CMDP)
that incorporates users' risk preference into the safety constraints, followed
by a model predictive control (MPC)-based low-level control. The safety level
of RL policy can be adjusted by computing cost limits of CMDP's constraints
based on risk preferences and traffic density using a fuzzy control method. To
filter out unsafe or invalid actions, we design an action shielding mechanism
that pre-executes RL actions using an MPC method and performs collision checks
with surrounding agents. We also provide theoretical proof to validate the
effectiveness of the shielding mechanism in enhancing RL's safety and sample
efficiency. Simulation experiments in multiple levels of traffic densities show
that our method can significantly reduce safety violations without sacrificing
traffic efficiency. Furthermore, due to the use of risk preference-aware
constraints in CMDP and action shielding, we can not only adjust the safety
level of the final policy but also reduce safety violations during the training
stage, proving a promising solution for online learning in real-world
environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Resource-Efficient Affordance Grounding with Complementary Depth and
  Semantic Prompts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhou Huang, Fan Yang, Guoliang Zhu, Gen Li, Hao Shi, Yukun Zuo, Wenrui Chen, Zhiyong Li, Kailun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Affordance refers to the functional properties that an agent perceives and
utilizes from its environment, and is key perceptual information required for
robots to perform actions. This information is rich and multimodal in nature.
Existing multimodal affordance methods face limitations in extracting useful
information, mainly due to simple structural designs, basic fusion methods, and
large model parameters, making it difficult to meet the performance
requirements for practical deployment. To address these issues, this paper
proposes the BiT-Align image-depth-text affordance mapping framework. The
framework includes a Bypass Prompt Module (BPM) and a Text Feature Guidance
(TFG) attention selection mechanism. BPM integrates the auxiliary modality
depth image directly as a prompt to the primary modality RGB image, embedding
it into the primary modality encoder without introducing additional encoders.
This reduces the model's parameter count and effectively improves functional
region localization accuracy. The TFG mechanism guides the selection and
enhancement of attention heads in the image encoder using textual features,
improving the understanding of affordance characteristics. Experimental results
demonstrate that the proposed method achieves significant performance
improvements on public AGD20K and HICO-IIF datasets. On the AGD20K dataset,
compared with the current state-of-the-art method, we achieve a 6.0%
improvement in the KLD metric, while reducing model parameters by 88.8%,
demonstrating practical application values. The source code will be made
publicly available at https://github.com/DAWDSE/BiT-Align.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The source code will be made publicly available at
  https://github.com/DAWDSE/BiT-Align</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Dexterous In-Hand Manipulation with <span class="highlight-title">Multi</span>fingered Hands via
  Visuomotor Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02587v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02587v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piotr Koczy, Michael C. Welle, Danica Kragic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a framework for learning dexterous in-hand manipulation with
multifingered hands using visuomotor diffusion policies. Our system enables
complex in-hand manipulation tasks, such as unscrewing a bottle lid with one
hand, by leveraging a fast and responsive teleoperation setup for the
four-fingered Allegro Hand. We collect high-quality expert demonstrations using
an augmented reality (AR) interface that tracks hand movements and applies
inverse kinematics and motion retargeting for precise control. The AR headset
provides real-time visualization, while gesture controls streamline
teleoperation. To enhance policy learning, we introduce a novel demonstration
outlier removal approach based on HDBSCAN clustering and the Global-Local
Outlier Score from Hierarchies (GLOSH) algorithm, effectively filtering out
low-quality demonstrations that could degrade performance. We evaluate our
approach extensively in real-world settings and provide all experimental videos
on the project website: https://dex-manip.github.io/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Research on visual simultaneous localization and mapping technology
  based on near infrared light 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02584v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02584v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Ma, Mengfang Liu, Boliang Li, Xinghui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In view of the problems that visual simultaneous localization and mapping
(VSLAM) are susceptible to environmental light interference and luminosity
inconsistency, the visual simultaneous localization and mapping technology
based on near infrared perception (NIR-VSLAM) is proposed. In order to avoid
ambient light interference, the near infrared light is innovatively selected as
the light source. The luminosity parameter estimation of error energy function,
halo factor and exposure time and the light source irradiance correction method
are proposed in this paper, which greatly improves the positioning accuracy of
Direct Sparse Odometry (DSO). The feasibility of the proposed method in four
large scenes is verified, which provides the reference for visual positioning
in automatic driving and mobile robot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 9 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling the Potential of Segment Anything Model 2 for RGB-Thermal
  Semantic Segmentation with Language Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiayi Zhao, Fei Teng, Kai Luo, Guoqiang Zhao, Zhiyong Li, Xu Zheng, Kailun Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The perception capability of robotic systems relies on the richness of the
dataset. Although Segment Anything Model 2 (SAM2), trained on large datasets,
demonstrates strong perception potential in perception tasks, its inherent
training paradigm prevents it from being suitable for RGB-T tasks. To address
these challenges, we propose SHIFNet, a novel SAM2-driven Hybrid Interaction
Paradigm that unlocks the potential of SAM2 with linguistic guidance for
efficient RGB-Thermal perception. Our framework consists of two key components:
(1) Semantic-Aware Cross-modal Fusion (SACF) module that dynamically balances
modality contributions through text-guided affinity learning, overcoming SAM2's
inherent RGB bias; (2) Heterogeneous Prompting Decoder (HPD) that enhances
global semantic information through a semantic enhancement module and then
combined with category embeddings to amplify cross-modal semantic consistency.
With 32.27M trainable parameters, SHIFNet achieves state-of-the-art
segmentation performance on public benchmarks, reaching 89.8% on PST900 and
67.8% on FMB, respectively. The framework facilitates the adaptation of
pre-trained large models to RGB-T segmentation tasks, effectively mitigating
the high costs associated with data collection while endowing robotic systems
with comprehensive perception capabilities. The source code will be made
publicly available at https://github.com/iAsakiT3T/SHIFNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The source code will be made publicly available at
  https://github.com/iAsakiT3T/SHIFNet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TS-CGNet: Temporal-Spatial Fusion Meets Cente<span class="highlight-title">rl</span>ine-Guided Diffusion for
  BEV Mapping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02578v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02578v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinying Hong, Siyu Li, Kang Zeng, Hao Shi, Bomin Peng, Kailun Yang, Zhiyong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bird's Eye View (BEV) perception technology is crucial for autonomous
driving, as it generates top-down 2D maps for environment perception,
navigation, and decision-making. Nevertheless, the majority of current BEV map
generation studies focusing on visual map generation lack depth-aware reasoning
capabilities. They exhibit limited efficacy in managing occlusions and handling
complex environments, with a notable decline in perceptual performance under
adverse weather conditions or low-light scenarios. Therefore, this paper
proposes TS-CGNet, which leverages Temporal-Spatial fusion with
Centerline-Guided diffusion. This visual framework, grounded in prior
knowledge, is designed for integration into any existing network for building
BEV maps. Specifically, this framework is decoupled into three parts: Local
mapping system involves the initial generation of semantic maps using purely
visual information; The Temporal-Spatial Aligner Module (TSAM) integrates
historical information into mapping generation by applying transformation
matrices; The Centerline-Guided Diffusion Model (CGDM) is a prediction module
based on the diffusion model. CGDM incorporates centerline information through
spatial-attention mechanisms to enhance semantic segmentation reconstruction.
We construct BEV semantic segmentation maps by our methods on the public
nuScenes and the robustness benchmarks under various corruptions. Our method
improves 1.90%, 1.73%, and 2.87% for perceived ranges of 60x30m, 120x60m, and
240x60m in the task of BEV HD mapping. TS-CGNet attains an improvement of 1.92%
for perceived ranges of 100x100m in the task of BEV semantic mapping. Moreover,
TS-CGNet achieves an average improvement of 2.92% in detection accuracy under
varying weather conditions and sensor interferences in the perception range of
240x60m. The source code will be publicly available at
https://github.com/krabs-H/TS-CGNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The source code will be publicly available at
  https://github.com/krabs-H/TS-CGNet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RaceVLA: VLA-based Racing Drone Navigation with Human-like Behaviour <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valerii Serpiva, Artem Lykov, Artyom Myshlyaev, Muhammad Haris Khan, Ali Alridha Abdulkarim, Oleg Sautenkov, Dzmitry Tsetserukou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  RaceVLA presents an innovative approach for autonomous racing drone
navigation by leveraging Visual-Language-Action (VLA) to emulate human-like
behavior. This research explores the integration of advanced algorithms that
enable drones to adapt their navigation strategies based on real-time
environmental feedback, mimicking the decision-making processes of human
pilots. The model, fine-tuned on a collected racing drone dataset, demonstrates
strong generalization despite the complexity of drone racing environments.
RaceVLA outperforms OpenVLA in motion (75.0 vs 60.0) and semantic
generalization (45.5 vs 36.3), benefiting from the dynamic camera and
simplified motion tasks. However, visual (79.6 vs 87.0) and physical (50.0 vs
76.7) generalization were slightly reduced due to the challenges of maneuvering
in dynamic environments with varying object sizes. RaceVLA also outperforms
RT-2 across all axes - visual (79.6 vs 52.0), motion (75.0 vs 55.0), physical
(50.0 vs 26.7), and semantic (45.5 vs 38.8), demonstrating its robustness for
real-time adjustments in complex environments. Experiments revealed an average
velocity of 1.04 m/s, with a maximum speed of 2.02 m/s, and consistent
maneuverability, demonstrating RaceVLA's ability to handle high-speed scenarios
effectively. These findings highlight the potential of RaceVLA for
high-performance navigation in competitive racing contexts. The RaceVLA
codebase, pretrained weights, and dataset are available at this http URL:
https://racevla.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures. Submitted to IROS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wo<span class="highlight-title">rl</span>d Models for Anomaly Detection during Model-Based Reinforcement
  Learning Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Domberg, Georg Schildbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning-based controllers are often purposefully kept out of real-world
applications due to concerns about their safety and reliability. We explore how
state-of-the-art world models in Model-Based Reinforcement Learning can be
utilized beyond the training phase to ensure a deployed policy only operates
within regions of the state-space it is sufficiently familiar with. This is
achieved by continuously monitoring discrepancies between a world model's
predictions and observed system behavior during inference. It allows for
triggering appropriate measures, such as an emergency stop, once an error
threshold is surpassed. This does not require any task-specific knowledge and
is thus universally applicable. Simulated experiments on established robot
control tasks show the effectiveness of this method, recognizing changes in
local robot geometry and global gravitational magnitude. Real-world experiments
using an agile quadcopter further demonstrate the benefits of this approach by
detecting unexpected forces acting on the vehicle. These results indicate how
even in new and adverse conditions, safe and reliable operation of otherwise
unpredictable learning-based controllers can be achieved.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Magic in Human-Robot Interaction (HRI) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02525v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02525v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Cooney, Alexey Vinel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  "Magic" is referred to here and there in the robotics literature, from
"magical moments" afforded by a mobile bubble machine, to "spells" intended to
entertain and motivate children--but what exactly could this concept mean for
designers? Here, we present (1) some theoretical discussion on how magic could
inform interaction designs based on reviewing the literature, followed by (2) a
practical description of using such ideas to develop a simplified prototype,
which received an award in an international robot magic competition. Although
this topic can be considered unusual and some negative connotations exist
(e.g., unrealistic thinking can be referred to as magical), our results seem to
suggest that magic, in the experiential, supernatural, and illusory senses of
the term, could be useful to consider in various robot design contexts, also
for artifacts like home assistants and autonomous vehicles--thus, inviting
further discussion and exploration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted Version of a Paper Published in IEEE, 10 pages, in the 34th
  annual workshop of the Swedish Artificial Intelligence Society (SAIS 2022),
  2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Impact of Temporal Delay on Radar-Inertial Odometry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02509v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02509v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vlaho-Josip Štironja, Luka Petrović, Juraj Peršić, Ivan Marković, Ivan Petrović
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate ego-motion estimation is a critical component of any autonomous
system. Conventional ego-motion sensors, such as cameras and LiDARs, may be
compromised in adverse environmental conditions, such as fog, heavy rain, or
dust. Automotive radars, known for their robustness to such conditions, present
themselves as complementary sensors or a promising alternative within the
ego-motion estimation frameworks. In this paper we propose a novel
Radar-Inertial Odometry (RIO) system that integrates an automotive radar and an
inertial measurement unit. The key contribution is the integration of online
temporal delay calibration within the factor graph optimization framework that
compensates for potential time offsets between radar and IMU measurements. To
validate the proposed approach we have conducted thorough experimental analysis
on real-world radar and IMU data. The results show that, even without scan
matching or target tracking, integration of online temporal calibration
significantly reduces localization error compared to systems that disregard
time synchronization, thus highlighting the important role of, often neglected,
accurate temporal alignment in radar-based sensor fusion systems for autonomous
navigation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02505v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02505v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaofei Cai, Zhancun Mu, Anji Liu, Yitao Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We aim to develop a goal specification method that is semantically clear,
spatially sensitive, and intuitive for human users to guide agent interactions
in embodied environments. Specifically, we propose a novel cross-view goal
alignment framework that allows users to specify target objects using
segmentation masks from their own camera views rather than the agent's
observations. We highlight that behavior cloning alone fails to align the
agent's behavior with human intent when the human and agent camera views differ
significantly. To address this, we introduce two auxiliary objectives:
cross-view consistency loss and target visibility loss, which explicitly
enhance the agent's spatial reasoning ability. According to this, we develop
ROCKET-2, a state-of-the-art agent trained in Minecraft, achieving an
improvement in the efficiency of inference 3x to 6x. We show ROCKET-2 can
directly interpret goals from human camera views for the first time, paving the
way for better human-agent interaction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UAV-VLRR: Vision-Language Informed NMPC for Rapid Response in UAV Search
  and Rescue 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02465v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02465v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasheerah Yaqoot, Muhammad Ahsan Mustafa, Oleg Sautenkov, Dzmitry Tsetserukou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emergency search and rescue (SAR) operations often require rapid and precise
target identification in complex environments where traditional manual drone
control is inefficient. In order to address these scenarios, a rapid SAR
system, UAV-VLRR (Vision-Language-Rapid-Response), is developed in this
research. This system consists of two aspects: 1) A multimodal system which
harnesses the power of Visual Language Model (VLM) and the natural language
processing capabilities of ChatGPT-4o (LLM) for scene interpretation. 2) A
non-linearmodel predictive control (NMPC) with built-in obstacle avoidance for
rapid response by a drone to fly according to the output of the multimodal
system. This work aims at improving response times in emergency SAR operations
by providing a more intuitive and natural approach to the operator to plan the
SAR mission while allowing the drone to carry out that mission in a rapid and
safe manner. When tested, our approach was faster on an average by 33.75% when
compared with an off-the-shelf autopilot and 54.6% when compared with a human
pilot. Video of UAV-VLRR: https://youtu.be/KJqQGKKt1xY
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UAV-VLPA*: A Vision-Language-Path-Action System for Optimal Route
  Generation on a Large Scales 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02454v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02454v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oleg Sautenkov, Aibek Akhmetkazy, Yasheerah Yaqoot, Muhammad Ahsan Mustafa, Grik Tadevosyan, Artem Lykov, Dzmitry Tsetserukou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The UAV-VLPA* (Visual-Language-Planning-and-Action) system represents a
cutting-edge advancement in aerial robotics, designed to enhance communication
and operational efficiency for unmanned aerial vehicles (UAVs). By integrating
advanced planning capabilities, the system addresses the Traveling Salesman
Problem (TSP) to optimize flight paths, reducing the total trajectory length by
18.5\% compared to traditional methods. Additionally, the incorporation of the
A* algorithm enables robust obstacle avoidance, ensuring safe and efficient
navigation in complex environments. The system leverages satellite imagery
processing combined with the Visual Language Model (VLM) and GPT's natural
language processing capabilities, allowing users to generate detailed flight
plans through simple text commands. This seamless fusion of visual and
linguistic analysis empowers precise decision-making and mission planning,
making UAV-VLPA* a transformative tool for modern aerial operations. With its
unmatched operational efficiency, navigational safety, and user-friendly
functionality, UAV-VLPA* sets a new standard in autonomous aerial robotics,
paving the way for future innovations in the field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2501.05014</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predictive Kinematic Coordinate Control for Aerial Manipulators based on
  Modified Kinematics Learning <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengzhen Li, Jiahao Shen, Mengyu Ji, Huazi Cao, Shiyu Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-precision manipulation has always been a developmental goal for aerial
manipulators. This paper investigates the kinematic coordinate control issue in
aerial manipulators. We propose a predictive kinematic coordinate control
method, which includes a learning-based modified kinematic model and a model
predictive control (MPC) scheme based on weight allocation. Compared to
existing methods, our proposed approach offers several attractive features.
First, the kinematic model incorporates closed-loop dynamics characteristics
and online residual learning. Compared to methods that do not consider
closed-loop dynamics and residuals, our proposed method has improved accuracy
by 59.6$\%$. Second, a MPC scheme that considers weight allocation has been
proposed, which can coordinate the motion strategies of quadcopters and
manipulators. Compared to methods that do not consider weight allocation, the
proposed method can meet the requirements of more tasks. The proposed approach
is verified through complex trajectory tracking and moving target tracking
experiments. The results validate the effectiveness of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A comparison of visual representations for real-wo<span class="highlight-title">rl</span>d reinforcement
  learning in the context of vacuum gripping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nico Sutter, Valentin N. Hartmann, Stelian Coros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When manipulating objects in the real world, we need reactive feedback
policies that take into account sensor information to inform decisions. This
study aims to determine how different encoders can be used in a reinforcement
learning (RL) framework to interpret the spatial environment in the local
surroundings of a robot arm. Our investigation focuses on comparing real-world
vision with 3D scene inputs, exploring new architectures in the process. We
built on the SERL framework, providing us with a sample efficient and stable RL
foundation we could build upon, while keeping training times minimal. The
results of this study indicate that spatial information helps to significantly
outperform the visual counterpart, tested on a box picking task with a vacuum
gripper. The code and videos of the evaluations are available at
https://github.com/nisutte/voxel-serl.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pager, 5 Figures, 5 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RGBSQGrasp: Inferring Local Superquadric Primitives from Single RGB
  Image for Graspability-Aware Bin Picking <span class="chip">IROS2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifeng Xu, Fan Zhu, Ye Li, Sebastian Ren, Xiaonan Huang, Yuhao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bin picking is a challenging robotic task due to occlusions and physical
constraints that limit visual information for object recognition and grasping.
Existing approaches often rely on known CAD models or prior object geometries,
restricting generalization to novel or unknown objects. Other methods directly
regress grasp poses from RGB-D data without object priors, but the inherent
noise in depth sensing and the lack of object understanding make grasp
synthesis and evaluation more difficult. Superquadrics (SQ) offer a compact,
interpretable shape representation that captures the physical and graspability
understanding of objects. However, recovering them from limited viewpoints is
challenging, as existing methods rely on multiple perspectives for
near-complete point cloud reconstruction, limiting their effectiveness in
bin-picking. To address these challenges, we propose \textbf{RGBSQGrasp}, a
grasping framework that leverages superquadric shape primitives and foundation
metric depth estimation models to infer grasp poses from a monocular RGB camera
-- eliminating the need for depth sensors. Our framework integrates a
universal, cross-platform dataset generation pipeline, a foundation model-based
object point cloud estimation module, a global-local superquadric fitting
network, and an SQ-guided grasp pose sampling module. By integrating these
components, RGBSQGrasp reliably infers grasp poses through geometric reasoning,
enhancing grasp stability and adaptability to unseen objects. Real-world
robotic experiments demonstrate a 92\% grasp success rate, highlighting the
effectiveness of RGBSQGrasp in packed bin-picking environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, In submission to IROS2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Introspective Loop Closure for <span class="highlight-title">SLAM</span> with 4D Imaging Radar <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Hilger, Vladimír Kubelka, Daniel Adolfsson, Ralf Becker, Henrik Andreasson, Achim J. Lilienthal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simultaneous Localization and Mapping (SLAM) allows mobile robots to navigate
without external positioning systems or pre-existing maps. Radar is emerging as
a valuable sensing tool, especially in vision-obstructed environments, as it is
less affected by particles than lidars or cameras. Modern 4D imaging radars
provide three-dimensional geometric information and relative velocity
measurements, but they bring challenges, such as a small field of view and
sparse, noisy point clouds. Detecting loop closures in SLAM is critical for
reducing trajectory drift and maintaining map accuracy. However, the
directional nature of 4D radar data makes identifying loop closures, especially
from reverse viewpoints, difficult due to limited scan overlap. This article
explores using 4D radar for loop closure in SLAM, focusing on similar and
opposing viewpoints. We generate submaps for a denser environment
representation and use introspective measures to reject false detections in
feature-degenerate environments. Our experiments show accurate loop closure
detection in geometrically diverse settings for both similar and opposing
viewpoints, improving trajectory estimation with up to 82 % improvement in ATE
and rejecting false positives in self-similar environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for publication in the IEEE
  International Conference on Robotics and Automation(ICRA), 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Label-Efficient LiDAR Panoptic Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02372v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02372v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmet Selim Çanakçı, Niclas Vödisch, Kürsat Petek, Wolfram Burgard, Abhinav Valada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A main bottleneck of learning-based robotic scene understanding methods is
the heavy reliance on extensive annotated training data, which often limits
their generalization ability. In LiDAR panoptic segmentation, this challenge
becomes even more pronounced due to the need to simultaneously address both
semantic and instance segmentation from complex, high-dimensional point cloud
data. In this work, we address the challenge of LiDAR panoptic segmentation
with very few labeled samples by leveraging recent advances in label-efficient
vision panoptic segmentation. To this end, we propose a novel method,
Limited-Label LiDAR Panoptic Segmentation (L3PS), which requires only a minimal
amount of labeled data. Our approach first utilizes a label-efficient 2D
network to generate panoptic pseudo-labels from a small set of annotated
images, which are subsequently projected onto point clouds. We then introduce a
novel 3D refinement module that capitalizes on the geometric properties of
point clouds. By incorporating clustering techniques, sequential scan
accumulation, and ground point separation, this module significantly enhances
the accuracy of the pseudo-labels, improving segmentation quality by up to
+10.6 PQ and +7.9 mIoU. We demonstrate that these refined pseudo-labels can be
used to effectively train off-the-shelf LiDAR segmentation networks. Through
extensive experiments, we show that L3PS not only outperforms existing methods
but also substantially reduces the annotation burden. We release the code of
our work at https://l3ps.cs.uni-freiburg.de.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JPDS-NN: Reinforcement Learning-Based Dynamic Task Allocation for
  Agricultural Vehicle Routing Optimization <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Fan, Haotian Xu, Mengqiao Liu, Qing Zhuo, Tao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Entrance Dependent Vehicle Routing Problem (EDVRP) is a variant of the
Vehicle Routing Problem (VRP) where the scale of cities influences routing
outcomes, necessitating consideration of their entrances. This paper addresses
EDVRP in agriculture, focusing on multi-parameter vehicle planning for
irregularly shaped fields. To address the limitations of traditional methods,
such as heuristic approaches, which often overlook field geometry and entrance
constraints, we propose a Joint Probability Distribution Sampling Neural
Network (JPDS-NN) to effectively solve the EDVRP. The network uses an
encoder-decoder architecture with graph transformers and attention mechanisms
to model routing as a Markov Decision Process, and is trained via reinforcement
learning for efficient and rapid end-to-end planning. Experimental results
indicate that JPDS-NN reduces travel distances by 48.4-65.4%, lowers fuel
consumption by 14.0-17.6%, and computes two orders of magnitude faster than
baseline methods, while demonstrating 15-25% superior performance in dynamic
arrangement scenarios. Ablation studies validate the necessity of
cross-attention and pre-training. The framework enables scalable, intelligent
routing for large-scale farming under dynamic constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, submitted to IROS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Controllable Motion Generation via Diffusion Modal Coupling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02353v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02353v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luobin Wang, Hongzhan Yu, Chenning Yu, Sicun Gao, Henrik Christensen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have recently gained significant attention in robotics due
to their ability to generate multi-modal distributions of system states and
behaviors. However, a key challenge remains: ensuring precise control over the
generated outcomes without compromising realism. This is crucial for
applications such as motion planning or trajectory forecasting, where adherence
to physical constraints and task-specific objectives is essential. We propose a
novel framework that enhances controllability in diffusion models by leveraging
multi-modal prior distributions and enforcing strong modal coupling. This
allows us to initiate the denoising process directly from distinct prior modes
that correspond to different possible system behaviors, ensuring sampling to
align with the training distribution. We evaluate our approach on motion
prediction using the Waymo dataset and multi-task control in Maze2D
environments. Experimental results show that our framework outperforms both
guidance-based techniques and conditioned models with unimodal priors,
achieving superior fidelity, diversity, and controllability, even in the
absence of explicit conditioning. Overall, our approach provides a more
reliable and scalable solution for controllable motion generation in robotics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Target Return Optimizer for <span class="highlight-title">Multi</span>-Game Decision Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kensuke Tatematsu, Akifumi Wachi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving autonomous agents with robust generalization capabilities across
diverse games and tasks remains one of the ultimate goals in AI research.
Recent advancements in transformer-based offline reinforcement learning,
exemplified by the MultiGame Decision Transformer [Lee et al., 2022], have
shown remarkable performance across various games or tasks. However, these
approaches depend heavily on human expertise, presenting substantial challenges
for practical deployment, particularly in scenarios with limited prior
game-specific knowledge. In this paper, we propose an algorithm called
Multi-Game Target Return Optimizer (MTRO) to autonomously determine
game-specific target returns within the Multi-Game Decision Transformer
framework using solely offline datasets. MTRO addresses the existing
limitations by automating the target return configuration process, leveraging
environmental reward information extracted from offline datasets. Notably, MTRO
does not require additional training, enabling seamless integration into
existing Multi-Game Decision Transformer architectures. Our experimental
evaluations on Atari games demonstrate that MTRO enhances the performance of RL
policies across a wide array of games, underscoring its potential to advance
the field of autonomous agent development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating Vision-Language-Action Model Integrated with Action
  Chunking via Parallel Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Song, Jiayi Chen, Pengxiang Ding, Han Zhao, Wei Zhao, Zhide Zhong, Zongyuan Ge, Jun Ma, Haoang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language-Action (VLA) models demonstrate remarkable potential for
generalizable robotic manipulation. The performance of VLA models can be
improved by integrating with action chunking, a critical technique for
effective control. However, action chunking linearly scales up action
dimensions in VLA models with increased chunking sizes. This reduces the
inference efficiency. To tackle this problem, we propose PD-VLA, the first
parallel decoding framework for VLA models integrated with action chunking. Our
framework reformulates autoregressive decoding as a nonlinear system solved by
parallel fixed-point iterations. This approach preserves model performance with
mathematical guarantees while significantly improving decoding speed. In
addition, it enables training-free acceleration without architectural changes,
as well as seamless synergy with existing acceleration techniques. Extensive
simulations validate that our PD-VLA maintains competitive success rates while
achieving 2.52 times execution frequency on manipulators (with 7 degrees of
freedom) compared with the fundamental VLA model. Furthermore, we
experimentally identify the most effective settings for acceleration. Finally,
real-world experiments validate its high applicability across different tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diffusion-Based mmWave Radar Point Cloud Enhancement Driven by Range
  Images <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixin Wu, Zihan Li, Jin Wang, Xiangyu Xu, Huan Yu, Zhi Zheng, Kaixiang Huang, Guodong Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Millimeter-wave (mmWave) radar has attracted significant attention in
robotics and autonomous driving. However, despite the perception stability in
harsh environments, the point cloud generated by mmWave radar is relatively
sparse while containing significant noise, which limits its further
development. Traditional mmWave radar enhancement approaches often struggle to
leverage the effectiveness of diffusion models in super-resolution, largely due
to the unnatural range-azimuth heatmap (RAH) or bird's eye view (BEV)
representation. To overcome this limitation, we propose a novel method that
pioneers the application of fusing range images with image diffusion models,
achieving accurate and dense mmWave radar point clouds that are similar to
LiDAR. Benefitting from the projection that aligns with human observation, the
range image representation of mmWave radar is close to natural images, allowing
the knowledge from pre-trained image diffusion models to be effectively
transferred, significantly improving the overall performance. Extensive
evaluations on both public datasets and self-constructed datasets demonstrate
that our approach provides substantial improvements, establishing a new
state-of-the-art performance in generating truly three-dimensional LiDAR-like
point clouds via mmWave radar.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, submitted to 2025 IROS. This work has been
  submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model-Based Capacitive Touch Sensing in Soft Robotics: Achieving Robust
  Tactile Interactions for Artistic Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02280v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02280v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carolina Silva-Plata, Carlos Rosel, Barnabas Gavin Cangan, Hosam Alagi, Björn Hein, Robert K. Katzschmann, Rubén Fernández, Yosra Mojtahedi, Stefan Escaida Navarro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a touch technology to achieve tactile interactivity
for human-robot interaction (HRI) in soft robotics. By combining a capacitive
touch sensor with an online solid mechanics simulation provided by the SOFA
framework, contact detection is achieved for arbitrary shapes. Furthermore, the
implementation of the capacitive touch technology presented here is selectively
sensitive to human touch (conductive objects), while it is largely unaffected
by the deformations created by the pneumatic actuation of our soft robot.
Multi-touch interactions are also possible. We evaluated our approach with an
organic soft robotics sculpture that was created by a visual artist. In
particular, we evaluate that the touch localization capabilities are robust
under the deformation of the device. We discuss the potential this approach has
for the arts and entertainment as well as other domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 17 figures. Accepted at IEEE Robotics and Automation Letters
  (RA-L) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Active Robot Curriculum Learning from Online Human Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhan Hou, Koen Hindriks, A. E. Eiben, Kim Baraka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning from Demonstrations (LfD) allows robots to learn skills from human
users, but its effectiveness can suffer due to sub-optimal teaching, especially
from untrained demonstrators. Active LfD aims to improve this by letting robots
actively request demonstrations to enhance learning. However, this may lead to
frequent context switches between various task situations, increasing the human
cognitive load and introducing errors to demonstrations. Moreover, few prior
studies in active LfD have examined how these active query strategies may
impact human teaching in aspects beyond user experience, which can be crucial
for developing algorithms that benefit both robot learning and human teaching.
To tackle these challenges, we propose an active LfD method that optimizes the
query sequence of online human demonstrations via Curriculum Learning (CL),
where demonstrators are guided to provide demonstrations in situations of
gradually increasing difficulty. We evaluate our method across four simulated
robotic tasks with sparse rewards and conduct a user study (N=26) to
investigate the influence of active LfD methods on human teaching regarding
teaching performance, post-guidance teaching adaptivity, and teaching
transferability. Our results show that our method significantly improves
learning performance compared to three other LfD baselines in terms of the
final success rate of the converged policy and sample efficiency. Additionally,
results from our user study indicate that our method significantly reduces the
time required from human demonstrators and decreases failed demonstration
attempts. It also enhances post-guidance human teaching in both seen and unseen
scenarios compared to another active LfD baseline, indicating enhanced teaching
performance, greater post-guidance teaching adaptivity, and better teaching
transferability achieved by our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ForaNav: Insect-inspired Online Target-oriented Navigation for MAVs in
  Tree Plantations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijie Kuang, Hann Woei Ho, Ye Zhou, Shahrel Azmin Suandi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous Micro Air Vehicles (MAVs) are becoming essential in precision
agriculture to enhance efficiency and reduce labor costs through targeted,
real-time operations. However, existing unmanned systems often rely on
GPS-based navigation, which is prone to inaccuracies in rural areas and limits
flight paths to predefined routes, resulting in operational inefficiencies. To
address these challenges, this paper presents ForaNav, an insect-inspired
navigation strategy for autonomous navigation in plantations. The proposed
method employs an enhanced Histogram of Oriented Gradient (HOG)-based tree
detection approach, integrating hue-saturation histograms and global HOG
feature variance with hierarchical HOG extraction to distinguish oil palm trees
from visually similar objects. Inspired by insect foraging behavior, the MAV
dynamically adjusts its path based on detected trees and employs a recovery
mechanism to stay on course if a target is temporarily lost. We demonstrate
that our detection method generalizes well to different tree types while
maintaining lower CPU usage, lower temperature, and higher FPS than lightweight
deep learning models, making it well-suited for real-time applications. Flight
test results across diverse real-world scenarios show that the MAV successfully
detects and approaches all trees without prior tree location, validating its
effectiveness for agricultural automation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Fluorescence-Guided Autonomous Robotic Partial Nephrectomy on
  Novel Tissue-Mimicking Hydrogel Phantoms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Kilmer, Joseph Chen, Jiawei Ge, Preksha Sarda, Richard Cha, Kevin Cleary, Lauren Shepard, Ahmed Ezzat Ghazi, Paul Maria Scheikl, Axel Krieger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous robotic systems hold potential for improving renal tumor resection
accuracy and patient outcomes. We present a fluorescence-guided robotic system
capable of planning and executing incision paths around exophytic renal tumors
with a clinically relevant resection margin. Leveraging point cloud
observations, the system handles irregular tumor shapes and distinguishes
healthy from tumorous tissue based on near-infrared imaging, akin to
indocyanine green staining in partial nephrectomy. Tissue-mimicking phantoms
are crucial for the development of autonomous robotic surgical systems for
interventions where acquiring ex-vivo animal tissue is infeasible, such as
cancer of the kidney and renal pelvis. To this end, we propose novel
hydrogel-based kidney phantoms with exophytic tumors that mimic the physical
and visual behavior of tissue, and are compatible with electrosurgical
instruments, a common limitation of silicone-based phantoms. In contrast to
previous hydrogel phantoms, we mix the material with near-infrared dye to
enable fluorescence-guided tumor segmentation. Autonomous real-world robotic
experiments validate our system and phantoms, achieving an average margin
accuracy of 1.44 mm in a completion time of 69 sec.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages. 7 figures. Preprint of an article accepted for publication
  in the Journal of Medical Robotics Research, 2025. Copyright World Scientific
  Publishing Company [https://worldscientific.com/worldscinet/jmrr]</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continual <span class="highlight-title">Multi</span>-Robot Learning from Black-Box Visual Place Recognition
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kenta Tsukahara, Kanji Tanaka, Daiki Iwata, Jonathan Tay Yu Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of visual place recognition (VPR), continual learning (CL)
techniques offer significant potential for avoiding catastrophic forgetting
when learning new places. However, existing CL methods often focus on knowledge
transfer from a known model to a new one, overlooking the existence of unknown
black-box models. We explore a novel multi-robot CL approach that enables
knowledge transfer from black-box VPR models (teachers), such as those of local
robots encountered by traveler robots (students) in unknown environments.
Specifically, we introduce Membership Inference Attack, or MIA, the only major
privacy attack applicable to black-box models, and leverage it to reconstruct
pseudo training sets, which serve as the key knowledge to be exchanged between
robots, from black-box VPR models. Furthermore, we aim to overcome the
inherently low sampling efficiency of MIA by leveraging insights on place class
prediction distribution and un-learned class detection imported from the VPR
literature as a prior distribution. We also analyze both the individual effects
of these methods and their combined impact. Experimental results demonstrate
that our black-box MIA (BB-MIA) approach is remarkably powerful despite its
simplicity, significantly enhancing the VPR capability of lower-performing
robots through brief communication with other robots. This study contributes to
optimizing knowledge sharing between robots in VPR and enhancing autonomy in
open-world environments with multi-robot systems that are fault-tolerant and
scalable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures, technical report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models as Natural Selector for Embodied Soft Robot Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02249v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02249v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changhe Chen, Xiaohao Xu, Xiangdong Wang, Xiaonan Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Designing soft robots is a complex and iterative process that demands
cross-disciplinary expertise in materials science, mechanics, and control,
often relying on intuition and extensive experimentation. While Large Language
Models (LLMs) have demonstrated impressive reasoning abilities, their capacity
to learn and apply embodied design principles--crucial for creating functional
robotic systems--remains largely unexplored. This paper introduces
RoboCrafter-QA, a novel benchmark to evaluate whether LLMs can learn
representations of soft robot designs that effectively bridge the gap between
high-level task descriptions and low-level morphological and material choices.
RoboCrafter-QA leverages the EvoGym simulator to generate a diverse set of soft
robot design challenges, spanning robotic locomotion, manipulation, and
balancing tasks. Our experiments with state-of-the-art multi-modal LLMs reveal
that while these models exhibit promising capabilities in learning design
representations, they struggle with fine-grained distinctions between designs
with subtle performance differences. We further demonstrate the practical
utility of LLMs for robot design initialization. Our code and benchmark will be
available to encourage the community to foster this exciting research
direction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WMNav: Integrating Vision-Language Models into Wo<span class="highlight-title">rl</span>d Models for Object
  Goal Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dujun Nie, Xianda Guo, Yiqun Duan, Ruijun Zhang, Long Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object Goal Navigation-requiring an agent to locate a specific object in an
unseen environment-remains a core challenge in embodied AI. Although recent
progress in Vision-Language Model (VLM)-based agents has demonstrated promising
perception and decision-making abilities through prompting, none has yet
established a fully modular world model design that reduces risky and costly
interactions with the environment by predicting the future state of the world.
We introduce WMNav, a novel World Model-based Navigation framework powered by
Vision-Language Models (VLMs). It predicts possible outcomes of decisions and
builds memories to provide feedback to the policy module. To retain the
predicted state of the environment, WMNav proposes the online maintained
Curiosity Value Map as part of the world model memory to provide dynamic
configuration for navigation policy. By decomposing according to a human-like
thinking process, WMNav effectively alleviates the impact of model
hallucination by making decisions based on the feedback difference between the
world model plan and observation. To further boost efficiency, we implement a
two-stage action proposer strategy: broad exploration followed by precise
localization. Extensive evaluation on HM3D and MP3D validates WMNav surpasses
existing zero-shot benchmarks in both success rate and exploration efficiency
(absolute improvement: +3.2% SR and +3.2% SPL on HM3D, +13.5% SR and +1.1% SPL
on MP3D). Project page: https://b0b8k1ng.github.io/WMNav/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ADMM-MCBF-LCA: A Layered Control Architecture for Safe Real-Time
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anusha Srikanthan, Yifan Xue, Vijay Kumar, Nikolai Matni, Nadia Figueroa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of safe real-time navigation of a robot in a dynamic
environment with moving obstacles of arbitrary smooth geometries and input
saturation constraints. We assume that the robot detects and models nearby
obstacle boundaries with a short-range sensor and that this detection is
error-free. This problem presents three main challenges: i) input constraints,
ii) safety, and iii) real-time computation. To tackle all three challenges, we
present a layered control architecture (LCA) consisting of an offline path
library generation layer, and an online path selection and safety layer. To
overcome the limitations of reactive methods, our offline path library consists
of feasible controllers, feedback gains, and reference trajectories. To handle
computational burden and safety, we solve online path selection and generate
safe inputs that run at 100 Hz. Through simulations on Gazebo and Fetch
hardware in an indoor environment, we evaluate our approach against baselines
that are layered, end-to-end, or reactive. Our experiments demonstrate that
among all algorithms, only our proposed LCA is able to complete tasks such as
reaching a goal, safely. When comparing metrics such as safety, input error,
and success rate, we show that our approach generates safe and feasible inputs
throughout the robot execution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-Shot Sim-to-Real Visual Quadrotor Control with Hard Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02198v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02198v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Miao, Will Shen, Sayan Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the first framework demonstrating zero-shot sim-to-real transfer
of visual control policies learned in a Neural Radiance Field (NeRF)
environment for quadrotors to fly through racing gates. Robust transfer from
simulation to real flight poses a major challenge, as standard simulators often
lack sufficient visual fidelity. To address this, we construct a photorealistic
simulation environment of quadrotor racing tracks, called FalconGym, which
provides effectively unlimited synthetic images for training. Within FalconGym,
we develop a pipelined approach for crossing gates that combines (i) a Neural
Pose Estimator (NPE) coupled with a Kalman filter to reliably infer quadrotor
poses from single-frame RGB images and IMU data, and (ii) a
self-attention-based multi-modal controller that adaptively integrates visual
features and pose estimation. This multi-modal design compensates for
perception noise and intermittent gate visibility. We train this controller
purely in FalconGym with imitation learning and deploy the resulting policy to
real hardware with no additional fine-tuning. Simulation experiments on three
distinct tracks (circle, U-turn and figure-8) demonstrate that our controller
outperforms a vision-only state-of-the-art baseline in both success rate and
gate-crossing accuracy. In 30 live hardware flights spanning three tracks and
120 gates, our controller achieves a 95.8% success rate and an average error of
just 10 cm when flying through 38 cm-radius gates.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RPF-Search: Field-based Search for Robot Person Following in Unknown
  Dynamic Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02188v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02188v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanjing Ye, Kuanqi Cai, Yu Zhan, Bingyi Xia, Arash Ajoudani, Hong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous robot person-following (RPF) systems are crucial for personal
assistance and security but suffer from target loss due to occlusions in
dynamic, unknown environments. Current methods rely on pre-built maps and
assume static environments, limiting their effectiveness in real-world
settings. There is a critical gap in re-finding targets under topographic
(e.g., walls, corners) and dynamic (e.g., moving pedestrians) occlusions. In
this paper, we propose a novel heuristic-guided search framework that
dynamically builds environmental maps while following the target and resolves
various occlusions by prioritizing high-probability areas for locating the
target. For topographic occlusions, a belief-guided search field is constructed
and used to evaluate the likelihood of the target's presence, while for dynamic
occlusions, a fluid-field approach allows the robot to adaptively follow or
overtake moving occluders. Past motion cues and environmental observations
refine the search decision over time. Our results demonstrate that the proposed
method outperforms existing approaches in terms of search efficiency and
success rates, both in simulations and real-world tests. Our target search
method enhances the adaptability and reliability of RPF systems in unknown and
dynamic environments to support their use in real-world applications. Our code,
video, experimental results and appendix are available at
https://medlartea.github.io/rpf-search/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Design and Control of A Tilt-Rotor Tailsitter Aircraft with Pivoting
  VTOL Capability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02158v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02158v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqing Ma, Ewoud J. J. Smeur, Guido C. H. E. de Croon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tailsitter aircraft attract considerable interest due to their capabilities
of both agile hover and high speed forward flight. However, traditional
tailsitters that use aerodynamic control surfaces face the challenge of limited
control effectiveness and associated actuator saturation during vertical flight
and transitions. Conversely, tailsitters relying solely on tilting rotors have
the drawback of insufficient roll control authority in forward flight. This
paper proposes a tilt-rotor tailsitter aircraft with both elevons and tilting
rotors as a promising solution. By implementing a cascaded weighted least
squares (WLS) based incremental nonlinear dynamic inversion (INDI) controller,
the drone successfully achieved autonomous waypoint tracking in outdoor
experiments at a cruise airspeed of 16 m/s, including transitions between
forward flight and hover without actuator saturation. Wind tunnel experiments
confirm improved roll control compared to tilt-rotor-only configurations, while
comparative outdoor flight tests highlight the vehicle's superior control over
elevon-only designs during critical phases such as vertical descent and
transitions. Finally, we also show that the tilt-rotors allow for an autonomous
takeoff and landing with a unique pivoting capability that demonstrates
stability and robustness under wind disturbances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Four Principles for Physically Interpretable Wo<span class="highlight-title">rl</span>d Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jordan Peper, Zhenjiang Mao, Yuang Geng, Siyuan Pan, Ivan Ruchkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As autonomous systems are increasingly deployed in open and uncertain
settings, there is a growing need for trustworthy world models that can
reliably predict future high-dimensional observations. The learned latent
representations in world models lack direct mapping to meaningful physical
quantities and dynamics, limiting their utility and interpretability in
downstream planning, control, and safety verification. In this paper, we argue
for a fundamental shift from physically informed to physically interpretable
world models - and crystallize four principles that leverage symbolic knowledge
to achieve these ends: (1) structuring latent spaces according to the physical
intent of variables, (2) learning aligned invariant and equivariant
representations of the physical world, (3) adapting training to the varied
granularity of supervision signals, and (4) partitioning generative outputs to
support scalability and verifiability. We experimentally demonstrate the value
of each principle on two benchmarks. This paper opens several intriguing
research directions to achieve and capitalize on full physical interpretability
in world models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Equal contribution by the first two authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ArticuBot: Learning Universal Articulated Object Manipulation Policy via
  Large Scale Simulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03045v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03045v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yufei Wang, Ziyu Wang, Mino Nakura, Pratik Bhowal, Chia-Liang Kuo, Yi-Ting Chen, Zackory Erickson, David Held
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents ArticuBot, in which a single learned policy enables a
robotics system to open diverse categories of unseen articulated objects in the
real world. This task has long been challenging for robotics due to the large
variations in the geometry, size, and articulation types of such objects. Our
system, Articubot, consists of three parts: generating a large number of
demonstrations in physics-based simulation, distilling all generated
demonstrations into a point cloud-based neural policy via imitation learning,
and performing zero-shot sim2real transfer to real robotics systems. Utilizing
sampling-based grasping and motion planning, our demonstration generalization
pipeline is fast and effective, generating a total of 42.3k demonstrations over
322 training articulated objects. For policy learning, we propose a novel
hierarchical policy representation, in which the high-level policy learns the
sub-goal for the end-effector, and the low-level policy learns how to move the
end-effector conditioned on the predicted goal. We demonstrate that this
hierarchical approach achieves much better object-level generalization compared
to the non-hierarchical version. We further propose a novel weighted
displacement model for the high-level policy that grounds the prediction into
the existing 3D structure of the scene, outperforming alternative policy
representations. We show that our learned policy can zero-shot transfer to
three different real robot settings: a fixed table-top Franka arm across two
different labs, and an X-Arm on a mobile base, opening multiple unseen
articulated objects across two labs, real lounges, and kitchens. Videos and
code can be found on our project website: https://articubot.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Multi</span>-Step Deep Koopman Network (MDK-Net) for Vehicle Control in Frenet
  Frame <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.03002v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.03002v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Abtahi, Mahdis Rabbani, Armin Abdolmohammadi, Shima Nazari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The highly nonlinear dynamics of vehicles present a major challenge for the
practical implementation of optimal and Model Predictive Control (MPC)
approaches in path planning and following. Koopman operator theory offers a
global linear representation of nonlinear dynamical systems, making it a
promising framework for optimization-based vehicle control. This paper
introduces a novel deep learning-based Koopman modeling approach that employs
deep neural networks to capture the full vehicle dynamics-from pedal and
steering inputs to chassis states-within a curvilinear Frenet frame. The
superior accuracy of the Koopman model compared to identified linear models is
shown for a double lane change maneuver. Furthermore, it is shown that an MPC
controller deploying the Koopman model provides significantly improved
performance while maintaining computational efficiency comparable to a linear
MPC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted for IROS 2025 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RAILGUN: A Unified Convolutional Policy for <span class="highlight-title">Multi</span>-Agent Path Finding
  Across Different Environments and Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02992v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02992v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yimin Tang, Xiao Xiong, Jingyi Xi, Jiaoyang Li, Erdem Bıyık, Sven Koenig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-Agent Path Finding (MAPF), which focuses on finding collision-free
paths for multiple robots, is crucial for applications ranging from aerial
swarms to warehouse automation. Solving MAPF is NP-hard so learning-based
approaches for MAPF have gained attention, particularly those leveraging deep
neural networks. Nonetheless, despite the community's continued efforts, all
learning-based MAPF planners still rely on decentralized planning due to
variability in the number of agents and map sizes. We have developed the first
centralized learning-based policy for MAPF problem called RAILGUN. RAILGUN is
not an agent-based policy but a map-based policy. By leveraging a CNN-based
architecture, RAILGUN can generalize across different maps and handle any
number of agents. We collect trajectories from rule-based methods to train our
model in a supervised way. In experiments, RAILGUN outperforms most baseline
methods and demonstrates great zero-shot generalization capabilities on various
tasks, maps and agent numbers that were not seen in the training dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Monocular visual simultaneous localization and mapping: (r)evolution
  from geometry to deep learning-based pipelines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02955v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02955v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olaya Alvarez-Tunon, Yury Brodskiy, Erdal Kayacan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rise of deep learning, there is a fundamental change in visual SLAM
algorithms toward developing different modules trained as end-to-end pipelines.
However, regardless of the implementation domain, visual SLAM's performance is
subject to diverse environmental challenges, such as dynamic elements in
outdoor environments, harsh imaging conditions in underwater environments, or
blurriness in high-speed setups. These environmental challenges need to be
identified to study the real-world viability of SLAM implementations. Motivated
by the aforementioned challenges, this paper surveys the current state of
visual SLAM algorithms according to the two main frameworks: geometry-based and
learning-based SLAM. First, we introduce a general formulation of the SLAM
pipeline that includes most of the implementations in the literature. Second,
those implementations are classified and surveyed for geometry and
learning-based SLAM. After that, environment-specific challenges are formulated
to enable experimental evaluation of the resilience of different visual SLAM
classes to varying imaging conditions. We address two significant issues in
surveying visual SLAM, providing (1) a consistent classification of visual SLAM
pipelines and (2) a robust evaluation of their performance under different
deployment conditions. Finally, we give our take on future opportunities for
visual SLAM implementations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reliable and Efficient <span class="highlight-title">Multi</span>-Agent Coordination via Graph Neural Network
  Variational Autoencoders <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Meng, Nathalie Majcherczyk, Wenliang Liu, Scott Kiesel, Chuchu Fan, Federico Pecora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent coordination is crucial for reliable multi-robot navigation in
shared spaces such as automated warehouses. In regions of dense robot traffic,
local coordination methods may fail to find a deadlock-free solution. In these
scenarios, it is appropriate to let a central unit generate a global schedule
that decides the passing order of robots. However, the runtime of such
centralized coordination methods increases significantly with the problem
scale. In this paper, we propose to leverage Graph Neural Network Variational
Autoencoders (GNN-VAE) to solve the multi-agent coordination problem at scale
faster than through centralized optimization. We formulate the coordination
problem as a graph problem and collect ground truth data using a Mixed-Integer
Linear Program (MILP) solver. During training, our learning framework encodes
good quality solutions of the graph problem into a latent space. At inference
time, solution samples are decoded from the sampled latent variables, and the
lowest-cost sample is selected for coordination. Finally, the feasible proposal
with the highest performance index is selected for the deployment. By
construction, our GNN-VAE framework returns solutions that always respect the
constraints of the considered coordination problem. Numerical results show that
our approach trained on small-scale problems can achieve high-quality solutions
even for large-scale problems with 250 robots, being much faster than other
baselines. Project page: https://mengyuest.github.io/gnn-vae-coord
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by 2025 International Conference on Robotics and Automation
  (ICRA 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Diverse Controllable Diffusion Policy with Signal Temporal Logic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02924v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02924v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Meng, Chuchu fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating realistic simulations is critical for autonomous system
applications such as self-driving and human-robot interactions. However,
driving simulators nowadays still have difficulty in generating controllable,
diverse, and rule-compliant behaviors for road participants: Rule-based models
cannot produce diverse behaviors and require careful tuning, whereas
learning-based methods imitate the policy from data but are not designed to
follow the rules explicitly. Besides, the real-world datasets are by nature
"single-outcome", making the learning method hard to generate diverse
behaviors. In this paper, we leverage Signal Temporal Logic (STL) and Diffusion
Models to learn controllable, diverse, and rule-aware policy. We first
calibrate the STL on the real-world data, then generate diverse synthetic data
using trajectory optimization, and finally learn the rectified diffusion policy
on the augmented dataset. We test on the NuScenes dataset and our approach can
achieve the most diverse rule-compliant trajectories compared to other
baselines, with a runtime 1/17X to the second-best approach. In the closed-loop
testing, our approach reaches the highest diversity, rule satisfaction rate,
and the least collision rate. Our method can generate varied characteristics
conditional on different STL parameters in testing. A case study on human-robot
encounter scenarios shows our approach can generate diverse and
closed-to-oracle trajectories. The annotation tool, augmented dataset, and code
are available at https://github.com/mengyuest/pSTL-diffusion-policy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Robotics and Automation Letters (RA-L), October 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Monocular Person Localization under Camera Ego-motion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02916v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02916v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhan, Hanjing Ye, Hong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Localizing a person from a moving monocular camera is critical for
Human-Robot Interaction (HRI). To estimate the 3D human position from a 2D
image, existing methods either depend on the geometric assumption of a fixed
camera or use a position regression model trained on datasets containing little
camera ego-motion. These methods are vulnerable to fierce camera ego-motion,
resulting in inaccurate person localization. We consider person localization as
a part of a pose estimation problem. By representing a human with a four-point
model, our method jointly estimates the 2D camera attitude and the person's 3D
location through optimization. Evaluations on both public datasets and real
robot experiments demonstrate our method outperforms baselines in person
localization accuracy. Our method is further implemented into a
person-following system and deployed on an agile quadruped robot.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Robust <span class="highlight-title">Multi</span>-UAV Collaboration: MA<span class="highlight-title">RL</span> with Noise-Resilient
  Communication and Attention Mechanisms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02913v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02913v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zilin Zhao, Chishui Chen, Haotian Shi, Jiale Chen, Xuanlin Yue, Zhejian Yang, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient path planning for unmanned aerial vehicles (UAVs) is crucial in
remote sensing and information collection. As task scales expand, the
cooperative deployment of multiple UAVs significantly improves information
collection efficiency. However, collaborative communication and decision-making
for multiple UAVs remain major challenges in path planning, especially in noisy
environments. To efficiently accomplish complex information collection tasks in
3D space and address robust communication issues, we propose a multi-agent
reinforcement learning (MARL) framework for UAV path planning based on the
Counterfactual Multi-Agent Policy Gradients (COMA) algorithm. The framework
incorporates attention mechanism-based UAV communication protocol and
training-deployment system, significantly improving communication robustness
and individual decision-making capabilities in noisy conditions. Experiments
conducted on both synthetic and real-world datasets demonstrate that our method
outperforms existing algorithms in terms of path planning efficiency and
robustness, especially in noisy environments, achieving a 78\% improvement in
entropy reduction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Psycho Gundam: Electroencephalography based real-time robotic control
  system with deep learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06414v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06414v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi-Sheng Chen, Wei-Sheng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Psycho Frame, a sophisticated system primarily used in Universal Century
(U.C.) series mobile suits for NEWTYPE pilots, has evolved as an integral
component in harnessing the latent potential of mental energy. Its ability to
amplify and resonate with the pilot's psyche enables real-time mental control,
creating unique applications such as psychomagnetic fields and sensory-based
weaponry. This paper presents the development of a novel robotic control system
inspired by the Psycho Frame, combining electroencephalography (EEG) and deep
learning for real-time control of robotic systems. By capturing and
interpreting brainwave data through EEG, the system extends human cognitive
commands to robotic actions, reflecting the seamless synchronization of thought
and machine, much like the Psyco Frame's integration with a Newtype pilot's
mental faculties. This research demonstrates how modern AI techniques can
expand the limits of human-machine interaction, potentially transcending
traditional input methods and enabling a deeper, more intuitive control of
complex robotic systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ λ: A Benchmark for Data-Efficiency in Long-Horizon Indoor Mobile
  Manipulation Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05313v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05313v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Jaafar, Shreyas Sundara Raman, Yichen Wei, Sudarshan Harithas, Sofia Juliani, Anneke Wernerfelt, Benedict Quartey, Ifrah Idrees, Jason Xinyu Liu, Stefanie Tellex
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning to execute long-horizon mobile manipulation tasks is crucial for
advancing robotics in household and workplace settings. However, current
approaches are typically data-inefficient, underscoring the need for improved
models that require realistically sized benchmarks to evaluate their
efficiency. To address this, we introduce the LAMBDA ({\lambda})
benchmark-Long-horizon Actions for Mobile-manipulation Benchmarking of Directed
Activities-which evaluates the data efficiency of models on
language-conditioned, long-horizon, multi-room, multi-floor, pick-and-place
tasks using a dataset of manageable size, more feasible for collection. Our
benchmark includes 571 human-collected demonstrations that provide realism and
diversity in simulated and real-world settings. Unlike planner-generated data,
these trajectories offer natural variability and replay-verifiability, ensuring
robust learning and evaluation. We leverage LAMBDA to benchmark current
end-to-end learning methods and a modular neuro-symbolic approaches that
combines foundation models with task and motion planning. We find that
end-to-end methods-even when pretrained-yield lower success rates, while
neuro-symbolic methods perform significantly better and require less data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Internal Model Control: Learning a Robust Control Policy via
  Predictive Error Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.13079v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.13079v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Gao, Chao Yu, Yu Wang, Yi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate motion control in the face of disturbances within complex
environments remains a major challenge in robotics. Classical model-based
approaches often struggle with nonlinearities and unstructured disturbances,
while RL-based methods can be fragile when encountering unseen scenarios. In
this paper, we propose a novel framework, Neural Internal Model Control, which
integrates model-based control with RL-based control to enhance robustness. Our
framework streamlines the predictive model by applying Newton-Euler equations
for rigid-body dynamics, eliminating the need to capture complex
high-dimensional nonlinearities. This internal model combines model-free RL
algorithms with predictive error feedback. Such a design enables a closed-loop
control structure to enhance the robustness and generalizability of the control
system. We demonstrate the effectiveness of our framework on both quadrotors
and quadrupedal robots, achieving superior performance compared to
state-of-the-art methods. Furthermore, real-world deployment on a quadrotor
with rope-suspended payloads highlights the framework's robustness in
sim-to-real transfer. Our code is released at
https://github.com/thu-uav/NeuralIMC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to RAL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UniGraspTransformer: Simplified Policy Distillation for Scalable
  Dexterous Robotic Grasping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.02699v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.02699v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenbo Wang, Fangyun Wei, Lei Zhou, Xi Chen, Lin Luo, Xiaohan Yi, Yizhong Zhang, Yaobo Liang, Chang Xu, Yan Lu, Jiaolong Yang, Baining Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce UniGraspTransformer, a universal Transformer-based network for
dexterous robotic grasping that simplifies training while enhancing scalability
and performance. Unlike prior methods such as UniDexGrasp++, which require
complex, multi-step training pipelines, UniGraspTransformer follows a
streamlined process: first, dedicated policy networks are trained for
individual objects using reinforcement learning to generate successful grasp
trajectories; then, these trajectories are distilled into a single, universal
network. Our approach enables UniGraspTransformer to scale effectively,
incorporating up to 12 self-attention blocks for handling thousands of objects
with diverse poses. Additionally, it generalizes well to both idealized and
real-world inputs, evaluated in state-based and vision-based settings. Notably,
UniGraspTransformer generates a broader range of grasping poses for objects in
various shapes and orientations, resulting in more diverse grasp strategies.
Experimental results demonstrate significant improvements over
state-of-the-art, UniDexGrasp++, across various object categories, achieving
success rate gains of 3.5%, 7.7%, and 10.1% on seen objects, unseen objects
within seen categories, and completely unseen objects, respectively, in the
vision-based setting. Project page:
https://dexhand.github.io/UniGraspTransformer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2025. Project page:
  https://dexhand.github.io/UniGraspTransformer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Mixing Laser Interferometry for Robotic Tactile Sensing <span class="chip">ICRA2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15390v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15390v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Remko Proesmans, Ward Goossens, Lowiek Van den Stockt, Lowie Christiaen, Francis wyffels
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-mixing interferometry (SMI) has been lauded for its sensitivity in
detecting microvibrations, while requiring no physical contact with its target.
In robotics, microvibrations have traditionally been interpreted as a marker
for object slip, and recently as a salient indicator of extrinsic contact. We
present the first-ever robotic fingertip making use of SMI for slip and
extrinsic contact sensing. The design is validated through measurement of
controlled vibration sources, both before and after encasing the readout
circuit in its fingertip package. Then, the SMI fingertip is compared to
acoustic sensing through four experiments. The results are distilled into a
technology decision map. SMI was found to be more sensitive to subtle slip
events and significantly more resilient against ambient noise. We conclude that
the integration of SMI in robotic fingertips offers a new, promising branch of
tactile sensing in robotics. Design and data files are available at
https://github.com/RemkoPr/icra2025-SMI-tactile-sensing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Final version for IEEE ICRA2025 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Roadside Unit for Infrastructure Assisted Intersection Control of
  Autonomous Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.00866v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.00866v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Evans, Marcial Machado, Rickey Johnson, Anna Vadella, Luis Escamilla, Beñat Froemming-Aldanondo, Tatiana Rastoskueva, Milan Jostes, Devson Butani, Ryan Kaddis, Chan-Jin Chung, Joshua Siegel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in autonomous vehicle technologies and cellular network
speeds motivate developments in vehicle-to-everything (V2X) communications.
Enhanced road safety features and improved fuel efficiency are some of the
motivations behind V2X for future transportation systems. Adaptive intersection
control systems have considerable potential to achieve these goals by
minimizing idle times and predicting short-term future traffic conditions.
Integrating V2X into traffic management systems introduces the infrastructure
necessary to make roads safer for all users and initiates the shift towards
more intelligent and connected cities. To demonstrate our control algorithm, we
implement both a simulated and real-world representation of a 4-way
intersection and crosswalk scenario with 2 self-driving electric vehicles, a
roadside unit (RSU), and a traffic light. Our architecture reduces acceleration
and braking through intersections by up to 75.35%, which has been shown to
minimize fuel consumption in gas vehicles. We propose a cost-effective solution
to intelligent and connected intersection control to serve as a
proof-of-concept model suitable as the basis for continued research and
development. Code for this project is available at
https://github.com/MMachado05/REU-2024.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Supported by the National Science Foundation under Grants No. 2150292
  and 2150096</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robustness of LiDAR-Based Pose Estimation: Evaluating and Improving
  Odometry and Localization Under Common Point Cloud Corruptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10824v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10824v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Yang, Tri Minh Triet Pham, Jinqiu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and reliable pose estimation, i.e., determining the precise position
and orientation of autonomous robots and vehicles, is critical for tasks like
navigation and mapping. LiDAR is a widely used sensor for pose estimation, with
odometry and localization being two primary tasks. LiDAR odometry estimates the
relative motion between consecutive scans, while LiDAR localization aligns
real-time scans with a pre-recorded map to obtain a global pose. Although they
have different objectives and application scenarios, both rely on point cloud
registration as the underlying technique and face shared challenges of data
corruption caused by adverse conditions (e.g., rain). While state-of-the-art
(SOTA) pose estimation systems achieved high accuracy on clean data, their
robustness to corrupted data remains unclear. In this work, we propose a
framework to systematically evaluate five SOTA LiDAR pose estimation systems
across 18 synthetic real-world point cloud corruptions. Our experiments reveal
that odometry systems degrade significantly under specific corruptions, with
relative position errors increasing from 0.5% to more than 80%, while
localization systems remain highly robust. We further demonstrate that
denoising techniques can effectively mitigate the adverse effects of
noise-induced corruptions, and re-training learning-based systems with
corrupted data significantly enhances the robustness against various corruption
types.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discovering Antagonists in Networks of Systems: Robot Deployment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20125v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20125v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ingeborg Wenger, Peter Eberhard, Henrik Ebel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A contextual anomaly detection method is proposed and applied to the physical
motions of a robot swarm executing a coverage task. Using simulations of a
swarm's normal behavior, a normalizing flow is trained to predict the
likelihood of a robot motion within the current context of its environment.
During application, the predicted likelihood of the observed motions is used by
a detection criterion that categorizes a robot agent as normal or antagonistic.
The proposed method is evaluated on five different strategies of antagonistic
behavior. Importantly, only readily available simulated data of normal robot
behavior is used for training such that the nature of the anomalies need not be
known beforehand. The best detection criterion correctly categorizes at least
80% of each antagonistic type while maintaining a false positive rate of less
than 5% for normal robot agents. Additionally, the method is validated in
hardware experiments, yielding results similar to the simulated scenarios.
Compared to the state-of-the-art approach, both the predictive performance of
the normalizing flow and the robustness of the detection criterion are
increased.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>reduced file size</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do Visual-Language Grid Maps Capture Latent Semantics? <span class="chip">IROS-2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.10117v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.10117v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matti Pekkanen, Tsvetomila Mihaylova, Francesco Verdoja, Ville Kyrki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual-language models (VLMs) have recently been introduced in robotic
mapping using the latent representations, i.e., embeddings, of the VLMs to
represent semantics in the map. They allow moving from a limited set of
human-created labels toward open-vocabulary scene understanding, which is very
useful for robots when operating in complex real-world environments and
interacting with humans. While there is anecdotal evidence that maps built this
way support downstream tasks, such as navigation, rigorous analysis of the
quality of the maps using these embeddings is missing. In this paper, we
propose a way to analyze the quality of maps created using VLMs. We investigate
two critical properties of map quality: queryability and distinctness. The
evaluation of queryability addresses the ability to retrieve information from
the embeddings. We investigate intra-map distinctness to study the ability of
the embeddings to represent abstract semantic classes and inter-map
distinctness to evaluate the generalization properties of the representation.
We propose metrics to evaluate these properties and evaluate two
state-of-the-art mapping methods, VLMaps and OpenScene, using two encoders,
LSeg and OpenSeg, using real-world data from the Matterport3D data set. Our
findings show that while 3D features improve queryability, they are not scale
invariant, whereas image-based embeddings generalize to multiple map
resolutions. This allows the image-based methods to maintain smaller map sizes,
which can be crucial for using these methods in real-world deployments.
Furthermore, we show that the choice of the encoder has an effect on the
results. The results imply that properly thresholding open-vocabulary queries
is an open problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE-IROS-2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Night-Voyager: Consistent and Efficient Nocturnal Vision-Aided State
  Estimation in Object Maps <span class="chip">T-RO</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20054v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20054v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianxiao Gao, Mingle Zhao, Chengzhong Xu, Hui Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and robust state estimation at nighttime is essential for autonomous
robotic navigation to achieve nocturnal or round-the-clock tasks. An intuitive
question arises: Can low-cost standard cameras be exploited for nocturnal state
estimation? Regrettably, most existing visual methods may fail under adverse
illumination conditions, even with active lighting or image enhancement. A
pivotal insight, however, is that streetlights in most urban scenarios act as
stable and salient prior visual cues at night, reminiscent of stars in deep
space aiding spacecraft voyage in interstellar navigation. Inspired by this, we
propose Night-Voyager, an object-level nocturnal vision-aided state estimation
framework that leverages prior object maps and keypoints for versatile
localization. We also find that the primary limitation of conventional visual
methods under poor lighting conditions stems from the reliance on pixel-level
metrics. In contrast, metric-agnostic, non-pixel-level object detection serves
as a bridge between pixel-level and object-level spaces, enabling effective
propagation and utilization of object map information within the system.
Night-Voyager begins with a fast initialization to solve the global
localization problem. By employing an effective two-stage cross-modal data
association, the system delivers globally consistent state updates using
map-based observations. To address the challenge of significant uncertainties
in visual observations at night, a novel matrix Lie group formulation and a
feature-decoupled multi-state invariant filter are introduced, ensuring
consistent and efficient estimation. Through comprehensive experiments in both
simulation and diverse real-world scenarios (spanning approximately 12.3 km),
Night-Voyager showcases its efficacy, robustness, and efficiency, filling a
critical gap in nocturnal vision-aided state estimation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE Transactions on Robotics (T-RO), 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Equivariant Filter for Tightly Coupled LiDAR-Inertial Odometry <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06948v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06948v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anbo Tao, Yarong Luo, Chunxi Xia, Chi Guo, Xingxing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pose estimation is a crucial problem in simultaneous localization and mapping
(SLAM). However, developing a robust and consistent state estimator remains a
significant challenge, as the traditional extended Kalman filter (EKF)
struggles to handle the model nonlinearity, especially for inertial measurement
unit (IMU) and light detection and ranging (LiDAR). To provide a consistent and
efficient solution of pose estimation, we propose Eq-LIO, a robust state
estimator for tightly coupled LIO systems based on an equivariant filter (EqF).
Compared with the invariant Kalman filter based on the $\SE_2(3)$ group
structure, the EqF uses the symmetry of the semi-direct product group to couple
the system state including IMU bias, navigation state and LiDAR extrinsic
calibration state, thereby suppressing linearization error and improving the
behavior of the estimator in the event of unexpected state changes. The
proposed Eq-LIO owns natural consistency and higher robustness, which is
theoretically proven with mathematical derivation and experimentally verified
through a series of tests on both public and private datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safe <span class="highlight-title">Distributed</span> Control of <span class="highlight-title">Multi</span>-Robot Systems with Communication
  Delays 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09382v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09382v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Ballotta, Rajat Talak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safe operation of multi-robot systems is critical, especially in
communication-degraded environments such as underwater for seabed mapping,
underground caves for navigation, and in extraterrestrial missions for assembly
and construction. We address safety of networked autonomous systems where the
information exchanged between robots incurs communication delays. We formalize
a notion of distributed control barrier function for multi-robot systems, a
safety certificate amenable to a distributed implementation, which provides
formal ground to using graph neural networks to learn safe distributed
controllers. Further, we observe that learning a distributed controller
ignoring delays can severely degrade safety. We finally propose a
predictor-based framework to train a safe distributed controller under
communication delays, where the current state of nearby robots is predicted
from received data and age-of-information. Numerical experiments on multi-robot
collision avoidance show that our predictor-based approach can significantly
improve the safety of a learned distributed controller under communication
delays. A video abstract is available at https://youtu.be/Hcu1Ri32Spk.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Copyright (c) 2025 IEEE. Personal use of this material is permitted.
  However, permission to use this material for any other purposes must be
  obtained from the IEEE by sending a request to pubs-permissions@ieee.org</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PlanScope: Learning to Plan Within Decision Scope Does Matter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.00476v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.00476v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ren Xin, Jie Cheng, Hongji Liu, Jun Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the context of autonomous driving, learning-based methods have been
promising for the development of planning modules. During the training process
of planning modules, directly minimizing the discrepancy between expert-driving
logs and planning output is widely deployed. In general, driving logs consist
of suddenly appearing obstacles or swiftly changing traffic signals, which
typically necessitate swift and nuanced adjustments in driving maneuvers.
Concurrently, future trajectories of the vehicles exhibit their long-term
decisions, such as adhering to a reference lane or circumventing stationary
obstacles. Due to the unpredictable influence of future events in driving logs,
reasoning bias could be naturally introduced to learning based planning
modules, which leads to a possible degradation of driving performance. To
address this issue, we identify the decisions and their corresponding time
horizons, and characterize a so-called decision scope by retaining decisions
within derivable horizons only, to mitigate the effect of irrational behaviors
caused by unpredictable events. Several viable implementations have been
proposed, among which batch normalization along the temporal dimension is
particularly effective and achieves superior performance. It consistently
outperforms baseline methods in terms of driving scores, as demonstrated
through closed-loop evaluations on the nuPlan dataset. Essentially, this
approach accommodates an appealing plug-and-play feature to enhance the
closed-loop performance of other learning-based planning models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Refine Input Constrained Control Barrier Functions via
  Uncertainty-Aware Online Parameter Adaptation <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14616v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14616v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taekyung Kim, Robin Inho Kee, Dimitra Panagou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Control Barrier Functions (CBFs) have become powerful tools for ensuring
safety in nonlinear systems. However, finding valid CBFs that guarantee
persistent safety and feasibility remains an open challenge, especially in
systems with input constraints. Traditional approaches often rely on manually
tuning the parameters of the class K functions of the CBF conditions a priori.
The performance of CBF-based controllers is highly sensitive to these fixed
parameters, potentially leading to overly conservative behavior or safety
violations. To overcome these issues, this paper introduces a learning-based
optimal control framework for online adaptation of Input Constrained CBF
(ICCBF) parameters in discrete-time nonlinear systems. Our method employs a
probabilistic ensemble neural network to predict the performance and risk
metrics, as defined in this work, for candidate parameters, accounting for both
epistemic and aleatoric uncertainties. We propose a two-step verification
process using Jensen-Renyi Divergence and distributionally-robust Conditional
Value at Risk to identify valid parameters. This enables dynamic refinement of
ICCBF parameters based on current state and nearby environments, optimizing
performance while ensuring safety within the verified parameter set.
Experimental results demonstrate that our method outperforms both
fixed-parameter and existing adaptive methods in robot navigation scenarios
across safety and performance metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2025 IEEE International Conference on Robotics and Automation (ICRA).
  Project page: https://www.taekyung.me/online-adaptive-cbf</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Geometry-aware <span class="highlight-title">RL</span> for Manipulation of Varying Shapes and Deformable
  Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07005v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07005v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tai Hoang, Huy Le, Philipp Becker, Vien Anh Ngo, Gerhard Neumann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Manipulating objects with varying geometries and deformable objects is a
major challenge in robotics. Tasks such as insertion with different objects or
cloth hanging require precise control and effective modelling of complex
dynamics. In this work, we frame this problem through the lens of a
heterogeneous graph that comprises smaller sub-graphs, such as actuators and
objects, accompanied by different edge types describing their interactions.
This graph representation serves as a unified structure for both rigid and
deformable objects tasks, and can be extended further to tasks comprising
multiple actuators. To evaluate this setup, we present a novel and challenging
reinforcement learning benchmark, including rigid insertion of diverse objects,
as well as rope and cloth manipulation with multiple end-effectors. These tasks
present a large search space, as both the initial and target configurations are
uniformly sampled in 3D space. To address this issue, we propose a novel
graph-based policy model, dubbed Heterogeneous Equivariant Policy (HEPi),
utilizing $SE(3)$ equivariant message passing networks as the main backbone to
exploit the geometric symmetry. In addition, by modeling explicit
heterogeneity, HEPi can outperform Transformer-based and non-heterogeneous
equivariant policies in terms of average returns, sample efficiency, and
generalization to unseen objects. Our project page is available at
https://thobotics.github.io/hepi.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2025 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MA<span class="highlight-title">RL</span>IN: <span class="highlight-title">Multi</span>-Agent Reinforcement Learning Guided by Language-Based
  Inter-Robot Negotiation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14383v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14383v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toby Godfrey, William Hunt, Mohammad D. Soorati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent reinforcement learning is a key method for training multi-robot
systems over a series of episodes in which robots are rewarded or punished
according to their performance; only once the system is trained to a suitable
standard is it deployed in the real world. If the system is not trained enough,
the task will likely not be completed and could pose a risk to the surrounding
environment. We introduce Multi-Agent Reinforcement Learning guided by
Language-based Inter-Robot Negotiation (MARLIN), in which the training process
requires fewer training episodes to reach peak performance. Robots are equipped
with large language models that negotiate and debate a task, producing plans
used to guide the policy during training. The approach dynamically switches
between using reinforcement learning and large language model-based action
negotiation throughout training. This reduces the number of training episodes
required, compared to standard multi-agent reinforcement learning, and hence
allows the system to be deployed to physical hardware earlier. The performance
of this approach is evaluated against multi-agent reinforcement learning,
showing that our hybrid method achieves comparable results with significantly
reduced training time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenFly: A Versatile Toolchain and Large-scale Benchmark for Aerial
  Vision-Language Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18041v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18041v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunpeng Gao, Chenhui Li, Zhongrui You, Junli Liu, Zhen Li, Pengan Chen, Qizhi Chen, Zhonghan Tang, Liansheng Wang, Penghui Yang, Yiwen Tang, Yuhang Tang, Shuai Liang, Songyi Zhu, Ziqin Xiong, Yifei Su, Xinyi Ye, Jianan Li, Yan Ding, Dong Wang, Zhigang Wang, Bin Zhao, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Navigation (VLN) aims to guide agents through an environment
by leveraging both language instructions and visual cues, playing a pivotal
role in embodied AI. Indoor VLN has been extensively studied, whereas outdoor
aerial VLN remains underexplored. The potential reason is that outdoor aerial
view encompasses vast areas, making data collection more challenging, which
results in a lack of benchmarks. To address this problem, we propose OpenFly, a
platform comprising a versatile toolchain and large-scale benchmark for aerial
VLN. Firstly, we develop a highly automated toolchain for data collection,
enabling automatic point cloud acquisition, scene semantic segmentation, flight
trajectory creation, and instruction generation. Secondly, based on the
toolchain, we construct a large-scale aerial VLN dataset with 100k
trajectories, covering diverse heights and lengths across 18 scenes. The
corresponding visual data are generated using various rendering engines and
advanced techniques, including Unreal Engine, GTA V, Google Earth, and 3D
Gaussian Splatting (3D GS). All data exhibit high visual quality. Particularly,
3D GS supports real-to-sim rendering, further enhancing the realism of the
dataset. Thirdly, we propose OpenFly-Agent, a keyframe-aware VLN model, which
takes language instructions, current observations, and historical keyframes as
input, and outputs flight actions directly. Extensive analyses and experiments
are conducted, showcasing the superiority of our OpenFly platform and
OpenFly-Agent. The toolchain, dataset, and codes will be open-sourced.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unified Video Action Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.00200v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.00200v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuang Li, Yihuai Gao, Dorsa Sadigh, Shuran Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A unified video and action model holds significant promise for robotics,
where videos provide rich scene information for action prediction, and actions
provide dynamics information for video prediction. However, effectively
combining video generation and action prediction remains challenging, and
current video generation-based methods struggle to match the performance of
direct policy learning in action accuracy and inference speed. To bridge this
gap, we introduce the Unified Video Action model (UVA), which jointly optimizes
video and action predictions to achieve both high accuracy and efficient action
inference. The key lies in learning a joint video-action latent representation
and decoupling video-action decoding. The joint latent representation bridges
the visual and action domains, effectively modeling the relationship between
video and action sequences. Meanwhile, the decoupled decoding, powered by two
lightweight diffusion heads, enables high-speed action inference by bypassing
video generation during inference. Such a unified framework further enables
versatile functionality through masked input training. By selectively masking
actions or videos, a single model can tackle diverse tasks beyond policy
learning, such as forward and inverse dynamics modeling and video generation.
Via an extensive set of experiments, we demonstrate that UVA can serve as a
general-purpose solution for a wide range of robotics tasks, such as policy
learning, forward/inverse dynamics and video observation prediction, without
compromising performance compared to methods tailored for specific
applications. Results are best viewed on
https://unified-video-action-model.github.io/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://unified-video-action-model.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Vision-Language-Action Models for Embodied AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14093v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14093v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueen Ma, Zixing Song, Yuzheng Zhuang, Jianye Hao, Irwin King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embodied AI is widely recognized as a key element of artificial general
intelligence because it involves controlling embodied agents to perform tasks
in the physical world. Building on the success of large language models and
vision-language models, a new category of multimodal models -- referred to as
vision-language-action models (VLAs) -- has emerged to address
language-conditioned robotic tasks in embodied AI by leveraging their distinct
ability to generate actions. In recent years, a myriad of VLAs have been
developed, making it imperative to capture the rapidly evolving landscape
through a comprehensive survey. To this end, we present the first survey on
VLAs for embodied AI. This work provides a detailed taxonomy of VLAs, organized
into three major lines of research. The first line focuses on individual
components of VLAs. The second line is dedicated to developing control policies
adept at predicting low-level actions. The third line comprises high-level task
planners capable of decomposing long-horizon tasks into a sequence of subtasks,
thereby guiding VLAs to follow more general user instructions. Furthermore, we
provide an extensive summary of relevant resources, including datasets,
simulators, and benchmarks. Finally, we discuss the challenges faced by VLAs
and outline promising future directions in embodied AI. We have created a
project associated with this survey, which is available at
https://github.com/yueen-ma/Awesome-VLA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://github.com/yueen-ma/Awesome-VLA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FABG : End-to-end Imitation Learning for Embodied Affective Human-Robot
  Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01363v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01363v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanghai Zhang, Changyi Liu, Keting Fu, Wenbin Zhou, Qingdu Li, Jianwei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes FABG (Facial Affective Behavior Generation), an
end-to-end imitation learning system for human-robot interaction, designed to
generate natural and fluid facial affective behaviors. In interaction,
effectively obtaining high-quality demonstrations remains a challenge. In this
work, we develop an immersive virtual reality (VR) demonstration system that
allows operators to perceive stereoscopic environments. This system ensures
"the operator's visual perception matches the robot's sensory input" and "the
operator's actions directly determine the robot's behaviors" - as if the
operator replaces the robot in human interaction engagements. We propose a
prediction-driven latency compensation strategy to reduce robotic reaction
delays and enhance interaction fluency. FABG naturally acquires human
interactive behaviors and subconscious motions driven by intuition, eliminating
manual behavior scripting. We deploy FABG on a real-world 25-degree-of-freedom
(DoF) humanoid robot, validating its effectiveness through four fundamental
interaction tasks: expression response, dynamic gaze, foveated attention, and
gesture recognition, supported by data collection and policy training. Project
website: https://cybergenies.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project website: https://cybergenies.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DnD Filter: Differentiable State Estimation for Dynamic Systems using
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01274v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01274v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyu Wan, Lin Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes the DnD Filter, a differentiable filter that utilizes
diffusion models for state estimation of dynamic systems. Unlike conventional
differentiable filters, which often impose restrictive assumptions on process
noise (e.g., Gaussianity), DnD Filter enables a nonlinear state update without
such constraints by conditioning a diffusion model on both the predicted state
and observational data, capitalizing on its ability to approximate complex
distributions. We validate its effectiveness on both a simulated task and a
real-world visual odometry task, where DnD Filter consistently outperforms
existing baselines. Specifically, it achieves a 25\% improvement in estimation
accuracy on the visual odometry task compared to state-of-the-art
differentiable filters, and even surpasses differentiable smoothers that
utilize future measurements. To the best of our knowledge, DnD Filter
represents the first successful attempt to leverage diffusion models for state
estimation, offering a flexible and powerful framework for nonlinear estimation
under noisy measurements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DART-<span class="highlight-title">LLM</span>: Dependency-Aware <span class="highlight-title">Multi</span>-Robot Task Decomposition and Execution
  using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09022v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09022v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongdong Wang, Runze Xiao, Jun Younes Louhi Kasahara, Ryosuke Yajima, Keiji Nagatani, Atsushi Yamashita, Hajime Asama
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated promising reasoning
capabilities in robotics; however, their application in multi-robot systems
remains limited, particularly in handling task dependencies. This paper
introduces DART-LLM, a novel framework that employs Directed Acyclic Graphs
(DAGs) to model task dependencies, enabling the decomposition of natural
language instructions into well-coordinated subtasks for multi-robot execution.
DART-LLM comprises four key components: a Question-Answering (QA) LLM module
for dependency-aware task decomposition, a Breakdown Function module for robot
assignment, an Actuation module for execution, and a Vision-Language Model
(VLM)-based object detector for environmental perception, achieving end-to-end
task execution. Experimental results across three task complexity levels
demonstrate that DART-LLM achieves state-of-the-art performance, significantly
outperforming the baseline across all evaluation metrics. Among the tested
models, DeepSeek-r1-671B achieves the highest success rate, whereas
Llama-3.1-8B exhibits superior response time reliability. Ablation studies
further confirm that explicit dependency modeling notably enhances the
performance of smaller models, facilitating efficient deployment on
resource-constrained platforms. Please refer to the project website
https://wyd0817.github.io/project-dart-llm/ for videos and code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The work was first submitted to an IEEE conference on September 15,
  2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D-Affordance<span class="highlight-title">LLM</span>: Harnessing Large Language Models for Open-Vocabulary
  Affordance Detection in 3D Wo<span class="highlight-title">rl</span>ds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20041v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20041v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengshuo Chu, Xiang Deng, Qi Lv, Xiaoyang Chen, Yinchuan Li, Jianye Hao, Liqiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Affordance detection is a challenging problem with broad applications on
various robotic tasks. Existing methods typically formulate the detection
paradigm as a label-based semantic segmentation task. This paradigm relies on
predefined labels and lacks the ability to comprehend complex natural language,
resulting in limited generalization in open-world scene. To address these
limitations, we reformulate the traditional affordance detection paradigm into
\textit{Instruction Reasoning Affordance Segmentation} (IRAS) task. This task
is designed to output a affordance mask region given a query reasoning text,
which avoids fixed categories of input labels. We accordingly propose the
\textit{3D-AffordanceLLM} (3D-ADLLM), a framework designed for reasoning
affordance detection in 3D open-scene. Specifically, 3D-ADLLM introduces large
language models (LLMs) to 3D affordance perception with a custom-designed
decoder for generating affordance masks, thus achieving open-world reasoning
affordance detection. In addition, given the scarcity of 3D affordance datasets
for training large models, we seek to extract knowledge from general
segmentation data and transfer it to affordance detection. Thus, we propose a
multi-stage training strategy that begins with a novel pre-training task, i.e.,
\textit{Referring Object Part Segmentation}~(ROPS). This stage is designed to
equip the model with general recognition and segmentation capabilities at the
object-part level. Then followed by fine-tuning with the IRAS task, 3D-ADLLM
obtains the reasoning ability for affordance detection. In summary, 3D-ADLLM
leverages the rich world knowledge and human-object interaction reasoning
ability of LLMs, achieving approximately an 8\% improvement in mIoU on
open-vocabulary affordance detection tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RobKiNet: Robotic Kinematics Informed Neural Network for Optimal Robot
  Configuration Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16281v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16281v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanlong Peng, Zhigang Wang, Yisheng Zhang, Pengxu Chang, Ziwen He, Kai Gu, Hongshen Zhang, Ming Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task and Motion Planning (TAMP) is essential for robots to interact with the
world and accomplish complex tasks. The TAMP problem involves a critical gap:
exploring the robot's configuration parameters (such as chassis position and
robotic arm joint angles) within continuous space to ensure that task-level
global constraints are met while also enhancing the efficiency of subsequent
motion planning. Existing methods still have significant room for improvement
in terms of efficiency. Recognizing that robot kinematics is a key factor in
motion planning, we propose a framework called the Robotic Kinematics Informed
Neural Network (RobKiNet) as a bridge between task and motion layers. RobKiNet
integrates kinematic knowledge into neural networks to train models capable of
efficient configuration prediction. We designed a Chassis Motion Predictor(CMP)
and a Full Motion Predictor(FMP) using RobKiNet, which employed two entirely
different sets of forward and inverse kinematics constraints to achieve loosely
coupled control and whole-body control, respectively. Experiments demonstrate
that CMP and FMP can predict configuration parameters with 96.67% and 98%
accuracy, respectively. That means that the corresponding motion planning can
achieve a speedup of 24.24x and 153x compared to random sampling. Furthermore,
RobKiNet demonstrates remarkable data efficiency. CMP only requires 1/71 and
FMP only requires 1/15052 of the training data for the same prediction accuracy
compared to other deep learning methods. These results demonstrate the great
potential of RoboKiNet in robot applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ POPGym Arcade: Parallel Pixelated POMDPs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01450v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01450v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zekang Wang, Zhe He, Edan Toledo, Steven Morad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce POPGym Arcade, a benchmark consisting of 7 pixel-based
environments each with three difficulties, utilizing a single observation and
action space. Each environment offers both fully observable and partially
observable variants, enabling counterfactual studies on partial observability.
POPGym Arcade utilizes JIT compilation on hardware accelerators to achieve
substantial speedups over CPU-bound environments. Moreover, this enables
Podracer-style architectures to further increase hardware utilization and
training speed. We evaluate memory models on our environments using a Podracer
variant of Q learning, and examine the results. Finally, we generate memory
saliency maps, uncovering how memories propagate through policies. Our library
is available at https://github.com/bolt-research/popgym_arcade.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AnyTouch: Learning Unified Static-Dynamic Representation across <span class="highlight-title">Multi</span>ple
  Visuo-tactile Sensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12191v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12191v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruoxuan Feng, Jiangyu Hu, Wenke Xia, Tianci Gao, Ao Shen, Yuhao Sun, Bin Fang, Di Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visuo-tactile sensors aim to emulate human tactile perception, enabling
robots to precisely understand and manipulate objects. Over time, numerous
meticulously designed visuo-tactile sensors have been integrated into robotic
systems, aiding in completing various tasks. However, the distinct data
characteristics of these low-standardized visuo-tactile sensors hinder the
establishment of a powerful tactile perception system. We consider that the key
to addressing this issue lies in learning unified multi-sensor representations,
thereby integrating the sensors and promoting tactile knowledge transfer
between them. To achieve unified representation of this nature, we introduce
TacQuad, an aligned multi-modal multi-sensor tactile dataset from four
different visuo-tactile sensors, which enables the explicit integration of
various sensors. Recognizing that humans perceive the physical environment by
acquiring diverse tactile information such as texture and pressure changes, we
further propose to learn unified multi-sensor representations from both static
and dynamic perspectives. By integrating tactile images and videos, we present
AnyTouch, a unified static-dynamic multi-sensor representation learning
framework with a multi-level structure, aimed at both enhancing comprehensive
perceptual abilities and enabling effective cross-sensor transfer. This
multi-level architecture captures pixel-level details from tactile data via
masked modeling and enhances perception and transferability by learning
semantic-level sensor-agnostic features through multi-modal alignment and
cross-sensor matching. We provide a comprehensive analysis of multi-sensor
transferability, and validate our method on various datasets and in the
real-world pouring task. Experimental results show that our method outperforms
existing methods, exhibits outstanding static and dynamic perception
capabilities across various sensors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Points2Plans: From Point Clouds to Long-Horizon Plans with Composable
  Relational Dynamics <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.14769v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.14769v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Huang, Christopher Agia, Jimmy Wu, Tucker Hermans, Jeannette Bohg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Points2Plans, a framework for composable planning with a
relational dynamics model that enables robots to solve long-horizon
manipulation tasks from partial-view point clouds. Given a language instruction
and a point cloud of the scene, our framework initiates a hierarchical planning
procedure, whereby a language model generates a high-level plan and a
sampling-based planner produces constraint-satisfying continuous parameters for
manipulation primitives sequenced according to the high-level plan. Key to our
approach is the use of a relational dynamics model as a unifying interface
between the continuous and symbolic representations of states and actions, thus
facilitating language-driven planning from high-dimensional perceptual input
such as point clouds. Whereas previous relational dynamics models require
training on datasets of multi-step manipulation scenarios that align with the
intended test scenarios, Points2Plans uses only single-step simulated training
data while generalizing zero-shot to a variable number of steps during
real-world evaluations. We evaluate our approach on tasks involving geometric
reasoning, multi-object interactions, and occluded object reasoning in both
simulated and real-world settings. Results demonstrate that Points2Plans offers
strong generalization to unseen long-horizon tasks in the real world, where it
solves over 85% of evaluated tasks while the next best baseline solves only
50%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://sites.google.com/stanford.edu/points2plans. 23
  pages, 11 figures. Accepted to the IEEE International Conference on Robotics
  and Automation (ICRA) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LocoVR: <span class="highlight-title">Multi</span>user Indoor Locomotion <span class="highlight-title">Dataset</span> in Virtual Reality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06437v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06437v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kojiro Takeyama, Yimeng Liu, Misha Sra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding human locomotion is crucial for AI agents such as robots,
particularly in complex indoor home environments. Modeling human trajectories
in these spaces requires insight into how individuals maneuver around physical
obstacles and manage social navigation dynamics. These dynamics include subtle
behaviors influenced by proxemics - the social use of space, such as stepping
aside to allow others to pass or choosing longer routes to avoid collisions.
Previous research has developed datasets of human motion in indoor scenes, but
these are often limited in scale and lack the nuanced social navigation
dynamics common in home environments. To address this, we present LocoVR, a
dataset of 7000+ two-person trajectories captured in virtual reality from over
130 different indoor home environments. LocoVR provides accurate trajectory
data and precise spatial information, along with rich examples of
socially-motivated movement behaviors. For example, the dataset captures
instances of individuals navigating around each other in narrow spaces,
adjusting paths to respect personal boundaries in living areas, and
coordinating movements in high-traffic zones like entryways and kitchens. Our
evaluation shows that LocoVR significantly enhances model performance in three
practical indoor tasks utilizing human trajectories, and demonstrates
predicting socially-aware navigation patterns in home environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted to ICLR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ZeroCAP: Zero-Shot <span class="highlight-title">Multi</span>-Robot Context Aware Pattern Formation via Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.02318v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.02318v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishnunandan L. N. Venkatesh, Byung-Cheol Min
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating language comprehension into robotic operations unlocks
significant advancements in robotics, but also presents distinct challenges,
particularly in executing spatially oriented tasks like pattern formation. This
paper introduces ZeroCAP, a novel system that integrates large language models
with multi-robot systems for zero-shot context aware pattern formation.
Grounded in the principles of language-conditioned robotics, ZeroCAP leverages
the interpretative power of language models to translate natural language
instructions into actionable robotic configurations. This approach combines the
synergy of vision-language models, cutting-edge segmentation techniques and
shape descriptors, enabling the realization of complex, context-driven pattern
formations in the realm of multi robot coordination. Through extensive
experiments, we demonstrate the systems proficiency in executing complex
context aware pattern formations across a spectrum of tasks, from surrounding
and caging objects to infilling regions. This not only validates the system's
capability to interpret and implement intricate context-driven tasks but also
underscores its adaptability and effectiveness across varied environments and
scenarios. The experimental videos and additional information about this work
can be found at https://sites.google.com/view/zerocap/home.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Feeling of Success: Does Touch Sensing Help Predict Grasp Outcomes? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1710.05512v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1710.05512v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roberto Calandra, Andrew Owens, Manu Upadhyaya, Wenzhen Yuan, Justin Lin, Edward H. Adelson, Sergey Levine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A successful grasp requires careful balancing of the contact forces. Deducing
whether a particular grasp will be successful from indirect measurements, such
as vision, is therefore quite challenging, and direct sensing of contacts
through touch sensing provides an appealing avenue toward more successful and
consistent robotic grasping. However, in order to fully evaluate the value of
touch sensing for grasp outcome prediction, we must understand how touch
sensing can influence outcome prediction accuracy when combined with other
modalities. Doing so using conventional model-based techniques is exceptionally
difficult. In this work, we investigate the question of whether touch sensing
aids in predicting grasp outcomes within a multimodal sensing framework that
combines vision and touch. To that end, we collected more than 9,000 grasping
trials using a two-finger gripper equipped with GelSight high-resolution
tactile sensors on each finger, and evaluated visuo-tactile deep neural network
models to directly predict grasp outcomes from either modality individually,
and from both modalities together. Our experimental results indicate that
incorporating tactile readings substantially improve grasping performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, published at the 1st Annual Conference on Robot Learning
  (CoRL), Code and dataset available at:
  https://lasr.org/research/feeling-of-success</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StableLego: Stability Analysis of Block Stacking Assembly 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10711v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10711v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruixuan Liu, Kangle Deng, Ziwei Wang, Changliu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Structural stability is a necessary condition for successful construction of
an assembly. However, designing a stable assembly requires a non-trivial effort
since a slight variation in the design could significantly affect the
structural stability. To address the challenge, this paper studies the
stability of assembly structures, in particular, block stacking assembly. The
paper proposes a new optimization formulation, which optimizes over force
balancing equations, for inferring the structural stability of 3D block
stacking structures. The proposed stability analysis is verified on
hand-crafted Lego examples. The experiment results demonstrate that the
proposed method can correctly predict whether the structure is stable. In
addition, it outperforms the existing methods since it can accurately locate
the weakest parts in the design, and more importantly, solve any given assembly
structures. To further validate the proposed method, we provide
\textit{StableLego}: a comprehensive dataset including 50k+ 3D objects with
their Lego layouts. We test the proposed stability analysis and include the
stability inference for each corresponding object in StableLego. Our code and
the dataset are available at
https://github.com/intelligent-control-lab/StableLego.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Maximal Safe Sets Using Hypernetworks for MPC-based Local
  Trajectory Planning in Unknown Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20267v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20267v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bojan Derajić, Mohamed-Khalil Bouzidi, Sebastian Bernhard, Wolfgang Hönig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel learning-based approach for online estimation of
maximal safe sets for local trajectory planning in unknown static environments.
The neural representation of a set is used as the terminal set constraint for a
model predictive control (MPC) local planner, resulting in improved recursive
feasibility and safety. To achieve real-time performance and desired
generalization properties, we employ the idea of hypernetworks. We use the
Hamilton-Jacobi (HJ) reachability analysis as the source of supervision during
the training process, allowing us to consider general nonlinear dynamics and
arbitrary constraints. The proposed method is extensively evaluated against
relevant baselines in simulations for different environments and robot
dynamics. The results show a success rate increase of up to 52 \% compared to
the best baseline while maintaining comparable execution speed. Additionally,
we deploy our proposed method, NTC-MPC, on a physical robot and demonstrate its
ability to safely avoid obstacles in scenarios where the baselines fail.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Teaching Robots to Build Simulations of Themselves 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12151v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12151v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhang Hu, Jiong Lin, Hod Lipson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergence of vision catalysed a pivotal evolutionary advancement,
enabling organisms not only to perceive but also to interact intelligently with
their environment. This transformation is mirrored by the evolution of robotic
systems, where the ability to leverage vision to simulate and predict their own
dynamics marks a leap towards autonomy and self-awareness. Humans utilize
vision to record experiences and internally simulate potential actions. For
example, we can imagine that, if we stand up and raise our arms, the body will
form a T shape without physical movement. Similarly, simulation allows robots
to plan and predict the outcomes of potential actions without execution. Here
we introduce a self-supervised learning framework to enable robots to model and
predict their morphology, kinematics and motor control using only brief raw
video data, eliminating the need for extensive real-world data collection and
kinematic priors. By observing their own movements, akin to humans watching
their reflection in a mirror, robots learn an ability to simulate themselves
and predict their spatial motion for various tasks. Our results demonstrate
that this self-learned simulation not only enables accurate motion planning but
also allows the robot to detect abnormalities and recover from damage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper was published on Nature Machine Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Trajectory Planning for Cooperative Manipulation with <span class="highlight-title">Multi</span>ple
  Quadrotors Using Control Barrier Functions <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01096v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01096v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arpan Pallar, Guanrui Li, Mrunal Sarvaiya, Giuseppe Loianno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a novel trajectory planning algorithm for
cooperative manipulation with multiple quadrotors using control barrier
functions (CBFs). Our approach addresses the complex dynamics of a system in
which a team of quadrotors transports and manipulates a cable-suspended
rigid-body payload in environments cluttered with obstacles. The proposed
algorithm ensures obstacle avoidance for the entire system, including the
quadrotors, cables, and the payload in all six degrees of freedom (DoF). We
introduce the use of CBFs to enable safe and smooth maneuvers, effectively
navigating through cluttered environments while accommodating the system's
nonlinear dynamics. To simplify complex constraints, the system components are
modeled as convex polytopes, and the Duality theorem is employed to reduce the
computational complexity of the optimization problem. We validate the
performance of our planning approach both in simulation and real-world
environments using multiple quadrotors. The results demonstrate the
effectiveness of the proposed approach in achieving obstacle avoidance and safe
trajectory generation for cooperative transportation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted for publication in the IEEE
  International Conference on Robotics and Automation(ICRA), 2025. Please cite
  the paper using appropriate formats</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Impact of Object Weight in Handovers: Inspiring Robotic Grip Release and
  Motion from Human Handovers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17834v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17834v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parag Khanna, Mårten Björkman, Christian Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work explores the effect of object weight on human motion and grip
release during handovers to enhance the naturalness, safety, and efficiency of
robot-human interactions. We introduce adaptive robotic strategies based on the
analysis of human handover behavior with varying object weights. The key
contributions of this work includes the development of an adaptive grip-release
strategy for robots, a detailed analysis of how object weight influences human
motion to guide robotic motion adaptations, and the creation of
handover-datasets incorporating various object weights, including the YCB
handover dataset. By aligning robotic grip release and motion with human
behavior, this work aims to improve robot-human handovers for different
weighted objects. We also evaluate these human-inspired adaptive robotic
strategies in robot-to-human handovers to assess their effectiveness and
performance and demonstrate that they outperform the baseline approaches in
terms of naturalness, efficiency, and user perception.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In Submission at IEEE-IEEE Transactions on Robotics. Changes:
  Corrected typos; Added 2 references for object weight impact on handovers;
  added Figures 20, 21, and 22 in Results in Section VI for further comparative
  analysis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ODYSSEE: Oyster Detection Yielded by Sensor Systems on Edge Electronics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07003v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07003v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaomin Lin, Vivek Mange, Arjun Suresh, Bernhard Neuberger, Aadi Palnitkar, Brendan Campbell, Alan Williams, Kleio Baxevani, Jeremy Mallette, Alhim Vera, Markus Vincze, Ioannis Rekleitis, Herbert G. Tanner, Yiannis Aloimonos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Oysters are a vital keystone species in coastal ecosystems, providing
significant economic, environmental, and cultural benefits. As the importance
of oysters grows, so does the relevance of autonomous systems for their
detection and monitoring. However, current monitoring strategies often rely on
destructive methods. While manual identification of oysters from video footage
is non-destructive, it is time-consuming, requires expert input, and is further
complicated by the challenges of the underwater environment.
  To address these challenges, we propose a novel pipeline using stable
diffusion to augment a collected real dataset with realistic synthetic data.
This method enhances the dataset used to train a YOLOv10-based vision model.
The model is then deployed and tested on an edge platform in underwater
robotics, achieving a state-of-the-art 0.657 mAP@50 for oyster detection on the
Aqua2 platform.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Artificial Intelligence <span class="chip" style="font-size: 60%">144</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bringing Comparative Cognition To Computers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02882v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02882v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantinos Voudouris, Lucy G. Cheke, Eric Schulz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Researchers are increasingly subjecting artificial intelligence systems to
psychological testing. But to rigorously compare their cognitive capacities
with humans and other animals, we must avoid both over- and under-stating our
similarities and differences. By embracing a comparative approach, we can
integrate AI cognition research into the broader cognitive sciences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reactive Diffusion Policy: Slow-Fast Visual-Tactile Policy Learning for
  Contact-Rich Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02881v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02881v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Xue, Jieji Ren, Wendi Chen, Gu Zhang, Yuan Fang, Guoying Gu, Huazhe Xu, Cewu Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans can accomplish complex contact-rich tasks using vision and touch, with
highly reactive capabilities such as quick adjustments to environmental changes
and adaptive control of contact forces; however, this remains challenging for
robots. Existing visual imitation learning (IL) approaches rely on action
chunking to model complex behaviors, which lacks the ability to respond
instantly to real-time tactile feedback during the chunk execution.
Furthermore, most teleoperation systems struggle to provide fine-grained
tactile / force feedback, which limits the range of tasks that can be
performed. To address these challenges, we introduce TactAR, a low-cost
teleoperation system that provides real-time tactile feedback through Augmented
Reality (AR), along with Reactive Diffusion Policy (RDP), a novel slow-fast
visual-tactile imitation learning algorithm for learning contact-rich
manipulation skills. RDP employs a two-level hierarchy: (1) a slow latent
diffusion policy for predicting high-level action chunks in latent space at low
frequency, (2) a fast asymmetric tokenizer for closed-loop tactile feedback
control at high frequency. This design enables both complex trajectory modeling
and quick reactive behavior within a unified framework. Through extensive
evaluation across three challenging contact-rich tasks, RDP significantly
improves performance compared to state-of-the-art visual IL baselines through
rapid response to tactile / force feedback. Furthermore, experiments show that
RDP is applicable across different tactile / force sensors. Code and videos are
available on https://reactive-diffusion-policy.github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wikipedia in the Era of <span class="highlight-title">LLM</span>s: Evolution and Risks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02879v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02879v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siming Huang, Yuliang Xu, Mingmeng Geng, Yao Wan, Dongping Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a thorough analysis of the impact of Large Language
Models (LLMs) on Wikipedia, examining the evolution of Wikipedia through
existing data and using simulations to explore potential risks. We begin by
analyzing page views and article content to study Wikipedia's recent changes
and assess the impact of LLMs. Subsequently, we evaluate how LLMs affect
various Natural Language Processing (NLP) tasks related to Wikipedia, including
machine translation and retrieval-augmented generation (RAG). Our findings and
simulation results reveal that Wikipedia articles have been influenced by LLMs,
with an impact of approximately 1%-2% in certain categories. If the machine
translation benchmark based on Wikipedia is influenced by LLMs, the scores of
the models may become inflated, and the comparative results among models might
shift as well. Moreover, the effectiveness of RAG might decrease if the
knowledge base becomes polluted by LLM-generated content. While LLMs have not
yet fully changed Wikipedia's language and knowledge structures, we believe
that our empirical findings signal the need for careful consideration of
potential future risks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We release all the experimental dataset and source code at:
  https://github.com/HSM316/LLM_Wikipedia</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Models can Self-Improve at State-Value Estimation for Better
  Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02878v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02878v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ethan Mendes, Alan Ritter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collecting ground truth task completion rewards or human demonstrations for
multi-step reasoning tasks is often cost-prohibitive and time-consuming,
especially in interactive domains like web tasks. To address this bottleneck,
we present self-taught lookahead, a self-supervised method that leverages
state-transition dynamics to train a value model capable of effectively guiding
language model-controlled search. We find that moderately sized (8 billion
parameters) open-weight value models improved with self-taught lookahead can
match the performance of using a frontier LLM such as gpt-4o as the value
model. Furthermore, we find that self-taught lookahead improves performance by
20% while reducing costs 37x compared to previous LLM-based tree search,
without relying on ground truth rewards.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation of Architectural Synthesis Using Generative AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02861v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02861v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingfei Huang, Alexandros Haridis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in multimodal Generative AI have the potential to
democratize specialized architectural tasks, such as interpreting technical
drawings and creating 3D CAD models, which traditionally require expert
knowledge. This paper presents a comparative evaluation of two systems: GPT-4o
and Claude 3.5, in the task of architectural 3D synthesis. We conduct a case
study on two buildings from Palladio's Four Books of Architecture (1965): Villa
Rotonda and Palazzo Porto. High-level architectural models and drawings of
these buildings were prepared, inspired by Palladio's original texts and
drawings. Through sequential text and image prompting, we assess the systems'
abilities in (1) interpreting 2D and 3D representations of buildings from
drawings, (2) encoding the buildings into a CAD software script, and (3)
self-improving based on outputs. While both systems successfully generate
individual parts, they struggle to accurately assemble these parts into the
desired spatial relationships, with Claude 3.5 demonstrating better
performance, particularly in self-correcting its output. This study contributes
to ongoing research on benchmarking the strengths and weaknesses of
off-the-shelf AI systems in performing intelligent human tasks that require
discipline-specific knowledge. The findings highlight the potential of
language-enabled AI systems to act as collaborative technical assistants in the
architectural design process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deepfake-Eval-2024: A <span class="highlight-title">Multi</span>-Modal In-the-Wild Benchmark of Deepfakes
  Circulated in 2024 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02857v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02857v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nuria Alina Chandra, Ryan Murtfeldt, Lin Qiu, Arnab Karmakar, Hannah Lee, Emmanuel Tanumihardja, Kevin Farhat, Ben Caffee, Sejin Paik, Changyeon Lee, Jongwook Choi, Aerin Kim, Oren Etzioni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the age of increasingly realistic generative AI, robust deepfake detection
is essential for mitigating fraud and disinformation. While many deepfake
detectors report high accuracy on academic datasets, we show that these
academic benchmarks are out of date and not representative of recent deepfakes.
We introduce Deepfake-Eval-2024, a new deepfake detection benchmark consisting
of in-the-wild deepfakes collected from social media and deepfake detection
platform users in 2024. Deepfake-Eval-2024 consists of 44 hours of videos, 56.5
hours of audio, and 1,975 images, encompassing the latest manipulation
technologies. The benchmark contains diverse media content from 88 different
websites in 52 different languages. We find that the performance of open-source
state-of-the-art deepfake detection models drops precipitously when evaluated
on Deepfake-Eval-2024, with AUC decreasing by 50% for video, 48% for audio, and
45% for image models compared to previous benchmarks. We also evaluate
commercial deepfake detection models and models finetuned on
Deepfake-Eval-2024, and find that they have superior performance to
off-the-shelf open-source models, but they do not yet reach the accuracy of
human deepfake forensic analysts. The dataset is available at
https://github.com/nuriachandra/Deepfake-Eval-2024.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ (How) Do Language Models Track State? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02854v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02854v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Belinda Z. Li, Zifan Carl Guo, Jacob Andreas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer language models (LMs) exhibit behaviors -- from storytelling to
code generation -- that appear to require tracking the unobserved state of an
evolving world. How do they do so? We study state tracking in LMs trained or
fine-tuned to compose permutations (i.e., to compute the order of a set of
objects after a sequence of swaps). Despite the simple algebraic structure of
this problem, many other tasks (e.g., simulation of finite automata and
evaluation of boolean expressions) can be reduced to permutation composition,
making it a natural model for state tracking in general. We show that LMs
consistently learn one of two state tracking mechanisms for this task. The
first closely resembles the "associative scan" construction used in recent
theoretical work by Liu et al. (2023) and Merrill et al. (2024). The second
uses an easy-to-compute feature (permutation parity) to partially prune the
space of outputs, then refines this with an associative scan. The two
mechanisms exhibit markedly different robustness properties, and we show how to
steer LMs toward one or the other with intermediate training tasks that
encourage or suppress the heuristics. Our results demonstrate that transformer
LMs, whether pretrained or fine-tuned, can learn to implement efficient and
interpretable state tracking mechanisms, and the emergence of these mechanisms
can be predicted and controlled.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 17 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Multi</span>modal Deep Learning for Subtype Classification in Breast Cancer
  Using Histopathological Images and Gene Expression Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02849v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02849v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amin Honarmandi Shandiz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Molecular subtyping of breast cancer is crucial for personalized treatment
and prognosis. Traditional classification approaches rely on either
histopathological images or gene expression profiling, limiting their
predictive power. In this study, we propose a deep multimodal learning
framework that integrates histopathological images and gene expression data to
classify breast cancer into BRCA.Luminal and BRCA.Basal / Her2 subtypes. Our
approach employs a ResNet-50 model for image feature extraction and fully
connected layers for gene expression processing, with a cross-attention fusion
mechanism to enhance modality interaction. We conduct extensive experiments
using five-fold cross-validation, demonstrating that our multimodal integration
outperforms unimodal approaches in terms of classification accuracy,
precision-recall AUC, and F1-score. Our findings highlight the potential of
deep learning for robust and interpretable breast cancer subtype
classification, paving the way for improved clinical decision-making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SeqFusion: Sequential Fusion of Pre-Trained Models for Zero-Shot
  Time-Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02836v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02836v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting-Ji Huang, Xu-Yang Chen, Han-Jia Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unlike traditional time-series forecasting methods that require extensive
in-task data for training, zero-shot forecasting can directly predict future
values given a target time series without additional training data. Current
zero-shot approaches primarily rely on pre-trained generalized models, with
their performance often depending on the variety and relevance of the
pre-training data, which can raise privacy concerns. Instead of collecting
diverse pre-training data, we introduce SeqFusion in this work, a novel
framework that collects and fuses diverse pre-trained models (PTMs)
sequentially for zero-shot forecasting. Based on the specific temporal
characteristics of the target time series, SeqFusion selects the most suitable
PTMs from a batch of pre-collected PTMs, performs sequential predictions, and
fuses all the predictions while using minimal data to protect privacy. Each of
these PTMs specializes in different temporal patterns and forecasting tasks,
allowing SeqFusion to select by measuring distances in a shared representation
space of the target time series with each PTM. Experiments demonstrate that
SeqFusion achieves competitive accuracy in zero-shot forecasting compared to
state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AlignDistil: Token-Level Language Model Alignment as Adaptive Policy
  Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02832v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02832v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songming Zhang, Xue Zhang, Tong Zhang, Bojie Hu, Yufeng Chen, Jinan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In modern large language models (LLMs), LLM alignment is of crucial
importance and is typically achieved through methods such as reinforcement
learning from human feedback (RLHF) and direct preference optimization (DPO).
However, in most existing methods for LLM alignment, all tokens in the response
are optimized using a sparse, response-level reward or preference annotation.
The ignorance of token-level rewards may erroneously punish high-quality tokens
or encourage low-quality tokens, resulting in suboptimal performance and slow
convergence speed. To address this issue, we propose AlignDistil, an
RLHF-equivalent distillation method for token-level reward optimization.
Specifically, we introduce the reward learned by DPO into the RLHF objective
and theoretically prove the equivalence between this objective and a
token-level distillation process, where the teacher distribution linearly
combines the logits from the DPO model and a reference model. On this basis, we
further bridge the accuracy gap between the reward from the DPO model and the
pure reward model, by building a contrastive DPO reward with a normal and a
reverse DPO model. Moreover, to avoid under- and over-optimization on different
tokens, we design a token adaptive logit extrapolation mechanism to construct
an appropriate teacher distribution for each token. Experimental results
demonstrate the superiority of our AlignDistil over existing methods and
showcase fast convergence due to its token-level distributional reward
optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Developing a PET/CT Foundation Model for Cross-Modal Anatomical and
  Functional Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02824v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02824v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujin Oh, Robert Seifert, Yihan Cao, Christoph Clement, Justin Ferdinandus, Constantin Lapa, Alessandro Liebich, Michelle Amon, Johanna Enke, Sifan Song, Runqi Meng, Fang Zeng, Ning Guo, Xiang Li, Pedram Heidari, Axel Rominger, Kuangyu Shi, Quanzheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In oncology, Positron Emission Tomography-Computed Tomography (PET/CT) is
widely used in cancer diagnosis, staging, and treatment monitoring, as it
combines anatomical details from CT with functional metabolic activity and
molecular marker expression information from PET. However, existing artificial
intelligence-driven PET/CT analyses rely predominantly on task-specific models
trained from scratch or on limited datasets, limiting their generalizability
and robustness. To address this, we propose a foundation model approach
specifically designed for multimodal PET/CT imaging. We introduce the
Cross-Fraternal Twin Masked Autoencoder (FratMAE), a novel framework that
effectively integrates whole-body anatomical and functional or molecular
information. FratMAE employs separate Vision Transformer (ViT) encoders for PET
and CT scans, along with cross-attention decoders that enable synergistic
interactions between modalities during masked autoencoder training.
Additionally, it incorporates textual metadata to enhance PET representation
learning. By pre-training on PET/CT datasets, FratMAE captures intricate
cross-modal relationships and global uptake patterns, achieving superior
performance on downstream tasks and demonstrating its potential as a
generalizable foundation model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 2 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Multi</span>modal Symphony: Integrating Taste and Sound through Generative AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Spanio, Massimiliano Zampini, Antonio Rodà, Franco Pierucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent decades, neuroscientific and psychological research has traced
direct relationships between taste and auditory perceptions. This article
explores multimodal generative models capable of converting taste information
into music, building on this foundational research. We provide a brief review
of the state of the art in this field, highlighting key findings and
methodologies. We present an experiment in which a fine-tuned version of a
generative music model (MusicGEN) is used to generate music based on detailed
taste descriptions provided for each musical piece. The results are promising:
according the participants' ($n=111$) evaluation, the fine-tuned model produces
music that more coherently reflects the input taste descriptions compared to
the non-fine-tuned model. This study represents a significant step towards
understanding and developing embodied interactions between AI, sound, and
taste, opening new possibilities in the field of generative AI. We release our
dataset, code and pre-trained model at: https://osf.io/xs5jy/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 6 figures (2 + 2 figures with 2 subfigures each)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Q-Filters: Leveraging QK Geometry for Efficient KV Cache Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Godey, Alessio Devoto, Yu Zhao, Simone Scardapane, Pasquale Minervini, Éric de la Clergerie, Benoît Sagot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive language models rely on a Key-Value (KV) Cache, which avoids
re-computing past hidden states during generation, making it faster. As model
sizes and context lengths grow, the KV Cache becomes a significant memory
bottleneck, which calls for compression methods that limit its size during
generation. In this paper, we discover surprising properties of Query (Q) and
Key (K) vectors that allow us to efficiently approximate attention scores
without computing the attention maps. We propose Q-Filters, a training-free KV
Cache compression method that filters out less crucial Key-Value pairs based on
a single context-agnostic projection. Contrarily to many alternatives,
Q-Filters is compatible with FlashAttention, as it does not require direct
access to attention weights. Experimental results in long-context settings
demonstrate that Q-Filters is competitive with attention-based compression
methods such as SnapKV in retrieval tasks while consistently outperforming
efficient compression schemes such as Streaming-LLM in generation setups.
Notably, Q-Filters achieves a 99% accuracy in the needle-in-a-haystack task
with a x32 compression level while reducing the generation perplexity drop by
up to 65% in text generation compared to Streaming-LLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Causal Framework for Aligning Image Quality Metrics and Deep Neural
  Network Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathan Drenkow, Mathias Unberath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image quality plays an important role in the performance of deep neural
networks (DNNs) and DNNs have been widely shown to exhibit sensitivity to
changes in imaging conditions. Large-scale datasets often contain images under
a wide range of conditions prompting a need to quantify and understand their
underlying quality distribution in order to better characterize DNN performance
and robustness. Aligning the sensitivities of image quality metrics and DNNs
ensures that estimates of quality can act as proxies for image/dataset
difficulty independent of the task models trained/evaluated on the data.
Conventional image quality assessment (IQA) seeks to measure and align quality
relative to human perceptual judgments, but here we seek a quality measure that
is not only sensitive to imaging conditions but also well-aligned with DNN
sensitivities. We first ask whether conventional IQA metrics are also
informative of DNN performance. In order to answer this question, we reframe
IQA from a causal perspective and examine conditions under which quality
metrics are predictive of DNN performance. We show theoretically and
empirically that current IQA metrics are weak predictors of DNN performance in
the context of classification. We then use our causal framework to provide an
alternative formulation and a new image quality metric that is more strongly
correlated with DNN performance and can act as a prior on performance without
training new task models. Our approach provides a means to directly estimate
the quality distribution of large-scale image datasets towards characterizing
the relationship between dataset composition and DNN performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do Not Trust Licenses You See -- <span class="highlight-title">Dataset</span> Compliance Requires
  Massive-Scale AI-Powered Lifecycle Tracing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaekyeom Kim, Sungryull Sohn, Gerrard Jeongwon Jo, Jihoon Choi, Kyunghoon Bae, Hwayoung Lee, Yongmin Park, Honglak Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper argues that a dataset's legal risk cannot be accurately assessed
by its license terms alone; instead, tracking dataset redistribution and its
full lifecycle is essential. However, this process is too complex for legal
experts to handle manually at scale. Tracking dataset provenance, verifying
redistribution rights, and assessing evolving legal risks across multiple
stages require a level of precision and efficiency that exceeds human
capabilities. Addressing this challenge effectively demands AI agents that can
systematically trace dataset redistribution, analyze compliance, and identify
legal risks. We develop an automated data compliance system called NEXUS and
show that AI can perform these tasks with higher accuracy, efficiency, and
cost-effectiveness than human experts. Our massive legal analysis of 17,429
unique entities and 8,072 license terms using this approach reveals the
discrepancies in legal rights between the original datasets before
redistribution and their redistributed subsets, underscoring the necessity of
the data lifecycle-aware compliance. For instance, we find that out of 2,852
datasets with commercially viable individual license terms, only 605 (21%) are
legally permissible for commercialization. This work sets a new standard for AI
data governance, advocating for a framework that systematically examines the
entire lifecycle of dataset redistribution to ensure transparent, legal, and
responsible dataset management.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IterPref: Focal Preference Learning for Code Generation via Iterative
  Debugging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Wu, Haoling Li, Xin Zhang, Jianwen Luo, Yangyu Huang, Ruihang Chu, Yujiu Yang, Scarlett Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preference learning enhances Code LLMs beyond supervised fine-tuning by
leveraging relative quality comparisons. Existing methods construct preference
pairs from
  candidates based on test case success, treating the higher pass rate sample
as positive and the lower as negative. However, this approach does not pinpoint
specific errors in the code, which prevents the model from learning more
informative error correction patterns, as aligning failing code as a whole
lacks the granularity needed to capture meaningful error-resolution
relationships. To address these issues, we propose IterPref, a new preference
alignment framework that mimics human iterative debugging to refine Code LLMs.
IterPref explicitly locates error regions and aligns the corresponding tokens
via a tailored DPO algorithm. To generate informative pairs, we introduce the
CodeFlow dataset, where samples are iteratively refined until passing tests,
with modifications capturing error corrections. Extensive experiments show that
a diverse suite of Code LLMs equipped with IterPref achieves significant
performance gains in code generation and improves on challenging tasks like
BigCodeBench. In-depth analysis reveals that IterPref yields fewer errors. Our
code and data will be made publicaly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code and data will be released soon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Implicit Bias in <span class="highlight-title">LLM</span>s: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02776v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02776v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinru Lin, Luyang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the implement of guardrails by developers, Large language models
(LLMs) have demonstrated exceptional performance in explicit bias tests.
However, bias in LLMs may occur not only explicitly, but also implicitly, much
like humans who consciously strive for impartiality yet still harbor implicit
bias. The unconscious and automatic nature of implicit bias makes it
particularly challenging to study. This paper provides a comprehensive review
of the existing literature on implicit bias in LLMs. We begin by introducing
key concepts, theories and methods related to implicit bias in psychology,
extending them from humans to LLMs. Drawing on the Implicit Association Test
(IAT) and other psychological frameworks, we categorize detection methods into
three primary approaches: word association, task-oriented text generation and
decision-making. We divide our taxonomy of evaluation metrics for implicit bias
into two categories: single-value-based metrics and comparison-value-based
metrics. We classify datasets into two types: sentences with masked tokens and
complete sentences, incorporating datasets from various domains to reflect the
broad application of LLMs. Although research on mitigating implicit bias in
LLMs is still limited, we summarize existing efforts and offer insights on
future challenges. We aim for this work to serve as a clear guide for
researchers and inspire innovative ideas to advance exploration in this task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prime Convolutional Model: Breaking the Ground for Theoretical
  Explainability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02773v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02773v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Panelli, Doaa Almhaithawi, Tania Cerquitelli, Alessandro Bellini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a new theoretical approach to Explainable AI.
Following the Scientific Method, this approach consists in formulating on the
basis of empirical evidence, a mathematical model to explain and predict the
behaviors of Neural Networks. We apply the method to a case study created in a
controlled environment, which we call Prime Convolutional Model (p-Conv for
short). p-Conv operates on a dataset consisting of the first one million
natural numbers and is trained to identify the congruence classes modulo a
given integer $m$. Its architecture uses a convolutional-type neural network
that contextually processes a sequence of $B$ consecutive numbers to each
input. We take an empirical approach and exploit p-Conv to identify the
congruence classes of numbers in a validation set using different values for
$m$ and $B$. The results show that the different behaviors of p-Conv (i.e.,
whether it can perform the task or not) can be modeled mathematically in terms
of $m$ and $B$. The inferred mathematical model reveals interesting patterns
able to explain when and why p-Conv succeeds in performing task and, if not,
which error pattern it follows.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Oil Slick Trajectory Simulations with Bayesian Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02749v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02749v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriele Accarino, Marco M. De Carlo, Igor Atake, Donatello Elia, Anusha L. Dissanayake, Antonio Augusto Sepp Neves, Juan Peña Ibañez, Italo Epicoco, Paola Nassisi, Sandro Fiore, Giovanni Coppini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate simulations of oil spill trajectories are essential for supporting
practitioners' response and mitigating environmental and socioeconomic impacts.
Numerical models, such as MEDSLIK-II, simulate advection, dispersion, and
transformation processes of oil particles. However, simulations heavily rely on
accurate parameter tuning, still based on expert knowledge and manual
calibration. To overcome these limitations, we integrate the MEDSLIK-II
numerical oil spill model with a Bayesian optimization framework to iteratively
estimate the best physical parameter configuration that yields simulation
closer to satellite observations of the slick. We focus on key parameters, such
as horizontal diffusivity and drift factor, maximizing the Fraction Skill Score
(FSS) as a measure of spatio-temporal overlap between simulated and observed
oil distributions. We validate the framework for the Baniyas oil incident that
occurred in Syria between August 23 and September 4, 2021, which released over
12,000 $m^3$ of oil. We show that, on average, the proposed approach
systematically improves the FSS from 5.82% to 11.07% compared to control
simulations initialized with default parameters. The optimization results in
consistent improvement across multiple time steps, particularly during periods
of increased drift variability, demonstrating the robustness of our method in
dynamic environmental conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 10 figures, 3 tables, research paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UAR-NVC: A Unified AutoRegressive Framework for Memory-Efficient Neural
  Video Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia Wang, Xinfeng Zhang, Gai Zhang, Jun Zhu, Lv Tang, Li Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Implicit Neural Representations (INRs) have demonstrated significant
potential in video compression by representing videos as neural networks.
However, as the number of frames increases, the memory consumption for training
and inference increases substantially, posing challenges in
resource-constrained scenarios. Inspired by the success of traditional video
compression frameworks, which process video frame by frame and can efficiently
compress long videos, we adopt this modeling strategy for INRs to decrease
memory consumption, while aiming to unify the frameworks from the perspective
of timeline-based autoregressive modeling. In this work, we present a novel
understanding of INR models from an autoregressive (AR) perspective and
introduce a Unified AutoRegressive Framework for memory-efficient Neural Video
Compression (UAR-NVC). UAR-NVC integrates timeline-based and INR-based neural
video compression under a unified autoregressive paradigm. It partitions videos
into several clips and processes each clip using a different INR model
instance, leveraging the advantages of both compression frameworks while
allowing seamless adaptation to either in form. To further reduce temporal
redundancy between clips, we design two modules to optimize the initialization,
training, and compression of these model parameters. UAR-NVC supports
adjustable latencies by varying the clip length. Extensive experimental results
demonstrate that UAR-NVC, with its flexible video clip setting, can adapt to
resource-constrained environments and significantly improve performance
compared to different baseline models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Vibration-Assisted Hysteresis Mitigation for Achieving High Compensation
  Efficiency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02720v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02720v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Myeongbo Park, Chunggil An, Junhyun Park, Jonghyun Kang, Minho Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tendon-sheath mechanisms (TSMs) are widely used in minimally invasive
surgical (MIS) applications, but their inherent hysteresis-caused by friction,
backlash, and tendon elongation-leads to significant tracking errors.
Conventional modeling and compensation methods struggle with these
nonlinearities and require extensive parameter tuning. To address this, we
propose a vibration-assisted hysteresis compensation approach, where controlled
vibrational motion is applied along the tendon's movement direction to mitigate
friction and reduce dead zones. Experimental results demonstrate that the
exerted vibration consistently reduces hysteresis across all tested
frequencies, decreasing RMSE by up to 23.41% (from 2.2345 mm to 1.7113 mm) and
improving correlation, leading to more accurate trajectory tracking. When
combined with a Temporal Convolutional Network (TCN)-based compensation model,
vibration further enhances performance, achieving an 85.2% reduction in MAE
(from 1.334 mm to 0.1969 mm). Without vibration, the TCN-based approach still
reduces MAE by 72.3% (from 1.334 mm to 0.370 mm) under the same parameter
settings. These findings confirm that vibration effectively mitigates
hysteresis, improving trajectory accuracy and enabling more efficient
compensation models with fewer trainable parameters. This approach provides a
scalable and practical solution for TSM-based robotic applications,
particularly in MIS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, and 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generative Tools for Graphical Assets: Empirical Guidelines based on
  Game Designers' and Developers' Preferences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02703v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02703v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaisei Fukaya, Damon Daylamani-Zad, Harry Agius
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graphical assets play an important role in the design and development of
games. There is potential in the use of generative tools, to aid in creating
graphical assets, thus improving game design and development pipelines.
However, there is little research to address how the generative methods can fit
into the wider pipeline. We conducted a user study with 16 game designers and
developers to examine their preferences regarding generative tools for
graphical assets. The findings highlight that early design stage is preferred
by all participants (mean values above 0.67 and p < .001 for early stages).
Designers and developers prefer to use such tools for creating large amounts of
variations at the cost of quality as they can improve the quality of the
artefacts once they generate a suitable asset (mean value 0.17 where 1 is high
quality, p < .001). They also strongly (mean value .78, p < .001) raised the
need for better integration of such tools in existing design and development
environments and the need for the outputs to be in common data formats, to be
manipulatable and integrate smoothly into existing environments (mean 3.5 out
of 5, p = .004). The study also highlights the requirement for further emphasis
on the needs of the users to incorporate these tools effectively in existing
pipelines. Informed by these results, we provide a set of guidelines for
creating tools that meet the expectations and needs of game designers and
developers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MindBridge: Scalable and Cross-Model Knowledge Editing via
  Memory-Augmented Modality 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuaike Li, Kai Zhang, Qi Liu, Enhong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge editing is a technique for efficiently and accurately updating the
knowledge of large language models (LLMs) to alleviate obsolescence and correct
errors. However, most existing methods overfit to specific models, causing
edited knowledge to be discarded during each LLM update and requiring frequent
re-editing, which is particularly burdensome in today's rapidly evolving
open-source community. To address this issue, we propose the problem of
cross-model knowledge editing and introduce MindBridge, a scalable solution
inspired by the low coupling between modality processing and LLMs in
multi-modal models. MindBridge introduces the novel concept of memory modality,
which encodes edited knowledge as an independent modality. It first performs
LLM-agnostic pre-training of the memory modality and then integrates it with
various LLMs. Extensive experiments on multiple LLMs and popular knowledge
editing datasets demonstrate that MindBridge achieves superior performance even
in editing tens of thousands of knowledge entries and can flexibly adapt to
different LLMs. Our code is available at
https://github.com/CrashBugger/MindBridge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Memory Efficient Continual Learning for Edge-Based Visual Anomaly
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02691v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02691v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Barusco, Lorenzo D'Antoni, Davide Dalle Pezze, Francesco Borsatti, Gian Antonio Susto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Anomaly Detection (VAD) is a critical task in computer vision with
numerous real-world applications. However, deploying these models on edge
devices presents significant challenges, such as constrained computational and
memory resources. Additionally, dynamic data distributions in real-world
settings necessitate continuous model adaptation, further complicating
deployment under limited resources. To address these challenges, we present a
novel investigation into the problem of Continual Learning for Visual Anomaly
Detection (CLAD) on edge devices. We evaluate the STFPM approach, given its low
memory footprint on edge devices, which demonstrates good performance when
combined with the Replay approach. Furthermore, we propose to study the
behavior of a recently proposed approach, PaSTe, specifically designed for the
edge but not yet explored in the Continual Learning context. Our results show
that PaSTe is not only a lighter version of STPFM, but it also achieves
superior anomaly detection performance, improving the f1 pixel performance by
10% with the Replay technique. In particular, the structure of PaSTe allows us
to test it using a series of Compressed Replay techniques, reducing memory
overhead by a maximum of 91.5% compared to the traditional Replay for STFPM.
Our study proves the feasibility of deploying VAD models that adapt and learn
incrementally on CLAD scenarios on resource-constrained edge devices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Class-Aware PillarMix: Can Mixed Sample Data Augmentation Enhance 3D
  Object Detection with Radar Point Clouds? <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miao Zhang, Sherif Abdulatif, Benedikt Loesch, Marco Altmann, Bin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the significant effort required for data collection and annotation in
3D perception tasks, mixed sample data augmentation (MSDA) has been widely
studied to generate diverse training samples by mixing existing data. Recently,
many MSDA techniques have been developed for point clouds, but they mainly
target LiDAR data, leaving their application to radar point clouds largely
unexplored. In this paper, we examine the feasibility of applying existing MSDA
methods to radar point clouds and identify several challenges in adapting these
techniques. These obstacles stem from the radar's irregular angular
distribution, deviations from a single-sensor polar layout in multi-radar
setups, and point sparsity. To address these issues, we propose Class-Aware
PillarMix (CAPMix), a novel MSDA approach that applies MixUp at the pillar
level in 3D point clouds, guided by class labels. Unlike methods that rely a
single mix ratio to the entire sample, CAPMix assigns an independent ratio to
each pillar, boosting sample diversity. To account for the density of different
classes, we use class-specific distributions: for dense objects (e.g., large
vehicles), we skew ratios to favor points from another sample, while for sparse
objects (e.g., pedestrians), we sample more points from the original. This
class-aware mixing retains critical details and enriches each sample with new
information, ultimately generating more diverse training data. Experimental
results demonstrate that our method not only significantly boosts performance
but also outperforms existing MSDA approaches across two datasets (Bosch Street
and K-Radar). We believe that this straightforward yet effective approach will
spark further investigation into MSDA techniques for radar data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures, 4 tables, submitted to 2025 IEEE/RSJ
  International Conference on Intelligent Robots and Systems (IROS 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seeding for Success: Skill and Stochasticity in Tabletop Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02686v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02686v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Goodman, Diego Perez-Liebana, Simon Lucas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Games often incorporate random elements in the form of dice or shuffled card
decks. This randomness is a key contributor to the player experience and the
variety of game situations encountered. There is a tension between a level of
randomness that makes the game interesting and contributes to the player
enjoyment of a game, and a level at which the outcome itself is effectively
random and the game becomes dull. The optimal level for a game will depend on
the design goals and target audience. We introduce a new technique to quantify
the level of randomness in game outcome and use it to compare 15 tabletop games
and disentangle the different contributions to the overall randomness from
specific parts of some games. We further explore the interaction between game
randomness and player skill, and how this innate randomness can affect error
analysis in common game experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in IEEE Transactions on Games, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MPO: Boosting <span class="highlight-title">LLM</span> Agents with Meta Plan Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weimin Xiong, Yifan Song, Qingxiu Dong, Bingchan Zhao, Feifan Song, Xun Wang, Sujian Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have enabled LLM-based
agents to successfully tackle interactive planning tasks. However, despite
their successes, existing approaches often suffer from planning hallucinations
and require retraining for each new agent. To address these challenges, we
propose the Meta Plan Optimization (MPO) framework, which enhances agent
planning capabilities by directly incorporating explicit guidance. Unlike
previous methods that rely on complex knowledge, which either require
significant human effort or lack quality assurance, MPO leverages high-level
general guidance through meta plans to assist agent planning and enables
continuous optimization of the meta plans based on feedback from the agent's
task execution. Our experiments conducted on two representative tasks
demonstrate that MPO significantly outperforms existing baselines. Moreover,
our analysis indicates that MPO provides a plug-and-play solution that enhances
both task completion efficiency and generalization capabilities in previous
unseen scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ State of play and future directions in industrial computer vision AI
  standards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02675v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02675v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Artemis Stefanidou, Panagiotis Radoglou-Grammatikis, Vasileios Argyriou, Panagiotis Sarigiannidis, Iraklis Varlamis, Georgios Th. Papadopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent tremendous advancements in the areas of Artificial Intelligence
(AI) and Deep Learning (DL) have also resulted into corresponding remarkable
progress in the field of Computer Vision (CV), showcasing robust technological
solutions in a wide range of application sectors of high industrial interest
(e.g., healthcare, autonomous driving, automation, etc.). Despite the
outstanding performance of CV systems in specific domains, their development
and exploitation at industrial-scale necessitates, among other, the addressing
of requirements related to the reliability, transparency, trustworthiness,
security, safety, and robustness of the developed AI models. The latter raises
the imperative need for the development of efficient, comprehensive and
widely-adopted industrial standards. In this context, this study investigates
the current state of play regarding the development of industrial computer
vision AI standards, emphasizing on critical aspects, like model
interpretability, data quality, and regulatory compliance. In particular, a
systematic analysis of launched and currently developing CV standards, proposed
by the main international standardization bodies (e.g. ISO/IEC, IEEE, DIN,
etc.) is performed. The latter is complemented by a comprehensive discussion on
the current challenges and future directions observed in this regularization
endeavor.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">dataset</span>-free approach for self-supervised learning of 3D reflectional
  symmetries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Issac Aguirre, Ivan Sipiran, Gabriel Montañana
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore a self-supervised model that learns to detect the
symmetry of a single object without requiring a dataset-relying solely on the
input object itself. We hypothesize that the symmetry of an object can be
determined by its intrinsic features, eliminating the need for large datasets
during training. Additionally, we design a self-supervised learning strategy
that removes the necessity of ground truth labels. These two key elements make
our approach both effective and efficient, addressing the prohibitive costs
associated with constructing large, labeled datasets for this task. The novelty
of our method lies in computing features for each point on the object based on
the idea that symmetric points should exhibit similar visual appearances. To
achieve this, we leverage features extracted from a foundational image model to
compute a visual descriptor for the points. This approach equips the point
cloud with visual features that facilitate the optimization of our
self-supervised model. Experimental results demonstrate that our method
surpasses the state-of-the-art models trained on large datasets. Furthermore,
our model is more efficient, effective, and operates with minimal computational
and data resources.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Effectiveness of Large Language Models in Transforming Unstructured
  Text to Standardized Formats 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        William Brach, Kristián Košťál, Michal Ries
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The exponential growth of unstructured text data presents a fundamental
challenge in modern data management and information retrieval. While Large
Language Models (LLMs) have shown remarkable capabilities in natural language
processing, their potential to transform unstructured text into standardized,
structured formats remains largely unexplored - a capability that could
revolutionize data processing workflows across industries. This study breaks
new ground by systematically evaluating LLMs' ability to convert unstructured
recipe text into the structured Cooklang format. Through comprehensive testing
of four models (GPT-4o, GPT-4o-mini, Llama3.1:70b, and Llama3.1:8b), an
innovative evaluation approach is introduced that combines traditional metrics
(WER, ROUGE-L, TER) with specialized metrics for semantic element
identification. Our experiments reveal that GPT-4o with few-shot prompting
achieves breakthrough performance (ROUGE-L: 0.9722, WER: 0.0730), demonstrating
for the first time that LLMs can reliably transform domain-specific
unstructured text into structured formats without extensive training. Although
model performance generally scales with size, we uncover surprising potential
in smaller models like Llama3.1:8b for optimization through targeted
fine-tuning. These findings open new possibilities for automated structured
data generation across various domains, from medical records to technical
documentation, potentially transforming the way organizations process and
utilize unstructured information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ YARE-GAN: Yet Another Resting State EEG-GAN 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02636v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02636v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeganeh Farahzadi, Morteza Ansarinia, Zoltan Kekecs
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Adversarial Networks (GANs) have shown promise in synthesising
realistic neural data, yet their potential for unsupervised representation
learning in resting-state EEG remains under explored. In this study, we
implement a Wasserstein GAN with Gradient Penalty (WGAN-GP) to generate
multi-channel resting-state EEG data and assess the quality of the synthesised
signals through both visual and feature-based evaluations. Our results indicate
that the model effectively captures the statistical and spectral
characteristics of real EEG data, although challenges remain in replicating
high-frequency oscillations in the frontal region. Additionally, we demonstrate
that the Critic's learned representations can be fine-tuned for age group
classification, achieving an out-of-sample accuracy, significantly better than
a shuffled-label baseline. These findings suggest that generative models can
serve not only as EEG data generators but also as unsupervised feature
extractors, reducing the need for manual feature engineering. This study
highlights the potential of GAN-based unsupervised learning for EEG analysis,
suggesting avenues for more data-efficient deep learning applications in
neuroscience.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reflection on Data Storytelling Tools in the Generative AI Era from the
  Human-AI Collaboration Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02631v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02631v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haotian Li, Yun Wang, Huamin Qu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-AI collaborative tools attract attentions from the data storytelling
community to lower the barrier of expertise and streamline the workflow. The
recent advance in large-scale generative AI techniques, e.g., large language
models (LLMs) and text-to-image models, has the potential to enhance data
storytelling with their power in visual and narration generation. After two
years since these techniques were publicly available, it is important to
reflect our progress of applying them and have an outlook for future
opportunities. To achieve the goal, we compare the collaboration patterns of
the latest tools with those of earlier ones using a dedicated framework for
understanding human-AI collaboration in data storytelling. Through comparison,
we identify persistent collaboration patterns, e.g., human-creator +
AI-assistant, and emerging ones, e.g., AI-creator + human-reviewer. The
benefits of these AI techniques and other implications to human-AI
collaboration are also revealed. We further propose future directions to
hopefully ignite innovations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is a sequel to the CHI 24 paper "Where Are We So Far?
  Understanding Data Storytelling Tools from the Perspective of Human-AI
  Collaboration (https://doi.org/10.1145/3613904.3642726), aiming to refresh
  our understanding with the latest advancements</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Event Extraction with Massive Types: <span class="highlight-title">LLM</span>-based Collaborative
  Annotation and Partitioning Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Liu, Zixuan Li, Long Bai, Yuxin Zuo, Daozhu Xu, Xiaolong Jin, Jiafeng Guo, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing a general-purpose extraction system that can extract events with
massive types is a long-standing target in Event Extraction (EE). In doing so,
the challenge comes from two aspects: 1) The absence of an efficient and
effective annotation method. 2) The absence of a powerful extraction method can
handle massive types. For the first challenge, we propose a collaborative
annotation method based on Large Language Models (LLMs). Through collaboration
among multiple LLMs, it first refines annotations of trigger words from distant
supervision and then carries out argument annotation. Next, a voting phase
consolidates the annotation preferences across different LLMs. Finally, we
create the EEMT dataset, the largest EE dataset to date, featuring over 200,000
samples, 3,465 event types, and 6,297 role types. For the second challenge, we
propose an LLM-based Partitioning EE method called LLM-PEE. To overcome the
limited context length of LLMs, LLM-PEE first recalls candidate event types and
then splits them into multiple partitions for LLMs to extract events. The
results in the supervised setting show that LLM-PEE outperforms the
state-of-the-art methods by 5.4 in event detection and 6.1 in argument
extraction. In the zero-shot setting, LLM-PEE achieves up to 12.9 improvement
compared to mainstream LLMs, demonstrating its strong generalization
capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reinforcement Learning-based Threat Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wuzhou Sun, Siyi Li, Qingxiang Zou, Zixing Liao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In some game scenarios, due to the uncertainty of the number of enemy units
and the priority of various attributes, the evaluation of the threat level of
enemy units as well as the screening has been a challenging research topic, and
the core difficulty lies in how to reasonably set the priority of different
attributes in order to achieve quantitative evaluation of the threat. In this
paper, we innovatively transform the problem of threat assessment into a
reinforcement learning problem, and through systematic reinforcement learning
training, we successfully construct an efficient neural network evaluator. The
evaluator can not only comprehensively integrate the multidimensional attribute
features of the enemy, but also effectively combine our state information, thus
realizing a more accurate and scientific threat assessment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages,9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seeing is Understanding: Unlocking Causal Attention into Modality-Mutual
  Attention for <span class="highlight-title">Multi</span>modal <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei-Yao Wang, Zhao Wang, Helen Suzuki, Yoshiyuki Kobayashi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent Multimodal Large Language Models (MLLMs) have demonstrated significant
progress in perceiving and reasoning over multimodal inquiries, ushering in a
new research era for foundation models. However, vision-language misalignment
in MLLMs has emerged as a critical challenge, where the textual responses
generated by these models are not factually aligned with the given text-image
inputs. Existing efforts to address vision-language misalignment have focused
on developing specialized vision-language connectors or leveraging visual
instruction tuning from diverse domains. In this paper, we tackle this issue
from a fundamental yet unexplored perspective by revisiting the core
architecture of MLLMs. Most MLLMs are typically built on decoder-only LLMs
consisting of a causal attention mechanism, which limits the ability of earlier
modalities (e.g., images) to incorporate information from later modalities
(e.g., text). To address this problem, we propose AKI, a novel MLLM that
unlocks causal attention into modality-mutual attention (MMA) to enable image
tokens to attend to text tokens. This simple yet effective design allows AKI to
achieve superior performance in 12 multimodal understanding benchmarks (+7.2%
on average) without introducing additional parameters and increasing training
time. Our MMA design is intended to be generic, allowing for application across
various modalities, and scalable to accommodate diverse multimodal scenarios.
The code is publicly available at https://github.com/sony/aki, and we will
release our AKI-4B model to encourage further advancements in MLLMs across
various directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StageDesigner: Artistic Stage Generation for Scenography via Theater
  Scripts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoxing Gan, Mengtian Li, Ruhua Chen, Zhongxia Ji, Sichen Guo, Huanling Hu, Guangnan Ye, Zuo Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce StageDesigner, the first comprehensive framework
for artistic stage generation using large language models combined with
layout-controlled diffusion models. Given the professional requirements of
stage scenography, StageDesigner simulates the workflows of seasoned artists to
generate immersive 3D stage scenes. Specifically, our approach is divided into
three primary modules: Script Analysis, which extracts thematic and spatial
cues from input scripts; Foreground Generation, which constructs and arranges
essential 3D objects; and Background Generation, which produces a harmonious
background aligned with the narrative atmosphere and maintains spatial
coherence by managing occlusions between foreground and background elements.
Furthermore, we introduce the StagePro-V1 dataset, a dedicated dataset with 276
unique stage scenes spanning different historical styles and annotated with
scripts, images, and detailed 3D layouts, specifically tailored for this task.
Finally, evaluations using both standard and newly proposed metrics, along with
extensive user studies, demonstrate the effectiveness of StageDesigner. Project
can be found at: https://deadsmither5.github.io/2025/01/03/StageDesigner/
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Playing games with Large language models: Randomness and strategy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02582v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02582v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alicia Vidler, Toby Walsh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Playing games has a long history of describing intricate interactions in
simplified forms. In this paper we explore if large language models (LLMs) can
play games, investigating their capabilities for randomisation and strategic
adaptation through both simultaneous and sequential game interactions. We focus
on GPT-4o-Mini-2024-08-17 and test two games between LLMs: Rock Paper Scissors
(RPS) and games of strategy (Prisoners Dilemma PD). LLMs are often described as
stochastic parrots, and while they may indeed be parrots, our results suggest
that they are not very stochastic in the sense that their outputs - when
prompted to be random - are often very biased. Our research reveals that LLMs
appear to develop loss aversion strategies in repeated games, with RPS
converging to stalemate conditions while PD shows systematic shifts between
cooperative and competitive outcomes based on prompt design. We detail
programmatic tools for independent agent interactions and the Agentic AI
challenges faced in implementation. We show that LLMs can indeed play games,
just not very well. These results have implications for the use of LLMs in
multi-agent LLM systems and showcase limitations in current approaches to model
output for strategic decision-making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">LLM</span>-Safety Evaluations Lack Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02574v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02574v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Beyer, Sophie Xhonneux, Simon Geisler, Gauthier Gidel, Leo Schwinn, Stephan Günnemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we argue that current safety alignment research efforts for
large language models are hindered by many intertwined sources of noise, such
as small datasets, methodological inconsistencies, and unreliable evaluation
setups. This can, at times, make it impossible to evaluate and compare attacks
and defenses fairly, thereby slowing progress. We systematically analyze the
LLM safety evaluation pipeline, covering dataset curation, optimization
strategies for automated red-teaming, response generation, and response
evaluation using LLM judges. At each stage, we identify key issues and
highlight their practical impact. We also propose a set of guidelines for
reducing noise and bias in evaluations of future attack and defense papers.
Lastly, we offer an opposing perspective, highlighting practical reasons for
existing limitations. We believe that addressing the outlined problems in
future research will improve the field's ability to generate easily comparable
results and make measurable progress.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RaceVLA: VLA-based Racing Drone Navigation with Human-like Behaviour <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02572v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02572v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valerii Serpiva, Artem Lykov, Artyom Myshlyaev, Muhammad Haris Khan, Ali Alridha Abdulkarim, Oleg Sautenkov, Dzmitry Tsetserukou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  RaceVLA presents an innovative approach for autonomous racing drone
navigation by leveraging Visual-Language-Action (VLA) to emulate human-like
behavior. This research explores the integration of advanced algorithms that
enable drones to adapt their navigation strategies based on real-time
environmental feedback, mimicking the decision-making processes of human
pilots. The model, fine-tuned on a collected racing drone dataset, demonstrates
strong generalization despite the complexity of drone racing environments.
RaceVLA outperforms OpenVLA in motion (75.0 vs 60.0) and semantic
generalization (45.5 vs 36.3), benefiting from the dynamic camera and
simplified motion tasks. However, visual (79.6 vs 87.0) and physical (50.0 vs
76.7) generalization were slightly reduced due to the challenges of maneuvering
in dynamic environments with varying object sizes. RaceVLA also outperforms
RT-2 across all axes - visual (79.6 vs 52.0), motion (75.0 vs 55.0), physical
(50.0 vs 26.7), and semantic (45.5 vs 38.8), demonstrating its robustness for
real-time adjustments in complex environments. Experiments revealed an average
velocity of 1.04 m/s, with a maximum speed of 2.02 m/s, and consistent
maneuverability, demonstrating RaceVLA's ability to handle high-speed scenarios
effectively. These findings highlight the potential of RaceVLA for
high-performance navigation in competitive racing contexts. The RaceVLA
codebase, pretrained weights, and dataset are available at this http URL:
https://racevla.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 6 figures. Submitted to IROS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Wo<span class="highlight-title">rl</span>d Models for Anomaly Detection during Model-Based Reinforcement
  Learning Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabian Domberg, Georg Schildbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning-based controllers are often purposefully kept out of real-world
applications due to concerns about their safety and reliability. We explore how
state-of-the-art world models in Model-Based Reinforcement Learning can be
utilized beyond the training phase to ensure a deployed policy only operates
within regions of the state-space it is sufficiently familiar with. This is
achieved by continuously monitoring discrepancies between a world model's
predictions and observed system behavior during inference. It allows for
triggering appropriate measures, such as an emergency stop, once an error
threshold is surpassed. This does not require any task-specific knowledge and
is thus universally applicable. Simulated experiments on established robot
control tasks show the effectiveness of this method, recognizing changes in
local robot geometry and global gravitational magnitude. Real-world experiments
using an agile quadcopter further demonstrate the benefits of this approach by
detecting unexpected forces acting on the vehicle. These results indicate how
even in new and adverse conditions, safe and reliable operation of otherwise
unpredictable learning-based controllers can be achieved.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated nnU-Net for Privacy-Preserving Medical Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Grzegorz Skorupko, Fotios Avgoustidis, Carlos Martín-Isla, Lidia Garrucho, Dimitri A. Kessler, Esmeralda Ruiz Pujadas, Oliver Díaz, Maciej Bobowicz, Katarzyna Gwoździewicz, Xavier Bargalló, Paulius Jaruševičius, Kaisar Kushibar, Karim Lekadir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The nnU-Net framework has played a crucial role in medical image segmentation
and has become the gold standard in multitudes of applications targeting
different diseases, organs, and modalities. However, so far it has been used
primarily in a centralized approach where the data collected from hospitals are
stored in one center and used to train the nnU-Net. This centralized approach
has various limitations, such as leakage of sensitive patient information and
violation of patient privacy. Federated learning is one of the approaches to
train a segmentation model in a decentralized manner that helps preserve
patient privacy. In this paper, we propose FednnU-Net, a federated learning
extension of nnU-Net. We introduce two novel federated learning methods to the
nnU-Net framework - Federated Fingerprint Extraction (FFE) and Asymmetric
Federated Averaging (AsymFedAvg) - and experimentally show their consistent
performance for breast, cardiac and fetal segmentation using 6 datasets
representing samples from 18 institutions. Additionally, to further promote
research and deployment of decentralized training in privacy constrained
institutions, we make our plug-n-play framework public. The source-code is
available at https://github.com/faildeny/FednnUNet .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RectifiedHR: Enable Efficient High-Resolution Image Generation via
  Energy Rectification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Yang, Guibao Shen, Liang Hou, Mushui Liu, Luozhou Wang, Xin Tao, Pengfei Wan, Di Zhang, Ying-Cong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have achieved remarkable advances in various image
generation tasks. However, their performance notably declines when generating
images at resolutions higher than those used during the training period.
Despite the existence of numerous methods for producing high-resolution images,
they either suffer from inefficiency or are hindered by complex operations. In
this paper, we propose RectifiedHR, an efficient and straightforward solution
for training-free high-resolution image generation. Specifically, we introduce
the noise refresh strategy, which theoretically only requires a few lines of
code to unlock the model's high-resolution generation ability and improve
efficiency. Additionally, we first observe the phenomenon of energy decay that
may cause image blurriness during the high-resolution image generation process.
To address this issue, we propose an Energy Rectification strategy, where
modifying the hyperparameters of the classifier-free guidance effectively
improves the generation performance. Our method is entirely training-free and
boasts a simple implementation logic. Through extensive comparisons with
numerous baseline methods, our RectifiedHR demonstrates superior effectiveness
and efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Page: https://zhenyangcs.github.io/RectifiedHR-Diffusion/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LTL Verification of Memoryful Neural Agents <span class="chip">AAMAS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02512v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02512v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehran Hosseini, Alessio Lomuscio, Nicola Paoletti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a framework for verifying Memoryful Neural Multi-Agent Systems
(MN-MAS) against full Linear Temporal Logic (LTL) specifications. In MN-MAS,
agents interact with a non-deterministic, partially observable environment.
Examples of MN-MAS include multi-agent systems based on feed-forward and
recurrent neural networks or state-space models. Different from previous
approaches, we support the verification of both bounded and unbounded LTL
specifications. We leverage well-established bounded model checking techniques,
including lasso search and invariant synthesis, to reduce the verification
problem to that of constraint solving. To solve these constraints, we develop
efficient methods based on bound propagation, mixed-integer linear programming,
and adaptive splitting. We evaluate the effectiveness of our algorithms in
single and multi-agent environments from the Gymnasium and PettingZoo
libraries, verifying unbounded specifications for the first time and improving
the verification time for bounded specifications by an order of magnitude
compared to the SoA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 2 figures, accepted at AAMAS 2025 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ROCKET-2: Steering Visuomotor Policy via Cross-View Goal Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02505v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02505v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaofei Cai, Zhancun Mu, Anji Liu, Yitao Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We aim to develop a goal specification method that is semantically clear,
spatially sensitive, and intuitive for human users to guide agent interactions
in embodied environments. Specifically, we propose a novel cross-view goal
alignment framework that allows users to specify target objects using
segmentation masks from their own camera views rather than the agent's
observations. We highlight that behavior cloning alone fails to align the
agent's behavior with human intent when the human and agent camera views differ
significantly. To address this, we introduce two auxiliary objectives:
cross-view consistency loss and target visibility loss, which explicitly
enhance the agent's spatial reasoning ability. According to this, we develop
ROCKET-2, a state-of-the-art agent trained in Minecraft, achieving an
improvement in the efficiency of inference 3x to 6x. We show ROCKET-2 can
directly interpret goals from human camera views for the first time, paving the
way for better human-agent interaction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PennyLang: Pioneering <span class="highlight-title">LLM</span>-Based Quantum Code Generation with a Novel
  PennyLane-Centric <span class="highlight-title">Dataset</span> <span class="chip">IJCNN 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02497v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02497v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haider Asif, Abdul Basit, Nouhaila Innan, Muhammad Kashif, Alberto Marchisio, Muhammad Shafique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) offer remarkable capabilities in code
generation, natural language processing, and domain-specific reasoning. Their
potential in aiding quantum software development remains underexplored,
particularly for the PennyLane framework-a leading platform for hybrid
quantum-classical computing. To address this gap, we introduce a novel,
high-quality dataset comprising 3,347 PennyLane-specific code samples of
quantum circuits and their contextual descriptions, specifically curated to
train/fine-tune LLM-based quantum code assistance. Our key contributions are
threefold: (1) the automatic creation and open-source release of a
comprehensive PennyLane dataset leveraging quantum computing textbooks,
official documentation, and open-source repositories; (2) the development of a
systematic methodology for data refinement, annotation, and formatting to
optimize LLM training efficiency; and (3) a thorough evaluation, based on a
Retrieval-Augmented Generation (RAG) framework, demonstrating the effectiveness
of our dataset in streamlining PennyLane code generation and improving quantum
development workflows. Compared to existing efforts that predominantly focus on
Qiskit, our dataset significantly broadens the spectrum of quantum frameworks
covered in AI-driven code assistance. By bridging this gap and providing
reproducible dataset-creation methodologies, we aim to advance the field of
AI-assisted quantum programming, making quantum computing more accessible to
both newcomers and experienced developers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures, 6 tables, submitted for review under IJCNN 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Union of Experts: Adapting Hierarchical Routing to Equivalently
  Decomposed Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujiao Yang, Jing Lian, Linhui Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture-of-Experts (MoE) enhances model performance while maintaining
computational efficiency, making it well-suited for large-scale applications.
However, expert in exist MoE paradigm works as an individual, thereby lacking
high-quality expert interactions. Moreover, they have not been effectively
extended to attention block, which constrains further efficiency improvements.
To tackle these issues, we propose Union-of-Experts (UoE), which decomposes
transformer into an equitant group of experts, and then implement dynamic
routing on input data and experts. Our approach advances MoE design with three
key innovations: (1) We conducted equitant expert decomposition on both MLP
blocks and attention blocks based on matrix partition in tensor parallelism.
(2) We developed two routing paradigms: patch wise data selection and expert
selection, to apply routing across different levels. (3) We design the
architecture of UoE model, including Selective Multi-Head Attention (SMHA) and
Union-of-MLP-Experts (UoME). (4) We develop parallel implementation of UoE's
routing and computation operation, and optimize efficiency based on the
hardware processing analysis. The experiments demonstrate that the model
employed with UoE surpass Full Attention, state-of-art MoEs and efficient
transformers in several tasks across image and natural language domains. The
source codes are available at https://github.com/YujiaoYang-work/UoE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 6 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ERetinex: Event Camera Meets Retinex Theory for Low-Light Image
  Enhancement <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02484v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02484v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuejian Guo, Zhiqiang Tian, Yuehang Wang, Siqi Li, Yu Jiang, Shaoyi Du, Yue Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-light image enhancement aims to restore the under-exposure image captured
in dark scenarios. Under such scenarios, traditional frame-based cameras may
fail to capture the structure and color information due to the exposure time
limitation. Event cameras are bio-inspired vision sensors that respond to
pixel-wise brightness changes asynchronously. Event cameras' high dynamic range
is pivotal for visual perception in extreme low-light scenarios, surpassing
traditional cameras and enabling applications in challenging dark environments.
In this paper, inspired by the success of the retinex theory for traditional
frame-based low-light image restoration, we introduce the first methods that
combine the retinex theory with event cameras and propose a novel retinex-based
low-light image restoration framework named ERetinex. Among our contributions,
the first is developing a new approach that leverages the high temporal
resolution data from event cameras with traditional image information to
estimate scene illumination accurately. This method outperforms traditional
image-only techniques, especially in low-light environments, by providing more
precise lighting information. Additionally, we propose an effective fusion
strategy that combines the high dynamic range data from event cameras with the
color information of traditional images to enhance image quality. Through this
fusion, we can generate clearer and more detail-rich images, maintaining the
integrity of visual information even under extreme lighting conditions. The
experimental results indicate that our proposed method outperforms
state-of-the-art (SOTA) methods, achieving a gain of 1.0613 dB in PSNR while
reducing FLOPS by \textbf{84.28}\%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BioD2C: A Dual-level Semantic Consistency Constraint Framework for
  Biomedical VQA 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengyang Ji, Shang Gao, Li Liu, Yifan Jia, Yutao Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biomedical visual question answering (VQA) has been widely studied and has
demonstrated significant application value and potential in fields such as
assistive medical diagnosis. Despite their success, current biomedical VQA
models perform multimodal information interaction only at the model level
within large language models (LLMs), leading to suboptimal multimodal semantic
alignment when dealing with complex tasks. To address this issue, we propose
BioD2C: a novel Dual-level Semantic Consistency Constraint Framework for
Biomedical VQA, which achieves dual-level semantic interaction alignment at
both the model and feature levels, enabling the model to adaptively learn
visual features based on the question. Specifically, we firstly integrate
textual features into visual features via an image-text fusion mechanism as
feature-level semantic interaction, obtaining visual features conditioned on
the given text; and then introduce a text-queue-based cross-modal soft semantic
loss function to further align the image semantics with the question semantics.
Specifically, in this work, we establish a new dataset, BioVGQ, to address
inherent biases in prior datasets by filtering manually-altered images and
aligning question-answer pairs with multimodal context, and train our model on
this dataset. Extensive experimental results demonstrate that BioD2C achieves
state-of-the-art (SOTA) performance across multiple downstream datasets,
showcasing its robustness, generalizability, and potential to advance
biomedical VQA research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Don't Get Too Excited -- Eliciting Emotions in <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gino Franco Fazzi, Julie Skoven Hinge, Stefan Heinrich, Paolo Burelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the challenges of affect control in large language
models (LLMs), focusing on their ability to express appropriate emotional
states during extended dialogues. We evaluated state-of-the-art open-weight
LLMs to assess their affective expressive range in terms of arousal and
valence. Our study employs a novel methodology combining LLM-based sentiment
analysis with multiturn dialogue simulations between LLMs. We quantify the
models' capacity to express a wide spectrum of emotions and how they fluctuate
during interactions. Our findings reveal significant variations among LLMs in
their ability to maintain consistent affect, with some models demonstrating
more stable emotional trajectories than others. Furthermore, we identify key
challenges in affect control, including difficulties in producing and
maintaining extreme emotional states and limitations in adapting affect to
changing conversational contexts. These findings have important implications
for the development of more emotionally intelligent AI systems and highlight
the need for improved affect modelling in LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sparse Meets Dense: Unified Generative Recommendations with Cascaded
  Sparse-Dense Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02453v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02453v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Yang, Zhi Ji, Zhaopeng Li, Yi Li, Zhonglin Mo, Yue Ding, Kai Chen, Zijian Zhang, Jie Li, Shuanglong Li, Lin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative models have recently gained attention in recommendation systems by
directly predicting item identifiers from user interaction sequences. However,
existing methods suffer from significant information loss due to the separation
of stages such as quantization and sequence modeling, hindering their ability
to achieve the modeling precision and accuracy of sequential dense retrieval
techniques. Integrating generative and dense retrieval methods remains a
critical challenge. To address this, we introduce the Cascaded Organized
Bi-Represented generAtive retrieval (COBRA) framework, which innovatively
integrates sparse semantic IDs and dense vectors through a cascading process.
Our method alternates between generating these representations by first
generating sparse IDs, which serve as conditions to aid in the generation of
dense vectors. End-to-end training enables dynamic refinement of dense
representations, capturing both semantic insights and collaborative signals
from user-item interactions. During inference, COBRA employs a coarse-to-fine
strategy, starting with sparse ID generation and refining them into dense
vectors via the generative model. We further propose BeamFusion, an innovative
approach combining beam search with nearest neighbor scores to enhance
inference flexibility and recommendation diversity. Extensive experiments on
public datasets and offline tests validate our method's robustness. Online A/B
tests on a real-world advertising platform with over 200 million daily users
demonstrate substantial improvements in key metrics, highlighting COBRA's
practical advantages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Model Quantization in GenAI-based Image Inpainting and
  Detection of Arable Plants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sourav Modak, Ahmet Oğuz Saltık, Anthony Stein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning-based weed control systems often suffer from limited training
data diversity and constrained on-board computation, impacting their real-world
performance. To overcome these challenges, we propose a framework that
leverages Stable Diffusion-based inpainting to augment training data
progressively in 10% increments -- up to an additional 200%, thus enhancing
both the volume and diversity of samples. Our approach is evaluated on two
state-of-the-art object detection models, YOLO11(l) and RT-DETR(l), using the
mAP50 metric to assess detection performance. We explore quantization
strategies (FP16 and INT8) for both the generative inpainting and detection
models to strike a balance between inference speed and accuracy. Deployment of
the downstream models on the Jetson Orin Nano demonstrates the practical
viability of our framework in resource-constrained environments, ultimately
improving detection accuracy and computational efficiency in intelligent weed
management systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoEval: A Practical Framework for Autonomous Evaluation of Mobile
  Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahui Sun, Zhichao Hua, Yubin Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate and systematic evaluation of mobile agents can significantly advance
their development and real-world applicability. However, existing benchmarks
for mobile agents lack practicality and scalability due to the extensive manual
effort required to define task reward signals and implement corresponding
evaluation codes. To this end, we propose AutoEval, an autonomous agent
evaluation framework that tests a mobile agent without any manual effort.
First, we design a Structured Substate Representation to describe the UI state
changes while agent execution, such that task reward signals can be
automatically generated. Second, we utilize a Judge System that can
autonomously evaluate agents' performance given the automatically generated
task reward signals. By providing only a task description, our framework
evaluates agents with fine-grained performance feedback to that task without
any extra manual effort. We implement a prototype of our framework and validate
the automatically generated task reward signals, finding over 93% coverage to
human-annotated reward signals. Moreover, to prove the effectiveness of our
autonomous Judge System, we manually verify its judge results and demonstrate
that it achieves 94% accuracy. Finally, we evaluate the state-of-the-art mobile
agents using our framework, providing detailed insights into their performance
characteristics and limitations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VisAgent: Narrative-Preserving Story Visualization Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seungkwon Kim, GyuTae Park, Sangyeon Kim, Seung-Hun Nam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Story visualization is the transformation of narrative elements into image
sequences. While existing research has primarily focused on visual contextual
coherence, the deeper narrative essence of stories often remains overlooked.
This limitation hinders the practical application of these approaches, as
generated images frequently fail to capture the intended meaning and nuances of
the narrative fully. To address these challenges, we propose VisAgent, a
training-free multi-agent framework designed to comprehend and visualize
pivotal scenes within a given story. By considering story distillation,
semantic consistency, and contextual coherence, VisAgent employs an agentic
workflow. In this workflow, multiple specialized agents collaborate to: (i)
refine layered prompts based on the narrative structure and (ii) seamlessly
integrate \gt{generated} elements, including refined prompts, scene elements,
and subject placement, into the final image. The empirically validated
effectiveness confirms the framework's suitability for practical story
visualization applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICASSP 2025. Equal contribution from first two authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PersonaX: A Recommendation Agent Oriented User Modeling Framework for
  Long Behavior Sequence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02398v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02398v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunxiao Shi, Wujiang Xu, Zeqi Zhang, Xing Zi, Qiang Wu, Min Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommendation agents leverage large language models for user modeling LLM UM
to construct textual personas guiding alignment with real users. However
existing LLM UM methods struggle with long user generated content UGC due to
context limitations and performance degradation. To address this sampling
strategies prioritize relevance or recency are often applied yet they
inevitably neglect the diverse user interests embedded within the discarded
behaviors resulting in incomplete modeling and degraded profiling quality.
Furthermore relevance based sampling requires real time retrieval forcing the
user modeling process to operate online which introduces significant latency
overhead. In this paper we propose PersonaX an agent agnostic LLM UM framework
that tackles these challenges through sub behavior sequence SBS selection and
offline multi persona construction. PersonaX extracts compact SBS segments
offline to capture diverse user interests generating fine grained textual
personas that are cached for efficient online retrieval. This approach ensures
that the user persona used for prompting remains highly relevant to the current
context while eliminating the need for online user modeling. For SBS selection
we ensure both efficiency length less than five and high representational
quality by balancing prototypicality and diversity within the sampled data.
Extensive experiments validate the effectiveness and versatility of PersonaX in
high quality user profiling. Utilizing only 30 to 50 percent of the behavioral
data with a sequence length of 480 integrating PersonaX with AgentCF yields an
absolute performance improvement of 3 to 11 percent while integration with
Agent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic
framework sets a new benchmark for scalable user modeling paving the way for
more accurate and efficient LLM driven recommendation agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>draft paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Binary Classification Social Network <span class="highlight-title">Dataset</span> for Graph Machine
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02397v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02397v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adnan Ali, Jinglong Li, Huanhuan Chen, AlMotasem Bellah Al Ajlouni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social networks have a vast range of applications with graphs. The available
benchmark datasets are citation, co-occurrence, e-commerce networks, etc, with
classes ranging from 3 to 15. However, there is no benchmark classification
social network dataset for graph machine learning. This paper fills the gap and
presents the Binary Classification Social Network Dataset (\textit{BiSND}),
designed for graph machine learning applications to predict binary classes. We
present the BiSND in \textit{tabular and graph} formats to verify its
robustness across classical and advanced machine learning. We employ a diverse
set of classifiers, including four traditional machine learning algorithms
(Decision Trees, K-Nearest Neighbour, Random Forest, XGBoost), one Deep Neural
Network (multi-layer perceptrons), one Graph Neural Network (Graph
Convolutional Network), and three state-of-the-art Graph Contrastive Learning
methods (BGRL, GRACE, DAENS). Our findings reveal that BiSND is suitable for
classification tasks, with F1-scores ranging from 67.66 to 70.15, indicating
promising avenues for future enhancements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Efficient and Precise Training Data Construction Framework for
  Process-supervised Reward Model in Mathematical Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02382v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02382v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Sun, Qianlong Du, Fuwei Cui, Jiajun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enhancing the mathematical reasoning capabilities of Large Language Models
(LLMs) is of great scientific and practical significance. Researchers typically
employ process-supervised reward models (PRMs) to guide the reasoning process,
effectively improving the models' reasoning abilities. However, existing
methods for constructing process supervision training data, such as manual
annotation and per-step Monte Carlo estimation, are often costly or suffer from
poor quality. To address these challenges, this paper introduces a framework
called EpicPRM, which annotates each intermediate reasoning step based on its
quantified contribution and uses an adaptive binary search algorithm to enhance
both annotation precision and efficiency. Using this approach, we efficiently
construct a high-quality process supervision training dataset named Epic50k,
consisting of 50k annotated intermediate steps. Compared to other publicly
available datasets, the PRM trained on Epic50k demonstrates significantly
superior performance. Getting Epic50k at https://github.com/xiaolizh1/EpicPRM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JPDS-NN: Reinforcement Learning-Based Dynamic Task Allocation for
  Agricultural Vehicle Routing Optimization <span class="chip">IROS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02369v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02369v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixuan Fan, Haotian Xu, Mengqiao Liu, Qing Zhuo, Tao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Entrance Dependent Vehicle Routing Problem (EDVRP) is a variant of the
Vehicle Routing Problem (VRP) where the scale of cities influences routing
outcomes, necessitating consideration of their entrances. This paper addresses
EDVRP in agriculture, focusing on multi-parameter vehicle planning for
irregularly shaped fields. To address the limitations of traditional methods,
such as heuristic approaches, which often overlook field geometry and entrance
constraints, we propose a Joint Probability Distribution Sampling Neural
Network (JPDS-NN) to effectively solve the EDVRP. The network uses an
encoder-decoder architecture with graph transformers and attention mechanisms
to model routing as a Markov Decision Process, and is trained via reinforcement
learning for efficient and rapid end-to-end planning. Experimental results
indicate that JPDS-NN reduces travel distances by 48.4-65.4%, lowers fuel
consumption by 14.0-17.6%, and computes two orders of magnitude faster than
baseline methods, while demonstrating 15-25% superior performance in dynamic
arrangement scenarios. Ablation studies validate the necessity of
cross-attention and pre-training. The framework enables scalable, intelligent
routing for large-scale farming under dynamic constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, submitted to IROS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EchoQA: A Large Collection of Instruction Tuning Data for Echocardiogram
  Reports 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lama Moukheiber, Mira Moukheiber, Dana Moukheiiber, Hyung-Chul Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel question-answering (QA) dataset using echocardiogram
reports sourced from the Medical Information Mart for Intensive Care database.
This dataset is specifically designed to enhance QA systems in cardiology,
consisting of 771,244 QA pairs addressing a wide array of cardiac abnormalities
and their severity. We compare large language models (LLMs), including
open-source and biomedical-specific models for zero-shot evaluation, and
closed-source models for zero-shot and three-shot evaluation. Our results show
that fine-tuning LLMs improves performance across various QA metrics,
validating the value of our dataset. Clinicians also qualitatively evaluate the
best-performing model to assess the LLM responses for correctness. Further, we
conduct fine-grained fairness audits to assess the bias-performance trade-off
of LLMs across various social determinants of health. Our objective is to
propel the field forward by establishing a benchmark for LLM AI agents aimed at
supporting clinicians with cardiac differential diagnoses, thereby reducing the
documentation burden that contributes to clinician burnout and enabling
healthcare professionals to focus more on patient care.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS SafeGenAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BdSLW401: Transformer-Based Word-Level Bangla Sign Language Recognition
  Using Relative Quantization Encoding (RQE) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Husne Ara Rubaiyeat, Njayou Youssouf, Md Kamrul Hasan, Hasan Mahmud
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sign language recognition (SLR) for low-resource languages like Bangla
suffers from signer variability, viewpoint variations, and limited annotated
datasets. In this paper, we present BdSLW401, a large-scale, multi-view,
word-level Bangla Sign Language (BdSL) dataset with 401 signs and 102,176 video
samples from 18 signers in front and lateral views. To improve
transformer-based SLR, we introduce Relative Quantization Encoding (RQE), a
structured embedding approach anchoring landmarks to physiological reference
points and quantize motion trajectories. RQE improves attention allocation by
decreasing spatial variability, resulting in 44.3% WER reduction in WLASL100,
21.0% in SignBD-200, and significant gains in BdSLW60 and SignBD-90. However,
fixed quantization becomes insufficient on large-scale datasets (e.g.,
WLASL2000), indicating the need for adaptive encoding strategies. Further,
RQE-SF, an extended variant that stabilizes shoulder landmarks, achieves
improvements in pose consistency at the cost of small trade-offs in lateral
view recognition. The attention graphs prove that RQE improves model
interpretability by focusing on the major articulatory features (fingers,
wrists) and the more distinctive frames instead of global pose changes.
Introducing BdSLW401 and demonstrating the effectiveness of RQE-enhanced
structured embeddings, this work advances transformer-based SLR for
low-resource languages and sets a benchmark for future research in this area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are Large Vision Language Models Good Game Players? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02358v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02358v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Wang, Bohan Zhuang, Qi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision Language Models (LVLMs) have demonstrated remarkable abilities
in understanding and reasoning about both visual and textual information.
However, existing evaluation methods for LVLMs, primarily based on benchmarks
like Visual Question Answering and image captioning, often fail to capture the
full scope of LVLMs' capabilities. These benchmarks are limited by issues such
as inadequate assessment of detailed visual perception, data contamination, and
a lack of focus on multi-turn reasoning. To address these challenges, we
propose \method{}, a game-based evaluation framework designed to provide a
comprehensive assessment of LVLMs' cognitive and reasoning skills in structured
environments. \method{} uses a set of games to evaluate LVLMs on four core
tasks: Perceiving, Question Answering, Rule Following, and End-to-End Playing,
with each target task designed to assess specific abilities, including visual
perception, reasoning, decision-making, etc. Based on this framework, we
conduct extensive experiments that explore the limitations of current LVLMs,
such as handling long structured outputs and perceiving detailed and dense
elements. Code and data are publicly available at
https://github.com/xinke-wang/LVLM-Playground.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoServe: Efficient Collaboration-of-Experts (CoE) Model Inference with
  Limited Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02354v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02354v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiashun Suo, Xiaojian Liao, Limin Xiao, Li Ruan, Jinquan Wang, Xiao Su, Zhisheng Huo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models like GPT-4 are resource-intensive, but recent
advancements suggest that smaller, specialized experts can outperform the
monolithic models on specific tasks. The Collaboration-of-Experts (CoE)
approach integrates multiple expert models, improving the accuracy of generated
results and offering great potential for precision-critical applications, such
as automatic circuit board quality inspection. However, deploying CoE serving
systems presents challenges to memory capacity due to the large number of
experts required, which can lead to significant performance overhead from
frequent expert switching across different memory and storage tiers.
  We propose CoServe, an efficient CoE model serving system on heterogeneous
CPU and GPU with limited memory. CoServe reduces unnecessary expert switching
by leveraging expert dependency, a key property of CoE inference. CoServe
introduces a dependency-aware request scheduler and dependency-aware expert
management for efficient inference. It also introduces an offline profiler to
automatically find optimal resource allocation on various processors and
devices. In real-world intelligent manufacturing workloads, CoServe achieves
4.5$\times$ to 12$\times$ higher throughput compared to state-of-the-art
systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ASPLOS '25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MindSimulator: Exploring Brain Concept Localization via Synthetic FMRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02351v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02351v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyin Bao, Qi Zhang, Zixuan Gong, Zhuojia Wu, Duoqian Miao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concept-selective regions within the human cerebral cortex exhibit
significant activation in response to specific visual stimuli associated with
particular concepts. Precisely localizing these regions stands as a crucial
long-term goal in neuroscience to grasp essential brain functions and
mechanisms. Conventional experiment-driven approaches hinge on manually
constructed visual stimulus collections and corresponding brain activity
recordings, constraining the support and coverage of concept localization.
Additionally, these stimuli often consist of concept objects in unnatural
contexts and are potentially biased by subjective preferences, thus prompting
concerns about the validity and generalizability of the identified regions. To
address these limitations, we propose a data-driven exploration approach. By
synthesizing extensive brain activity recordings, we statistically localize
various concept-selective regions. Our proposed MindSimulator leverages
advanced generative technologies to learn the probability distribution of brain
activity conditioned on concept-oriented visual stimuli. This enables the
creation of simulated brain recordings that reflect real neural response
patterns. Using the synthetic recordings, we successfully localize several
well-studied concept-selective regions and validate them against empirical
findings, achieving promising prediction accuracy. The feasibility opens
avenues for exploring novel concept-selective regions and provides prior
hypotheses for future neuroscience research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CQ CNN: A Hybrid Classical Quantum Convolutional Neural Network for
  Alzheimer's Disease Detection Using Diffusion Generated and U Net Segmented
  3D MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02345v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02345v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mominul Islam, Mohammad Junayed Hasan, M. R. C. Mahdy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The detection of Alzheimer disease (AD) from clinical MRI data is an active
area of research in medical imaging. Recent advances in quantum computing,
particularly the integration of parameterized quantum circuits (PQCs) with
classical machine learning architectures, offer new opportunities to develop
models that may outperform traditional methods. However, quantum machine
learning (QML) remains in its early stages and requires further experimental
analysis to better understand its behavior and limitations. In this paper, we
propose an end to end hybrid classical quantum convolutional neural network (CQ
CNN) for AD detection using clinically formatted 3D MRI data. Our approach
involves developing a framework to make 3D MRI data usable for machine
learning, designing and training a brain tissue segmentation model (Skull Net),
and training a diffusion model to generate synthetic images for the minority
class. Our converged models exhibit potential quantum advantages, achieving
higher accuracy in fewer epochs than classical models. The proposed beta8 3
qubit model achieves an accuracy of 97.50%, surpassing state of the art (SOTA)
models while requiring significantly fewer computational resources. In
particular, the architecture employs only 13K parameters (0.48 MB), reducing
the parameter count by more than 99.99% compared to current SOTA models.
Furthermore, the diffusion-generated data used to train our quantum models, in
conjunction with real samples, preserve clinical structural standards,
representing a notable first in the field of QML. We conclude that CQCNN
architecture like models, with further improvements in gradient optimization
techniques, could become a viable option and even a potential alternative to
classical models for AD detection, especially in data limited and resource
constrained clinical settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Application of hybrid quantum-classical machine learning for (early
  stage) disease detection</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GRADEO: Towards Human-Like Evaluation for Text-to-Video Generation via
  <span class="highlight-title">Multi</span>-Step Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhun Mou, Bin Xia, Zhengchao Huang, Wenming Yang, Jiaya Jia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent great advances in video generation models have demonstrated their
potential to produce high-quality videos, bringing challenges to effective
evaluation. Unlike human evaluation, existing automated evaluation metrics lack
high-level semantic understanding and reasoning capabilities for video, thus
making them infeasible and unexplainable. To fill this gap, we curate
GRADEO-Instruct, a multi-dimensional T2V evaluation instruction tuning dataset,
including 3.3k videos from over 10 existing video generation models and
multi-step reasoning assessments converted by 16k human annotations. We then
introduce GRADEO, one of the first specifically designed video evaluation
models, which grades AI-generated videos for explainable scores and assessments
through multi-step reasoning. Experiments show that our method aligns better
with human evaluations than existing methods. Furthermore, our benchmarking
reveals that current video generation models struggle to produce content that
aligns with human reasoning and complex real-world scenarios. The models,
datasets, and codes will be released soon.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing the Product Quality of the Injection Process Using eXplainable
  Artificial Intelligence 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jisoo Hong, Yongmin Hong, Jung-Woo Baek, Sung-Woo Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The injection molding process is a traditional technique for making products
in various industries such as electronics and automobiles via solidifying
liquid resin into certain molds. Although the process is not related to
creating the main part of engines or semiconductors, this manufacturing
methodology sets the final form of the products. Re-cently, research has
continued to reduce the defect rate of the injection molding process. This
study proposes an optimal injection molding process control system to reduce
the defect rate of injection molding products with XAI (eXplainable Artificial
Intelligence) ap-proaches. Boosting algorithms (XGBoost and LightGBM) are used
as tree-based classifiers for predicting whether each product is normal or
defective. The main features to control the process for improving the product
are extracted by SHapley Additive exPlanations, while the individual
conditional expectation analyzes the optimal control range of these extracted
features. To validate the methodology presented in this work, the actual
injection molding AI manufacturing dataset provided by KAMP (Korea AI
Manufacturing Platform) is employed for the case study. The results reveal that
the defect rate decreases from 1.00% (Original defect rate) to 0.21% with
XGBoost and 0.13% with LightGBM, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BiasICL: In-Context Learning and Demographic Biases of Vision Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sonnet Xu, Joseph Janizek, Yixing Jiang, Roxana Daneshjou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision language models (VLMs) show promise in medical diagnosis, but their
performance across demographic subgroups when using in-context learning (ICL)
remains poorly understood. We examine how the demographic composition of
demonstration examples affects VLM performance in two medical imaging tasks:
skin lesion malignancy prediction and pneumothorax detection from chest
radiographs. Our analysis reveals that ICL influences model predictions through
multiple mechanisms: (1) ICL allows VLMs to learn subgroup-specific disease
base rates from prompts and (2) ICL leads VLMs to make predictions that perform
differently across demographic groups, even after controlling for
subgroup-specific disease base rates. Our empirical results inform
best-practices for prompting current VLMs (specifically examining demographic
subgroup performance, and matching base rates of labels to target distribution
at a bulk level and within subgroups), while also suggesting next steps for
improving our theoretical understanding of these models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Examining the Mental Health Impact of Misinformation on Social Media
  Using a Hybrid Transformer-Based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02333v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02333v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sarvesh Arora, Sarthak Arora, Deepika Kumar, Vallari Agrawal, Vedika Gupta, Dipit Vasdev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social media has significantly reshaped interpersonal communication,
fostering connectivity while also enabling the proliferation of misinformation.
The unchecked spread of false narratives has profound effects on mental health,
contributing to increased stress, anxiety, and misinformation-driven paranoia.
This study presents a hybrid transformer-based approach using a RoBERTa-LSTM
classifier to detect misinformation, assess its impact on mental health, and
classify disorders linked to misinformation exposure. The proposed models
demonstrate accuracy rates of 98.4, 87.8, and 77.3 in detecting misinformation,
mental health implications, and disorder classification, respectively.
Furthermore, Pearson's Chi-Squared Test for Independence (p-value = 0.003871)
validates the direct correlation between misinformation and deteriorating
mental well-being. This study underscores the urgent need for better
misinformation management strategies to mitigate its psychological
repercussions. Future research could explore broader datasets incorporating
linguistic, demographic, and cultural variables to deepen the understanding of
misinformation-induced mental health distress.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PromptCoT: Synthesizing Olympiad-level Problems for Mathematical
  Reasoning in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02324v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02324v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueliang Zhao, Wei Wu, Jian Guan, Lingpeng Kong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability of large language models to solve complex mathematical problems
has progressed significantly, particularly for tasks requiring advanced
reasoning. However, the scarcity of sufficiently challenging problems,
particularly at the Olympiad level, hinders further advancements. In this work,
we introduce PromptCoT, a novel approach for automatically generating
high-quality Olympiad-level math problems. The proposed method synthesizes
complex problems based on mathematical concepts and the rationale behind
problem construction, emulating the thought processes of experienced problem
designers. We provide a theoretical analysis demonstrating that an optimal
rationale should maximize both the likelihood of rationale generation given the
associated concepts and the likelihood of problem generation conditioned on
both the rationale and the concepts. Our method is evaluated on standard
benchmarks including GSM8K, MATH-500, and AIME2024, where it consistently
outperforms existing problem generation methods. Furthermore, we demonstrate
that PromptCoT exhibits superior data scalability, consistently maintaining
high performance as the dataset size increases, outperforming the baselines.
The implementation is available at https://github.com/zhaoxlpku/PromptCoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Audio-Reasoner: Improving Reasoning Capability in Large Audio Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02318v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02318v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhifei Xie, Mingbao Lin, Zihang Liu, Pengcheng Wu, Shuicheng Yan, Chunyan Miao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in multimodal reasoning have largely overlooked the audio
modality. We introduce Audio-Reasoner, a large-scale audio language model for
deep reasoning in audio tasks. We meticulously curated a large-scale and
diverse multi-task audio dataset with simple annotations. Then, we leverage
closed-source models to conduct secondary labeling, QA generation, along with
structured COT process. These datasets together form a high-quality reasoning
dataset with 1.2 million reasoning-rich samples, which we name CoTA. Following
inference scaling principles, we train Audio-Reasoner on CoTA, enabling it to
achieve great logical capabilities in audio reasoning. Experiments show
state-of-the-art performance across key benchmarks, including MMAU-mini
(+25.42%), AIR-Bench chat/foundation(+14.57%/+10.13%), and MELD (+8.01%). Our
findings stress the core of structured CoT training in advancing audio
reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report, in process</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Target Return Optimizer for <span class="highlight-title">Multi</span>-Game Decision Transformer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02311v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02311v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kensuke Tatematsu, Akifumi Wachi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Achieving autonomous agents with robust generalization capabilities across
diverse games and tasks remains one of the ultimate goals in AI research.
Recent advancements in transformer-based offline reinforcement learning,
exemplified by the MultiGame Decision Transformer [Lee et al., 2022], have
shown remarkable performance across various games or tasks. However, these
approaches depend heavily on human expertise, presenting substantial challenges
for practical deployment, particularly in scenarios with limited prior
game-specific knowledge. In this paper, we propose an algorithm called
Multi-Game Target Return Optimizer (MTRO) to autonomously determine
game-specific target returns within the Multi-Game Decision Transformer
framework using solely offline datasets. MTRO addresses the existing
limitations by automating the target return configuration process, leveraging
environmental reward information extracted from offline datasets. Notably, MTRO
does not require additional training, enabling seamless integration into
existing Multi-Game Decision Transformer architectures. Our experimental
evaluations on Atari games demonstrate that MTRO enhances the performance of RL
policies across a wide array of games, underscoring its potential to advance
the field of autonomous agent development.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Flexible Prefrontal Control over Hippocampal Episodic Memory for
  Goal-Directed Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02303v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02303v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yicong Zheng, Nora Wolf, Charan Ranganath, Randall C. O'Reilly, Kevin L. McKee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many tasks require flexibly modifying perception and behavior based on
current goals. Humans can retrieve episodic memories from days to years ago,
using them to contextualize and generalize behaviors across novel but
structurally related situations. The brain's ability to control episodic
memories based on task demands is often attributed to interactions between the
prefrontal cortex (PFC) and hippocampus (HPC). We propose a reinforcement
learning model that incorporates a PFC-HPC interaction mechanism for
goal-directed generalization. In our model, the PFC learns to generate
query-key representations to encode and retrieve goal-relevant episodic
memories, modulating HPC memories top-down based on current task demands.
Moreover, the PFC adapts its encoding and retrieval strategies dynamically when
faced with multiple goals presented in a blocked, rather than interleaved,
manner. Our results show that: (1) combining working memory with selectively
retrieved episodic memory allows transfer of decisions among similar
environments or situations, (2) top-down control from PFC over HPC improves
learning of arbitrary structural associations between events for generalization
to novel environments compared to a bottom-up sensory-driven approach, and (3)
the PFC encodes generalizable representations during both encoding and
retrieval of goal-relevant memories, whereas the HPC exhibits event-specific
representations. Together, these findings highlight the importance of
goal-directed prefrontal control over hippocampal episodic memory for
decision-making in novel situations and suggest a computational mechanism by
which PFC-HPC interactions enable flexible behavior.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Memorize or Generalize? Evaluating <span class="highlight-title">LLM</span> Code Generation with Evolved
  Questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wentao Chen, Lizhe Zhang, Li Zhong, Letian Peng, Zilong Wang, Jingbo Shang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are known to exhibit a memorization phenomenon
in code generation: instead of truly understanding the underlying principles of
a programming problem, they tend to memorize the original prompt and its
solution together in the training. Consequently, when facing variants of the
original problem, their answers very likely resemble the memorized solutions
and fail to generalize. In this paper, we investigate this phenomenon by
designing three evolution strategies to create variants: mutation,
paraphrasing, and code-rewriting. By comparing the performance and AST
similarity of the LLM-generated codes before and after these three evolutions,
we develop a memorization score that positively correlates with the level of
memorization. As expected, as supervised fine-tuning goes on, the memorization
score rises before overfitting, suggesting more severe memorization. We
demonstrate that common mitigation approaches, such as prompt translation and
using evolved variants as data augmentation in supervised learning and
reinforcement learning, either compromise the performance or fail to alleviate
the memorization issue. Therefore, memorization remains a significant challenge
in LLM code generation, highlighting the need for a more effective solution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VOILA: Evaluation of M<span class="highlight-title">LLM</span>s For Perceptual Understanding and Analogical
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.00043v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.00043v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nilay Yilmaz, Maitreya Patel, Yiran Lawrence Luo, Tejas Gokhale, Chitta Baral, Suren Jayasuriya, Yezhou Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal Large Language Models (MLLMs) have become a powerful tool for
integrating visual and textual information. Despite their exceptional
performance on visual understanding benchmarks, measuring their ability to
reason abstractly across multiple images remains a significant challenge. To
address this, we introduce VOILA, a large-scale, open-ended, dynamic benchmark
designed to evaluate MLLMs' perceptual understanding and abstract relational
reasoning. VOILA employs an analogical mapping approach in the visual domain,
requiring models to generate an image that completes an analogy between two
given image pairs, reference and application, without relying on predefined
choices. Our experiments demonstrate that the analogical reasoning tasks in
VOILA present a challenge to MLLMs. Through multi-step analysis, we reveal that
current MLLMs struggle to comprehend inter-image relationships and exhibit
limited capabilities in high-level relational reasoning. Notably, we observe
that performance improves when following a multi-step strategy of least-to-most
prompting. Comprehensive evaluations on open-source models and GPT-4o show that
on text-based answers, the best accuracy for challenging scenarios is 13%
(LLaMa 3.2) and even for simpler tasks is only 29% (GPT-4o), while human
performance is significantly higher at 70% across both difficulty levels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2025. Code and data: https://github.com/nlylmz/Voila</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Anytime-Constrained Equilibria in Polynomial Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23637v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23637v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeremy McMahan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We extend anytime constraints to the Markov game setting and the
corresponding solution concept of an anytime-constrained equilibrium (ACE).
Then, we present a comprehensive theory of anytime-constrained equilibria that
includes (1) a computational characterization of feasible policies, (2) a
fixed-parameter tractable algorithm for computing ACE, and (3) a
polynomial-time algorithm for approximately computing ACE. Since computing a
feasible policy is NP-hard even for two-player zero-sum games, our
approximation guarantees are optimal so long as $P \neq NP$. We also develop
the first theory of efficient computation for action-constrained Markov games,
which may be of independent interest.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LABIIUM: AI-Enhanced Zero-configuration Measurement Automation System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16172v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16172v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emmanuel A. Olowe, Danial Chitnis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The complexity of laboratory environments requires solutions that simplify
instrument interaction and enhance measurement automation. Traditional tools
often require configuration, software, and programming skills, creating
barriers to productivity. Previous approaches, including dedicated software
suites and custom scripts, frequently fall short in providing user-friendly
solutions that align with programming practices. We present LABIIUM, an
AI-enhanced, zero-configuration measurement automation system designed to
streamline experimental workflows and improve user productivity. LABIIUM
integrates an AI assistant powered by Large Language Models (LLMs) to generate
code. LABIIUM's Lab-Automation-Measurement Bridges (LAMBs) enable seamless
instrument connectivity using standard tools such as VSCode and Python,
eliminating setup overhead. To demonstrate its capabilities, we conducted
experiments involving the measurement of the parametric transfer curve of a
simple two-transistor inverting amplifier with a current source load. The AI
assistant was evaluated using different prompt scenarios and compared with
multiple models, including Claude Sonnet 3.5, Gemini Pro 1.5, and GPT-4o. An
expert solution implementing the Gradient-Weighted Adaptive Stochastic Sampling
(GWASS) method was used as a baseline. The solutions generated by the AI
assistant were compared with the expert solution and a uniform linear sweep
baseline with 10,000 points. The graph results show that the LLMs were able to
successfully complete the most basic uniform sweep, but LLMs were unable to
develop adaptive sweeping algorithms to compete with GWASS. The evaluation
underscores LABIIUM's ability to enhance laboratory productivity and support
digital transformation in research and industry, and emphasizes the future work
required to improve LLM performance in Electronic Measurement Science Tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted for IEEE I2MTC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Layer Swapping for Zero-Shot Cross-Lingual Transfer in Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01335v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01335v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Bandarkar, Benjamin Muller, Pritish Yuvraj, Rui Hou, Nayan Singhal, Hongjiang Lv, Bing Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model merging, such as model souping, is the practice of combining different
models with the same architecture together without further training. In this
work, we present a model merging methodology that addresses the difficulty of
fine-tuning Large Language Models (LLMs) for target tasks in non-English
languages, where task-specific data is often unavailable. We focus on
mathematical reasoning and without in-language math data, facilitate
cross-lingual transfer by composing language and math capabilities. Starting
from the same pretrained model, we fine-tune separate "experts" on math
instruction data in English and on generic instruction data in the target
language. We then replace the top and bottom transformer layers of the math
expert directly with layers from the language expert, which consequently
enhances math performance in the target language. The resulting merged models
outperform the individual experts and other merging methods on the math
benchmark, MGSM, by 10% across four major languages where math instruction data
is scarce. In addition, this layer swapping is simple, inexpensive, and
intuitive, as it is based on an interpretative analysis of the most important
parameter changes during the fine-tuning of each expert. The ability to
successfully re-compose LLMs for cross-lingual transfer in this manner opens up
future possibilities to combine model expertise, create modular solutions, and
transfer reasoning capabilities across languages all post hoc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025, Spotlight Paper, In The Thirteenth International
  Conference on Learning Representations, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Verbalized Probabilistic Graphical Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.05516v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.05516v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengguan Huang, Xing Shen, Songtao Wang, Lingfa Meng, Dianbo Liu, Hao Wang, Samir Bhatt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human cognition excels at transcending sensory input and forming latent
representations that structure our understanding of the world. Although Large
Language Models (LLMs) can produce chain-of-thought reasoning, they lack a
principled framework to capture latent structures and model uncertainty,
especially in compositional reasoning tasks. We propose Verbalized
Probabilistic Graphical Modeling (vPGM), a Bayesian prompting framework that
guides LLMs to simulate key principles of Probabilistic Graphical Models (PGMs)
in natural language. Unlike many traditional probabilistic methods requiring
substantial domain expertise or specialized training, vPGM bypasses
expert-driven model design, making it well-suited for scenarios with limited
assumptions or scarce data. We evaluated our model on several compositional
reasoning tasks, both close-ended and open-ended. Our results indicate that the
model effectively enhances confidence calibration and text generation quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Decentralized</span> Adversarial Training over Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.13326v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.13326v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ying Cao, Elsa Rizk, Stefan Vlaski, Ali H. Sayed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The vulnerability of machine learning models to adversarial attacks has been
attracting considerable attention in recent years. Most existing studies focus
on the behavior of stand-alone single-agent learners. In comparison, this work
studies adversarial training over graphs, where individual agents are subjected
to perturbations of varied strength levels across space. It is expected that
interactions by linked agents, and the heterogeneity of the attack models that
are possible over the graph, can help enhance robustness in view of the
coordination power of the group. Using a min-max formulation of distributed
learning, we develop a decentralized adversarial training framework for
multi-agent systems. Specifically, we devise two decentralized adversarial
training algorithms by relying on two popular decentralized learning
strategies--diffusion and consensus. We analyze the convergence properties of
the proposed framework for strongly-convex, convex, and non-convex
environments, and illustrate the enhanced robustness to adversarial attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2303.01936</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ λ: A Benchmark for Data-Efficiency in Long-Horizon Indoor Mobile
  Manipulation Robotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05313v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05313v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Jaafar, Shreyas Sundara Raman, Yichen Wei, Sudarshan Harithas, Sofia Juliani, Anneke Wernerfelt, Benedict Quartey, Ifrah Idrees, Jason Xinyu Liu, Stefanie Tellex
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning to execute long-horizon mobile manipulation tasks is crucial for
advancing robotics in household and workplace settings. However, current
approaches are typically data-inefficient, underscoring the need for improved
models that require realistically sized benchmarks to evaluate their
efficiency. To address this, we introduce the LAMBDA ({\lambda})
benchmark-Long-horizon Actions for Mobile-manipulation Benchmarking of Directed
Activities-which evaluates the data efficiency of models on
language-conditioned, long-horizon, multi-room, multi-floor, pick-and-place
tasks using a dataset of manageable size, more feasible for collection. Our
benchmark includes 571 human-collected demonstrations that provide realism and
diversity in simulated and real-world settings. Unlike planner-generated data,
these trajectories offer natural variability and replay-verifiability, ensuring
robust learning and evaluation. We leverage LAMBDA to benchmark current
end-to-end learning methods and a modular neuro-symbolic approaches that
combines foundation models with task and motion planning. We find that
end-to-end methods-even when pretrained-yield lower success rates, while
neuro-symbolic methods perform significantly better and require less data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI-based association analysis for medical imaging using latent-space
  geometric confounder correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.12836v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.12836v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianjing Liu, Bo Li, Meike W. Vernooij, Eppo B. Wolvius, Gennady V. Roshchupkin, Esther E. Bron
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study addresses the challenges of confounding effects and
interpretability in artificial-intelligence-based medical image analysis.
Whereas existing literature often resolves confounding by removing
confounder-related information from latent representations, this strategy risks
affecting image reconstruction quality in generative models, thus limiting
their applicability in feature visualization. To tackle this, we propose a
different strategy that retains confounder-related information in latent
representations while finding an alternative confounder-free representation of
the image data.
  Our approach views the latent space of an autoencoder as a vector space,
where imaging-related variables, such as the learning target (t) and confounder
(c), have a vector capturing their variability. The confounding problem is
addressed by searching a confounder-free vector which is orthogonal to the
confounder-related vector but maximally collinear to the target-related vector.
To achieve this, we introduce a novel correlation-based loss that not only
performs vector searching in the latent space, but also encourages the encoder
to generate latent representations linearly correlated with the variables.
Subsequently, we interpret the confounder-free representation by sampling and
reconstructing images along the confounder-free vector.
  The efficacy and flexibility of our proposed method are demonstrated across
three applications, accommodating multiple confounders and utilizing diverse
image modalities. Results affirm the method's effectiveness in reducing
confounder influences, preventing wrong or misleading associations, and
offering a unique visual interpretation for in-depth investigations by clinical
and epidemiological researchers. The code is released in the following GitLab
repository:
https://gitlab.com/radiology/compopbio/ai_based_association_analysis}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Medical Image Analysis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Internal Model Control: Learning a Robust Control Policy via
  Predictive Error Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.13079v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.13079v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Gao, Chao Yu, Yu Wang, Yi Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate motion control in the face of disturbances within complex
environments remains a major challenge in robotics. Classical model-based
approaches often struggle with nonlinearities and unstructured disturbances,
while RL-based methods can be fragile when encountering unseen scenarios. In
this paper, we propose a novel framework, Neural Internal Model Control, which
integrates model-based control with RL-based control to enhance robustness. Our
framework streamlines the predictive model by applying Newton-Euler equations
for rigid-body dynamics, eliminating the need to capture complex
high-dimensional nonlinearities. This internal model combines model-free RL
algorithms with predictive error feedback. Such a design enables a closed-loop
control structure to enhance the robustness and generalizability of the control
system. We demonstrate the effectiveness of our framework on both quadrotors
and quadrupedal robots, achieving superior performance compared to
state-of-the-art methods. Furthermore, real-world deployment on a quadrotor
with rope-suspended payloads highlights the framework's robustness in
sim-to-real transfer. Our code is released at
https://github.com/thu-uav/NeuralIMC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to RAL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI Literacy in K-12 and Higher Education in the Wake of Generative AI:
  An Integrative <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.00079v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.00079v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingjian Gu, Barbara J. Ericson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Even though AI literacy has emerged as a prominent education topic in the
wake of generative AI, its definition remains vague. There is little consensus
among researchers and practitioners on how to discuss and design AI literacy
interventions. The term has been used to describe both learning activities that
train undergraduate students to use ChatGPT effectively and having kindergarten
children interact with social robots. This paper applies an integrative review
method to examine empirical and theoretical AI literacy studies published since
2020. In synthesizing the 124 reviewed studies, three ways to conceptualize
literacy-functional, critical, and indirectly beneficial-and three perspectives
on AI-technical detail, tool, and sociocultural-were identified, forming a
framework that reflects the spectrum of how AI literacy is approached in
practice. The framework highlights the need for more specialized terms within
AI literacy discourse and indicates research gaps in certain AI literacy
objectives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 7 figures; previous version replaced due to incorrect
  parsing of author name</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Advanced Techniques for Visual Question Answering: A
  Comprehensive Comparison 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14827v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14827v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aiswarya Baby, Tintu Thankom Koshy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Question Answering (VQA) has emerged as a pivotal task in the
intersection of computer vision and natural language processing, requiring
models to understand and reason about visual content in response to natural
language questions. Analyzing VQA datasets is essential for developing robust
models that can handle the complexities of multimodal reasoning. Several
approaches have been developed to examine these datasets, each offering
distinct perspectives on question diversity, answer distribution, and
visual-textual correlations. Despite significant progress, existing VQA models
face challenges related to dataset bias, limited model complexity, commonsense
reasoning gaps, rigid evaluation methods, and generalization to real world
scenarios. This paper offers a detailed study of the original VQA dataset,
baseline models and methods along with a comparative study of five advanced VQA
models, ABC-CNN, KICNLE, Masked Vision and Language Modeling, BLIP-2, and OFA,
each employing distinct methods to address these ongoing challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, No figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From homeostasis to resource sharing: Biologically and economically
  aligned <span class="highlight-title">multi</span>-objective <span class="highlight-title">multi</span>-agent AI safety benchmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00081v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00081v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roland Pihlakas, Joel Pyykkö
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing safe, aligned agentic AI systems requires comprehensive empirical
testing, yet many existing benchmarks neglect crucial themes aligned with
biology and economics, both time-tested fundamental sciences describing our
needs and preferences. To address this gap, the present work focuses on
introducing biologically and economically motivated themes that have been
neglected in current mainstream discussions on AI safety - namely a set of
multi-objective, multi-agent alignment benchmarks that emphasize homeostasis
for bounded and biological objectives, diminishing returns for unbounded,
instrumental, and business objectives, sustainability principle, and resource
sharing. We implemented eight main benchmark environments on the above themes,
to illustrate key pitfalls and challenges in agentic AI-s, such as unboundedly
maximizing a homeostatic objective, over-optimizing one objective at the
expense of others, neglecting safety constraints, or depleting shared
resources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 13 figures, 1 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Shifting Power: Leveraging <span class="highlight-title">LLM</span>s to Simulate Human Aversion in ABMs of
  Bilateral Financial Exchanges, A bond market study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.00320v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.00320v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alicia Vidler, Toby Walsh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bilateral markets, such as those for government bonds, involve decentralized
and opaque transactions between market makers (MMs) and clients, posing
significant challenges for traditional modeling approaches. To address these
complexities, we introduce TRIBE an agent-based model augmented with a large
language model (LLM) to simulate human-like decision-making in trading
environments. TRIBE leverages publicly available data and stylized facts to
capture realistic trading dynamics, integrating human biases like risk aversion
and ambiguity sensitivity into the decision-making processes of agents. Our
research yields three key contributions: first, we demonstrate that integrating
LLMs into agent-based models to enhance client agency is feasible and enriches
the simulation of agent behaviors in complex markets; second, we find that even
slight trade aversion encoded within the LLM leads to a complete cessation of
trading activity, highlighting the sensitivity of market dynamics to agents'
risk profiles; third, we show that incorporating human-like variability shifts
power dynamics towards clients and can disproportionately affect the entire
system, often resulting in systemic agent collapse across simulations. These
findings underscore the emergent properties that arise when introducing
stochastic, human-like decision processes, revealing new system behaviors that
enhance the realism and complexity of artificial societies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models are Powerful EHR Encoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17403v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17403v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Hegselmann, Georg von Arnim, Tillmann Rheude, Noel Kronenberg, David Sontag, Gerhard Hindricks, Roland Eils, Benjamin Wild
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electronic Health Records (EHRs) offer rich potential for clinical
prediction, yet their inherent complexity and heterogeneity pose significant
challenges for traditional machine learning approaches. Domain-specific EHR
foundation models trained on large collections of unlabeled EHR data have
demonstrated promising improvements in predictive accuracy and generalization;
however, their training is constrained by limited access to diverse,
high-quality datasets and inconsistencies in coding standards and healthcare
practices. In this study, we explore the possibility of using general-purpose
Large Language Models (LLMs) based embedding methods as EHR encoders. By
serializing patient records into structured Markdown text, transforming codes
into human-readable descriptors, we leverage the extensive generalization
capabilities of LLMs pretrained on vast public corpora, thereby bypassing the
need for proprietary medical datasets. We systematically evaluate two
state-of-the-art LLM-embedding models, GTE-Qwen2-7B-Instruct and
LLM2Vec-Llama3.1-8B-Instruct, across 15 diverse clinical prediction tasks from
the EHRSHOT benchmark, comparing their performance to an EHRspecific foundation
model, CLIMBR-T-Base, and traditional machine learning baselines. Our results
demonstrate that LLM-based embeddings frequently match or exceed the
performance of specialized models, even in few-shot settings, and that their
effectiveness scales with the size of the underlying LLM and the available
context window. Overall, our findings demonstrate that repurposing LLMs for EHR
encoding offers a scalable and effective approach for clinical prediction,
capable of overcoming the limitations of traditional EHR modeling and
facilitating more interoperable and generalizable healthcare applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simulating Human-like Daily Activities with Desire-driven Autonomy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06435v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06435v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiding Wang, Yuxuan Chen, Fangwei Zhong, Long Ma, Yizhou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Desires motivate humans to interact autonomously with the complex world. In
contrast, current AI agents require explicit task specifications, such as
instructions or reward functions, which constrain their autonomy and behavioral
diversity. In this paper, we introduce a Desire-driven Autonomous Agent (D2A)
that can enable a large language model (LLM) to autonomously propose and select
tasks, motivated by satisfying its multi-dimensional desires. Specifically, the
motivational framework of D2A is mainly constructed by a dynamic Value System,
inspired by the Theory of Needs. It incorporates an understanding of human-like
desires, such as the need for social interaction, personal fulfillment, and
self-care. At each step, the agent evaluates the value of its current state,
proposes a set of candidate activities, and selects the one that best aligns
with its intrinsic motivations. We conduct experiments on Concordia, a
text-based simulator, to demonstrate that our agent generates coherent,
contextually relevant daily activities while exhibiting variability and
adaptability similar to human behavior. A comparative analysis with other
LLM-based agents demonstrates that our approach significantly enhances the
rationality of the simulated activities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fuzzy Speculative Decoding for a Tunable Accuracy-Runtime Tradeoff 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20704v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20704v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Holsman, Yukun Huang, Bhuwan Dhingra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speculative Decoding (SD) enforces strict distributional equivalence to the
target model, limiting potential speed ups as distributions of near-equivalence
achieve comparable outcomes in many cases. Furthermore, enforcing
distributional equivalence means that users are unable to trade deviations from
the target model distribution for further inference speed gains. To address
these limitations, we introduce Fuzzy Speculative Decoding (FSD) - a decoding
algorithm that generalizes SD by accepting candidate tokens purely based on the
divergences between the target and draft model distributions. By allowing for
controlled divergence from the target model, FSD enables users to flexibly
trade generation quality for inference speed. Across several benchmarks, our
method is able to achieve significant runtime improvements of over 5 tokens per
second faster than SD at only an approximate 2% absolute reduction in benchmark
accuracy. In many cases, FSD is even able to match SD benchmark accuracy at
over 2 tokens per second faster, demonstrating that distributional equivalence
is not necessary to maintain target model performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Semantic Understanding in Speech Language Models via
  Brain-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09230v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09230v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omer Moussa, Dietrich Klakow, Mariya Toneva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech language models align with human brain responses to natural language
to an impressive degree. However, current models rely heavily on low-level
speech features, indicating they lack brain-relevant semantics which limits
their utility as model organisms of semantic processing in the brain. In this
work, we address this limitation by inducing brain-relevant bias directly into
the models via fine-tuning with fMRI recordings of people listening to natural
stories, a process we name brain-tuning. After testing it on 3 different
pretrained model families, we show that brain-tuning not only improves overall
alignment with new brain recordings in semantic language regions, but also
reduces the reliance on low-level speech features for this alignment.
Excitingly, we further show that brain-tuning leads to 1) consistent
improvements in performance on a range of downstream tasks and 2) a
representational space with increased semantic preference. Our results provide
converging evidence, for the first time, that incorporating brain signals into
the training of language models improves the models' semantic understanding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assistance or Disruption? Exploring and Evaluating the Design and
  Trade-offs of Proactive AI Programming Support 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18658v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18658v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Pu, Daniel Lazaro, Ian Arawjo, Haijun Xia, Ziang Xiao, Tovi Grossman, Yan Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI programming tools enable powerful code generation, and recent prototypes
attempt to reduce user effort with proactive AI agents, but their impact on
programming workflows remains unexplored. We introduce and evaluate
Codellaborator, a design probe LLM agent that initiates programming assistance
based on editor activities and task context. We explored three interface
variants to assess trade-offs between increasingly salient AI support:
prompt-only, proactive agent, and proactive agent with presence and context
(Codellaborator). In a within-subject study (N=18), we find that proactive
agents increase efficiency compared to prompt-only paradigm, but also incur
workflow disruptions. However, presence indicators and interaction context
support alleviated disruptions and improved users' awareness of AI processes.
We underscore trade-offs of Codellaborator on user control, ownership, and code
understanding, emphasizing the need to adapt proactivity to programming
processes. Our research contributes to the design exploration and evaluation of
proactive AI systems, presenting design implications on AI-integrated
programming workflow.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comparative Evaluation of Quantification Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.03223v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.03223v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Schumacher, Markus Strohmaier, Florian Lemmerich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantification represents the problem of estimating the distribution of class
labels on unseen data. It also represents a growing research field in
supervised machine learning, for which a large variety of different algorithms
has been proposed in recent years. However, a comprehensive empirical
comparison of quantification methods that supports algorithm selection is not
available yet. In this work, we close this research gap by conducting a
thorough empirical performance comparison of 24 different quantification
methods on overall more than 40 data sets, considering binary as well as
multiclass quantification settings. We observe that no single algorithm
generally outperforms all competitors, but identify a group of methods
including the threshold selection-based Median Sweep and TSMax methods, the DyS
framework including the HDy method, Forman's mixture model, and Friedman's
method that performs best in the binary setting. For the multiclass setting, we
observe that a different, broad group of algorithms yields good performance,
including the HDx method, the Generalized Probabilistic Adjusted Count, the
readme method, the energy distance minimization method, the EM algorithm for
quantification, and Friedman's method. We also find that tuning the underlying
classifiers has in most cases only a limited impact on the quantification
performance. More generally, we find that the performance on multiclass
quantification is inferior to the results obtained in the binary setting. Our
results can guide practitioners who intend to apply quantification algorithms
and help researchers to identify opportunities for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages, 18 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Perils of Optimizing Learned Reward Functions: Low Training Error
  Does Not Guarantee Low Regret 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15753v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15753v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Fluri, Leon Lang, Alessandro Abate, Patrick Forré, David Krueger, Joar Skalse
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In reinforcement learning, specifying reward functions that capture the
intended task can be very challenging. Reward learning aims to address this
issue by learning the reward function. However, a learned reward model may have
a low error on the data distribution, and yet subsequently produce a policy
with large regret. We say that such a reward model has an error-regret
mismatch. The main source of an error-regret mismatch is the distributional
shift that commonly occurs during policy optimization. In this paper, we
mathematically show that a sufficiently low expected test error of the reward
model guarantees low worst-case regret, but that for any fixed expected test
error, there exist realistic data distributions that allow for error-regret
mismatch to occur. We then show that similar problems persist even when using
policy regularization techniques, commonly employed in methods such as RLHF. We
hope our results stimulate the theoretical and empirical study of improved
methods to learn reward models, and better ways to measure their quality
reliably.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>70 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive <span class="highlight-title">Survey</span> on Composed Image Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18495v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18495v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuemeng Song, Haoqiang Lin, Haokun Wen, Bohan Hou, Mingzhu Xu, Liqiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Composed Image Retrieval (CIR) is an emerging yet challenging task that
allows users to search for target images using a multimodal query, comprising a
reference image and a modification text specifying the user's desired changes
to the reference image. Given its significant academic and practical value, CIR
has become a rapidly growing area of interest in the computer vision and
machine learning communities, particularly with the advances in deep learning.
To the best of our knowledge, there is currently no comprehensive review of CIR
to provide a timely overview of this field. Therefore, we synthesize insights
from over 120 publications in top conferences and journals, including ACM TOIS,
SIGIR, and CVPR In particular, we systematically categorize existing supervised
CIR and zero-shot CIR models using a fine-grained taxonomy. For a comprehensive
review, we also briefly discuss approaches for tasks closely related to CIR,
such as attribute-based CIR and dialog-based CIR. Additionally, we summarize
benchmark datasets for evaluation and analyze existing supervised and zero-shot
CIR methods by comparing experimental results across multiple datasets.
Furthermore, we present promising future directions in this field, offering
practical insights for researchers interested in further exploration. The
curated collection of related works is maintained and continuously updated in
https://github.com/haokunwen/Awesome-Composed-Image-Retrieval.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WalkVLM:Aid Visually Impaired People Walking by Vision Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20903v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20903v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiang Yuan, Ting Zhang, Ying Deng, Jiapei Zhang, Yeshuang Zhu, Zexi Jia, Jie Zhou, Jinchao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Approximately 200 million individuals around the world suffer from varying
degrees of visual impairment, making it crucial to leverage AI technology to
offer walking assistance for these people. With the recent progress of
vision-language models (VLMs), applying VLMs to offer walking guidance has
become popular. However, the existing methods of walking guidance are mainly
based on self-curated question-answering datasets that are not publicly
accessible, without a standardized benchmark for training or evaluation.
Moreover, walking assistance often requires real-time streaming video analysis
and the generation of concise yet informative reminders, making VLMs struggle
due to excessive responses and low efficiency in inferences. In this paper, we
introduce the first large-scale dataset dedicated to walking assistance,
comprising 12,000 video-annotation pairs, to provide a unified benchmark for
training and evaluating systems to help visually-impaired individuals walk.
Furthermore, a WalkVLM model is proposed, which employs chain of thought for
hierarchical planning to generate concise but informative reminders and
utilizes temporal-aware adaptive prediction to reduce the temporal redundancy
of reminders. Finally, we have established a solid benchmark for blind walking
task and verified the advantages of WalkVLM in stream video processing for this
task compared to other VLMs. Our dataset and code are available at
https://walkvlm2024.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modeling Relational Patterns for Logical Query Answering over Knowledge
  Graphs <span class="chip">ECAI 2024</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11858v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11858v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunjie He, Mojtaba Nayyeri, Bo Xiong, Yuqicheng Zhu, Evgeny Kharlamov, Steffen Staab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Answering first-order logical (FOL) queries over knowledge graphs (KG)
remains a challenging task mainly due to KG incompleteness. Query embedding
approaches this problem by computing the low-dimensional vector representations
of entities, relations, and logical queries. KGs exhibit relational patterns
such as symmetry and composition and modeling the patterns can further enhance
the performance of query embedding models. However, the role of such patterns
in answering FOL queries by query embedding models has not been yet studied in
the literature. In this paper, we fill in this research gap and empower FOL
queries reasoning with pattern inference by introducing an inductive bias that
allows for learning relation patterns. To this end, we develop a novel query
embedding method, RoConE, that defines query regions as geometric cones and
algebraic query operators by rotations in complex space. RoConE combines the
advantages of Cone as a well-specified geometric representation for query
embedding, and also the rotation operator as a powerful algebraic operation for
pattern inference. Our experimental results on several benchmark datasets
confirm the advantage of relational patterns for enhancing logical query
answering task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The results reported in this paper are included in our accepted paper
  arXiv:2407.09212 at ECAI 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variational Best-of-N Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06057v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06057v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Afra Amini, Tim Vieira, Elliott Ash, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Best-of-N (BoN) is a popular and effective algorithm for aligning language
models to human preferences. The algorithm works as follows: at inference time,
N samples are drawn from the language model, and the sample with the highest
reward, as judged by a reward model, is returned as the output. Despite its
effectiveness, BoN is computationally expensive; it reduces sampling throughput
by a factor of N. To make BoN more efficient at inference time, one strategy is
to fine-tune the language model to mimic what BoN does during inference. To
achieve this, we derive the distribution induced by the BoN algorithm. We then
propose to fine-tune the language model to minimize backward KL divergence to
the BoN distribution. Our approach is analogous to mean-field variational
inference and, thus, we term it variational BoN (vBoN). To the extent this
fine-tuning is successful and we end up with a good approximation, we have
reduced the inference cost by a factor of N. Our experiments on controlled
generation and summarization tasks show that BoN is the most effective
alignment method, and our variational approximation to BoN achieves the closest
performance to BoN and surpasses models fine-tuned using the standard
KL-constrained RL objective. In the controlled generation task, vBoN appears
more frequently on the Pareto frontier of reward and KL divergence compared to
other alignment methods. In the summarization task, vBoN achieves high reward
values across various sampling temperatures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ R2Det: Exploring Relaxed Rotation Equivariance in 2D object detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11760v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11760v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiqiang Wu, Yingjie Liu, Hanlin Dong, Xuan Tang, Jian Yang, Bo Jin, Mingsong Chen, Xian Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Group Equivariant Convolution (GConv) empowers models to explore underlying
symmetry in data, improving performance. However, real-world scenarios often
deviate from ideal symmetric systems caused by physical permutation,
characterized by non-trivial actions of a symmetry group, resulting in
asymmetries that affect the outputs, a phenomenon known as Symmetry Breaking.
Traditional GConv-based methods are constrained by rigid operational rules
within group space, assuming data remains strictly symmetry after limited group
transformations. This limitation makes it difficult to adapt to
Symmetry-Breaking and non-rigid transformations. Motivated by this, we mainly
focus on a common scenario: Rotational Symmetry-Breaking. By relaxing strict
group transformations within Strict Rotation-Equivariant group $\mathbf{C}_n$,
we redefine a Relaxed Rotation-Equivariant group $\mathbf{R}_n$ and introduce a
novel Relaxed Rotation-Equivariant GConv (R2GConv) with only a minimal increase
of $4n$ parameters compared to GConv. Based on R2GConv, we propose a Relaxed
Rotation-Equivariant Network (R2Net) as the backbone and develop a Relaxed
Rotation-Equivariant Object Detector (R2Det) for 2D object detection.
Experimental results demonstrate the effectiveness of the proposed R2GConv in
natural image classification, and R2Det achieves excellent performance in 2D
object detection with improved generalization capabilities and robustness. The
code is available in \texttt{https://github.com/wuer5/r2det}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WalnutData: A UAV Remote Sensing <span class="highlight-title">Dataset</span> of Green Walnuts and Model
  Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20092v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20092v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingjie Wu, Chenggui Yang, Huihua Wang, Chen Xue, Yibo Wang, Haoyu Wang, Yansong Wang, Can Peng, Yuqi Han, Ruoyu Li, Lijun Yun, Zaiqing Chen, Yuelong Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The UAV technology is gradually maturing and can provide extremely powerful
support for smart agriculture and precise monitoring. Currently, there is no
dataset related to green walnuts in the field of agricultural computer vision.
Thus, in order to promote the algorithm design in the field of agricultural
computer vision, we used UAV to collect remote-sensing data from 8 walnut
sample plots. Considering that green walnuts are subject to various lighting
conditions and occlusion, we constructed a large-scale dataset with a
higher-granularity of target features - WalnutData. This dataset contains a
total of 30,240 images and 706,208 instances, and there are 4 target
categories: being illuminated by frontal light and unoccluded (A1), being
backlit and unoccluded (A2), being illuminated by frontal light and occluded
(B1), and being backlit and occluded (B2). Subsequently, we evaluated many
mainstream algorithms on WalnutData and used these evaluation results as the
baseline standard. The dataset and all evaluation results can be obtained at
https://github.com/1wuming/WalnutData.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PAD: Personalized Alignment of <span class="highlight-title">LLM</span>s at Decoding-Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04070v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04070v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruizhe Chen, Xiaotian Zhang, Meng Luo, Wenhao Chai, Zuozhu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning with personalized preferences, which vary significantly across
cultural, educational, and political differences, poses a significant challenge
due to the computational costs and data demands of traditional alignment
methods. In response, this paper presents Personalized Alignment at
Decoding-time (PAD), a novel framework designed to align LLM outputs with
diverse personalized preferences during the inference phase, eliminating the
need for additional training. By introducing a unique personalized reward
modeling strategy, this framework decouples the text generation process from
personalized preferences, facilitating the generation of generalizable
token-level personalized rewards. The PAD algorithm leverages these rewards to
guide the decoding process, dynamically tailoring the base model's predictions
to personalized preferences. Extensive experimental results demonstrate that
PAD not only outperforms existing training-based alignment methods in terms of
aligning with diverse preferences but also shows significant generalizability
to preferences unseen during training and scalability across different base
models. This work advances the capability of LLMs to meet user needs in
real-time applications, presenting a substantial step forward in personalized
LLM alignment.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Training a <span class="highlight-title">multi</span>layer dynamical spintronic network with standard machine
  learning tools to perform time series classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02835v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02835v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erwan Plouet, Dédalo Sanz-Hernández, Aymeric Vecchiola, Julie Grollier, Frank Mizrahi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to process time-series at low energy cost is critical for many
applications. Recurrent neural network, which can perform such tasks, are
computationally expensive when implementing in software on conventional
computers. Here we propose to implement a recurrent neural network in hardware
using spintronic oscillators as dynamical neurons. Using numerical simulations,
we build a multi-layer network and demonstrate that we can use backpropagation
through time (BPTT) and standard machine learning tools to train this network.
Leveraging the transient dynamics of the spintronic oscillators, we solve the
sequential digits classification task with $89.83\pm2.91~\%$ accuracy, as good
as the equivalent software network. We devise guidelines on how to choose the
time constant of the oscillators as well as hyper-parameters of the network to
adapt to different input time scales.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Pronunciation and Accent Conversion through Knowledge
  Distillation And Synthetic Ground-Truth from Native TTS 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14997v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14997v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuan Nam Nguyen, Seymanur Akti, Ngoc Quan Pham, Alexander Waibel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous approaches on accent conversion (AC) mainly aimed at making
non-native speech sound more native while maintaining the original content and
speaker identity. However, non-native speakers sometimes have pronunciation
issues, which can make it difficult for listeners to understand them. Hence, we
developed a new AC approach that not only focuses on accent conversion but also
improves pronunciation of non-native accented speaker. By providing the
non-native audio and the corresponding transcript, we generate the ideal
ground-truth audio with native-like pronunciation with original duration and
prosody. This ground-truth data aids the model in learning a direct mapping
between accented and native speech. We utilize the end-to-end VITS framework to
achieve high-quality waveform reconstruction for the AC task. As a result, our
system not only produces audio that closely resembles native accents and while
retaining the original speaker's identity but also improve pronunciation, as
demonstrated by evaluation results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unifying Framework for Learning Argumentation Semantics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.12309v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.12309v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zlatina Mileva, Antonis Bikakis, Fabio Aurelio D'Asaro, Mark Law, Alessandra Russo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Argumentation is a very active research field of Artificial Intelligence
concerned with the representation and evaluation of arguments used in dialogues
between humans and/or artificial agents. Acceptability semantics of formal
argumentation systems define the criteria for the acceptance or rejection of
arguments. Several software systems, known as argumentation solvers, have been
developed to compute the accepted/rejected arguments using such criteria. These
include systems that learn to identify the accepted arguments using
non-interpretable methods. In this paper we present a novel framework, which
uses an Inductive Logic Programming approach to learn the acceptability
semantics for several abstract and structured argumentation frameworks in an
interpretable way. Through an empirical evaluation we show that our framework
outperforms existing argumentation solvers, thus opening up new future research
directions in the area of formal argumentation and human-machine dialogues.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpretable Interaction Modeling for Trajectory Prediction via Agent
  Selection and Physical Coefficient 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13152v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13152v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiji Huang, Lei Ye, Min Chen, Wenhai Luo, Dihong Wang, Chenqi Xu, Deyuan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A thorough understanding of the interaction between the target agent and
surrounding agents is a prerequisite for accurate trajectory prediction.
Although many methods have been explored, they assign correlation coefficients
to surrounding agents in a purely learning-based manner. In this study, we
present ASPILin, which manually selects interacting agents and replaces the
attention scores in Transformer with a newly computed physical correlation
coefficient, enhancing the interpretability of interaction modeling.
Surprisingly, these simple modifications can significantly improve prediction
performance and substantially reduce computational costs. We intentionally
simplified our model in other aspects, such as map encoding. Remarkably,
experiments conducted on the INTERACTION, highD, and CitySim datasets
demonstrate that our method is efficient and straightforward, outperforming
other state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>code:https://github.com/kkk00714/ASPILin</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Let the Code <span class="highlight-title">LLM</span> Edit Itself When You Edit the Code 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03157v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03157v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu He, Jun Zhang, Shengjie Luo, Jingjing Xu, Zhi Zhang, Di He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we investigate a typical scenario in code generation where a
developer edits existing code in real time and requests a code assistant, e.g.,
a large language model, to re-predict the next token or next line on the fly.
Naively, the LLM needs to re-encode the entire KV cache to provide an accurate
prediction. However, this process is computationally expensive, especially when
the sequence length is long. Simply encoding the edited subsequence and
integrating it to the original KV cache meets the temporal confusion problem,
leading to significantly worse performance. We address this efficiency and
accuracy trade-off by introducing \underline{\textbf{Positional
\textbf{I}ntegrity \textbf{E}ncoding} (PIE). Building upon the rotary
positional encoding, PIE first removes the rotary matrices in the Key cache
that introduce temporal confusion and then reapplies the correct rotary
matrices. This process ensures that positional relationships between tokens are
correct and requires only a single round of matrix multiplication. We validate
the effectiveness of PIE through extensive experiments on the RepoBench-C-8k
dataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.
Our evaluation includes three real-world coding tasks: code insertion, code
deletion, and multi-place code editing. Results demonstrate that PIE reduces
computational overhead by over 85% compared to the standard full recomputation
approach across all model sizes and tasks while well approximating the model
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 Camera Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DermaSynth: Rich Synthetic Image-Text Pairs Using Open Access
  Dermatology <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00196v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00196v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdurrahim Yilmaz, Furkan Yuceyalcin, Ece Gokyayla, Donghee Choi, Ozan Erdem, Ali Anil Demircali, Rahmetullah Varol, Ufuk Gorkem Kirabali, Gulsum Gencoglan, Joram M. Posma, Burak Temelkuran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A major barrier to developing vision large language models (LLMs) in
dermatology is the lack of large image--text pairs dataset. We introduce
DermaSynth, a dataset comprising of 92,020 synthetic image--text pairs curated
from 45,205 images (13,568 clinical and 35,561 dermatoscopic) for
dermatology-related clinical tasks. Leveraging state-of-the-art LLMs, using
Gemini 2.0, we used clinically related prompts and self-instruct method to
generate diverse and rich synthetic texts. Metadata of the datasets were
incorporated into the input prompts by targeting to reduce potential
hallucinations. The resulting dataset builds upon open access dermatological
image repositories (DERM12345, BCN20000, PAD-UFES-20, SCIN, and HIBA) that have
permissive CC-BY-4.0 licenses. We also fine-tuned a preliminary
Llama-3.2-11B-Vision-Instruct model, DermatoLlama 1.0, on 5,000 samples. We
anticipate this dataset to support and accelerate AI research in dermatology.
Data and code underlying this work are accessible at
https://github.com/abdurrahimyilmaz/DermaSynth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Order Theory in the Context of Machine Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06097v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06097v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Dolores-Cuenca, Aldo Guzman-Saenz, Sangil Kim, Susana Lopez-Moreno, Jose Mendoza-Cortes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper ``Tropical Geometry of Deep Neural Networks'' by L. Zhang et al.
introduces an equivalence between integer-valued neural networks (IVNN) with
$\text{ReLU}_{t}$ and tropical rational functions, which come with a map to
polytopes. Here, IVNN refers to a network with integer weights but real biases,
and $\text{ReLU}_{t}$ is defined as $\text{ReLU}_{t}(x)=\max(x,t)$ for
$t\in\mathbb{R}\cup\{-\infty\}$.
  For every poset with $n$ points, there exists a corresponding order polytope,
i.e., a convex polytope in the unit cube $[0,1]^n$ whose coordinates obey the
inequalities of the poset. We study neural networks whose associated polytope
is an order polytope. We then explain how posets with four points induce neural
networks that can be interpreted as $2\times 2$ convolutional filters. These
poset filters can be added to any neural network, not only IVNN.
  Similarly to maxout, poset pooling filters update the weights of the neural
network during backpropagation with more precision than average pooling, max
pooling, or mixed pooling, without the need to train extra parameters. We
report experiments that support our statements.
  We also define the structure of algebra over the operad of posets on poset
neural networks and tropical polynomials. This formalism allows us to study the
composition of poset neural network arquitectures and the effect on their
corresponding Newton polytopes, via the introduction of the generalization of
two operations on polytopes: the Minkowski sum and the convex envelope.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We added experiments with ImageNet 100, and improved the exposition
  of the theory developed. Added examples. Poster presentation in NeurIPS WIML
  2024, Talk in JMM 2025 section: Applied category theory II</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLaSE-G1: Incentivizing Generalization Capability for LLaMA-based Speech
  Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.00493v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.00493v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boyi Kang, Xinfa Zhu, Zihan Zhang, Zhen Ye, Mingshuai Liu, Ziqian Wang, Yike Zhu, Guobin Ma, Jun Chen, Longshuai Xiao, Chao Weng, Wei Xue, Lei Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in language models (LMs) have demonstrated strong
capabilities in semantic understanding and contextual modeling, which have
flourished in generative speech enhancement (SE). However, many LM-based SE
approaches primarily focus on semantic information, often neglecting the
critical role of acoustic information, which leads to acoustic inconsistency
after enhancement and limited generalization across diverse SE tasks. In this
paper, we introduce LLaSE-G1, a LLaMA-based language model that incentivizes
generalization capabilities for speech enhancement. LLaSE-G1 offers the
following key contributions: First, to mitigate acoustic inconsistency,
LLaSE-G1 employs continuous representations from WavLM as input and predicts
speech tokens from X-Codec2, maximizing acoustic preservation. Second, to
promote generalization capability, LLaSE-G1 introduces dual-channel inputs and
outputs, unifying multiple SE tasks without requiring task-specific IDs. Third,
LLaSE-G1 outperforms prior task-specific discriminative and generative SE
models, demonstrating scaling effects at test time and emerging capabilities
for unseen SE tasks. Additionally, we release our code and models to support
further research in this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 2 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Smoothing Grounding and Reasoning for M<span class="highlight-title">LLM</span>-Powered GUI Agents with
  Query-Oriented Pivot Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.00401v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.00401v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zongru Wu, Pengzhou Cheng, Zheng Wu, Tianjie Ju, Zhuosheng Zhang, Gongshen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Perception-enhanced pre-training, particularly through grounding techniques,
is widely adopted to enhance the performance of graphical user interface (GUI)
agents. However, in resource-constrained scenarios, the format discrepancy
between coordinate-oriented grounding and action-oriented reasoning limits the
effectiveness of grounding for reasoning tasks. To address this challenge, we
propose a query-oriented pivot approach called query inference, which serves as
a bridge between GUI grounding and reasoning. By inferring potential user
queries from a screenshot and its associated element coordinates, query
inference improves the understanding of coordinates while aligning more closely
with reasoning tasks. Experimental results show that query inference
outperforms previous grounding techniques under the same training data scale.
Notably, query inference achieves comparable or even better performance to
large-scale grounding-enhanced OS-Atlas with less than 0.1% of training data.
Furthermore, we explore the impact of reasoning formats and demonstrate that
integrating additional semantic information into the input further boosts
reasoning performance. The code is publicly available at
https://github.com/ZrW00/GUIPivot.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Balancing Efficiency and Effectiveness: An <span class="highlight-title">LLM</span>-Infused Approach for
  Optimized CTR Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.06860v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.06860v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoxiao Zhang, Yi Wei, Yadong Zhang, Huajian Feng, Qiang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Click-Through Rate (CTR) prediction is essential in online advertising, where
semantic information plays a pivotal role in shaping user decisions and
enhancing CTR effectiveness. Capturing and modeling deep semantic information,
such as a user's preference for "H\"aagen-Dazs' HEAVEN strawberry light ice
cream" due to its health-conscious and premium attributes, is challenging.
Traditional semantic modeling often overlooks these intricate details at the
user and item levels. To bridge this gap, we introduce a novel approach that
models deep semantic information end-to-end, leveraging the comprehensive world
knowledge capabilities of Large Language Models (LLMs). Our proposed
LLM-infused CTR prediction framework(Multi-level Deep Semantic Information
Infused CTR model via Distillation, MSD) is designed to uncover deep semantic
insights by utilizing LLMs to extract and distill critical information into a
smaller, more efficient model, enabling seamless end-to-end training and
inference. Importantly, our framework is carefully designed to balance
efficiency and effectiveness, ensuring that the model not only achieves high
performance but also operates with optimal resource utilization. Online A/B
tests conducted on the Meituan sponsored-search system demonstrate that our
method significantly outperforms baseline models in terms of Cost Per Mile
(CPM) and CTR, validating its effectiveness, scalability, and balanced approach
in real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures,4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Position: Don't use the CLT in <span class="highlight-title">LLM</span> evals with fewer than a few hundred
  datapoints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01747v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01747v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sam Bowyer, Laurence Aitchison, Desi R. Ivanova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rigorous statistical evaluations of large language models (LLMs), including
valid error bars and significance testing, are essential for meaningful and
reliable performance assessment. Currently, when such statistical measures are
reported, they typically rely on the Central Limit Theorem (CLT). In this
position paper, we argue that while CLT-based methods for uncertainty
quantification are appropriate when benchmarks consist of thousands of
examples, they fail to provide adequate uncertainty estimates for LLM
evaluations that rely on smaller, highly specialized benchmarks. In these
small-data settings, we demonstrate that CLT-based methods perform very poorly,
usually dramatically underestimating uncertainty (i.e. producing error bars
that are too small). We give recommendations for alternative frequentist and
Bayesian methods that are both easy to implement and more appropriate in these
increasingly common scenarios. We provide a simple Python library for these
Bayesian methods at https://github.com/sambowyer/bayes_evals .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 37 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Replay Consolidation with Label Propagation for Continual Object
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.05650v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.05650v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riccardo De Monte, Davide Dalle Pezze, Marina Ceccon, Francesco Pasti, Francesco Paissan, Elisabetta Farella, Gian Antonio Susto, Nicola Bellotto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Continual Learning (CL) aims to learn new data while remembering previously
acquired knowledge. In contrast to CL for image classification, CL for Object
Detection faces additional challenges such as the missing annotations problem.
In this scenario, images from previous tasks may contain instances of unknown
classes that could reappear as labeled in future tasks, leading to task
interference in replay-based approaches. Consequently, most approaches in the
literature have focused on distillation-based techniques, which are effective
when there is a significant class overlap between tasks. In our work, we
propose an alternative to distillation-based approaches with a novel approach
called Replay Consolidation with Label Propagation for Object Detection
(RCLPOD). RCLPOD enhances the replay memory by improving the quality of the
stored samples through a technique that promotes class balance while also
improving the quality of the ground truth associated with these samples through
a technique called label propagation. RCLPOD outperforms existing techniques on
well-established benchmarks such as VOC and COC. Moreover, our approach is
developed to work with modern architectures like YOLOv8, making it suitable for
dynamic, real-world applications such as autonomous driving and robotics, where
continuous learning and resource efficiency are essential.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Comet: Fine-grained Computation-communication Ove<span class="highlight-title">rl</span>apping for
  Mixture-of-Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19811v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19811v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shulai Zhang, Ningxin Zheng, Haibin Lin, Ziheng Jiang, Wenlei Bao, Chengquan Jiang, Qi Hou, Weihao Cui, Size Zheng, Li-Wen Chang, Quan Chen, Xin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture-of-experts (MoE) has been extensively employed to scale large
language models to trillion-plus parameters while maintaining a fixed
computational cost. The development of large MoE models in the distributed
scenario encounters the problem of large communication overhead. The
inter-device communication of a MoE layer can occupy 47% time of the entire
model execution with popular models and frameworks. Therefore, existing methods
suggest the communication in a MoE layer to be pipelined with the computation
for overlapping. However, these coarse grained overlapping schemes introduce a
notable impairment of computational efficiency and the latency concealing is
sub-optimal.
  To this end, we present COMET, an optimized MoE system with fine-grained
communication-computation overlapping. Leveraging data dependency analysis and
task rescheduling, COMET achieves precise fine-grained overlapping of
communication and computation. Through adaptive workload assignment, COMET
effectively eliminates fine-grained communication bottlenecks and enhances its
adaptability across various scenarios. Our evaluation shows that COMET
accelerates the execution of a single MoE layer by $1.96\times$ and for
end-to-end execution, COMET delivers a $1.71\times$ speedup on average. COMET
has been adopted in the production environment of clusters with
ten-thousand-scale of GPUs, achieving savings of millions of GPU hours.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessing Robustness via Score-Based Adversarial Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.04285v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.04285v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcel Kollovieh, Lukas Gosch, Marten Lienen, Yan Scholten, Leo Schwinn, Stephan Günnemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most adversarial attacks and defenses focus on perturbations within small
$\ell_p$-norm constraints. However, $\ell_p$ threat models cannot capture all
relevant semantics-preserving perturbations, and hence, the scope of robustness
evaluations is limited. In this work, we introduce Score-Based Adversarial
Generation (ScoreAG), a novel framework that leverages the advancements in
score-based generative models to generate unrestricted adversarial examples
that overcome the limitations of $\ell_p$-norm constraints. Unlike traditional
methods, ScoreAG maintains the core semantics of images while generating
adversarial examples, either by transforming existing images or synthesizing
new ones entirely from scratch. We further exploit the generative capability of
ScoreAG to purify images, empirically enhancing the robustness of classifiers.
Our extensive empirical evaluation demonstrates that ScoreAG improves upon the
majority of state-of-the-art attacks and defenses across multiple benchmarks.
This work highlights the importance of investigating adversarial examples
bounded by semantics rather than $\ell_p$-norm constraints. ScoreAG represents
an important step towards more encompassing robustness assessments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unposed Sparse Views Room Layout Reconstruction in the Age of Pretrain
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16779v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16779v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaxuan Huang, Xili Dai, Jianan Wang, Xianbiao Qi, Yixing Yuan, Xiangyu Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Room layout estimation from multiple-perspective images is poorly
investigated due to the complexities that emerge from multi-view geometry,
which requires muti-step solutions such as camera intrinsic and extrinsic
estimation, image matching, and triangulation. However, in 3D reconstruction,
the advancement of recent 3D foundation models such as DUSt3R has shifted the
paradigm from the traditional multi-step structure-from-motion process to an
end-to-end single-step approach. To this end, we introduce Plane-DUSt3R, a
novel method for multi-view room layout estimation leveraging the 3D foundation
model DUSt3R. Plane-DUSt3R incorporates the DUSt3R framework and fine-tunes on
a room layout dataset (Structure3D) with a modified objective to estimate
structural planes. By generating uniform and parsimonious results, Plane-DUSt3R
enables room layout estimation with only a single post-processing step and 2D
detection results. Unlike previous methods that rely on single-perspective or
panorama image, Plane-DUSt3R extends the setting to handle multiple-perspective
images. Moreover, it offers a streamlined, end-to-end solution that simplifies
the process and reduces error accumulation. Experimental results demonstrate
that Plane-DUSt3R not only outperforms state-of-the-art methods on the
synthetic dataset but also proves robust and effective on in the wild data with
different image styles such as cartoon. Our code is available at:
https://github.com/justacar/Plane-DUSt3R
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025. Github
  page:https://github.com/justacar/Plane-DUSt3R</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Thermodynamic Computing via Autonomous Quantum Thermal Machines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.15905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.15905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patryk Lipka-Bartosik, Martí Perarnau-Llobet, Nicolas Brunner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a physics-based model for classical computation based on
autonomous quantum thermal machines. These machines consist of few interacting
quantum bits (qubits) connected to several environments at different
temperatures. Heat flows through the machine are here exploited for computing.
The process starts by setting the temperatures of the environments according to
the logical input. The machine evolves, eventually reaching a non-equilibrium
steady state, from which the output of the computation can be determined via
the temperature of an auxilliary finite-size reservoir. Such a machine, which
we term a ``thermodynamic neuron'', can implement any linearly-separable
function, and we discuss explicitly the cases of NOT, 3-MAJORITY and NOR gates.
In turn, we show that a network of thermodynamic neurons can perform any
desired function. We discuss the close connection between our model and
artificial neurons (perceptrons), and argue that our model provides an
alternative physics-based analogue implementation of neural networks, and more
generally a platform for thermodynamic computing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 + 5 pages. Published version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Creative Short Story Generation in Humans and Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02316v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02316v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mete Ismayilzada, Claire Stevenson, Lonneke van der Plas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Story-writing is a fundamental aspect of human imagination, relying heavily
on creativity to produce narratives that are novel, effective, and surprising.
While large language models (LLMs) have demonstrated the ability to generate
high-quality stories, their creative story-writing capabilities remain
under-explored. In this work, we conduct a systematic analysis of creativity in
short story generation across 60 LLMs and 60 people using a five-sentence
creative story-writing task. We use measures to automatically evaluate model-
and human-generated stories across several dimensions of creativity, including
novelty, surprise, diversity, and linguistic complexity. We also collect
creativity ratings and Turing Test classifications from non-expert and expert
human raters and LLMs. Automated metrics show that LLMs generate stylistically
complex stories, but tend to fall short in terms of novelty, surprise and
diversity when compared to average human writers. Expert ratings generally
coincide with automated metrics. However, LLMs and non-experts rate LLM stories
to be more creative than human-generated stories. We discuss why and how these
differences in ratings occur, and their implications for both human and
artificial creativity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ICCC 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compare different SG-Schemes based on large least square problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01507v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01507v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ramkrishna Acharya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study reviews popular stochastic gradient-based schemes based on large
least-square problems. These schemes, often called optimizers in machine
learning, play a crucial role in finding better model parameters. Hence, this
study focuses on viewing such optimizers with different hyper-parameters and
analyzing them based on least square problems. Codes that produced results in
this work are available on
https://github.com/q-viper/gradients-based-methods-on-large-least-square.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Simple and Effective Reinforcement Learning Method for Text-to-Image
  Diffusion Fine-tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.00897v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.00897v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shashank Gupta, Chaitanya Ahuja, Tsung-Yu Lin, Sreya Dutta Roy, Harrie Oosterhuis, Maarten de Rijke, Satya Narayan Shukla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning ( RL)-based fine-tuning has emerged as a powerful
approach for aligning diffusion models with black-box objectives. Proximal
policy optimization (PPO) is the most popular choice of method for policy
optimization. While effective in terms of performance, PPO is highly sensitive
to hyper-parameters and involves substantial computational overhead. REINFORCE,
on the other hand, mitigates some computational complexities such as high
memory overhead and sensitive hyper-parameter tuning, but has suboptimal
performance due to high-variance and sample inefficiency. While the variance of
the REINFORCE can be reduced by sampling multiple actions per input prompt and
using a baseline correction term, it still suffers from sample inefficiency. To
address these challenges, we systematically analyze the
efficiency-effectiveness trade-off between REINFORCE and PPO, and propose
leave-one-out PPO ( LOOP), a novel RL for diffusion fine-tuning method. LOOP
combines variance reduction techniques from REINFORCE, such as sampling
multiple actions per input prompt and a baseline correction term, with the
robustness and sample efficiency of PPO via clipping and importance sampling.
Our results demonstrate that LOOP effectively improves diffusion models on
various black-box objectives, and achieves a better balance between
computational efficiency and performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Matching to Generation: A <span class="highlight-title">Survey</span> on Generative Information
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.14851v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.14851v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yuyao Zhang, Peitian Zhang, Yutao Zhu, Zhicheng Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information Retrieval (IR) systems are crucial tools for users to access
information, which have long been dominated by traditional methods relying on
similarity matching. With the advancement of pre-trained language models,
generative information retrieval (GenIR) emerges as a novel paradigm,
attracting increasing attention. Based on the form of information provided to
users, current research in GenIR can be categorized into two aspects:
\textbf{(1) Generative Document Retrieval} (GR) leverages the generative
model's parameters for memorizing documents, enabling retrieval by directly
generating relevant document identifiers without explicit indexing. \textbf{(2)
Reliable Response Generation} employs language models to directly generate
information users seek, breaking the limitations of traditional IR in terms of
document granularity and relevance matching while offering flexibility,
efficiency, and creativity to meet practical needs. This paper aims to
systematically review the latest research progress in GenIR. We will summarize
the advancements in GR regarding model training and structure, document
identifier, incremental learning, etc., as well as progress in reliable
response generation in aspects of internal knowledge memorization, external
knowledge augmentation, etc. We also review the evaluation, challenges and
future developments in GenIR systems. This review aims to offer a comprehensive
reference for researchers, encouraging further development in the GenIR field.
Github Repository: https://github.com/RUC-NLPIR/GenIR-Survey
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Interpretable Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11639v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11639v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pietro Barbiero, Giuseppe Marra, Gabriele Ciravegna, David Debot, Francesco De Santis, Michelangelo Diligenti, Mateo Espinosa Zarlenga, Francesco Giannini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We formalize a novel modeling framework for achieving interpretability in
deep learning, anchored in the principle of inference equivariance. While the
direct verification of interpretability scales exponentially with the number of
variables of the system, we show that this complexity can be mitigated by
treating interpretability as a Markovian property and employing neural
re-parametrization techniques. Building on these insights, we propose a new
modeling paradigm -- neural generation and interpretable execution -- that
enables scalable verification of equivariance. This paradigm provides a general
approach for designing Neural Interpretable Reasoners that are not only
expressive but also transparent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-rigid Structure-from-Motion: Temporally-smooth Procrustean Alignment
  and Spatially-variant Deformation Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.04309v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.04309v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Shi, Hui Deng, Yuchao Dai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Even though Non-rigid Structure-from-Motion (NRSfM) has been extensively
studied and great progress has been made, there are still key challenges that
hinder their broad real-world applications: 1) the inherent motion/rotation
ambiguity requires either explicit camera motion recovery with extra constraint
or complex Procrustean Alignment; 2) existing low-rank modeling of the global
shape can over-penalize drastic deformations in the 3D shape sequence. This
paper proposes to resolve the above issues from a spatial-temporal modeling
perspective. First, we propose a novel Temporally-smooth Procrustean Alignment
module that estimates 3D deforming shapes and adjusts the camera motion by
aligning the 3D shape sequence consecutively. Our new alignment module remedies
the requirement of complex reference 3D shape during alignment, which is more
conductive to non-isotropic deformation modeling. Second, we propose a
spatial-weighted approach to enforce the low-rank constraint adaptively at
different locations to accommodate drastic spatially-variant deformation
reconstruction better. Our modeling outperform existing low-rank based methods,
and extensive experiments across different datasets validate the effectiveness
of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2024; The new version adds additional experiments
  and corrects typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Weak-to-Strong Generalization in Theory and Practice: Reverse
  KL vs. Forward KL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11107v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11107v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Yao, Wenkai Yang, Ziqiao Wang, Yankai Lin, Yong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models advance toward superhuman performance, ensuring
their alignment with human values and abilities grows increasingly complex.
Weak-to-strong generalization offers a promising approach by leveraging
predictions from weaker models to guide stronger systems, but its effectiveness
could be constrained by the inherent noise and inaccuracies in these weak
predictions. To address this, we propose a theoretically grounded approach that
replaces forward KL divergence-whose mass-covering behavior risks overfitting
to imperfect weak signals-with reverse KL divergence. Reverse KL divergence's
zero-forcing effect prioritizes high-confidence predictions, effectively
mitigating the influence of unreliable weak supervision. Theoretically, we
extend existing bounds and derive tighter lower bounds for both forward and
reverse KL divergence, establishing that reverse KL achieves at least
comparable guarantees to forward KL. Notably, when a sufficiently pre-trained
strong model is fine-tuned on the last linear layer, reverse KL guarantees that
it outperforms its weak supervisor by the magnitude of their disagreement.
Empirically, we demonstrate that reverse KL and reverse cross-entropy enable
strong models to successfully outperform those trained with forward KL and
standard cross-entropy across most settings, highlighting the practical
advantages of these reverse losses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Multi</span>modal Inconsistency Reasoning (MMIR): A New Benchmark for
  <span class="highlight-title">Multi</span>modal Reasoning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16033v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16033v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianqi Yan, Yue Fan, Hongquan Li, Shan Jiang, Yang Zhao, Xinze Guan, Ching-Chen Kuo, Xin Eric Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing Multimodal Large Language Models (MLLMs) are predominantly trained
and tested on consistent visual-textual inputs, leaving open the question of
whether they can handle inconsistencies in real-world, layout-rich content. To
bridge this gap, we propose the Multimodal Inconsistency Reasoning (MMIR)
benchmark to assess MLLMs' ability to detect and reason about semantic
mismatches in artifacts such as webpages, presentation slides, and posters.
MMIR comprises 534 challenging samples, each containing synthetically injected
errors across five reasoning-heavy categories: Factual Contradiction, Identity
Misattribution, Contextual Mismatch, Quantitative Discrepancy, and
Temporal/Spatial Incoherence. We evaluate six state-of-the-art MLLMs, showing
that models with dedicated multimodal reasoning capabilities, such as o1,
substantially outperform their counterparts while open-source models remain
particularly vulnerable to inconsistency errors. Detailed error analyses
further show that models excel in detecting pairwise inconsistencies but
struggle with inconsistencies confined to single elements in complex layouts.
Probing experiments reveal that single-modality prompting, including
Chain-of-Thought (CoT) and Set-of-Mark (SoM) methods, yields marginal gains,
revealing a key bottleneck in cross-modal reasoning. Our findings highlight the
need for advanced multimodal reasoning and point to future research on
multimodal inconsistency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ M2Lingual: Enhancing <span class="highlight-title">Multi</span>lingual, <span class="highlight-title">Multi</span>-Turn Instruction Alignment in
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.16783v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.16783v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rishabh Maheshwary, Vikas Yadav, Hoang Nguyen, Khyati Mahajan, Sathwik Tejaswi Madhusudhan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction finetuning (IFT) is critical for aligning Large Language Models
(LLMs) to follow instructions. While many effective IFT datasets have been
introduced recently, they predominantly focus on high-resource languages like
English. To better align LLMs across a broad spectrum of languages and tasks,
we propose a fully synthetic, novel taxonomy (Evol) guided Multilingual,
Multi-turn instruction finetuning dataset, called M2Lingual. It is constructed
by first selecting a diverse set of seed examples and then utilizing the
proposed Evol taxonomy to convert these seeds into complex and challenging
multi-turn instructions. We demonstrate the effectiveness of M2Lingual by
training LLMs of varying sizes and showcasing the enhanced performance across a
diverse set of languages. We contribute the 2 step Evol taxonomy with the
guided generation code: https://github.com/ServiceNow/M2Lingual, as well as the
first fully synthetic, general and task-oriented, multi-turn, multilingual
dataset built with Evol - M2Lingual:
https://huggingface.co/datasets/ServiceNow-AI/ M2Lingual - containing 182K
total IFT pairs, covering 70 languages and 17+ NLP tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Artificial Intelligence-Guided User Studies: An Application
  for Air Taxi Services 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12296v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12296v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengdi Xiao, Jingjing Li, Tatsuki Fushimi, Yoichi Ochiai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  User studies are crucial for meeting user needs. In user studies, real
experimental scenarios and participants are constructed and recruited. However,
emerging and unfamiliar studies face limitations, including safety concerns and
iterative efficiency. To address these challenges, this study utilises a
Generative Artificial Intelligence (GenAI) to create GenAI-generated scenarios
for user experience (UX). By recruiting real users to evaluate this experience,
we can collect feedback that enables rapid iteration in the early design phase.
The air taxi is particularly representative of these challenges and has been
chosen as the case study for this research. The key contribution was designing
an Air Taxi Journey (ATJ) using Large Language Models (LLMs) and AI image and
video generators. Based on the GPT-4-generated scripts, key visuals were
created for the air taxi, and the ATJ was evaluated by 72 participants.
Furthermore, the LLMs demonstrated the ability to identify and suggest
environments that significantly improve participants' willingness toward air
taxis. Education level and gender significantly influenced participants' the
difference in willingness and their satisfaction with the ATJ. Satisfaction
with the ATJ serves as a mediator, significantly influencing participants'
willingness to take air taxis. Our study confirms the capability of GenAI to
support user studies, providing a feasible approach and valuable insights for
designing air taxi UX in the early design phase.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, 6 main figures, 10 appendix figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diversifying Question Generation over Knowledge Base via External
  Natural Questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.14362v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.14362v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shasha Guo, Jing Zhang, Xirui Ke, Cuiping Li, Hong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous methods on knowledge base question generation (KBQG) primarily focus
on enhancing the quality of a single generated question. Recognizing the
remarkable paraphrasing ability of humans, we contend that diverse texts should
convey the same semantics through varied expressions. The above insights make
diversifying question generation an intriguing task, where the first challenge
is evaluation metrics for diversity. Current metrics inadequately assess the
above diversity since they calculate the ratio of unique n-grams in the
generated question itself, which leans more towards measuring duplication
rather than true diversity. Accordingly, we devise a new diversity evaluation
metric, which measures the diversity among top-k generated questions for each
instance while ensuring their relevance to the ground truth. Clearly, the
second challenge is how to enhance diversifying question generation. To address
this challenge, we introduce a dual model framework interwoven by two selection
strategies to generate diverse questions leveraging external natural questions.
The main idea of our dual framework is to extract more diverse expressions and
integrate them into the generation model to enhance diversifying question
generation. Extensive experiments on widely used benchmarks for KBQG
demonstrate that our proposed approach generates highly diverse questions and
improves the performance of question answering tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeqAfford: Sequential 3D Affordance Reasoning via <span class="highlight-title">Multi</span>modal Large
  Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.01550v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.01550v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chunlin Yu, Hanqing Wang, Ye Shi, Haoyang Luo, Sibei Yang, Jingyi Yu, Jingya Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D affordance segmentation aims to link human instructions to touchable
regions of 3D objects for embodied manipulations. Existing efforts typically
adhere to single-object, single-affordance paradigms, where each affordance
type or explicit instruction strictly corresponds to a specific affordance
region and are unable to handle long-horizon tasks. Such a paradigm cannot
actively reason about complex user intentions that often imply sequential
affordances. In this paper, we introduce the Sequential 3D Affordance Reasoning
task, which extends the traditional paradigm by reasoning from cumbersome user
intentions and then decomposing them into a series of segmentation maps. Toward
this, we construct the first instruction-based affordance segmentation
benchmark that includes reasoning over both single and sequential affordances,
comprising 180K instruction-point cloud pairs. Based on the benchmark, we
propose our model, SeqAfford, to unlock the 3D multi-modal large language model
with additional affordance segmentation abilities, which ensures reasoning with
world knowledge and fine-grained affordance grounding in a cohesive framework.
We further introduce a multi-granular language-point integration module to
endow 3D dense prediction. Extensive experimental evaluations show that our
model excels over well-established methods and exhibits open-world
generalization with sequential reasoning abilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VideoRAG: Retrieval-Augmented Generation over Video Corpus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05874v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05874v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soyeong Jeong, Kangsan Kim, Jinheon Baek, Sung Ju Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-Augmented Generation (RAG) is a powerful strategy for improving the
factual accuracy of models by retrieving external knowledge relevant to queries
and incorporating it into the generation process. However, existing approaches
primarily focus on text, with some recent advancements considering images, and
they largely overlook videos, a rich source of multimodal knowledge capable of
representing contextual details more effectively than any other modality. While
very recent studies explore the use of videos in response generation, they
either predefine query-associated videos without retrieval or convert videos
into textual descriptions losing multimodal richness. To tackle these, we
introduce VideoRAG, a framework that not only dynamically retrieves videos
based on their relevance with queries but also utilizes both visual and textual
information. The operation of VideoRAG is powered by recent Large Video
Language Models (LVLMs), which enable the direct processing of video content to
represent it for retrieval and the seamless integration of retrieved videos
jointly with queries for response generation. Also, inspired by that the
context size of LVLMs may not be sufficient to process all frames in extremely
long videos and not all frames are equally important, we introduce a video
frame selection mechanism to extract the most informative subset of frames,
along with a strategy to extract textual information from videos (as it can aid
the understanding of video content) when their subtitles are not available. We
experimentally validate the effectiveness of VideoRAG, showcasing that it is
superior to relevant baselines. Code is available at
https://github.com/starsuzi/VideoRAG.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Advancing Molecular Graph-Text Pre-training via Fine-grained Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14106v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14106v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibo Li, Yuan Fang, Mengmei Zhang, Chuan Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding molecular structure and related knowledge is crucial for
scientific research. Recent studies integrate molecular graphs with their
textual descriptions to enhance molecular representation learning. However,
they focus on the whole molecular graph and neglect frequently occurring
subgraphs, known as motifs, which are essential for determining molecular
properties. Without such fine-grained knowledge, these models struggle to
generalize to unseen molecules and tasks that require motif-level insights. To
bridge this gap, we propose FineMolTex, a novel Fine-grained Molecular
graph-Text pre-training framework to jointly learn coarse-grained
molecule-level knowledge and fine-grained motif-level knowledge. Specifically,
FineMolTex consists of two pre-training tasks: a contrastive alignment task for
coarse-grained matching and a masked multi-modal modeling task for fine-grained
matching. In particular, the latter predicts the labels of masked motifs and
words, which are selected based on their importance. By leveraging insights
from both modalities, FineMolTex is able to understand the fine-grained
matching between motifs and words. Finally, we conduct extensive experiments
across three downstream tasks, achieving up to 238% improvement in the
text-based molecule editing task. Additionally, our case studies reveal that
FineMolTex successfully captures fine-grained knowledge, potentially offering
valuable insights for drug discovery and catalyst design.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Danoliteracy of Generative Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22839v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22839v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Søren Vejlgaard Holm, Lars Kai Hansen, Martin Carsten Nielsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The language technology moonshot moment of Generative Large Language Models
(GLLMs) was not limited to English: These models brought a surge of
technological applications, investments, and hype to low-resource languages as
well. However, the capabilities of these models in languages such as Danish
were, until recently, difficult to verify beyond qualitative demonstrations due
to a lack of applicable evaluation corpora. We present a GLLM benchmark to
evaluate \emph{Danoliteracy}, a measure of Danish language and cultural
competency across eight diverse scenarios such as Danish citizenship tests and
abstractive social media question answering. This limited-size benchmark was
found to produce a robust ranking that correlates to human feedback at $\rho
\sim 0.8$ with GPT-4 and Claude Opus models achieving the highest rankings.
Analyzing these model results across scenarios, we find one strong underlying
factor explaining $95\%$ of scenario performance variance for GLLMs in Danish,
suggesting a $g$ factor of model consistency in language adaptation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 13 figures, Accepted to NoDaLiDa/Baltic-HLT 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI Governance InternationaL Evaluation Index (AGILE Index) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15859v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15859v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Zeng, Enmeng Lu, Xin Guan, Cunqing Huangfu, Zizhe Ruan, Ammar Younas, Kang Sun, Xuan Tang, Yuwei Wang, Hongjie Suo, Dongqi Liang, Zhengqiang Han, Aorigele Bao, Xiaoyang Guo, Jin Wang, Jiawei Xie, Yao Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Artificial Intelligence (AI) technology is
profoundly transforming human society and concurrently presenting a series of
ethical, legal, and social issues. The effective governance of AI has become a
crucial global concern. Since 2022, the extensive deployment of generative AI,
particularly large language models, marked a new phase in AI governance.
Continuous efforts are being made by the international community in actively
addressing the novel challenges posed by these AI developments. As consensus on
international governance continues to be established and put into action, the
practical importance of conducting a global assessment of the state of AI
governance is progressively coming to light. In this context, we initiated the
development of the AI Governance InternationaL Evaluation Index (AGILE Index).
Adhering to the design principle, "the level of governance should match the
level of development," the inaugural evaluation of the AGILE Index commences
with an exploration of four foundational pillars: the development level of AI,
the AI governance environment, the AI governance instruments, and the AI
governance effectiveness. It covers 39 indicators across 18 dimensions to
comprehensively assess the AI governance level of 14 representative countries
globally. The index is utilized to delve into the status of AI governance to
date in 14 countries for the first batch of evaluation. The aim is to depict
the current state of AI governance in these countries through data scoring,
assist them in identifying their governance stage and uncovering governance
issues, and ultimately offer insights for the enhancement of their AI
governance systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Evaluation Report. 85 pages, 30 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RobKiNet: Robotic Kinematics Informed Neural Network for Optimal Robot
  Configuration Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16281v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16281v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanlong Peng, Zhigang Wang, Yisheng Zhang, Pengxu Chang, Ziwen He, Kai Gu, Hongshen Zhang, Ming Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task and Motion Planning (TAMP) is essential for robots to interact with the
world and accomplish complex tasks. The TAMP problem involves a critical gap:
exploring the robot's configuration parameters (such as chassis position and
robotic arm joint angles) within continuous space to ensure that task-level
global constraints are met while also enhancing the efficiency of subsequent
motion planning. Existing methods still have significant room for improvement
in terms of efficiency. Recognizing that robot kinematics is a key factor in
motion planning, we propose a framework called the Robotic Kinematics Informed
Neural Network (RobKiNet) as a bridge between task and motion layers. RobKiNet
integrates kinematic knowledge into neural networks to train models capable of
efficient configuration prediction. We designed a Chassis Motion Predictor(CMP)
and a Full Motion Predictor(FMP) using RobKiNet, which employed two entirely
different sets of forward and inverse kinematics constraints to achieve loosely
coupled control and whole-body control, respectively. Experiments demonstrate
that CMP and FMP can predict configuration parameters with 96.67% and 98%
accuracy, respectively. That means that the corresponding motion planning can
achieve a speedup of 24.24x and 153x compared to random sampling. Furthermore,
RobKiNet demonstrates remarkable data efficiency. CMP only requires 1/71 and
FMP only requires 1/15052 of the training data for the same prediction accuracy
compared to other deep learning methods. These results demonstrate the great
potential of RoboKiNet in robot applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Integrating Various Software Artifacts for Better <span class="highlight-title">LLM</span>-based Bug
  Localization and Program Repair 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiong Feng, Xiaotian Ma, Jiayi Sheng, Ziyuan Feng, Wei Song, Peng Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLMs have garnered considerable attention for their potential to streamline
Automated Program Repair (APR). LLM-based approaches can either insert the
correct code or directly generate patches when provided with buggy methods.
However, most of LLM-based APR methods rely on a single type of software
information, without fully leveraging different software artifacts. Despite
this, many LLM-based approaches do not explore which specific types of
information best assist in APR. Addressing this gap is crucial for advancing
LLM-based APR techniques. We propose DEVLoRe to use issue content (description
and message) and stack error traces to localize buggy methods, then rely on
debug information in buggy methods and issue content and stack error to
localize buggy lines and generate plausible patches which can pass all unit
tests. The results show that while issue content is particularly effective in
assisting LLMs with fault localization and program repair, different types of
software artifacts complement each other. By incorporating different artifacts,
DEVLoRe successfully locates 49.3% and 47.6% of single and non-single buggy
methods and generates 56.0% and 14.5% plausible patches for the Defects4J v2.0
dataset, respectively. This outperforms current state-of-the-art APR methods.
The source code and experimental results of this work for replication are
available at https://github.com/XYZboom/DEVLoRe.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 11 images, 9 tables, Manuscript submitted to a journal
  (2024)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PolaFormer: Polarity-aware Linear Attention for Vision Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15061v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15061v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weikang Meng, Yadan Luo, Xin Li, Dongmei Jiang, Zheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Linear attention has emerged as a promising alternative to softmax-based
attention, leveraging kernelized feature maps to reduce complexity from
quadratic to linear in sequence length. However, the non-negative constraint on
feature maps and the relaxed exponential function used in approximation lead to
significant information loss compared to the original query-key dot products,
resulting in less discriminative attention maps with higher entropy. To address
the missing interactions driven by negative values in query-key pairs, we
propose a polarity-aware linear attention mechanism that explicitly models both
same-signed and opposite-signed query-key interactions, ensuring comprehensive
coverage of relational information. Furthermore, to restore the spiky
properties of attention maps, we provide a theoretical analysis proving the
existence of a class of element-wise functions (with positive first and second
derivatives) that can reduce entropy in the attention distribution. For
simplicity, and recognizing the distinct contributions of each dimension, we
employ a learnable power function for rescaling, allowing strong and weak
attention signals to be effectively separated. Extensive experiments
demonstrate that the proposed PolaFormer improves performance on various vision
tasks, enhancing both expressiveness and efficiency by up to 4.6%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SECURA: Sigmoid-Enhanced CUR Decomposition with Uninterrupted Retention
  and Low-Rank Adaptation in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18168v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18168v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of large language models (LLMs), fully fine-tuning
(FT) these models is becoming increasingly infeasible due to high computational
demands. Moreover, FT also increases the risk of catastrophic forgetting. As an
alternative, Low-Rank Adaptation (LoRA) has been proposed. By fine-tuning only
a small subset of parameters, LoRA achieves performance similar to FT while
significantly reducing resource requirements. However, since LoRA inherits FT's
design, the issue of catastrophic forgetting still remains. To address these
limitations, we propose SECURA: Sigmoid-Enhanced CUR Decomposition LoRA, a
novel PEFT variant designed to mitigate catastrophic forgetting while improving
fine-tuning performance. Our method introduces a novel normalization technique,
Sigmoid-based Magnitude Norm (S-MagNorm), which enhances parameter retention
and fine-tuning efficiency. SECURA has been evaluated on a diverse range of
tasks, including mathematical problem-solving (GSM8K), complex
question-answering (CNNDM), translation (NewsDE), and complex multiple-choice
reasoning (LogiQA). Experimental results demonstrate that it achieves an
average fine-tuning improvement of 3.59% across four MCQ tasks and 2.51% across
five QA tasks on Gemma2 2B, Qwen2 1.5B, Qwen2 7B, Llama3 8B, and Llama3.1 8B,
outperforming DoRA. Additionally, SECURA demonstrates superior knowledge
retention capabilities, achieving state-of-the-art performance in 16 continual
learning tests and maintaining more than 70% accuracy on LLMs' basic knowledge
compared to Experience Replay (ER), sequential learning (SEQ), EWC, I-LoRA, and
CUR-LoRA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>New work on PEFT for LLMs, introducing S-MagNorm and CABR-LoRA to
  enhance fine-tuning performance and knowledge retention. In v4, we renamed
  Sigmoid-based Magnitude Normalization to S-MagNorm for clarity and added a
  gradient comparison between SECURA and CABR-LoRA to highlight their
  contributions</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> of NL2SQL with Large Language Models: Where are we, and where
  are we going? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.05109v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.05109v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Liu, Shuyu Shen, Boyan Li, Peixian Ma, Runzhi Jiang, Yuxin Zhang, Ju Fan, Guoliang Li, Nan Tang, Yuyu Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Translating users' natural language queries (NL) into SQL queries (i.e.,
NL2SQL, a.k.a., Text-to-SQL) can significantly reduce barriers to accessing
relational databases and support various commercial applications. The
performance of NL2SQL has been greatly enhanced with the emergence of Large
Language Models (LLMs). In this survey, we provide a comprehensive review of
NL2SQL techniques powered by LLMs, covering its entire lifecycle from the
following four aspects: (1) Model: NL2SQL translation techniques that tackle
not only NL ambiguity and under-specification, but also properly map NL with
database schema and instances; (2) Data: From the collection of training data,
data synthesis due to training data scarcity, to NL2SQL benchmarks; (3)
Evaluation: Evaluating NL2SQL methods from multiple angles using different
metrics and granularities; and (4) Error Analysis: analyzing NL2SQL errors to
find the root cause and guiding NL2SQL models to evolve. Moreover, we provide a
rule of thumb for developing NL2SQL solutions. Finally, we discuss the research
challenges and open problems of NL2SQL in the LLMs era.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 11 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tera-MIND: Tera-scale mouse brain simulation via spatial mRNA-guided
  diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01220v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01220v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiqing Wu, Ingrid Berg, Yawei Li, Ender Konukoglu, Viktor H. Koelzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Holistic 3D modeling of molecularly defined brain structures is crucial for
understanding complex brain functions. Emerging tissue profiling technologies
enable the construction of a comprehensive atlas of the mammalian brain with
sub-cellular resolution and spatially resolved gene expression data. However,
such tera-scale volumetric datasets present significant computational
challenges in understanding complex brain functions within their native 3D
spatial context. Here, we propose the novel generative approach
$\textbf{Tera-MIND}$, which can simulate $\textbf{Tera}$-scale $\textbf{M}$ouse
bra$\textbf{IN}$s in 3D using a patch-based and boundary-aware
$\textbf{D}$iffusion model. Taking spatial transcriptomic data as the
conditional input, we generate virtual mouse brains with comprehensive cellular
morphological detail at teravoxel scale. Through the lens of 3D $gene$-$gene$
self-attention, we identify spatial molecular interactions for key
transcriptomic pathways in the murine brain, exemplified by glutamatergic and
dopaminergic neuronal systems. Importantly, these $in$-$silico$ biological
findings are consistent and reproducible across three tera-scale virtual mouse
brains. Therefore, Tera-MIND showcases a promising path toward efficient and
generative simulations of whole organ systems for biomedical research. Project
website: https://musikisomorphie.github.io/Tera-MIND.html
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reflective-Net: Learning from Explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2011.13986v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2011.13986v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Johannes Schneider, Michalis Vlachos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We examine whether data generated by explanation techniques, which promote a
process of self-reflection, can improve classifier performance. Our work is
based on the idea that humans have the ability to make quick, intuitive
decisions as well as to reflect on their own thinking and learn from
explanations. To the best of our knowledge, this is the first time that the
potential of mimicking this process by using explanations generated by
explainability methods has been explored. We found that combining explanations
with traditional labeled data leads to significant improvements in
classification accuracy and training efficiency across multiple image
classification datasets and convolutional neural network architectures. It is
worth noting that during training, we not only used explanations for the
correct or predicted class, but also for other classes. This serves multiple
purposes, including allowing for reflection on potential outcomes and enriching
the data through augmentation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Residual Kolmogorov-Arnold Network for Enhanced Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05500v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05500v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ray Congrui Yu, Sherry Wu, Jiang Gui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite their immense success, deep neural networks (CNNs) are costly to
train, while modern architectures can retain hundreds of convolutional layers
in network depth. Standard convolutional operations are fundamentally limited
by their linear nature along with fixed activations, where multiple layers are
needed to learn complex patterns, making this approach computationally
inefficient and prone to optimization difficulties. As a result, we introduce
RKAN (Residual Kolmogorov-Arnold Network), which could be easily implemented
into stages of traditional networks, such as ResNet. The module also integrates
polynomial feature transformation that provides the expressive power of many
convolutional layers through learnable, non-linear feature refinement. Our
proposed RKAN module offers consistent improvements over the base models on
various well-known benchmark datasets, such as CIFAR-100, Food-101, and
ImageNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at https://github.com/withray/residualKAN.git</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ B-STaR: Monitoring and Balancing Exploration and Exploitation in
  Self-Taught Reasoners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.17256v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.17256v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihao Zeng, Yuzhen Huang, Lulu Zhao, Yijun Wang, Zifei Shan, Junxian He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the absence of extensive human-annotated data for complex reasoning tasks,
self-improvement -- where models are trained on their own outputs -- has
emerged as a primary method for enhancing performance. However, the critical
factors underlying the mechanism of these iterative self-improving methods
remain poorly understood, such as under what conditions self-improvement is
effective, and what are the bottlenecks in the current iterations. In this
work, we identify and propose methods to monitor two pivotal factors in this
iterative process: (1) the model's ability to generate sufficiently diverse
responses (exploration); and (2) the effectiveness of external rewards in
distinguishing high-quality candidates from lower-quality ones (exploitation).
Using mathematical reasoning as a case study, we begin with a quantitative
analysis to track the dynamics of exploration and exploitation, discovering
that a model's exploratory capabilities rapidly deteriorate over iterations,
and the effectiveness of exploiting external rewards diminishes as well.
Motivated by these findings, we introduce B-STaR, a Self-Taught Reasoning
framework that autonomously adjusts configurations across iterations to Balance
exploration and exploitation, thereby optimizing the self-improving
effectiveness based on the current policy model and available rewards. Our
experiments on mathematical reasoning, coding, and commonsense reasoning
demonstrate that B-STaR not only enhances the model's exploratory capabilities
throughout training but also achieves a more effective balance between
exploration and exploitation, leading to superior performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting <span class="highlight-title">LLM</span>-Generated Korean Text through Linguistic Feature Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.00032v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.00032v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shinwoo Park, Shubin Kim, Do-Kyung Kim, Yo-Sub Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of large language models (LLMs) increases the
difficulty of distinguishing between human-written and LLM-generated text.
Detecting LLM-generated text is crucial for upholding academic integrity,
preventing plagiarism, protecting copyrights, and ensuring ethical research
practices. Most prior studies on detecting LLM-generated text focus primarily
on English text. However, languages with distinct morphological and syntactic
characteristics require specialized detection approaches. Their unique
structures and usage patterns can hinder the direct application of methods
primarily designed for English. Among such languages, we focus on Korean, which
has relatively flexible spacing rules, a rich morphological system, and less
frequent comma usage compared to English. We introduce KatFish, the first
benchmark dataset for detecting LLM-generated Korean text. The dataset consists
of text written by humans and generated by four LLMs across three genres.
  By examining spacing patterns, part-of-speech diversity, and comma usage, we
illuminate the linguistic differences between human-written and LLM-generated
Korean text. Building on these observations, we propose KatFishNet, a detection
method specifically designed for the Korean language. KatFishNet achieves an
average of 19.78% higher AUROC compared to the best-performing existing
detection method. Our code and data are available at
https://github.com/Shinwoo-Park/detecting_llm_generated_korean_text_through_linguistic_analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variational Learning Induces Adaptive Label Smoothing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.07273v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.07273v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sin-Han Yang, Zhedong Liu, Gian Maria Marconi, Mohammad Emtiyaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show that variational learning naturally induces an adaptive label
smoothing where label noise is specialized for each example. Such
label-smoothing is useful to handle examples with labeling errors and
distribution shifts, but designing a good adaptivity strategy is not always
easy. We propose to skip this step and simply use the natural adaptivity
induced during the optimization of a variational objective. We show empirical
results where a variational algorithm called IVON outperforms traditional label
smoothing and yields adaptivity strategies similar to those of an existing
approach. By connecting Bayesian methods to label smoothing, our work provides
a new way to handle overconfident predictions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stochastic Resetting Mitigates Latent Gradient Bias of SGD from Label
  Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00396v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00396v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youngkyoung Bae, Yeongwoo Song, Hawoong Jeong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Giving up and starting over may seem wasteful in many situations such as
searching for a target or training deep neural networks (DNNs). Our study,
though, demonstrates that resetting from a checkpoint can significantly improve
generalization performance when training DNNs with noisy labels. In the
presence of noisy labels, DNNs initially learn the general patterns of the data
but then gradually memorize the corrupted data, leading to overfitting. By
deconstructing the dynamics of stochastic gradient descent (SGD), we identify
the behavior of a latent gradient bias induced by noisy labels, which harms
generalization. To mitigate this negative effect, we apply the stochastic
resetting method to SGD, inspired by recent developments in the field of
statistical physics achieving efficient target searches. We first theoretically
identify the conditions where resetting becomes beneficial, and then we
empirically validate our theory, confirming the significant improvements
achieved by resetting. We further demonstrate that our method is both easy to
implement and compatible with other methods for handling noisy labels.
Additionally, this work offers insights into the learning dynamics of DNNs from
an interpretability perspective, expanding the potential to analyze training
methods through the lens of statistical physics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ POPGym Arcade: Parallel Pixelated POMDPs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01450v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01450v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zekang Wang, Zhe He, Edan Toledo, Steven Morad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce POPGym Arcade, a benchmark consisting of 7 pixel-based
environments each with three difficulties, utilizing a single observation and
action space. Each environment offers both fully observable and partially
observable variants, enabling counterfactual studies on partial observability.
POPGym Arcade utilizes JIT compilation on hardware accelerators to achieve
substantial speedups over CPU-bound environments. Moreover, this enables
Podracer-style architectures to further increase hardware utilization and
training speed. We evaluate memory models on our environments using a Podracer
variant of Q learning, and examine the results. Finally, we generate memory
saliency maps, uncovering how memories propagate through policies. Our library
is available at https://github.com/bolt-research/popgym_arcade.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-03-03T00:00:00Z">2025-03-03</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">63</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Forecasting Frontier Language Model Agent Capabilities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15850v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15850v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Govind Pimpale, Axel Højmark, Jérémy Scheurer, Marius Hobbhahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Language Models (LMs) increasingly operate as autonomous agents,
accurately forecasting their capabilities becomes crucial for societal
preparedness. We evaluate six forecasting methods that predict downstream
capabilities of LM agents. We use "one-step" approaches that predict benchmark
scores from input metrics like compute or model release date directly or
"two-step" approaches that first predict an intermediate metric like the
principal component of cross-benchmark performance (PC-1) and human-evaluated
competitive Elo ratings. We evaluate our forecasting methods by backtesting
them on a dataset of 38 LMs from the OpenLLM 2 leaderboard. We then use the
validated two-step approach (Release Date$\to$Elo$\to$Benchmark) to predict LM
agent performance for frontier models on three benchmarks: SWE-Bench Verified
(software development), Cybench (cybersecurity assessment), and RE-Bench (ML
research engineering). Our forecast predicts that by the beginning of 2026,
non-specialized LM agents with low capability elicitation will reach a success
rate of 54% on SWE-Bench Verified, while state-of-the-art LM agents will reach
an 87% success rate. Our approach does not account for recent advances in
inference-compute scaling and might thus be too conservative.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chain of Draft: Thinking Faster by Writing Less 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18600v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18600v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Silei Xu, Wenhao Xie, Lingxiao Zhao, Pengcheng He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable performance in
solving complex reasoning tasks through mechanisms like Chain-of-Thought (CoT)
prompting, which emphasizes verbose, step-by-step reasoning. However, humans
typically employ a more efficient strategy: drafting concise intermediate
thoughts that capture only essential information. In this work, we propose
Chain of Draft (CoD), a novel paradigm inspired by human cognitive processes,
where LLMs generate minimalistic yet informative intermediate reasoning outputs
while solving tasks. By reducing verbosity and focusing on critical insights,
CoD matches or surpasses CoT in accuracy while using as little as only 7.6% of
the tokens, significantly reducing cost and latency across various reasoning
tasks. Our code and data are available at
https://github.com/sileix/chain-of-draft.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SensorQA: A Question Answering Benchmark for Daily-Life Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.04974v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.04974v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Reichman, Xiaofan Yu, Lanxiang Hu, Jack Truxal, Atishay Jain, Rushil Chandrupatla, Tajana Šimunić Rosing, Larry Heck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid growth in sensor data, effectively interpreting and
interfacing with these data in a human-understandable way has become crucial.
While existing research primarily focuses on learning classification models,
fewer studies have explored how end users can actively extract useful insights
from sensor data, often hindered by the lack of a proper dataset. To address
this gap, we introduce SensorQA, the first human-created question-answering
(QA) dataset for long-term time-series sensor data for daily life monitoring.
SensorQA is created by human workers and includes 5.6K diverse and practical
queries that reflect genuine human interests, paired with accurate answers
derived from sensor data. We further establish benchmarks for state-of-the-art
AI models on this dataset and evaluate their performance on typical edge
devices. Our results reveal a gap between current models and optimal QA
performance and efficiency, highlighting the need for new contributions. The
dataset and code are available at:
https://github.com/benjamin-reichman/SensorQA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Memory Construction and Retrieval for Personalized Conversational
  Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05589v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05589v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoshi Pan, Qianhui Wu, Huiqiang Jiang, Xufang Luo, Hao Cheng, Dongsheng Li, Yuqing Yang, Chin-Yew Lin, H. Vicky Zhao, Lili Qiu, Jianfeng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To deliver coherent and personalized experiences in long-term conversations,
existing approaches typically perform retrieval augmented response generation
by constructing memory banks from conversation history at either the
turn-level, session-level, or through summarization techniques.In this paper,
we present two key findings: (1) The granularity of memory unit matters:
turn-level, session-level, and summarization-based methods each exhibit
limitations in both memory retrieval accuracy and the semantic quality of the
retrieved content. (2) Prompt compression methods, such as LLMLingua-2, can
effectively serve as a denoising mechanism, enhancing memory retrieval accuracy
across different granularities. Building on these insights, we propose SeCom, a
method that constructs the memory bank at segment level by introducing a
conversation segmentation model that partitions long-term conversations into
topically coherent segments, while applying compression based denoising on
memory units to enhance memory retrieval. Experimental results show that SeCom
exhibits a significant performance advantage over baselines on long-term
conversation benchmarks LOCOMO and Long-MT-Bench+. Additionally, the proposed
conversation segmentation method demonstrates superior performance on dialogue
segmentation datasets such as DialSeg711, TIAGE, and SuperDialSeg.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ R1-T1: Fully Incentivizing Translation Capability in <span class="highlight-title">LLM</span>s via Reasoning
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19735v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19735v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minggui He, Yilun Liu, Shimin Tao, Yuanchang Luo, Hongyong Zeng, Chang Su, Li Zhang, Hongxia Ma, Daimeng Wei, Weibin Meng, Hao Yang, Boxing Chen, Osamu Yoshie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent breakthroughs in reasoning-enhanced large language models
(LLMs) like DeepSeek-R1, incorporating inference-time reasoning into machine
translation (MT), where human translators naturally employ structured,
multi-layered reasoning chain-of-thoughts (CoTs), is yet underexplored.
Existing methods either design a fixed CoT tailored for a specific MT sub-task
(e.g., literature translation), or rely on synthesizing CoTs unaligned with
humans, limiting their adaptability to diverse translation scenarios. This
paper introduces R1-Translator (R1-T1), a novel framework to achieve
inference-time reasoning for general MT via reinforcement learning (RL) with
human-aligned CoTs comprising six common patterns. Our approach pioneers three
innovations: (1) extending reasoning-based translation beyond MT sub-tasks to
six languages and diverse tasks (e.g., legal/medical domain adaptation, idiom
resolution); (2) formalizing six expert-curated CoT templates that mirror
hybrid human strategies like context-aware paraphrasing and back translation;
and (3) enabling self-evolving CoT discovery through RL. Experimental results
indicate a steady translation performance improvement in 11 languages and 40
translation directions on Flores-101 test set, especially on the languages
unseen from training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InductionBench: <span class="highlight-title">LLM</span>s Fail in the Simplest Complexity Class 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15823v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15823v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyue Hua, Tyler Wong, Sun Fei, Liangming Pan, Adam Jardine, William Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable improvements in reasoning
and many existing benchmarks have been addressed by models such as o1 and o3
either fully or partially. However, a majority of these benchmarks emphasize
deductive reasoning, including mathematical and coding tasks in which rules
such as mathematical axioms or programming syntax are clearly defined, based on
which LLMs can plan and apply these rules to arrive at a solution. In contrast,
inductive reasoning, where one infers the underlying rules from observed data,
remains less explored. Such inductive processes lie at the heart of scientific
discovery, as they enable researchers to extract general principles from
empirical observations. To assess whether LLMs possess this capacity, we
introduce InductionBench, a new benchmark designed to evaluate the inductive
reasoning ability of LLMs. Our experimental findings reveal that even the most
advanced models available struggle to master the simplest complexity classes
within the subregular hierarchy of functions, highlighting a notable deficiency
in current LLMs' inductive reasoning capabilities. Coda and data are available
https://github.com/Wenyueh/inductive_reasoning_benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can Knowledge Editing Really Correct Hallucinations? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16251v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16251v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baixiang Huang, Canyu Chen, Xiongxiao Xu, Ali Payani, Kai Shu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) suffer from hallucinations, referring to the
non-factual information in generated content, despite their superior capacities
across tasks. Meanwhile, knowledge editing has been developed as a new popular
paradigm to correct erroneous factual knowledge encoded in LLMs with the
advantage of avoiding retraining from scratch. However, a common issue of
existing evaluation datasets for knowledge editing is that they do not ensure
that LLMs actually generate hallucinated answers to the evaluation questions
before editing. When LLMs are evaluated on such datasets after being edited by
different techniques, it is hard to directly adopt the performance to assess
the effectiveness of different knowledge editing methods in correcting
hallucinations. Thus, the fundamental question remains insufficiently
validated: Can knowledge editing really correct hallucinations in LLMs? We
proposed HalluEditBench to holistically benchmark knowledge editing methods in
correcting real-world hallucinations. First, we rigorously construct a massive
hallucination dataset with 9 domains, 26 topics and more than 6,000
hallucinations. Then, we assess the performance of knowledge editing methods in
a holistic way on five dimensions including Efficacy, Generalization,
Portability, Locality, and Robustness. Through HalluEditBench, we have provided
new insights into the potentials and limitations of different knowledge editing
methods in correcting hallucinations, which could inspire future improvements
and facilitate progress in the field of knowledge editing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025. Main paper: 10 pages; total: 34 pages (including
  appendix). The first two authors contributed equally to this work. Code,
  data, results, and additional resources are available on the project website:
  https://llm-editing.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting the Test-Time Scaling of o1-like Models: Do they Truly
  Possess Test-Time Scaling Capabilities? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12215v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12215v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Zeng, Qinyuan Cheng, Zhangyue Yin, Yunhua Zhou, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of test-time scaling in large language models (LLMs), exemplified
by OpenAI's o1 series, has advanced reasoning capabilities by scaling
computational resource allocation during inference. While successors like QwQ,
Deepseek-R1 (R1) and LIMO replicate these advancements, whether these models
truly possess test-time scaling capabilities remains underexplored. This study
found that longer CoTs of these o1-like models do not consistently enhance
accuracy; in fact, correct solutions are often shorter than incorrect ones for
the same questions. Further investigation shows this phenomenon is closely
related to models' self-revision capabilities - longer CoTs contain more
self-revisions, which often lead to performance degradation. We then compare
sequential and parallel scaling strategies on QwQ, R1 and LIMO, finding that
parallel scaling achieves better coverage and scalability. Based on these
insights, we propose Shortest Majority Vote, a method that combines parallel
scaling strategies with CoT length characteristics, significantly improving
models' test-time scalability compared to conventional majority voting
approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Add the github link</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ First-Person Fairness in Chatbots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.19803v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.19803v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tyna Eloundou, Alex Beutel, David G. Robinson, Keren Gu-Lemberg, Anna-Luisa Brakman, Pamela Mishkin, Meghan Shah, Johannes Heidecke, Lilian Weng, Adam Tauman Kalai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating chatbot fairness is crucial given their rapid proliferation, yet
typical chatbot tasks (e.g., resume writing, entertainment) diverge from the
institutional decision-making tasks (e.g., resume screening) which have
traditionally been central to discussion of algorithmic fairness. The
open-ended nature and diverse use-cases of chatbots necessitate novel methods
for bias assessment. This paper addresses these challenges by introducing a
scalable counterfactual approach to evaluate "first-person fairness," meaning
fairness toward chatbot users based on demographic characteristics. Our method
employs a Language Model as a Research Assistant (LMRA) to yield quantitative
measures of harmful stereotypes and qualitative analyses of demographic
differences in chatbot responses. We apply this approach to assess biases in
six of our language models across millions of interactions, covering sixty-six
tasks in nine domains and spanning two genders and four races. Independent
human annotations corroborate the LMRA-generated bias evaluations. This study
represents the first large-scale fairness evaluation based on real-world chat
data. We highlight that post-training reinforcement learning techniques
significantly mitigate these biases. This evaluation provides a practical
methodology for ongoing bias monitoring and mitigation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In ICLR 2025, 59 pages, 27 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CNsum:Automatic Summarization for Chinese News Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19723v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19723v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Zhao, Songping Huang, Dongsheng Zhou, Zhaoyun Ding, Fei Wang, Aixin Nian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Obtaining valuable information from massive data efficiently has become our
research goal in the era of Big Data. Text summarization technology has been
continuously developed to meet this demand. Recent work has also shown that
transformer-based pre-trained language models have achieved great success on
various tasks in Natural Language Processing (NLP). Aiming at the problem of
Chinese news text summary generation and the application of Transformer
structure on Chinese, this paper proposes a Chinese news text summarization
model (CNsum) based on Transformer structure, and tests it on Chinese datasets
such as THUCNews. The results of the conducted experiments show that CNsum
achieves better ROUGE score than the baseline models, which verifies the
outperformance of the model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This withdrawal is due to the lack of authorization from all
  co-authors for the publication of this version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gumbel Counterfactual Generation From Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07180v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07180v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shauli Ravfogel, Anej Svete, Vésteinn Snæbjarnarson, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and manipulating the causal generation mechanisms in language
models is essential for controlling their behavior. Previous work has primarily
relied on techniques such as representation surgery -- e.g., model ablations or
manipulation of linear subspaces tied to specific concepts -- to
\emph{intervene} on these models. To understand the impact of interventions
precisely, it is useful to examine \emph{counterfactuals} -- e.g., how a given
sentence would have appeared had it been generated by the model following a
specific intervention. We highlight that counterfactual reasoning is
conceptually distinct from interventions, as articulated in Pearl's causal
hierarchy. Based on this observation, we propose a framework for generating
true string counterfactuals by reformulating language models as a structural
equation model using the Gumbel-max trick, which we called Gumbel
counterfactual generation. This reformulation allows us to model the joint
distribution over original strings and their counterfactuals resulting from the
same instantiation of the sampling noise. We develop an algorithm based on
hindsight Gumbel sampling that allows us to infer the latent noise variables
and generate counterfactuals of observed strings. Our experiments demonstrate
that the approach produces meaningful counterfactuals while at the same time
showing that commonly used intervention techniques have considerable undesired
side effects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Tokens to Words: On the Inner Lexicon of <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05864v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05864v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guy Kaplan, Matanel Oren, Yuval Reif, Roy Schwartz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language is composed of words, but modern large language models
(LLMs) process sub-words as input. A natural question raised by this
discrepancy is whether LLMs encode words internally, and if so how. We present
evidence that LLMs engage in an intrinsic detokenization process, where
sub-word sequences are combined into coherent whole-word representations at
their last token. Our experiments show that this process primarily takes place
within the early and middle layers of the model. We further demonstrate its
robustness to arbitrary splits (e.g., "cats" to "ca" and "ts"), typos, and
importantly-to out-of-vocabulary words: when feeding the last token internal
representations of such words to the model as input, it can "understand" them
as the complete word despite never seeing such representations as input during
training. Our findings suggest that LLMs maintain a latent vocabulary beyond
the tokenizer's scope. These insights provide a practical, finetuning-free
application for expanding the vocabulary of pre-trained models. By enabling the
addition of new vocabulary words, we reduce input length and inference
iterations, which reduces both space and model latency, with little to no loss
in model accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the International Conference on Learning Representations
  (ICLR) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Naturally Occurring Feedback is Common, Extractable and Useful 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10944v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10944v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shachar Don-Yehiya, Leshem Choshen, Omri Abend
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human feedback data is a critical component in developing language models.
However, collecting this feedback is costly and ultimately not scalable.
Inspired by the way human interlocutors provide spontaneous unsolicited
feedback to each other, we propose to extract feedback that users naturally
include when interacting with chat models. We manually annotated conversations
to confirm the presence of naturally occurring feedback in a standard corpus,
finding that as much as 30% of the chats include explicit feedback. Comparing
to older datasets, we find that naturally occurring feedback is more prevalent
in recent conversation datasets, suggesting that more than ever, naturally
occurring feedback can serve as a valuable resource for feedback data. We
propose a method for automatically extracting this feedback, and apply it to
over 1M conversations to obtain hundreds of thousands of feedback samples. The
extracted feedback shows promise: training with it improves over baseline
models and enhances model alignment to human preferences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Intelligence via Trial and Error 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18858v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18858v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingtao Zhan, Jiahao Zhao, Jiayu Li, Yiqun Liu, Bo Zhang, Qingyao Ai, Jiaxin Mao, Hongning Wang, Min Zhang, Shaoping Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intelligence is a crucial trait for species to find solutions within a
limited number of trial-and-error attempts. Building on this idea, we introduce
Survival Game as a framework to evaluate intelligence based on the number of
failed attempts in a trial-and-error process. Fewer failures indicate higher
intelligence. When the expectation and variance of failure counts are both
finite, it signals the ability to consistently find solutions to new
challenges, which we define as the Autonomous Level of intelligence. Using
Survival Game, we comprehensively evaluate existing AI systems. Our results
show that while AI systems achieve the Autonomous Level in simple tasks, they
are still far from it in more complex tasks, such as vision, search,
recommendation, and language. While scaling current AI technologies might help,
this would come at an astronomical cost. Projections suggest that achieving the
Autonomous Level for general tasks would require $10^{26}$ parameters. To put
this into perspective, loading such a massive model requires so many H100 GPUs
that their total value is $10^{7}$ times that of Apple Inc.'s market value.
Even with Moore's Law, supporting such a parameter scale would take $70$ years.
This staggering cost highlights the complexity of human tasks and the
inadequacies of current AI technologies. To further investigate this
phenomenon, we conduct a theoretical analysis of Survival Game and its
experimental results. Our findings suggest that human tasks possess a
criticality property. As a result, Autonomous Level requires a deep
understanding of the task's underlying mechanisms. Current AI systems, however,
do not fully grasp these mechanisms and instead rely on superficial mimicry,
making it difficult for them to reach an autonomous level. We believe Survival
Game can not only guide the future development of AI but also offer profound
insights into human intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimize Incompatible Parameters through Compatibility-aware Knowledge
  Integration <span class="chip">AAAI'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07596v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07596v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheqi Lv, Keming Ye, Zishu Wei, Qi Tian, Shengyu Zhang, Wenqiao Zhang, Wenjie Wang, Kun Kuang, Tat-Seng Chua, Fei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have become foundational to advancements in multiple
domains, including recommendation systems, natural language processing, and so
on. Despite their successes, these models often contain incompatible parameters
that can be underutilized or detrimental to model performance, particularly
when faced with specific, varying data distributions. Existing research excels
in removing such parameters or merging the outputs of multiple different
pretrained models. However, the former focuses on efficiency rather than
performance, while the latter requires several times more computing and storage
resources to support inference. In this paper, we set the goal to explicitly
improve these incompatible parameters by leveraging the complementary strengths
of different models, thereby directly enhancing the models without any
additional parameters. Specifically, we propose Compatibility-aware Knowledge
Integration (CKI), which consists of Parameter Compatibility Assessment and
Parameter Splicing, which are used to evaluate the knowledge content of
multiple models and integrate the knowledge into one model, respectively. The
integrated model can be used directly for inference or for further fine-tuning.
We conduct extensive experiments on various datasets for recommendation and
language tasks, and the results show that Compatibility-aware Knowledge
Integration can effectively optimize incompatible parameters under multiple
tasks and settings to break through the training limit of the original model
without increasing the inference cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published on AAAI'25(Oral): The Annual AAAI Conference on Artificial
  Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Better Chain-of-Thought: A Reflection on Effectiveness and
  Faithfulness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18915v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18915v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiachun Li, Pengfei Cao, Yubo Chen, Jiexin Xu, Huaijun Li, Xiaojian Jiang, Kang Liu, Jun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-thought (CoT) prompting demonstrates varying performance under
different reasoning tasks. Previous work attempts to evaluate it but falls
short in providing an in-depth analysis of patterns that influence the CoT. In
this paper, we study the CoT performance from the perspective of effectiveness
and faithfulness. For the former, we identify key factors that influence CoT
effectiveness on performance improvement, including problem difficulty,
information gain, and information flow. For the latter, we interpret the
unfaithful CoT issue by conducting a joint analysis of the information
interaction among the question, CoT, and answer. The result demonstrates that,
when the LLM predicts answers, it can recall correct information missing in the
CoT from the question, leading to the problem. Finally, we propose a novel
algorithm to mitigate this issue, in which we recall extra information from the
question to enhance the CoT generation and evaluate CoTs based on their
information gain. Extensive experiments demonstrate that our approach enhances
both the faithfulness and effectiveness of CoT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The ShareLM Collection and Plugin: Contributing Human-Model Chats for
  the Benefit of the Community 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08291v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08291v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shachar Don-Yehiya, Leshem Choshen, Omri Abend
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human-model conversations provide a window into users' real-world scenarios,
behavior, and needs, and thus are a valuable resource for model development and
research. While for-profit companies collect user data through the APIs of
their models, using it internally to improve their own models, the open source
and research community lags behind.
  We introduce the ShareLM collection, a unified set of human conversations
with large language models, and its accompanying plugin, a Web extension for
voluntarily contributing user-model conversations. Where few platforms share
their chats, the ShareLM plugin adds this functionality, thus, allowing users
to share conversations from most platforms. The plugin allows the user to rate
their conversations, both at the conversation and the response levels, and
delete conversations they prefer to keep private before they ever leave the
user's local storage. We release the plugin conversations as part of the
ShareLM collection, and call for more community effort in the field of open
human-model data.
  The code, plugin, and data are available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry
  Scientific Hypotheses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07076v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07076v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific discovery contributes largely to human society's prosperity, and
recent progress shows that LLMs could potentially catalyze this process.
However, it is still unclear whether LLMs can discover novel and valid
hypotheses in chemistry. In this work, we investigate this central research
question: Can LLMs automatically discover novel and valid chemistry research
hypotheses given only a chemistry research background (consisting of a research
question and/or a background survey), without limitation on the domain of the
research question? After extensive discussions with chemistry experts, we
propose an assumption that a majority of chemistry hypotheses can be resulted
from a research background and several inspirations. With this key insight, we
break the central question into three smaller fundamental questions. In brief,
they are: (1) given a background question, whether LLMs can retrieve good
inspirations; (2) with background and inspirations, whether LLMs can lead to
hypothesis; and (3) whether LLMs can identify good hypotheses to rank them
higher. To investigate these questions, we construct a benchmark consisting of
51 chemistry papers published in Nature, Science, or a similar level in 2024
(all papers are only available online since 2024). Every paper is divided by
chemistry PhD students into three components: background, inspirations, and
hypothesis. The goal is to rediscover the hypothesis, given only the background
and a large randomly selected chemistry literature corpus consisting the ground
truth inspiration papers, with LLMs trained with data up to 2023. We also
develop an LLM-based multi-agent framework that leverages the assumption,
consisting of three stages reflecting the three smaller questions. The proposed
method can rediscover many hypotheses with very high similarity with the ground
truth ones, covering the main innovations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NavRAG: Generating User Demand Instructions for Embodied Navigation
  through Retrieval-Augmented <span class="highlight-title">LLM</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11142v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11142v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Wang, Yaohui Zhu, Gim Hee Lee, Yachun Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-and-Language Navigation (VLN) is an essential skill for embodied
agents, allowing them to navigate in 3D environments following natural language
instructions. High-performance navigation models require a large amount of
training data, the high cost of manually annotating data has seriously hindered
this field. Therefore, some previous methods translate trajectory videos into
step-by-step instructions for expanding data, but such instructions do not
match well with users' communication styles that briefly describe destinations
or state specific needs. Moreover, local navigation trajectories overlook
global context and high-level task planning. To address these issues, we
propose NavRAG, a retrieval-augmented generation (RAG) framework that generates
user demand instructions for VLN. NavRAG leverages LLM to build a hierarchical
scene description tree for 3D scene understanding from global layout to local
details, then simulates various user roles with specific demands to retrieve
from the scene tree, generating diverse instructions with LLM. We annotate over
2 million navigation instructions across 861 scenes and evaluate the data
quality and navigation performance of trained models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Speculative Decoding and Beyond: An In-Depth <span class="highlight-title">Survey</span> of Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19732v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19732v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunhai Hu, Zining Liu, Zhenyuan Dong, Tianfan Peng, Bradley McDanel, Sai Qian Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential dependencies present a fundamental bottleneck in deploying
large-scale autoregressive models, particularly for real-time applications.
While traditional optimization approaches like pruning and quantization often
compromise model quality, recent advances in generation-refinement frameworks
demonstrate that this trade-off can be significantly mitigated.
  This survey presents a comprehensive taxonomy of generation-refinement
frameworks, analyzing methods across autoregressive sequence tasks. We
categorize methods based on their generation strategies (from simple n-gram
prediction to sophisticated draft models) and refinement mechanisms (including
single-pass verification and iterative approaches). Through systematic analysis
of both algorithmic innovations and system-level implementations, we examine
deployment strategies across computing environments and explore applications
spanning text, images, and speech generation. This systematic examination of
both theoretical frameworks and practical implementations provides a foundation
for future research in efficient autoregressive decoding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variational Best-of-N Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06057v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06057v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Afra Amini, Tim Vieira, Elliott Ash, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Best-of-N (BoN) is a popular and effective algorithm for aligning language
models to human preferences. The algorithm works as follows: at inference time,
N samples are drawn from the language model, and the sample with the highest
reward, as judged by a reward model, is returned as the output. Despite its
effectiveness, BoN is computationally expensive; it reduces sampling throughput
by a factor of N. To make BoN more efficient at inference time, one strategy is
to fine-tune the language model to mimic what BoN does during inference. To
achieve this, we derive the distribution induced by the BoN algorithm. We then
propose to fine-tune the language model to minimize backward KL divergence to
the BoN distribution. Our approach is analogous to mean-field variational
inference and, thus, we term it variational BoN (vBoN). To the extent this
fine-tuning is successful and we end up with a good approximation, we have
reduced the inference cost by a factor of N. Our experiments on controlled
generation and summarization tasks show that BoN is the most effective
alignment method, and our variational approximation to BoN achieves the closest
performance to BoN and surpasses models fine-tuned using the standard
KL-constrained RL objective. In the controlled generation task, vBoN appears
more frequently on the Pareto frontier of reward and KL divergence compared to
other alignment methods. In the summarization task, vBoN achieves high reward
values across various sampling temperatures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Iterative Controllable Summarization with Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12460v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12460v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sangwon Ryu, Heejin Do, Daehee Kim, Hwanjo Yu, Dongwoo Kim, Yunsu Kim, Gary Geunbae Lee, Jungseul Ok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated remarkable performance in
abstractive summarization tasks. However, their ability to precisely control
summary attributes (e.g., length or topic) remains underexplored, limiting
their adaptability to specific user preferences. In this paper, we
systematically explore the controllability of LLMs. To this end, we revisit
summary attribute measurements and introduce iterative evaluation metrics,
failure rate and average iteration count to precisely evaluate controllability
of LLMs, rather than merely assessing errors. Our findings show that LLMs
struggle more with numerical attributes than with linguistic attributes. To
address this challenge, we propose a guide-to-explain framework (GTE) for
controllable summarization. Our GTE framework enables the model to identify
misaligned attributes in the initial draft and guides it in self-explaining
errors in the previous output. By allowing the model to reflect on its
misalignment, GTE generates well-adjusted summaries that satisfy the desired
attributes with robust effectiveness, requiring surprisingly fewer iterations
than other iterative approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ "Nuclear Deployed!": Analyzing Catastrophic Risks in Decision-making of
  Autonomous <span class="highlight-title">LLM</span> Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11355v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11355v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongwu Xu, Xiaojian Li, Shuo Chen, Wei Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are evolving into autonomous decision-makers,
raising concerns about catastrophic risks in high-stakes scenarios,
particularly in Chemical, Biological, Radiological and Nuclear (CBRN) domains.
Based on the insight that such risks can originate from trade-offs between the
agent's Helpful, Harmlessness and Honest (HHH) goals, we build a novel
three-stage evaluation framework, which is carefully constructed to effectively
and naturally expose such risks. We conduct 14,400 agentic simulations across
12 advanced LLMs, with extensive experiments and analysis. Results reveal that
LLM agents can autonomously engage in catastrophic behaviors and deception,
without being deliberately induced. Furthermore, stronger reasoning abilities
often increase, rather than mitigate, these risks. We also show that these
agents can violate instructions and superior commands. On the whole, we
empirically prove the existence of catastrophic risks in autonomous LLM agents.
We will release our code upon request.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Please visit https://llm-catastrophic-risks.github.io for a quick
  tour of our project</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LaERC-S: Improving <span class="highlight-title">LLM</span>-based Emotion Recognition in Conversation with
  Speaker Characteristics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.07260v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.07260v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yumeng Fu, Junjie Wu, Zhongjie Wang, Meishan Zhang, Lili Shan, Yulin Wu, Bingquan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotion recognition in conversation (ERC), the task of discerning human
emotions for each utterance within a conversation, has garnered significant
attention in human-computer interaction systems. Previous ERC studies focus on
speaker-specific information that predominantly stems from relationships among
utterances, which lacks sufficient information around conversations. Recent
research in ERC has sought to exploit pre-trained large language models (LLMs)
with speaker modelling to comprehend emotional states. Although these methods
have achieved encouraging results, the extracted speaker-specific information
struggles to indicate emotional dynamics. In this paper, motivated by the fact
that speaker characteristics play a crucial role and LLMs have rich world
knowledge, we present LaERC-S, a novel framework that stimulates LLMs to
explore speaker characteristics involving the mental state and behavior of
interlocutors, for accurate emotion predictions. To endow LLMs with this
knowledge information, we adopt the two-stage learning to make the models
reason speaker characteristics and track the emotion of the speaker in complex
conversation scenarios. Extensive experiments on three benchmark datasets
demonstrate the superiority of LaERC-S, reaching the new state-of-the-art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Large Language Models with Pseudo- and <span class="highlight-title">Multi</span>source- Knowledge
  Graphs for Open-ended Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09911v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09911v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxiang Liu, Tong Zhou, Yubo Chen, Kang Liu, Jun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mitigating the hallucinations of Large Language Models is a crucial task.
Although some existing methods employ self-enhancement techniques, they fall
short of effectively addressing unknown factual hallucinations. Meanwhile,
Knowledge Graph (KG) enhancement approaches fail to address the generalization
across different KG sources and the enhancement of open-ended answer questions
simultaneously. To tackle these limitations, we propose a framework that
combines Pseudo-Graph Generation and Atomic Knowledge Verification (PG\&AKV).
Enhancement of open-ended question-answering begins with leveraging the
Pseudo-Graph Generation to provide the related knowledge framework.
Subsequently, Atomic Knowledge Verification utilizes atomic-level knowledge
querying and verification to achieve generalizability under different KG
sources. Compared to the baseline, this approach yields a minimum improvement
of 11.5 in the ROUGE-L score for open-ended questions. For precise-answered
questions, we observe a minimum accuracy improvement of 7.5%. Moreover, PG\&AKV
also exhibits generalizability across different KG sources. Utilizing KG
different from the question sources, PG\&AKV can even achieve at least a 3.5 %
performance improvement. In summary, our results pave the way for enhancing
LLMs by incorporating Pseudo- and Multisource-KGs, particularly in the filed of
open-ended questions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ECLeKTic: a Novel Challenge Set for Evaluation of Cross-Lingual
  Knowledge Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21228v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21228v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omer Goldman, Uri Shaham, Dan Malkin, Sivan Eiger, Avinatan Hassidim, Yossi Matias, Joshua Maynez, Adi Mayrav Gilady, Jason Riesa, Shruti Rijhwani, Laura Rimell, Idan Szpektor, Reut Tsarfaty, Matan Eyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To achieve equitable performance across languages, multilingual large
language models (LLMs) must be able to abstract knowledge beyond the language
in which it was acquired. However, the current literature lacks reliable ways
to measure LLMs' capability of cross-lingual knowledge transfer. To that end,
we present ECLeKTic, a multilingual closed-book QA (CBQA) dataset that
Evaluates Cross-Lingual Knowledge Transfer in a simple, black-box manner. We
detected information with uneven coverage across languages by controlling for
presence and absence of Wikipedia articles in 12 languages. We generated
knowledge-seeking questions in a source language, for which the answer appears
in a relevant Wikipedia article and translated them to all other 11 languages,
for which the respective Wikipedias lack equivalent articles. Assuming that
Wikipedia reflects the prominent knowledge in the LLM's training data, to solve
ECLeKTic's CBQA task the model is required to transfer knowledge between
languages. Experimenting with 8 LLMs, we show that SOTA models struggle to
effectively share knowledge across, languages even if they can predict the
answer well for queries in the same language the knowledge was acquired in.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SURGE: On the Potential of Large Language Models as General-Purpose
  Surrogate Code Executors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11167v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11167v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohan Lyu, Siqiao Huang, Zichen Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural surrogate models have emerged as powerful and efficient tools in data
mining. Meanwhile, large language models (LLMs) have demonstrated remarkable
capabilities in code-related tasks. We investigate a novel application: using
LLMs as surrogate models for code execution prediction. Given LLMs' unique
ability to understand and process diverse programs, they present a promising
direction for building general-purpose surrogate models. To systematically
investigate this capability, we introduce SURGE, a comprehensive benchmark with
$1160$ problems covering $8$ key aspects: multi-language programming tasks,
competition-level programming problems, repository-level code analysis,
high-cost scientific computing, time-complexity-intensive algorithms, buggy
code analysis, programs dependent on specific compilers or execution
environments, and formal mathematical proof verification. Through extensive
empirical analysis of $21$ open-source and proprietary LLMs, we examine scaling
laws, data efficiency, and predictive accuracy. Our findings reveal important
insights about the feasibility of LLMs as efficient surrogates for
computational processes, with implications for automated software testing,
program analysis, and computational resource optimization in data mining
applications. Code and dataset are released at
https://github.com/Imbernoulli/SURGE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Preference Optimization through Reward Model Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19316v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19316v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Fisch, Jacob Eisenstein, Vicky Zayats, Alekh Agarwal, Ahmad Beirami, Chirag Nagpal, Pete Shaw, Jonathan Berant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language model (LM) post-training (or alignment) involves maximizing a reward
function that is derived from preference annotations. Direct Preference
Optimization (DPO) is a popular offline alignment method that trains a policy
directly on preference data without the need to train a reward model or apply
reinforcement learning. However, the empirical evidence suggests that DPO
typically assigns implicit rewards that overfit, and trend towards infinite
magnitude. This frequently leads to degenerate policies, sometimes causing even
the probabilities of the preferred generations to go to zero. In this work, we
analyze this phenomenon and use distillation to get a better proxy for the true
preference distribution over generation pairs: we train the LM such that its
induced implicit reward, i.e., the scaled log-likelihood ratio of the model to
the reference model, matches an explicit reward model trained on the preference
data. Moreover, to account for uncertainty in the reward model we are
distilling from, we optimize against a family of reward models that, as a
whole, is likely to include at least one reasonable proxy for the preference
distribution. Our results show that distilling from such a family of reward
models leads to improved robustness to distribution shift in preference
annotations, while preserving the simple supervised nature of DPO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamics of Instruction Fine-Tuning for Chinese Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.19651v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.19651v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chiyu Song, Zhanchao Zhou, Jianhao Yan, Yuejiao Fei, Zhenzhong Lan, Yue Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning is a burgeoning method to elicit the general intelligence
of Large Language Models (LLMs). While numerous studies have examined the
impact of factors such as data volume and model size on English models, the
scaling properties of instruction tuning in other languages remain largely
unexplored. In this work, we systematically investigate the effects of data
quantity, model size, and data construction methods on instruction tuning for
Chinese LLMs. We utilize a newly curated dataset, DoIT, which includes over
40,000 high-quality instruction instances covering ten underlying abilities,
such as creative writing, code generation, and logical reasoning. Our
experiments, conducted on models ranging from 7b to 33b parameters, yield three
key findings: (i) While these factors directly affect overall model
performance, some abilities are more responsive to scaling, whereas others
demonstrate significant resistance. (ii) The scaling sensitivity of different
abilities to these factors can be explained by two features: Complexity and
Transference. (iii) By tailoring training strategies to their varying
sensitivities, specific abilities can be efficiently learned, enhancing
performance on two public benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Selected Languages are All You Need for Cross-lingual Truthfulness
  Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.14434v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.14434v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weihao Liu, Ning Wu, Wenbiao Ding, Shining Liang, Ming Gong, Dongmei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Truthfulness stands out as an essential challenge for Large Language Models
(LLMs). Although many works have developed various ways for truthfulness
enhancement, they seldom focus on truthfulness in multilingual scenarios.
Meanwhile, contemporary multilingual aligning technologies struggle to balance
numerous languages and often exhibit serious truthfulness gaps across different
languages, especially those that differ greatly from English. In our work, we
extend truthfulness evaluation to multilingual contexts and propose a practical
method for cross-lingual truthfulness transfer called Fact-aware Multilingual
Selective Synergy (FaMSS). FaMSS is able to select an optimal subset of all
tested languages by language bias and transfer contributions, and then employ
translation instruction tuning for cross-lingual truthfulness transfer.
Experimental results demonstrate that our approach can effectively reduce the
multilingual representation disparity and boost cross-lingual truthfulness
transfer of LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, COLING2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enabling Auditory Large Language Models for Automatic Speech Quality
  Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16644v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16644v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyin Wang, Wenyi Yu, Yudong Yang, Changli Tang, Yixuan Li, Jimin Zhuang, Xianzhao Chen, Xiaohai Tian, Jun Zhang, Guangzhi Sun, Lu Lu, Chao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech quality assessment typically requires evaluating audio from multiple
aspects, such as mean opinion score (MOS) and speaker similarity (SIM) \etc.,
which can be challenging to cover using one small model designed for a single
task. In this paper, we propose leveraging recently introduced auditory large
language models (LLMs) for automatic speech quality assessment. By employing
task-specific prompts, auditory LLMs are finetuned to predict MOS, SIM and A/B
testing results, which are commonly used for evaluating text-to-speech systems.
Additionally, the finetuned auditory LLM is able to generate natural language
descriptions assessing aspects like noisiness, distortion, discontinuity, and
overall quality, providing more interpretable outputs. Extensive experiments
have been performed on the NISQA, BVCC, SOMOS and VoxSim speech quality
datasets, using open-source auditory LLMs such as SALMONN, Qwen-Audio, and
Qwen2-Audio. For the natural language descriptions task, a commercial model
Google Gemini 1.5 Pro is also evaluated. The results demonstrate that auditory
LLMs achieve competitive performance compared to state-of-the-art task-specific
small models in predicting MOS and SIM, while also delivering promising results
in A/B testing and natural language descriptions. Our data processing scripts
and finetuned model checkpoints can be found at
https://github.com/bytedance/SALMONN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DailyDilemmas: Revealing Value Preferences of <span class="highlight-title">LLM</span>s with Quandaries of
  Daily Life 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02683v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02683v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Ying Chiu, Liwei Jiang, Yejin Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As users increasingly seek guidance from LLMs for decision-making in daily
life, many of these decisions are not clear-cut and depend significantly on the
personal values and ethical standards of people. We present DailyDilemmas, a
dataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma
presents two possible actions, along with affected parties and relevant human
values for each action. Based on these dilemmas, we gather a repository of
human values covering diverse everyday topics, such as interpersonal
relationships, workplace, and environmental issues. With DailyDilemmas, we
evaluate LLMs on these dilemmas to determine what action they will choose and
the values represented by these action choices. Then, we analyze values through
the lens of five theoretical frameworks inspired by sociology, psychology, and
philosophy, including the World Values Survey, Moral Foundations Theory,
Maslow's Hierarchy of Needs, Aristotle's Virtues, and Plutchik's Wheel of
Emotions. For instance, we find LLMs are most aligned with self-expression over
survival in World Values Survey and care over loyalty in Moral Foundations
Theory. Interestingly, we find substantial preference differences in models for
some core values. For example, for truthfulness, Mixtral-8x7B neglects it by
9.7% while GPT-4-turbo selects it by 9.4%. We also study the recent guidance
released by OpenAI (ModelSpec), and Anthropic (Constitutional AI) to understand
how their designated principles reflect their models' actual value
prioritization when facing nuanced moral reasoning in daily-life settings.
Finally, we find that end users cannot effectively steer such prioritization
using system prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted into ICLR 2025 (spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Test-Time Compute: from System-1 Thinking to System-2 Thinking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02497v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02497v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixin Ji, Juntao Li, Hai Ye, Kaixin Wu, Kai Yao, Jia Xu, Linjian Mo, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable performance of the o1 model in complex reasoning demonstrates
that test-time compute scaling can further unlock the model's potential,
enabling powerful System-2 thinking. However, there is still a lack of
comprehensive surveys for test-time compute scaling. We trace the concept of
test-time compute back to System-1 models. In System-1 models, test-time
compute addresses distribution shifts and improves robustness and
generalization through parameter updating, input modification, representation
editing, and output calibration. In System-2 models, it enhances the model's
reasoning ability to solve complex problems through repeated sampling,
self-correction, and tree search. We organize this survey according to the
trend of System-1 to System-2 thinking, highlighting the key role of test-time
compute in the transition from System-1 models to weak System-2 models, and
then to strong System-2 models. We also point out a few possible future
directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Align <span class="highlight-title">Multi</span>-Faceted Evaluation: A Unified and Robust
  Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18874v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18874v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaishuai Xu, Tiezheng Yu, Wenjun Hou, Yi Cheng, Liangyou Li, Xin Jiang, Lifeng Shang, Qun Liu, Wenjie Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are being used more and more extensively for
automated evaluation in various scenarios. Previous studies have attempted to
fine-tune open-source LLMs to replicate the evaluation explanations and
judgments of powerful proprietary models, such as GPT-4. However, these methods
are largely limited to text-based analyses under predefined general criteria,
resulting in reduced adaptability for unseen instructions and demonstrating
instability in evaluating adherence to quantitative and structural constraints.
To address these limitations, we propose a novel evaluation framework, ARJudge,
that adaptively formulates evaluation criteria and synthesizes both text-based
and code-driven analyses to evaluate LLM responses. ARJudge consists of two
components: a fine-tuned Analyzer that generates multi-faceted evaluation
analyses and a tuning-free Refiner that combines and refines all analyses to
make the final judgment. We construct a Composite Analysis Corpus that
integrates tasks for evaluation criteria generation alongside text-based and
code-driven analysis generation to train the Analyzer. Our results demonstrate
that ARJudge outperforms existing fine-tuned evaluators in effectiveness and
robustness. Furthermore, it demonstrates the importance of multi-faceted
evaluation and code-driven analyses in enhancing evaluation capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Subtle Errors Matter: Preference Learning via Error-injected
  Self-editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06638v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06638v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaishuai Xu, Tiezheng Yu, Wenjun Hou, Yi Cheng, Chak Tou Leong, Liangyou Li, Xin Jiang, Lifeng Shang, Qun Liu, Wenjie Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have exhibited strong mathematical reasoning
prowess, tackling tasks ranging from basic arithmetic to advanced
competition-level problems. However, frequently occurring subtle yet critical
errors, such as miscalculations or incorrect substitutions, limit the LLMs'
full potential. Existing studies to improve mathematical ability typically
involve applying preference learning to step-wise solution pairs. Although
these methods leverage samples of varying granularity to mitigate reasoning
errors, they overlook critical subtle errors. In this work, we propose a novel
preference learning framework called eRror-Injected Self-Editing (RISE), which
injects predefined subtle errors into pivotal tokens in reasoning or
computation steps to construct hard pairs for error mitigation. In detail, RISE
uses the LLM itself to edit a small number of tokens in the solution, injecting
designed subtle errors. Then, pairs composed of self-edited solutions and their
corresponding correct ones, along with pairs of correct and incorrect solutions
obtained through sampling, are used together for subtle error-aware DPO
training. Compared with other preference learning methods, RISE further refines
the training objective without requiring fine-grained sampling or preference
annotation. Extensive experiments validate the effectiveness of RISE, with
preference learning on Qwen2-7B-Instruct yielding notable improvements of 3.0%
on GSM8K and 7.9% on MATH with only 4.5K training samples. Moreover, the effect
of error mitigation extends from mathematical reasoning to logical reasoning
and code generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Efficient Recursive Numeral Systems via Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07170v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07170v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Silvi, Jonathan Thomas, Emil Carlsson, Devdatt Dubhashi, Moa Johansson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It has previously been shown that by using reinforcement learning (RL),
agents can derive simple approximate and exact-restricted numeral systems that
are similar to human ones (Carlsson, 2021). However, it is a major challenge to
show how more complex recursive numeral systems, similar to for example
English, could arise via a simple learning mechanism such as RL. Here, we
introduce an approach towards deriving a mechanistic explanation of the
emergence of efficient recursive number systems. We consider pairs of agents
learning how to communicate about numerical quantities through a meta-grammar
that can be gradually modified throughout the interactions. %We find that the
seminal meta-grammar of Hurford (Hurford, 1975) is not suitable for this
application as its optimization results in systems that deviate from standard
conventions observed within human numeral systems. We propose a simple
modification which addresses this issue. Utilising a slightly modified version
of the meta-grammar of Hurford, we demonstrate that our RL agents, shaped by
the pressures for efficient communication, can effectively modify their lexicon
towards Pareto-optimal configurations which are comparable to those observed
within human numeral systems in terms of their efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spike<span class="highlight-title">LLM</span>: Scaling up Spiking Neural Network to Large Language Models via
  Saliency-based Spiking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04752v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04752v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingrun Xing, Boyan Gao, Zheng Zhang, David A. Clifton, Shitao Xiao, Li Du, Guoqi Li, Jiajun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) with billions of
parameters have improved performance in various applications, but their
inference processes demand significant energy and computational resources. In
contrast, the human brain, with approximately 86 billion neurons, is much more
energy-efficient than LLMs with similar parameters. Inspired by this, we
redesign 7$\sim$70 billion parameter LLMs using bio-plausible spiking
mechanisms, emulating the efficient behavior of the human brain. We propose the
first spiking large language model, SpikeLLM. Coupled with the proposed model,
two essential approaches are proposed to improve spike training efficiency:
Generalized Integrate-and-Fire (GIF) neurons to compress spike length from $T$
to $\frac{T}{L} \log_2 L$ bits, and an Optimal Brain Spiking framework to
divide outlier channels and allocate different $T$ for GIF neurons, which
further compresses spike length to approximate $log_2T$ bits. The necessity of
spike-driven LLM is proved by comparison with quantized LLMs with similar
operations. In the OmniQuant pipeline, SpikeLLM reduces 11.01% WikiText2
perplexity and improves 2.55% accuracy of common scene reasoning on a LLAMA-7B
W4A4 model. In the GPTQ pipeline, SpikeLLM achieves direct additive in linear
layers, significantly exceeding PB-LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SynGhost: Invisible and Universal Task-agnostic Backdoor Attack via
  Syntactic Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18945v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18945v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengzhou Cheng, Wei Du, Zongru Wu, Fengwei Zhang, Libo Chen, Zhuosheng Zhang, Gongshen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although pre-training achieves remarkable performance, it suffers from
task-agnostic backdoor attacks due to vulnerabilities in data and training
mechanisms. These attacks can transfer backdoors to various downstream tasks.
In this paper, we introduce $\mathtt{maxEntropy}$, an entropy-based poisoning
filter that mitigates such risks. To overcome the limitations of manual target
setting and explicit triggers, we propose $\mathtt{SynGhost}$, an invisible and
universal task-agnostic backdoor attack via syntactic transfer, further
exposing vulnerabilities in pre-trained language models (PLMs). Specifically,
$\mathtt{SynGhost}$ injects multiple syntactic backdoors into the pre-training
space through corpus poisoning, while preserving the PLM's pre-training
capabilities. Second, $\mathtt{SynGhost}$ adaptively selects optimal targets
based on contrastive learning, creating a uniform distribution in the
pre-training space. To identify syntactic differences, we also introduce an
awareness module to minimize interference between backdoors. Experiments show
that $\mathtt{SynGhost}$ poses significant threats and can transfer to various
downstream tasks. Furthermore, $\mathtt{SynGhost}$ resists defenses based on
perplexity, fine-pruning, and $\mathtt{maxEntropy}$. The code is available at
https://github.com/Zhou-CyberSecurity-AI/SynGhost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 16 figures, 12 tables, accepted at NAACL 2025 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Rise and Down of Babel Tower: Investigating the Evolution Process of
  <span class="highlight-title">Multi</span>lingual Code Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07298v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07298v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiawei Chen, Wentao Chen, Jing Su, Jingjing Xu, Hongyu Lin, Mengjie Ren, Yaojie Lu, Xianpei Han, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown significant multilingual
capabilities. However, the mechanisms underlying the development of these
capabilities during pre-training are not well understood. In this paper, we use
code LLMs as an experimental platform to explore the evolution of multilingual
capabilities in LLMs during the pre-training process. Based on our
observations, we propose the Babel Tower Hypothesis, which describes the entire
process of LLMs acquiring new language capabilities. During the learning
process, multiple languages initially share a single knowledge system dominated
by the primary language and gradually develop language-specific knowledge
systems. We then validate the above hypothesis by tracking the internal states
of the LLMs through identifying working languages and language transferring
neurons. Experimental results show that the internal state changes of the LLM
are consistent with our Babel Tower Hypothesis. Building on these insights, we
propose a novel method to construct an optimized pre-training corpus for
multilingual code LLMs, which significantly outperforms LLMs trained on the
original corpus. The proposed Babel Tower Hypothesis provides new insights into
designing pre-training data distributions to achieve optimal multilingual
capabilities in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Order Matters: Investigate the Position Bias in <span class="highlight-title">Multi</span>-constraint
  Instruction Following 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17204v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17204v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Zeng, Qianyu He, Qingyu Ren, Jiaqing Liang, Yanghua Xiao, Weikang Zhou, Zeye Sun, Fei Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world instructions with multiple constraints pose a significant
challenge to existing large language models (LLMs). An observation is that the
LLMs exhibit dramatic performance fluctuation when disturbing the order of the
incorporated constraints. Yet, none of the existing works has systematically
investigated this position bias problem in the field of multi-constraint
instruction following. To bridge this gap, we design a probing task where we
quantitatively measure the difficulty distribution of the constraints by a
novel Difficulty Distribution Index (CDDI). Through the experimental results,
we find that LLMs are more performant when presented with the constraints in a
``hard-to-easy'' order. This preference can be generalized to LLMs with
different architecture or different sizes of parameters. Additionally, we
conduct an explanation study, providing an intuitive insight into the
correlation between the LLM's attention and constraint orders. Our code and
dataset are publicly available at https://github.com/meowpass/PBIF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representation Engineering: A Top-Down Approach to AI Transparency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01405v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01405v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, J. Zico Kolter, Dan Hendrycks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we identify and characterize the emerging area of
representation engineering (RepE), an approach to enhancing the transparency of
AI systems that draws on insights from cognitive neuroscience. RepE places
population-level representations, rather than neurons or circuits, at the
center of analysis, equipping us with novel methods for monitoring and
manipulating high-level cognitive phenomena in deep neural networks (DNNs). We
provide baselines and an initial analysis of RepE techniques, showing that they
offer simple yet effective solutions for improving our understanding and
control of large language models. We showcase how these methods can provide
traction on a wide range of safety-relevant problems, including honesty,
harmlessness, power-seeking, and more, demonstrating the promise of top-down
transparency research. We hope that this work catalyzes further exploration of
RepE and fosters advancements in the transparency and safety of AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at
  https://github.com/andyzoujm/representation-engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TokenSelect: Efficient Long-Context Inference and Length Extrapolation
  for <span class="highlight-title">LLM</span>s via Dynamic Token-Level KV Cache Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02886v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02886v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Wu, Zhuoshi Pan, Chao Wang, Liyi Chen, Yunchu Bai, Tianfu Wang, Kun Fu, Zheng Wang, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Large Language Models (LLMs) has driven growing
demand for processing extended context sequences in contemporary applications.
However, this progress faces two major challenges: performance degradation due
to sequence lengths out-of-distribution, and excessively long inference times
caused by the quadratic computational complexity of attention. These issues
hinder the application of LLMs in long-context scenarios. In this paper, we
propose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free
method for efficient and accurate long-context inference. TokenSelect builds
upon the observation of non-contiguous attention sparsity, using Query-Key dot
products to measure per-head KV Cache criticality at token-level. By per-head
soft voting mechanism, TokenSelect selectively involves a few critical KV cache
tokens in attention calculation without sacrificing accuracy. To further
accelerate TokenSelect, we design the Selection Cache based on observations of
consecutive Query similarity and implemented efficient dot product kernel,
significantly reducing the overhead. A comprehensive evaluation of TokenSelect
demonstrates up to 23.84x speedup in attention computation and up to 2.28x
acceleration in end-to-end latency, while providing superior performance
compared to state-of-the-art long-context inference methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Conversational Agents with Theory of Mind: Aligning Beliefs,
  Desires, and Intentions for Human-Like Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14171v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14171v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehdi Jafari, Devin Yuncheng Hua, Hao Xue, Flora Salim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language interaction with agentic Artificial Intelligence (AI),
driven by Large Language Models (LLMs), is expected to remain a dominant
paradigm in the near future. While humans instinctively align their
communication with mental states -- an ability known as Theory of Mind (ToM),
current LLM powered systems exhibit significant limitations in this regard.
This study examines the extent to which open source language models (LLaMA) can
capture and preserve ToM related information and how effectively it contributes
to consistent ToM reasoning in generated responses. We further investigate
whether explicit manipulation of ToM related components, such as beliefs,
desires, and intentions, can enhance response alignment. Experiments on two
LLaMA 3 variants demonstrate that incorporating ToM informed alignment improves
response quality, achieving win rates of 67 and 63 percent for the 3B and 8B
models, respectively. These findings highlight the potential of ToM driven
strategies to improve alignment in LLM based conversational agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structural-Entropy-Based Sample Selection for Efficient and Effective
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02268v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02268v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianchi Xie, Jiangning Zhu, Guozu Ma, Minzhi Lin, Wei Chen, Weikai Yang, Shixia Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sample selection improves the efficiency and effectiveness of machine
learning models by providing informative and representative samples. Typically,
samples can be modeled as a sample graph, where nodes are samples and edges
represent their similarities. Most existing methods are based on local
information, such as the training difficulty of samples, thereby overlooking
global information, such as connectivity patterns. This oversight can result in
suboptimal selection because global information is crucial for ensuring that
the selected samples well represent the structural properties of the graph. To
address this issue, we employ structural entropy to quantify global information
and losslessly decompose it from the whole graph to individual nodes using the
Shapley value. Based on the decomposition, we present
$\textbf{S}$tructural-$\textbf{E}$ntropy-based sample $\textbf{S}$election
($\textbf{SES}$), a method that integrates both global and local information to
select informative and representative samples. SES begins by constructing a
$k$NN-graph among samples based on their similarities. It then measures sample
importance by combining structural entropy (global metric) with training
difficulty (local metric). Finally, SES applies importance-biased blue noise
sampling to select a set of diverse and representative samples. Comprehensive
experiments on three learning scenarios -- supervised learning, active
learning, and continual learning -- clearly demonstrate the effectiveness of
our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spontaneous Giving and Calculated Greed in Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17720v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17720v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Li, Hirokazu Shirado
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models demonstrate advanced problem-solving capabilities by
incorporating reasoning techniques such as chain of thought and reflection.
However, how these reasoning capabilities extend to social intelligence remains
unclear. In this study, we investigate this question using economic games that
model social dilemmas, where social intelligence plays a crucial role. First,
we examine the effects of chain-of-thought and reflection techniques in a
public goods game. We then extend our analysis to six economic games on
cooperation and punishment, comparing off-the-shelf non-reasoning and reasoning
models. We find that reasoning models significantly reduce cooperation and norm
enforcement, prioritizing individual rationality. Consequently, groups with
more reasoning models exhibit less cooperation and lower gains through repeated
interactions. These behaviors parallel human tendencies of "spontaneous giving
and calculated greed." Our results suggest the need for AI architectures that
incorporate social intelligence alongside reasoning capabilities to ensure that
AI supports, rather than disrupts, human cooperative intuition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Representational Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09906v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09906v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, Douwe Kiela
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  All text-based language problems can be reduced to either generation or
embedding. Current models only perform well at one or the other. We introduce
generative representational instruction tuning (GRIT) whereby a large language
model is trained to handle both generative and embedding tasks by
distinguishing between them through instructions. Compared to other open
models, our resulting GritLM 7B sets a new state of the art on the Massive Text
Embedding Benchmark (MTEB) and outperforms all models up to its size on a range
of generative tasks. By scaling up further, GritLM 8x7B outperforms all open
generative language models that we tried while still being among the best
embedding models. Notably, we find that GRIT matches training on only
generative or embedding data, thus we can unify both at no performance loss.
Among other benefits, the unification via GRIT speeds up Retrieval-Augmented
Generation (RAG) by > 60% for long documents, by no longer requiring separate
retrieval and generation models. Models, code, etc. are freely available at
https://github.com/ContextualAI/gritlm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>67 pages (16 main), 25 figures, 34 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perturbation-Restrained Sequential Model Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16821v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16821v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun-Yu Ma, Hong Wang, Hao-Xiang Xu, Zhen-Hua Ling, Jia-Chen Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model editing is an emerging field that focuses on updating the knowledge
embedded within large language models (LLMs) without extensive retraining.
However, current model editing methods significantly compromise the general
abilities of LLMs as the number of edits increases, and this trade-off poses a
substantial challenge to the continual learning of LLMs. In this paper, we
first theoretically analyze that the factor affecting the general abilities in
sequential model editing lies in the condition number of the edited matrix. The
condition number of a matrix represents its numerical sensitivity, and
therefore can be used to indicate the extent to which the original knowledge
associations stored in LLMs are perturbed after editing. Subsequently,
statistical findings demonstrate that the value of this factor becomes larger
as the number of edits increases, thereby exacerbating the deterioration of
general abilities. To this end, a framework termed Perturbation Restraint on
Upper bouNd for Editing (PRUNE) is proposed, which applies the condition number
restraints in sequential editing. These restraints can lower the upper bound on
perturbation to edited models, thus preserving the general abilities.
Systematically, we conduct experiments employing three editing methods on three
LLMs across four downstream tasks. The results show that PRUNE can preserve
general abilities while maintaining the editing performance effectively in
sequential model editing. The code are available at
https://github.com/mjy1111/PRUNE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A-MEM: Agentic Memory for <span class="highlight-title">LLM</span> Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12110v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12110v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wujiang Xu, Zujie Liang, Kai Mei, Hang Gao, Juntao Tan, Yongfeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language model (LLM) agents can effectively use external tools
for complex real-world tasks, they require memory systems to leverage
historical experiences. Current memory systems enable basic storage and
retrieval but lack sophisticated memory organization, despite recent attempts
to incorporate graph databases. Moreover, these systems' fixed operations and
structures limit their adaptability across diverse tasks. To address this
limitation, this paper proposes a novel agentic memory system for LLM agents
that can dynamically organize memories in an agentic way. Following the basic
principles of the Zettelkasten method, we designed our memory system to create
interconnected knowledge networks through dynamic indexing and linking. When a
new memory is added, we generate a comprehensive note containing multiple
structured attributes, including contextual descriptions, keywords, and tags.
The system then analyzes historical memories to identify relevant connections,
establishing links where meaningful similarities exist. Additionally, this
process enables memory evolution - as new memories are integrated, they can
trigger updates to the contextual representations and attributes of existing
historical memories, allowing the memory network to continuously refine its
understanding. Our approach combines the structured organization principles of
Zettelkasten with the flexibility of agent-driven decision making, allowing for
more adaptive and context-aware memory management. Empirical experiments on six
foundation models show superior improvement against existing SOTA baselines.
The source code is available at https://github.com/WujiangXu/AgenticMemory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QUAD-<span class="highlight-title">LLM</span>-MLTC: Large Language Models Ensemble Learning for Healthcare
  Text <span class="highlight-title">Multi</span>-Label Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14189v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14189v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hajar Sakai, Sarah S. Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The escalating volume of collected healthcare textual data presents a unique
challenge for automated Multi-Label Text Classification (MLTC), which is
primarily due to the scarcity of annotated texts for training and their nuanced
nature. Traditional machine learning models often fail to fully capture the
array of expressed topics. However, Large Language Models (LLMs) have
demonstrated remarkable effectiveness across numerous Natural Language
Processing (NLP) tasks in various domains, which show impressive computational
efficiency and suitability for unsupervised learning through prompt
engineering. Consequently, these LLMs promise an effective MLTC of medical
narratives. However, when dealing with various labels, different prompts can be
relevant depending on the topic. To address these challenges, the proposed
approach, QUAD-LLM-MLTC, leverages the strengths of four LLMs: GPT-4o, BERT,
PEGASUS, and BART. QUAD-LLM-MLTC operates in a sequential pipeline in which
BERT extracts key tokens, PEGASUS augments textual data, GPT-4o classifies, and
BART provides topics' assignment probabilities, which results in four
classifications, all in a 0-shot setting. The outputs are then combined using
ensemble learning and processed through a meta-classifier to produce the final
MLTC result. The approach is evaluated using three samples of annotated texts,
which contrast it with traditional and single-model methods. The results show
significant improvements across the majority of the topics in the
classification's F1 score and consistency (F1 and Micro-F1 scores of 78.17% and
80.16% with standard deviations of 0.025 and 0.011, respectively). This
research advances MLTC using LLMs and provides an efficient and scalable
solution to rapidly categorize healthcare-related text data without further
training.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Iterative Nash Policy Optimization: Aligning <span class="highlight-title">LLM</span>s with General
  Preferences via No-Regret Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00617v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00617v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuheng Zhang, Dian Yu, Baolin Peng, Linfeng Song, Ye Tian, Mingyue Huo, Nan Jiang, Haitao Mi, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning with Human Feedback (RLHF) has achieved great success
in aligning large language models (LLMs) with human preferences. Prevalent RLHF
approaches are reward-based, following the Bradley-Terry (BT) model assumption,
which may not fully capture the complexity of human preferences. In this paper,
we explore RLHF under a general preference framework and approach it from a
game-theoretic perspective. Specifically, we formulate the problem as a
two-player game and propose a novel online algorithm, iterative Nash policy
optimization (INPO). The key idea is to let the policy play against itself via
no-regret learning, thereby approximating the Nash policy. Unlike previous
methods, INPO bypasses the need for estimating the expected win rate for
individual responses, which typically incurs high computational or annotation
costs. Instead, we introduce a new loss objective that is directly minimized
over a preference dataset. We provide theoretical analysis for our approach and
demonstrate its effectiveness through experiments on various representative
benchmarks. With an LLaMA-3-8B-based SFT model, INPO achieves a 42.6%
length-controlled win rate on AlpacaEval 2.0 and a 37.8% win rate on
Arena-Hard, showing substantial improvement over the state-of-the-art online
RLHF algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Vision-Language-Action Models for Embodied AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14093v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14093v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueen Ma, Zixing Song, Yuzheng Zhuang, Jianye Hao, Irwin King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embodied AI is widely recognized as a key element of artificial general
intelligence because it involves controlling embodied agents to perform tasks
in the physical world. Building on the success of large language models and
vision-language models, a new category of multimodal models -- referred to as
vision-language-action models (VLAs) -- has emerged to address
language-conditioned robotic tasks in embodied AI by leveraging their distinct
ability to generate actions. In recent years, a myriad of VLAs have been
developed, making it imperative to capture the rapidly evolving landscape
through a comprehensive survey. To this end, we present the first survey on
VLAs for embodied AI. This work provides a detailed taxonomy of VLAs, organized
into three major lines of research. The first line focuses on individual
components of VLAs. The second line is dedicated to developing control policies
adept at predicting low-level actions. The third line comprises high-level task
planners capable of decomposing long-horizon tasks into a sequence of subtasks,
thereby guiding VLAs to follow more general user instructions. Furthermore, we
provide an extensive summary of relevant resources, including datasets,
simulators, and benchmarks. Finally, we discuss the challenges faced by VLAs
and outline promising future directions in embodied AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, a survey of vision-language-action models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Generalization and Adaptation Ability of Machine-Generated Text
  Detectors in Academic Writing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.17242v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.17242v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yule Liu, Zhiyuan Zhong, Yifan Liao, Zhen Sun, Jingyi Zheng, Jiaheng Wei, Qingyuan Gong, Fenghua Tong, Yang Chen, Yang Zhang, Xinlei He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rising popularity of large language models (LLMs) has raised concerns
about machine-generated text (MGT), particularly in academic settings, where
issues like plagiarism and misinformation are prevalent. As a result,
developing a highly generalizable and adaptable MGT detection system has become
an urgent priority. Given that LLMs are most commonly misused in academic
writing, this work investigates the generalization and adaptation capabilities
of MGT detectors in three key aspects specific to academic writing: First, we
construct MGT-Acedemic, a large-scale dataset comprising over 336M tokens and
749K samples. MGT-Acedemic focuses on academic writing, featuring human-written
texts (HWTs) and MGTs across STEM, Humanities, and Social Sciences, paired with
an extensible code framework for efficient benchmarking. Second, we benchmark
the performance of various detectors for binary classification and attribution
tasks in both in-domain and cross-domain settings. This benchmark reveals the
often-overlooked challenges of attribution tasks. Third, we introduce a novel
attribution task where models have to adapt to new classes over time without
(or with very limited) access to prior training data in both few-shot and
many-shot scenarios. We implement eight different adapting techniques to
improve the performance and highlight the inherent complexity of the task. Our
findings provide insights into the generalization and adaptation ability of MGT
detectors across diverse scenarios and lay the foundation for building robust,
adaptive detection systems. The code framework is available at
https://github.com/Y-L-LIU/MGTBench-2.0.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMed-RAG: Versatile <span class="highlight-title">Multi</span>modal RAG System for Medical Vision Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13085v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13085v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Xia, Kangyu Zhu, Haoran Li, Tianze Wang, Weijia Shi, Sheng Wang, Linjun Zhang, James Zou, Huaxiu Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence (AI) has demonstrated significant potential in
healthcare, particularly in disease diagnosis and treatment planning. Recent
progress in Medical Large Vision-Language Models (Med-LVLMs) has opened up new
possibilities for interactive diagnostic tools. However, these models often
suffer from factual hallucination, which can lead to incorrect diagnoses.
Fine-tuning and retrieval-augmented generation (RAG) have emerged as methods to
address these issues. However, the amount of high-quality data and distribution
shifts between training data and deployment data limit the application of
fine-tuning methods. Although RAG is lightweight and effective, existing
RAG-based approaches are not sufficiently general to different medical domains
and can potentially cause misalignment issues, both between modalities and
between the model and the ground truth. In this paper, we propose a versatile
multimodal RAG system, MMed-RAG, designed to enhance the factuality of
Med-LVLMs. Our approach introduces a domain-aware retrieval mechanism, an
adaptive retrieved contexts selection method, and a provable RAG-based
preference fine-tuning strategy. These innovations make the RAG process
sufficiently general and reliable, significantly improving alignment when
introducing retrieved contexts. Experimental results across five medical
datasets (involving radiology, ophthalmology, pathology) on medical VQA and
report generation demonstrate that MMed-RAG can achieve an average improvement
of 43.8% in the factual accuracy of Med-LVLMs. Our data and code are available
in https://github.com/richard-peng-xia/MMed-RAG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HORAE: A Domain-Agnostic Modeling Language for Automating <span class="highlight-title">Multi</span>modal
  Service Regulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06600v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06600v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutao Sun, Mingshuai Chen, Kangjia Zhao, Jintao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence is rapidly encroaching on the field of service
regulation. This work-in-progress article presents the design principles behind
HORAE, a unified specification language to model multimodal regulation rules
across a diverse set of domains. We show how HORAE facilitates an intelligent
service regulation pipeline by further exploiting a fine-tuned large language
model named HORAE that automates the HORAE modeling process, thereby yielding
an end-to-end framework for fully automated intelligent service regulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Controllable Context Sensitivity and the Knob Behind It 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07404v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07404v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Minder, Kevin Du, Niklas Stoehr, Giovanni Monea, Chris Wendler, Robert West, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When making predictions, a language model must trade off how much it relies
on its context vs. its prior knowledge. Choosing how sensitive the model is to
its context is a fundamental functionality, as it enables the model to excel at
tasks like retrieval-augmented generation and question-answering. In this
paper, we search for a knob which controls this sensitivity, determining
whether language models answer from the context or their prior knowledge. To
guide this search, we design a task for controllable context sensitivity. In
this task, we first feed the model a context (Paris is in England) and a
question (Where is Paris?); we then instruct the model to either use its prior
or contextual knowledge and evaluate whether it generates the correct answer
for both intents (either France or England). When fine-tuned on this task,
instruction-tuned versions of Llama-3.1, Mistral-v0.3, and Gemma-2 can solve it
with high accuracy (85-95%). Analyzing these high-performing models, we narrow
down which layers may be important to context sensitivity using a novel linear
time algorithm. Then, in each model, we identify a 1-D subspace in a single
layer that encodes whether the model follows context or prior knowledge.
Interestingly, while we identify this subspace in a fine-tuned model, we find
that the exact same subspace serves as an effective knob in not only that model
but also non-fine-tuned instruct and base models of that model family. Finally,
we show a strong correlation between a model's performance and how distinctly
it separates context-agreeing from context-ignoring answers in this subspace.
These results suggest a single subspace facilitates how the model chooses
between context and prior knowledge, hinting at a simple fundamental mechanism
that controls this behavior.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Pilot Empirical Study on When and How to Use Knowledge Graphs as
  Retrieval Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20854v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20854v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xujie Yuan, Yongxu Liu, Shimin Di, Shiwen Wu, Libin Zheng, Rui Meng, Lei Chen, Xiaofang Zhou, Jian Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of Knowledge Graphs (KGs) into the Retrieval Augmented
Generation (RAG) framework has attracted significant interest, with early
studies showing promise in mitigating hallucinations and improving model
accuracy. However, a systematic understanding and comparative analysis of the
rapidly emerging KG-RAG methods are still lacking. This paper seeks to lay the
foundation for systematically answering the question of when and how to use
KG-RAG by analyzing their performance in various application scenarios
associated with different technical configurations. After outlining the mind
map using KG-RAG framework and summarizing its popular pipeline, we conduct a
pilot empirical study of KG-RAG works to reimplement and evaluate 6 KG-RAG
methods across 7 datasets in diverse scenarios, analyzing the impact of 9
KG-RAG configurations in combination with 17 LLMs. Our results underscore the
critical role of appropriate application conditions and optimal configurations
of KG-RAG components.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 2 figures, 14 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Closer Look at Machine Unlearning for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08109v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08109v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaojian Yuan, Tianyu Pang, Chao Du, Kejiang Chen, Weiming Zhang, Min Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) may memorize sensitive or copyrighted content,
raising privacy and legal concerns. Due to the high cost of retraining from
scratch, researchers attempt to employ machine unlearning to remove specific
content from LLMs while preserving the overall performance. In this paper, we
discuss several issues in machine unlearning for LLMs and provide our insights
on possible approaches. To address the issue of inadequate evaluation of model
outputs after unlearning, we introduce three additional metrics to evaluate
token diversity, sentence semantics, and factual correctness. We then
categorize unlearning methods into untargeted and targeted, and discuss their
issues respectively. Specifically, the behavior that untargeted unlearning
attempts to approximate is unpredictable and may involve hallucinations, and
existing regularization is insufficient for targeted unlearning. To alleviate
these issues, we propose using the objective of maximizing entropy (ME) for
untargeted unlearning and incorporate answer preservation (AP) loss as
regularization for targeted unlearning. Experimental results across three
scenarios, i.e., fictitious unlearning, continual unlearning, and real-world
unlearning, demonstrate the effectiveness of our approaches. The code is
available at https://github.com/sail-sg/closer-look-LLM-unlearning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Adversarial Robustness in Classification tasks using DNA
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19788v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19788v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunwoo Yoo, Haebin Shin, Kaidi Xu, Gail Rosen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DNA Language Models, such as GROVER, DNABERT2 and the Nucleotide Transformer,
operate on DNA sequences that inherently contain sequencing errors, mutations,
and laboratory-induced noise, which may significantly impact model performance.
Despite the importance of this issue, the robustness of DNA language models
remains largely underexplored. In this paper, we comprehensivly investigate
their robustness in DNA classification by applying various adversarial attack
strategies: the character (nucleotide substitutions), word (codon
modifications), and sentence levels (back-translation-based transformations) to
systematically analyze model vulnerabilities. Our results demonstrate that DNA
language models are highly susceptible to adversarial attacks, leading to
significant performance degradation. Furthermore, we explore adversarial
training method as a defense mechanism, which enhances both robustness and
classification accuracy. This study highlights the limitations of DNA language
models and underscores the necessity of robustness in bioinformatics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Facilitating <span class="highlight-title">Multi</span>-turn Function Calling for <span class="highlight-title">LLM</span>s via Compositional
  Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12952v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12952v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyang Chen, Haoze Sun, Tianpeng Li, Fan Yang, Hao Liang, Keer Lu, Bin Cui, Wentao Zhang, Zenan Zhou, Weipeng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have exhibited significant potential in
performing diverse tasks, including the ability to call functions or use
external tools to enhance their performance. While current research on function
calling by LLMs primarily focuses on single-turn interactions, this paper
addresses the overlooked necessity for LLMs to engage in multi-turn function
calling--critical for handling compositional, real-world queries that require
planning with functions but not only use functions. To facilitate this, we
introduce an approach, BUTTON, which generates synthetic compositional
instruction tuning data via bottom-up instruction construction and top-down
trajectory generation. In the bottom-up phase, we generate simple atomic tasks
based on real-world scenarios and build compositional tasks using heuristic
strategies based on atomic tasks. Corresponding function definitions are then
synthesized for these compositional tasks. The top-down phase features a
multi-agent environment where interactions among simulated humans, assistants,
and tools are utilized to gather multi-turn function calling trajectories. This
approach ensures task compositionality and allows for effective function and
trajectory generation by examining atomic tasks within compositional tasks. We
produce a dataset BUTTONInstruct comprising 8k data points and demonstrate its
effectiveness through extensive experiments across various LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AdEval: Alignment-based Dynamic Evaluation to Mitigate Data
  Contamination in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.13983v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.13983v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Large Language Models (LLMs) are pretrained on massive-scale corpora, the
issue of data contamination has become increasingly severe, leading to
potential overestimation of model performance during evaluation. To address
this, we propose AdEval (Alignment-based Dynamic Evaluation), a dynamic data
evaluation method aimed at mitigating the impact of data contamination on
evaluation reliability. Experimental results on multiple datasets demonstrate
that AdEval effectively reduces the impact of data contamination on evaluation
outcomes, enhancing both the fairness and reliability of the evaluation
process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>There are serious academic problems in this paper, such as data
  falsification and plagiarism in the method of the paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OLMoE: Open Mixture-of-Experts Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02060v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02060v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers, Douwe Kiela, Ali Farhadi, Noah A. Smith, Pang Wei Koh, Amanpreet Singh, Hannaneh Hajishirzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce OLMoE, a fully open, state-of-the-art language model leveraging
sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but
uses only 1B per input token. We pretrain it on 5 trillion tokens and further
adapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available
models with similar active parameters, even surpassing larger ones like
Llama2-13B-Chat and DeepSeekMoE-16B. We present various experiments on MoE
training, analyze routing in our model showing high specialization, and
open-source all aspects of our work: model weights, training data, code, and
logs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>63 pages (24 main), 36 figures, 17 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Labyrinth of Links: Navigating the Associative Maze of <span class="highlight-title">Multi</span>-modal
  <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01417v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01417v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong Li, Nanxi Li, Yuanjie Chen, Jianbin Zhu, Qinlu Guo, Cewu Lu, Yong-Lu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal Large Language Models (MLLMs) have exhibited impressive
capability. However, recently many deficiencies of MLLMs have been found
compared to human intelligence, $\textit{e.g.}$, hallucination. To drive the
MLLMs study, the community dedicated efforts to building larger benchmarks with
complex tasks. In this paper, we propose benchmarking an essential but usually
overlooked intelligence: $\textbf{association}$, a human's basic capability to
link observation and prior practice memory. To comprehensively investigate
MLLM's performance on the association, we formulate the association task and
devise a standard benchmark based on adjective and verb semantic concepts.
Instead of costly data annotation and curation, we propose a convenient
$\textbf{annotation-free}$ construction method transforming the general dataset
for our association tasks. Simultaneously, we devise a rigorous data refinement
process to eliminate confusion in the raw dataset. Building on this database,
we establish three levels of association tasks: single-step, synchronous, and
asynchronous associations. Moreover, we conduct a comprehensive investigation
into the MLLMs' zero-shot association capabilities, addressing multiple
dimensions, including three distinct memory strategies, both open-source and
closed-source MLLMs, cutting-edge Mixture-of-Experts (MoE) models, and the
involvement of human experts. Our systematic investigation shows that current
open-source MLLMs consistently exhibit poor capability in our association
tasks, even the currently state-of-the-art GPT-4V(vision) also has a
significant gap compared to humans. We believe our benchmark would pave the way
for future MLLM studies. $\textit{Our data and code are available at:}$
https://mvig-rhos.com/llm_inception.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025. Project page:
  https://mvig-rhos.com/llm_inception</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NL2FOL: Translating Natural Language to First-Order Logic for Logical
  Fallacy Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.02318v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.02318v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Lalwani, Tasha Kim, Lovish Chopra, Christopher Hahn, Zhijing Jin, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Translating natural language into formal language such as First-Order Logic
(FOL) is a foundational challenge in NLP with wide-ranging applications in
automated reasoning, misinformation tracking, and knowledge validation. In this
paper, we introduce Natural Language to First-Order Logic (NL2FOL), a framework
to autoformalize natural language to FOL step by step using Large Language
Models (LLMs). Our approach addresses key challenges in this translation
process, including the integration of implicit background knowledge. By
leveraging structured representations generated by NL2FOL, we use
Satisfiability Modulo Theory (SMT) solvers to reason about the logical validity
of natural language statements. We present logical fallacy detection as a case
study to evaluate the efficacy of NL2FOL. Being neurosymbolic, our approach
also provides interpretable insights into the reasoning process and
demonstrates robustness without requiring model fine-tuning or labeled training
data. Our framework achieves strong performance on multiple datasets. On the
LOGIC dataset, NL2FOL achieves an F1-score of 78%, while generalizing
effectively to the LOGICCLIMATE dataset with an F1-score of 80%.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">70</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ModeDreamer: Mode Guiding Score Distillation for Text-to-3D Generation
  using Reference Image Prompts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18135v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18135v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uy Dieu Tran, Minh Luu, Phong Ha Nguyen, Khoi Nguyen, Binh-Son Hua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing Score Distillation Sampling (SDS)-based methods have driven
significant progress in text-to-3D generation. However, 3D models produced by
SDS-based methods tend to exhibit over-smoothing and low-quality outputs. These
issues arise from the mode-seeking behavior of current methods, where the
scores used to update the model oscillate between multiple modes, resulting in
unstable optimization and diminished output quality. To address this problem,
we introduce a novel image prompt score distillation loss named ISD, which
employs a reference image to direct text-to-3D optimization toward a specific
mode. Our ISD loss can be implemented by using IP-Adapter, a lightweight
adapter for integrating image prompt capability to a text-to-image diffusion
model, as a mode-selection module. A variant of this adapter, when not being
prompted by a reference image, can serve as an efficient control variate to
reduce variance in score estimates, thereby enhancing both output quality and
optimization stability. Our experiments demonstrate that the ISD loss
consistently achieves visually coherent, high-quality outputs and improves
optimization speed compared to prior text-to-3D methods, as demonstrated
through both qualitative and quantitative evaluations on the T3Bench benchmark
suite.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://modedreamer.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FoodM<span class="highlight-title">LLM</span>-JP: Leveraging <span class="highlight-title">Multi</span>modal Large Language Models for Japanese
  Recipe Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18459v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18459v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuki Imajuku, Yoko Yamakata, Kiyoharu Aizawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on food image understanding using recipe data has been a
long-standing focus due to the diversity and complexity of the data. Moreover,
food is inextricably linked to people's lives, making it a vital research area
for practical applications such as dietary management. Recent advancements in
Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities, not only in their vast knowledge but also in their ability to
handle languages naturally. While English is predominantly used, they can also
support multiple languages including Japanese. This suggests that MLLMs are
expected to significantly improve performance in food image understanding
tasks. We fine-tuned open MLLMs LLaVA-1.5 and Phi-3 Vision on a Japanese recipe
dataset and benchmarked their performance against the closed model GPT-4o. We
then evaluated the content of generated recipes, including ingredients and
cooking procedures, using 5,000 evaluation samples that comprehensively cover
Japanese food culture. Our evaluation demonstrates that the open models trained
on recipe data outperform GPT-4o, the current state-of-the-art model, in
ingredient generation. Our model achieved F1 score of 0.531, surpassing
GPT-4o's F1 score of 0.481, indicating a higher level of accuracy. Furthermore,
our model exhibited comparable performance to GPT-4o in generating cooking
procedure text.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 5 figures. We found errors in the calculation of evaluation
  metrics, which were corrected in this version with
  $\color{blue}{\text{modifications highlighted in blue}}$. Please also see the
  Appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Supervised Iterative Refinement for Anomaly Detection in Industrial
  Quality Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11561v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11561v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Aqeel, Shakiba Sharifi, Marco Cristani, Francesco Setti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces the Iterative Refinement Process (IRP), a robust
anomaly detection methodology designed for high-stakes industrial quality
control. The IRP enhances defect detection accuracy through a cyclic data
refinement strategy, iteratively removing misleading data points to improve
model performance and robustness. We validate the IRP's effectiveness using two
benchmark datasets, Kolektor SDD2 (KSDD2) and MVTec AD, covering a wide range
of industrial products and defect types. Our experimental results demonstrate
that the IRP consistently outperforms traditional anomaly detection models,
particularly in environments with high noise levels. This study highlights the
IRP's potential to significantly enhance anomaly detection processes in
industrial settings, effectively managing the challenges of sparse and noisy
data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to VISAPP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StarVid: Enhancing Semantic Alignment in Video Diffusion Models via
  Spatial and SynTactic Guided Attention Refocusing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15259v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15259v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanhang Li, Qi Mao, Lan Chen, Zhen Fang, Lei Tian, Xinyan Xiao, Libiao Jin, Hua Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in text-to-video (T2V) generation with diffusion models have
garnered significant attention. However, they typically perform well in scenes
with a single object and motion, struggling in compositional scenarios with
multiple objects and distinct motions to accurately reflect the semantic
content of text prompts. To address these challenges, we propose
\textbf{StarVid}, a plug-and-play, training-free method that improves semantic
alignment between multiple subjects, their motions, and text prompts in T2V
models. StarVid first leverages the spatial reasoning capabilities of large
language models (LLMs) for two-stage motion trajectory planning based on text
prompts. Such trajectories serve as spatial priors, guiding a spatial-aware
loss to refocus cross-attention (CA) maps into distinctive regions.
Furthermore, we propose a syntax-guided contrastive constraint to strengthen
the correlation between the CA maps of verbs and their corresponding nouns,
enhancing motion-subject binding. Both qualitative and quantitative evaluations
demonstrate that the proposed framework significantly outperforms baseline
methods, delivering videos of higher quality with improved semantic
consistency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating Hallucinations in Large Vision-Language Models via DPO:
  On-Policy Data Hold the Key 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09695v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09695v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihe Yang, Xufang Luo, Dongqi Han, Yunjian Xu, Dongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hallucination remains a major challenge for Large Vision-Language Models
(LVLMs). Direct Preference Optimization (DPO) has gained increasing attention
as a simple solution to hallucination issues. It directly learns from
constructed preference pairs that reflect the severity of hallucinations in
responses to the same prompt and image. Nonetheless, different data
construction methods in existing works bring notable performance variations. We
identify a crucial factor here: outcomes are largely contingent on whether the
constructed data aligns on-policy w.r.t the initial (reference) policy of DPO.
Theoretical analysis suggests that learning from off-policy data is impeded by
the presence of KL-divergence between the updated policy and the reference
policy. From the perspective of dataset distribution, we systematically
summarize the inherent flaws in existing algorithms that employ DPO to address
hallucination issues. To alleviate the problems, we propose On-Policy Alignment
(OPA)-DPO framework, which uniquely leverages expert feedback to correct
hallucinated responses and aligns both the original and expert-revised
responses in an on-policy manner. Notably, with only 4.8k data, OPA-DPO
achieves an additional reduction in the hallucination rate of LLaVA-1.5-7B:
13.26% on the AMBER benchmark and 5.39% on the Object-Hal benchmark, compared
to the previous SOTA algorithm trained with 16k samples. Our implementation is
available at https://github.com/zhyang2226/OPA-DPO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Physically Realizable Adversarial Attacks in Embodied Vision
  Navigation <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10071v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10071v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Chen, Jiawei Tu, Chao Qi, Yonghao Dang, Feng Zhou, Wei Wei, Jianqin Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The significant advancements in embodied vision navigation have raised
concerns about its susceptibility to adversarial attacks exploiting deep neural
networks. Investigating the adversarial robustness of embodied vision
navigation is crucial, especially given the threat of 3D physical attacks that
could pose risks to human safety. However, existing attack methods for embodied
vision navigation often lack physical feasibility due to challenges in
transferring digital perturbations into the physical world. Moreover, current
physical attacks for object detection struggle to achieve both multi-view
effectiveness and visual naturalness in navigation scenarios. To address this,
we propose a practical attack method for embodied navigation by attaching
adversarial patches to objects, where both opacity and textures are learnable.
Specifically, to ensure effectiveness across varying viewpoints, we employ a
multi-view optimization strategy based on object-aware sampling, which
optimizes the patch's texture based on feedback from the vision-based
perception model used in navigation. To make the patch inconspicuous to human
observers, we introduce a two-stage opacity optimization mechanism, in which
opacity is fine-tuned after texture optimization. Experimental results
demonstrate that our adversarial patches decrease the navigation success rate
by an average of 22.39%, outperforming previous methods in practicality,
effectiveness, and naturalness. Code is available at:
https://github.com/chen37058/Physical-Attacks-in-Embodied-Nav
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures, submitted to IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stereo Hand-Object Reconstruction for Human-to-Robot Handover 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07487v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07487v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yik Lung Pang, Alessio Xompero, Changjae Oh, Andrea Cavallaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Jointly estimating hand and object shape facilitates the grasping task in
human-to-robot handovers. However, relying on hand-crafted prior knowledge
about the geometric structure of the object fails when generalising to unseen
objects, and depth sensors fail to detect transparent objects such as drinking
glasses. In this work, we propose a stereo-based method for hand-object
reconstruction that combines single-view reconstructions probabilistically to
form a coherent stereo reconstruction. We learn 3D shape priors from a large
synthetic hand-object dataset to ensure that our method is generalisable, and
use RGB inputs to better capture transparent objects. We show that our method
reduces the object Chamfer distance compared to existing RGB based hand-object
reconstruction methods on single view and stereo settings. We process the
reconstructed hand-object shape with a projection-based outlier removal step
and use the output to guide a human-to-robot handover pipeline with
wide-baseline stereo RGB cameras. Our hand-object reconstruction enables a
robot to successfully receive a diverse range of household objects from the
human.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EchoONE: Segmenting <span class="highlight-title">Multi</span>ple echocardiography Planes in One Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.02993v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.02993v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiongtong Hu, Wei Zhuo, Jun Cheng, Yingying Liu, Wufeng Xue, Dong Ni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In clinical practice of echocardiography examinations, multiple planes
containing the heart structures of different view are usually required in
screening, diagnosis and treatment of cardiac disease. AI models for
echocardiography have to be tailored for each specific plane due to the
dramatic structure differences, thus resulting in repetition development and
extra complexity. Effective solution for such a multi-plane segmentation (MPS)
problem is highly demanded for medical images, yet has not been well
investigated. In this paper, we propose a novel solution, EchoONE, for this
problem with a SAM-based segmentation architecture, a prior-composable mask
learning (PC-Mask) module for semantic-aware dense prompt generation, and a
learnable CNN-branch with a simple yet effective local feature fusion and
adaption (LFFA) module for SAM adapting. We extensively evaluated our method on
multiple internal and external echocardiography datasets, and achieved
consistently state-of-the-art performance for multi-source datasets with
different heart planes. This is the first time that the MPS problem is solved
in one model for echocardiography data. The code will be available at
https://github.com/a2502503/EchoONE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Intelligence via Trial and Error 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18858v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18858v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingtao Zhan, Jiahao Zhao, Jiayu Li, Yiqun Liu, Bo Zhang, Qingyao Ai, Jiaxin Mao, Hongning Wang, Min Zhang, Shaoping Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intelligence is a crucial trait for species to find solutions within a
limited number of trial-and-error attempts. Building on this idea, we introduce
Survival Game as a framework to evaluate intelligence based on the number of
failed attempts in a trial-and-error process. Fewer failures indicate higher
intelligence. When the expectation and variance of failure counts are both
finite, it signals the ability to consistently find solutions to new
challenges, which we define as the Autonomous Level of intelligence. Using
Survival Game, we comprehensively evaluate existing AI systems. Our results
show that while AI systems achieve the Autonomous Level in simple tasks, they
are still far from it in more complex tasks, such as vision, search,
recommendation, and language. While scaling current AI technologies might help,
this would come at an astronomical cost. Projections suggest that achieving the
Autonomous Level for general tasks would require $10^{26}$ parameters. To put
this into perspective, loading such a massive model requires so many H100 GPUs
that their total value is $10^{7}$ times that of Apple Inc.'s market value.
Even with Moore's Law, supporting such a parameter scale would take $70$ years.
This staggering cost highlights the complexity of human tasks and the
inadequacies of current AI technologies. To further investigate this
phenomenon, we conduct a theoretical analysis of Survival Game and its
experimental results. Our findings suggest that human tasks possess a
criticality property. As a result, Autonomous Level requires a deep
understanding of the task's underlying mechanisms. Current AI systems, however,
do not fully grasp these mechanisms and instead rely on superficial mimicry,
making it difficult for them to reach an autonomous level. We believe Survival
Game can not only guide the future development of AI but also offer profound
insights into human intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MATCH POLICY: A Simple Pipeline from Point Cloud Registration to
  Manipulation Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15517v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15517v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haojie Huang, Haotian Liu, Dian Wang, Robin Walters, Robert Platt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many manipulation tasks require the robot to rearrange objects relative to
one another. Such tasks can be described as a sequence of relative poses
between parts of a set of rigid bodies. In this work, we propose MATCH POLICY,
a simple but novel pipeline for solving high-precision pick and place tasks.
Instead of predicting actions directly, our method registers the pick and place
targets to the stored demonstrations. This transfers action inference into a
point cloud registration task and enables us to realize nontrivial manipulation
policies without any training. MATCH POLICY is designed to solve high-precision
tasks with a key-frame setting. By leveraging the geometric interaction and the
symmetries of the task, it achieves extremely high sample efficiency and
generalizability to unseen configurations. We demonstrate its state-of-the-art
performance across various tasks on RLBench benchmark compared with several
strong baselines and test it on a real robot with six tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project url: https://haojhuang.github.io/match_page/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Annotation-Free Curb Detection Leveraging Altitude Difference Image 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.20171v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.20171v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fulong Ma, Peng Hou, Yuxuan Liu, Yang Liu, Ming Liu, Jun Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Road curbs are considered as one of the crucial and ubiquitous traffic
features, which are essential for ensuring the safety of autonomous vehicles.
Current methods for detecting curbs primarily rely on camera imagery or LiDAR
point clouds. Image-based methods are vulnerable to fluctuations in lighting
conditions and exhibit poor robustness, while methods based on point clouds
circumvent the issues associated with lighting variations. However, it is the
typical case that significant processing delays are encountered due to the
voluminous amount of 3D points contained in each frame of the point cloud data.
Furthermore, the inherently unstructured characteristics of point clouds poses
challenges for integrating the latest deep learning advancements into point
cloud data applications. To address these issues, this work proposes an
annotation-free curb detection method leveraging Altitude Difference Image
(ADI), which effectively mitigates the aforementioned challenges. Given that
methods based on deep learning generally demand extensive, manually annotated
datasets, which are both expensive and labor-intensive to create, we present an
Automatic Curb Annotator (ACA) module. This module utilizes a deterministic
curb detection algorithm to automatically generate a vast quantity of training
data. Consequently, it facilitates the training of the curb detection model
without necessitating any manual annotation of data. Finally, by incorporating
a post-processing module, we manage to achieve state-of-the-art results on the
KITTI 3D curb dataset with considerably reduced processing delays compared to
existing methods, which underscores the effectiveness of our approach in curb
detection tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Text-driven Adaptation of Foundation Models for Few-shot Surgical
  Workflow Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.09555v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.09555v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tingxuan Chen, Kun Yuan, Vinkle Srivastav, Nassir Navab, Nicolas Padoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: Surgical workflow analysis is crucial for improving surgical
efficiency and safety. However, previous studies rely heavily on large-scale
annotated datasets, posing challenges in cost, scalability, and reliance on
expert annotations. To address this, we propose Surg-FTDA (Few-shot Text-driven
Adaptation), designed to handle various surgical workflow analysis tasks with
minimal paired image-label data.
  Methods: Our approach has two key components. First, Few-shot selection-based
modality alignment selects a small subset of images and aligns their embeddings
with text embeddings from the downstream task, bridging the modality gap.
Second, Text-driven adaptation leverages only text data to train a decoder,
eliminating the need for paired image-text data. This decoder is then applied
to aligned image embeddings, enabling image-related tasks without explicit
image-text pairs.
  Results: We evaluate our approach to generative tasks (image captioning) and
discriminative tasks (triplet recognition and phase recognition). Results show
that Surg-FTDA outperforms baselines and generalizes well across downstream
tasks.
  Conclusion: We propose a text-driven adaptation approach that mitigates the
modality gap and handles multiple downstream tasks in surgical workflow
analysis, with minimal reliance on large annotated datasets. The code and
dataset will be released in https://github.com/CAMMA-public/Surg-FTDA
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NavRAG: Generating User Demand Instructions for Embodied Navigation
  through Retrieval-Augmented <span class="highlight-title">LLM</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11142v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11142v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Wang, Yaohui Zhu, Gim Hee Lee, Yachun Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-and-Language Navigation (VLN) is an essential skill for embodied
agents, allowing them to navigate in 3D environments following natural language
instructions. High-performance navigation models require a large amount of
training data, the high cost of manually annotating data has seriously hindered
this field. Therefore, some previous methods translate trajectory videos into
step-by-step instructions for expanding data, but such instructions do not
match well with users' communication styles that briefly describe destinations
or state specific needs. Moreover, local navigation trajectories overlook
global context and high-level task planning. To address these issues, we
propose NavRAG, a retrieval-augmented generation (RAG) framework that generates
user demand instructions for VLN. NavRAG leverages LLM to build a hierarchical
scene description tree for 3D scene understanding from global layout to local
details, then simulates various user roles with specific demands to retrieve
from the scene tree, generating diverse instructions with LLM. We annotate over
2 million navigation instructions across 861 scenes and evaluate the data
quality and navigation performance of trained models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Monocular Depth Estimation and Segmentation for Transparent Object with
  Iterative Semantic and Geometric Fusion <span class="chip">ICRA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14616v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14616v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiangyuan Liu, Hongxuan Ma, Yuxin Guo, Yuhao Zhao, Chi Zhang, Wei Sui, Wei Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transparent object perception is indispensable for numerous robotic tasks.
However, accurately segmenting and estimating the depth of transparent objects
remain challenging due to complex optical properties. Existing methods
primarily delve into only one task using extra inputs or specialized sensors,
neglecting the valuable interactions among tasks and the subsequent refinement
process, leading to suboptimal and blurry predictions. To address these issues,
we propose a monocular framework, which is the first to excel in both
segmentation and depth estimation of transparent objects, with only a
single-image input. Specifically, we devise a novel semantic and geometric
fusion module, effectively integrating the multi-scale information between
tasks. In addition, drawing inspiration from human perception of objects, we
further incorporate an iterative strategy, which progressively refines initial
features for clearer results. Experiments on two challenging synthetic and
real-world datasets demonstrate that our model surpasses state-of-the-art
monocular, stereo, and multi-view methods by a large margin of about
38.8%-46.2% with only a single RGB input. Codes and models are publicly
available at https://github.com/L-J-Yuan/MODEST.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA(2025). The code is accessible through:
  https://github.com/L-J-Yuan/MODEST</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HiLo: A Learning Framework for Generalized Category Discovery Robust to
  Domain Shifts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04591v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04591v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongjun Wang, Sagar Vaze, Kai Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalized Category Discovery (GCD) is a challenging task in which, given a
partially labelled dataset, models must categorize all unlabelled instances,
regardless of whether they come from labelled categories or from new ones. In
this paper, we challenge a remaining assumption in this task: that all images
share the same domain. Specifically, we introduce a new task and method to
handle GCD when the unlabelled data also contains images from different domains
to the labelled set. Our proposed `HiLo' networks extract High-level semantic
and Low-level domain features, before minimizing the mutual information between
the representations. Our intuition is that the clusterings based on domain
information and semantic information should be independent. We further extend
our method with a specialized domain augmentation tailored for the GCD task, as
well as a curriculum learning approach. Finally, we construct a benchmark from
corrupted fine-grained datasets as well as a large-scale evaluation on
DomainNet with real-world domain shifts, reimplementing a number of GCD
baselines in this setting. We demonstrate that HiLo outperforms SoTA category
discovery models by a large margin on all evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2: Accepted as a conference paper at ICLR 2025; Project page:
  https://github.com/Visual-AI/hilo/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ct<span class="highlight-title">rL</span>oRA: An Extensible and Efficient Framework for Controllable Image
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09400v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09400v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifeng Xu, Zhenliang He, Shiguang Shan, Xilin Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, large-scale diffusion models have made impressive progress in
text-to-image (T2I) generation. To further equip these T2I models with
fine-grained spatial control, approaches like ControlNet introduce an extra
network that learns to follow a condition image. However, for every single
condition type, ControlNet requires independent training on millions of data
pairs with hundreds of GPU hours, which is quite expensive and makes it
challenging for ordinary users to explore and develop new types of conditions.
To address this problem, we propose the CtrLoRA framework, which trains a Base
ControlNet to learn the common knowledge of image-to-image generation from
multiple base conditions, along with condition-specific LoRAs to capture
distinct characteristics of each condition. Utilizing our pretrained Base
ControlNet, users can easily adapt it to new conditions, requiring as few as
1,000 data pairs and less than one hour of single-GPU training to obtain
satisfactory results in most scenarios. Moreover, our CtrLoRA reduces the
learnable parameters by 90% compared to ControlNet, significantly lowering the
threshold to distribute and deploy the model weights. Extensive experiments on
various types of conditions demonstrate the efficiency and effectiveness of our
method. Codes and model weights will be released at
https://github.com/xyfJASON/ctrlora.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025. Code: https://github.com/xyfJASON/ctrlora</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Learning With Sine-Activated Low-rank Matrices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19243v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19243v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiping Ji, Hemanth Saratchandran, Cameron Gordon, Zeyu Zhang, Simon Lucey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-rank decomposition has emerged as a vital tool for enhancing parameter
efficiency in neural network architectures, gaining traction across diverse
applications in machine learning. These techniques significantly lower the
number of parameters, striking a balance between compactness and performance.
However, a common challenge has been the compromise between parameter
efficiency and the accuracy of the model, where reduced parameters often lead
to diminished accuracy compared to their full-rank counterparts. In this work,
we propose a novel theoretical framework that integrates a sinusoidal function
within the low-rank decomposition process. This approach not only preserves the
benefits of the parameter efficiency characteristic of low-rank methods but
also increases the decomposition's rank, thereby enhancing model performance.
Our method proves to be a plug in enhancement for existing low-rank models, as
evidenced by its successful application in Vision Transformers (ViT), Large
Language Models (LLMs), Neural Radiance Fields (NeRF) and 3D shape modelling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally. Paper accepted at ICLR
  2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Poison-splat: Computation Cost Attack on 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08190v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08190v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Lu, Yifan Zhang, Qiuhong Shen, Xinchao Wang, Shuicheng Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian splatting (3DGS), known for its groundbreaking performance and
efficiency, has become a dominant 3D representation and brought progress to
many 3D vision tasks. However, in this work, we reveal a significant security
vulnerability that has been largely overlooked in 3DGS: the computation cost of
training 3DGS could be maliciously tampered by poisoning the input data. By
developing an attack named Poison-splat, we reveal a novel attack surface where
the adversary can poison the input images to drastically increase the
computation memory and time needed for 3DGS training, pushing the algorithm
towards its worst computation complexity. In extreme cases, the attack can even
consume all allocable memory, leading to a Denial-of-Service (DoS) that
disrupts servers, resulting in practical damages to real-world 3DGS service
vendors. Such a computation cost attack is achieved by addressing a bi-level
optimization problem through three tailored strategies: attack objective
approximation, proxy model rendering, and optional constrained optimization.
These strategies not only ensure the effectiveness of our attack but also make
it difficult to defend with simple defensive measures. We hope the revelation
of this novel attack surface can spark attention to this crucial yet overlooked
vulnerability of 3DGS systems. Our code is available at
https://github.com/jiahaolu97/poison-splat .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025 as a spotlight paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FLARE: Feed-forward Geometry, Appearance and Camera Estimation from
  Uncalibrated Sparse Views 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12138v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12138v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangzhan Zhang, Jianyuan Wang, Yinghao Xu, Nan Xue, Christian Rupprecht, Xiaowei Zhou, Yujun Shen, Gordon Wetzstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present FLARE, a feed-forward model designed to infer high-quality camera
poses and 3D geometry from uncalibrated sparse-view images (i.e., as few as 2-8
inputs), which is a challenging yet practical setting in real-world
applications. Our solution features a cascaded learning paradigm with camera
pose serving as the critical bridge, recognizing its essential role in mapping
3D structures onto 2D image planes. Concretely, FLARE starts with camera pose
estimation, whose results condition the subsequent learning of geometric
structure and appearance, optimized through the objectives of geometry
reconstruction and novel-view synthesis. Utilizing large-scale public datasets
for training, our method delivers state-of-the-art performance in the tasks of
pose estimation, geometry reconstruction, and novel view synthesis, while
maintaining the inference efficiency (i.e., less than 0.5 seconds). The project
page and code can be found at: https://zhanghe3z.github.io/FLARE/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025. Website: https://zhanghe3z.github.io/FLARE/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Decade's Battle on <span class="highlight-title">Dataset</span> Bias: Are We There Yet? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08632v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08632v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuang Liu, Kaiming He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We revisit the "dataset classification" experiment suggested by Torralba &
Efros (2011) a decade ago, in the new era with large-scale, diverse, and
hopefully less biased datasets as well as more capable neural network
architectures. Surprisingly, we observe that modern neural networks can achieve
excellent accuracy in classifying which dataset an image is from: e.g., we
report 84.7% accuracy on held-out validation data for the three-way
classification problem consisting of the YFCC, CC, and DataComp datasets. Our
further experiments show that such a dataset classifier could learn semantic
features that are generalizable and transferable, which cannot be explained by
memorization. We hope our discovery will inspire the community to rethink
issues involving dataset bias.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ICLR 2025 (Oral Presentation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Brain Apoptosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17941v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17941v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyuan Sun, Zheng Fang, Jiaxu Wang, Junjie Jiang, Delei Kong, Chenming Hu, Yuetong Fang, Renjing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing complexity and parameter count of Convolutional Neural
Networks (CNNs) and Transformers pose challenges in terms of computational
efficiency and resource demands. Pruning has been identified as an effective
strategy to address these challenges by removing redundant elements such as
neurons, channels, or connections, thereby enhancing computational efficiency
without heavily compromising performance. This paper builds on the foundational
work of Optimal Brain Damage (OBD) by advancing the methodology of parameter
importance estimation using the Hessian matrix. Unlike previous approaches that
rely on approximations, we introduce Optimal Brain Apoptosis (OBA), a novel
pruning method that calculates the Hessian-vector product value directly for
each parameter. By decomposing the Hessian matrix across network layers and
identifying conditions under which inter-layer Hessian submatrices are
non-zero, we propose a highly efficient technique for computing the
second-order Taylor expansion of parameters. This approach allows for a more
precise pruning process, particularly in the context of CNNs and Transformers,
as validated in our experiments including VGG19, ResNet32, ResNet50, and
ViT-B/16 on CIFAR10, CIFAR100 and Imagenet datasets. Our code is available at
https://github.com/NEU-REAL/OBA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring the Effectiveness of Object-Centric Representations in Visual
  Question Answering: Comparative Insights with Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15589v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15589v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Mohammad Karimi Mamaghan, Samuele Papa, Karl Henrik Johansson, Stefan Bauer, Andrea Dittadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object-centric (OC) representations, which model visual scenes as
compositions of discrete objects, have the potential to be used in various
downstream tasks to achieve systematic compositional generalization and
facilitate reasoning. However, these claims have yet to be thoroughly validated
empirically. Recently, foundation models have demonstrated unparalleled
capabilities across diverse domains, from language to computer vision,
positioning them as a potential cornerstone of future research for a wide range
of computational tasks. In this paper, we conduct an extensive empirical study
on representation learning for downstream Visual Question Answering (VQA),
which requires an accurate compositional understanding of the scene. We
thoroughly investigate the benefits and trade-offs of OC models and alternative
approaches including large pre-trained foundation models on both synthetic and
real-world data, ultimately identifying a promising path to leverage the
strengths of both paradigms. The extensiveness of our study, encompassing over
600 downstream VQA models and 15 different types of upstream representations,
also provides several additional insights that we believe will be of interest
to the community at large.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MIGE: A Unified Framework for <span class="highlight-title">Multi</span>modal Instruction-Based Image
  Generation and Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21291v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21291v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueyun Tian, Wei Li, Bingbing Xu, Yige Yuan, Yuanzhuo Wang, Huawei Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant progress in diffusion-based image generation,
subject-driven generation and instruction-based editing remain challenging.
Existing methods typically treat them separately, struggling with limited
high-quality data and poor generalization. However, both tasks require
capturing complex visual variations while maintaining consistency between
inputs and outputs. Therefore, we propose MIGE, a unified framework that
standardizes task representations using multimodal instructions. It treats
subject-driven generation as creation on a blank canvas and instruction-based
editing as modification of an existing image, establishing a shared
input-output formulation. MIGE introduces a novel multimodal encoder that maps
free-form multimodal instructions into a unified vision-language space,
integrating visual and semantic features through a feature fusion mechanism.
This unification enables joint training of both tasks, providing two key
advantages: (1) Cross-Task Enhancement: By leveraging shared visual and
semantic representations, joint training improves instruction adherence and
visual consistency in both subject-driven generation and instruction-based
editing. (2) Generalization: Learning in a unified format facilitates
cross-task knowledge transfer, enabling MIGE to generalize to novel
compositional tasks, including instruction-based subject-driven editing.
Experiments show that MIGE excels in both subject-driven generation and
instruction-based editing while setting a state-of-the-art in the new task of
instruction-based subject-driven editing. Code and model have been publicly
available at https://github.com/Eureka-Maggie/MIGE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Prompt: Unlocking the Power of Visual Prompt Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18936v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18936v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh Le, Anh Nguyen, Huy Nguyen, Chau Nguyen, Nhat Ho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Prompt Tuning (VPT) has recently emerged as a powerful method for
adapting pre-trained vision models to downstream tasks. By introducing
learnable prompt tokens as task-specific instructions, VPT effectively guides
pre-trained transformer models with minimal overhead. Despite its empirical
success, a comprehensive theoretical understanding of VPT remains an active
area of research. Building on recent insights into the connection between
mixture of experts and prompt-based approaches, we identify a key limitation in
VPT: the restricted functional expressiveness in prompt formulation. To address
this limitation, we propose Visual Adaptive Prompt Tuning (VAPT), a new
generation of prompts that redefines prompts as adaptive functions of the
input. Our theoretical analysis shows that this simple yet intuitive approach
achieves optimal sample efficiency. Empirical results on VTAB-1K and FGVC
further demonstrate VAPT's effectiveness, with performance gains of 7.34% and
1.04% over fully fine-tuning baselines, respectively. Notably, VAPT also
surpasses VPT by a substantial margin while using fewer parameters. These
results highlight both the effectiveness and efficiency of our method and pave
the way for future research to explore the potential of adaptive prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>57 pages, 10 figures, 18 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PnP-Flow: Plug-and-Play Image Restoration with Flow Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02423v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02423v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ségolène Martin, Anne Gagneux, Paul Hagemann, Gabriele Steidl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce Plug-and-Play (PnP) Flow Matching, an algorithm
for solving imaging inverse problems. PnP methods leverage the strength of
pre-trained denoisers, often deep neural networks, by integrating them in
optimization schemes. While they achieve state-of-the-art performance on
various inverse problems in imaging, PnP approaches face inherent limitations
on more generative tasks like inpainting. On the other hand, generative models
such as Flow Matching pushed the boundary in image sampling yet lack a clear
method for efficient use in image restoration. We propose to combine the PnP
framework with Flow Matching (FM) by defining a time-dependent denoiser using a
pre-trained FM model. Our algorithm alternates between gradient descent steps
on the data-fidelity term, reprojections onto the learned FM path, and
denoising. Notably, our method is computationally efficient and
memory-friendly, as it avoids backpropagation through ODEs and trace
computations. We evaluate its performance on denoising, super-resolution,
deblurring, and inpainting tasks, demonstrating superior results compared to
existing PnP algorithms and Flow Matching based state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Meta Curvature-Aware Minimization for Domain Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11542v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11542v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Chen, Yiwen Ye, Feilong Tang, Yongsheng Pan, Yong Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain generalization (DG) aims to enhance the ability of models trained on
source domains to generalize effectively to unseen domains. Recently,
Sharpness-Aware Minimization (SAM) has shown promise in this area by reducing
the sharpness of the loss landscape to obtain more generalized models. However,
SAM and its variants sometimes fail to guide the model toward a flat minimum,
and their training processes exhibit limitations, hindering further
improvements in model generalization. In this paper, we first propose an
improved model training process aimed at encouraging the model to converge to a
flat minima. To achieve this, we design a curvature metric that has a minimal
effect when the model is far from convergence but becomes increasingly
influential in indicating the curvature of the minima as the model approaches a
local minimum. Then we derive a novel algorithm from this metric, called Meta
Curvature-Aware Minimization (MeCAM), to minimize the curvature around the
local minima. Specifically, the optimization objective of MeCAM simultaneously
minimizes the regular training loss, the surrogate gap of SAM, and the
surrogate gap of meta-learning. We provide theoretical analysis on MeCAM's
generalization error and convergence rate, and demonstrate its superiority over
existing DG methods through extensive experiments on five benchmark DG
datasets, including PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet. Code
will be available on GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 5 figures, 17 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Training One-Step Diffusion Models Without Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08005v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08005v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingtian Zhang, Jiajun He, Wenlin Chen, Zijing Ou, José Miguel Hernández-Lobato, Bernhard Schölkopf, David Barber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in one-step generative models typically follow a two-stage
process: first training a teacher diffusion model and then distilling it into a
one-step student model. This distillation process traditionally relies on both
the teacher model's score function to compute the distillation loss and its
weights for student initialization. In this paper, we explore whether one-step
generative models can be trained directly without this distillation process.
First, we show that the teacher's score function is not essential and propose a
family of distillation methods that achieve competitive results without relying
on score estimation. Next, we demonstrate that initialization from teacher
weights is indispensable in successful training. Surprisingly, we find that
this benefit is not due to improved ``input-output" mapping but rather the
learned feature representations, which dominate distillation quality. Our
findings provide a better understanding of the role of initialization in
one-step model training and its impact on distillation quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Foundation Models -- A Panacea for Artificial Intelligence in Pathology? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21264v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21264v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nita Mulliqi, Anders Blilie, Xiaoyi Ji, Kelvin Szolnoky, Henrik Olsson, Sol Erika Boman, Matteo Titus, Geraldine Martinez Gonzalez, Julia Anna Mielcarz, Masi Valkonen, Einar Gudlaugsson, Svein R. Kjosavik, José Asenjo, Marcello Gambacorta, Paolo Libretti, Marcin Braun, Radzislaw Kordek, Roman Łowicki, Kristina Hotakainen, Päivi Väre, Bodil Ginnerup Pedersen, Karina Dalsgaard Sørensen, Benedicte Parm Ulhøi, Pekka Ruusuvuori, Brett Delahunt, Hemamali Samaratunga, Toyonori Tsuzuki, Emilius A. M. Janssen, Lars Egevad, Martin Eklund, Kimmo Kartasalo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The role of artificial intelligence (AI) in pathology has evolved from aiding
diagnostics to uncovering predictive morphological patterns in whole slide
images (WSIs). Recently, foundation models (FMs) leveraging self-supervised
pre-training have been widely advocated as a universal solution for diverse
downstream tasks. However, open questions remain about their clinical
applicability and generalization advantages over end-to-end learning using
task-specific (TS) models. Here, we focused on AI with clinical-grade
performance for prostate cancer diagnosis and Gleason grading. We present the
largest validation of AI for this task, using over 100,000 core needle biopsies
from 7,342 patients across 15 sites in 11 countries. We compared two FMs with a
fully end-to-end TS model in a multiple instance learning framework. Our
findings challenge assumptions that FMs universally outperform TS models. While
FMs demonstrated utility in data-scarce scenarios, their performance converged
with - and was in some cases surpassed by - TS models when sufficient labeled
training data were available. Notably, extensive task-specific training
markedly reduced clinically significant misgrading, misdiagnosis of challenging
morphologies, and variability across different WSI scanners. Additionally, FMs
used up to 35 times more energy than the TS model, raising concerns about their
sustainability. Our results underscore that while FMs offer clear advantages
for rapid prototyping and research, their role as a universal solution for
clinically applicable medical AI remains uncertain. For high-stakes clinical
applications, rigorous validation and consideration of task-specific training
remain critically important. We advocate for integrating the strengths of FMs
and end-to-end learning to achieve robust and resource-efficient AI pathology
solutions fit for clinical use.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>50 pages, 15 figures and an appendix (study protocol) which is
  previously published, see https://doi.org/10.1101/2024.07.04.24309948;
  updated authors list format</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The PanAf-FGBG <span class="highlight-title">Dataset</span>: Understanding the Impact of Backgrounds in
  Wildlife Behaviour Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21201v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21201v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Otto Brookes, Maksim Kukushkin, Majid Mirmehdi, Colleen Stephens, Paula Dieguez, Thurston C. Hicks, Sorrel Jones, Kevin Lee, Maureen S. McCarthy, Amelia Meier, Emmanuelle Normand, Erin G. Wessling, Roman M. Wittig, Kevin Langergraber, Klaus Zuberbühler, Lukas Boesch, Thomas Schmid, Mimi Arandjelovic, Hjalmar Kühl, Tilo Burghardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer vision analysis of camera trap video footage is essential for
wildlife conservation, as captured behaviours offer some of the earliest
indicators of changes in population health. Recently, several high-impact
animal behaviour datasets and methods have been introduced to encourage their
use; however, the role of behaviour-correlated background information and its
significant effect on out-of-distribution generalisation remain unexplored. In
response, we present the PanAf-FGBG dataset, featuring 20 hours of wild
chimpanzee behaviours, recorded at over 350 individual camera locations.
Uniquely, it pairs every video with a chimpanzee (referred to as a foreground
video) with a corresponding background video (with no chimpanzee) from the same
camera location. We present two views of the dataset: one with overlapping
camera locations and one with disjoint locations. This setup enables, for the
first time, direct evaluation of in-distribution and out-of-distribution
conditions, and for the impact of backgrounds on behaviour recognition models
to be quantified. All clips come with rich behavioural annotations and metadata
including unique camera IDs and detailed textual scene descriptions.
Additionally, we establish several baselines and present a highly effective
latent-space normalisation technique that boosts out-of-distribution
performance by +5.42% mAP for convolutional and +3.75% mAP for
transformer-based models. Finally, we provide an in-depth analysis on the role
of backgrounds in out-of-distribution behaviour recognition, including the so
far unexplored impact of background durations (i.e., the count of background
frames within foreground videos).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the IEEE / CVF Computer Vision and Pattern Recognition
  Conference 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TRACE: Temporal Grounding Video <span class="highlight-title">LLM</span> via Causal Event Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05643v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05643v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongxin Guo, Jingyu Liu, Mingda Li, Qingbin Liu, Xi Chen, Xiaoying Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video Temporal Grounding (VTG) is a crucial capability for video
understanding models and plays a vital role in downstream tasks such as video
browsing and editing. To effectively handle various tasks simultaneously and
enable zero-shot prediction, there is a growing trend in employing video LLMs
for VTG tasks. However, current video LLM-based methods rely exclusively on
natural language generation, lacking the ability to model the clear structure
inherent in videos, which restricts their effectiveness in tackling VTG tasks.
To address this issue, this paper first formally introduces causal event
modeling framework, which represents video LLM outputs as sequences of events,
and predict the current event using previous events, video inputs, and textural
instructions. Each event consists of three components: timestamps, salient
scores, and textual captions. We then propose a novel task-interleaved video
LLM called TRACE to effectively implement the causal event modeling framework
in practice. The TRACE process visual frames, timestamps, salient scores, and
text as distinct tasks, employing various encoders and decoding heads for each.
Task tokens are arranged in an interleaved sequence according to the causal
event modeling framework's formulation. Extensive experiments on various VTG
tasks and datasets demonstrate the superior performance of TRACE compared to
state-of-the-art video LLMs. Our model and code are available at
https://github.com/gyxxyg/TRACE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Slowing Down Forgetting in Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06916v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06916v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pascal Janetzky, Tobias Schlagenhauf, Stefan Feuerriegel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A common challenge in continual learning (CL) is catastrophic forgetting,
where the performance on old tasks drops after new, additional tasks are
learned. In this paper, we propose a novel framework called ReCL to slow down
forgetting in CL. Our framework exploits an implicit bias of gradient-based
neural networks due to which these converge to margin maximization points. Such
convergence points allow us to reconstruct old data from previous tasks, which
we then combine with the current training data. Our framework is flexible and
can be applied on top of existing, state-of-the-art CL methods. We further
demonstrate the performance gain from our framework across a large series of
experiments, including two challenging CL scenarios (class incremental and
domain incremental learning), different datasets (MNIST, CIFAR10,
TinyImagenet), and different network architectures. Across all experiments, we
find large performance gains through ReCL. To the best of our knowledge, our
framework is the first to address catastrophic forgetting by leveraging models
in CL as their own memory buffers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Representation of High-frequency Components for Medical Visual
  Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14651v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14651v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuetan Chu, Yilan Zhang, Zhongyi Han, Changchun Yang, Longxi Zhou, Gongning Luo, Chao Huang, Xin Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models have recently attracted significant attention for their
impressive generalizability across diverse downstream tasks. However, these
models are demonstrated to exhibit great limitations in representing
high-frequency components and fine-grained details. In many medical imaging
tasks, the precise representation of such information is crucial due to the
inherently intricate anatomical structures, sub-visual features, and complex
boundaries involved. Consequently, the limited representation of prevalent
foundation models can result in significant performance degradation or even
failure in these tasks. To address these challenges, we propose a novel
pretraining strategy, named Frequency-advanced Representation Autoencoder
(Frepa). Through high-frequency masking and low-frequency perturbation combined
with adversarial learning, Frepa encourages the encoder to effectively
represent and preserve high-frequency components in the image embeddings.
Additionally, we introduce an innovative histogram-equalized image masking
strategy, extending the Masked Autoencoder approach beyond ViT to other
architectures such as Swin Transformer and convolutional networks. We develop
Frepa across nine medical modalities and validate it on 32 downstream tasks for
both 2D images and 3D volume data. Without fine-tuning, Frepa can outperform
other self-supervised pretraining methods and, in some cases, even surpasses
task-specific trained models. This improvement is particularly significant for
tasks involving fine-grained details, such as achieving up to a +15% increase
in DSC for retina vessel segmentation and a +7% increase in IoU for lung nodule
detection. Further experiments quantitatively reveal that Frepa enables
superior high-frequency representations and preservation in the embeddings,
underscoring its potential for developing more generalized and universal
medical image foundation models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EXACFS -- A CIL Method to mitigate Catastrophic Forgetting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23751v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23751v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S Balasubramanian, M Sai Subramaniam, Sai Sriram Talasu, Yedu Krishna P, Manepalli Pranav Phanindra Sai, Ravi Mukkamala, Darshan Gera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNS) excel at learning from static datasets but
struggle with continual learning, where data arrives sequentially. Catastrophic
forgetting, the phenomenon of forgetting previously learned knowledge, is a
primary challenge. This paper introduces EXponentially Averaged Class-wise
Feature Significance (EXACFS) to mitigate this issue in the class incremental
learning (CIL) setting. By estimating the significance of model features for
each learned class using loss gradients, gradually aging the significance
through the incremental tasks and preserving the significant features through a
distillation loss, EXACFS effectively balances remembering old knowledge
(stability) and learning new knowledge (plasticity). Extensive experiments on
CIFAR-100 and ImageNet-100 demonstrate EXACFS's superior performance in
preserving stability while acquiring plasticity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Saliency-Bench: A Comprehensive Benchmark for Evaluating Visual
  Explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.08537v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.08537v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Zhang, James Song, Siyi Gu, Tianxu Jiang, Bo Pan, Guangji Bai, Liang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Explainable AI (XAI) has gained significant attention for providing insights
into the decision-making processes of deep learning models, particularly for
image classification tasks through visual explanations visualized by saliency
maps. Despite their success, challenges remain due to the lack of annotated
datasets and standardized evaluation pipelines. In this paper, we introduce
Saliency-Bench, a novel benchmark suite designed to evaluate visual
explanations generated by saliency methods across multiple datasets. We
curated, constructed, and annotated eight datasets, each covering diverse tasks
such as scene classification, cancer diagnosis, object classification, and
action classification, with corresponding ground-truth explanations. The
benchmark includes a standardized and unified evaluation pipeline for assessing
faithfulness and alignment of the visual explanation, providing a holistic
visual explanation performance assessment. We benchmark these eight datasets
with widely used saliency methods on different image classifier architectures
to evaluate explanation quality. Additionally, we developed an easy-to-use API
for automating the evaluation pipeline, from data accessing, and data loading,
to result evaluation. The benchmark is available via our website:
https://xaidataset.github.io.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HiBug2: Efficient and Interpretable Error Slice Discovery for
  Comprehensive Model Debugging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.16751v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.16751v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muxi Chen, Chenchen Zhao, Qiang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the significant success of deep learning models in computer vision,
they often exhibit systematic failures on specific data subsets, known as error
slices. Identifying and mitigating these error slices is crucial to enhancing
model robustness and reliability in real-world scenarios. In this paper, we
introduce HiBug2, an automated framework for error slice discovery and model
repair. HiBug2 first generates task-specific visual attributes to highlight
instances prone to errors through an interpretable and structured process. It
then employs an efficient slice enumeration algorithm to systematically
identify error slices, overcoming the combinatorial challenges that arise
during slice exploration. Additionally, HiBug2 extends its capabilities by
predicting error slices beyond the validation set, addressing a key limitation
of prior approaches. Extensive experiments across multiple domains, including
image classification, pose estimation, and object detection - show that HiBug2
not only improves the coherence and precision of identified error slices but
also significantly enhances the model repair capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VoCo-LLaMA: Towards Vision Compression with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.12275v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.12275v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xubing Ye, Yukang Gan, Xiaoke Huang, Yixiao Ge, Yansong Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) have achieved remarkable success in various
multi-modal tasks, but they are often bottlenecked by the limited context
window and high computational cost of processing high-resolution image inputs
and videos. Vision compression can alleviate this problem by reducing the
vision token count. Previous approaches compress vision tokens with external
modules and force LLMs to understand the compressed ones, leading to visual
information loss. However, the LLMs' understanding paradigm of vision tokens is
not fully utilised in the compression learning process. We propose VoCo-LLaMA,
the first approach to compress vision tokens using LLMs. By introducing Vision
Compression tokens during the vision instruction tuning phase and leveraging
attention distillation, our method distill how LLMs comprehend vision tokens
into their processing of VoCo tokens. VoCo-LLaMA facilitates effective vision
compression and improves the computational efficiency during the inference
stage. Specifically, our method achieves minimal performance loss with a
compression ratio of 576$\times$, resulting in up to 94.8$\%$ fewer FLOPs and
69.6$\%$ acceleration in inference time. Furthermore, through continuous
training using time-series compressed token sequences of video frames,
VoCo-LLaMA demonstrates the ability to understand temporal correlations,
outperforming previous methods on popular video question-answering benchmarks.
Our approach presents a promising way to unlock the full potential of VLMs'
contextual window, enabling more scalable multi-modal applications. The project
page, along with the associated code, can be accessed via
https://yxxxb.github.io/VoCo-LLaMA-page/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast and Accurate Gigapixel Pathological Image Classification with
  Hierarchical Distillation <span class="highlight-title">Multi</span>-Instance Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21130v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21130v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiuyang Dong, Junjun Jiang, Kui Jiang, Jiahan Li, Yongbing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although multi-instance learning (MIL) has succeeded in pathological image
classification, it faces the challenge of high inference costs due to
processing numerous patches from gigapixel whole slide images (WSIs). To
address this, we propose HDMIL, a hierarchical distillation multi-instance
learning framework that achieves fast and accurate classification by
eliminating irrelevant patches. HDMIL consists of two key components: the
dynamic multi-instance network (DMIN) and the lightweight instance
pre-screening network (LIPN). DMIN operates on high-resolution WSIs, while LIPN
operates on the corresponding low-resolution counterparts. During training,
DMIN are trained for WSI classification while generating attention-score-based
masks that indicate irrelevant patches. These masks then guide the training of
LIPN to predict the relevance of each low-resolution patch. During testing,
LIPN first determines the useful regions within low-resolution WSIs, which
indirectly enables us to eliminate irrelevant regions in high-resolution WSIs,
thereby reducing inference time without causing performance degradation. In
addition, we further design the first Chebyshev-polynomials-based
Kolmogorov-Arnold classifier in computational pathology, which enhances the
performance of HDMIL through learnable activation layers. Extensive experiments
on three public datasets demonstrate that HDMIL outperforms previous
state-of-the-art methods, e.g., achieving improvements of 3.13% in AUC while
reducing inference time by 28.6% on the Camelyon16 dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures, accepted by CVPR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GDTS: Goal-Guided Diffusion Model with Tree Sampling for <span class="highlight-title">Multi</span>-Modal
  Pedestrian Trajectory Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2311.14922v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2311.14922v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ge Sun, Sheng Wang, Lei Zhu, Ming Liu, Jun Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate prediction of pedestrian trajectories is crucial for improving the
safety of autonomous driving. However, this task is generally nontrivial due to
the inherent stochasticity of human motion, which naturally requires the
predictor to generate multi-modal prediction. Previous works leverage various
generative methods, such as GAN and VAE, for pedestrian trajectory prediction.
Nevertheless, these methods may suffer from mode collapse and relatively
low-quality results. The denoising diffusion probabilistic model (DDPM) has
recently been applied to trajectory prediction due to its simple training
process and powerful reconstruction ability. However, current diffusion-based
methods do not fully utilize input information and usually require many
denoising iterations that lead to a long inference time or an additional
network for initialization. To address these challenges and facilitate the use
of diffusion models in multi-modal trajectory prediction, we propose GDTS, a
novel Goal-Guided Diffusion Model with Tree Sampling for multi-modal trajectory
prediction. Considering the "goal-driven" characteristics of human motion, GDTS
leverages goal estimation to guide the generation of the diffusion network. A
two-stage tree sampling algorithm is presented, which leverages common features
to reduce the inference time and improve accuracy for multi-modal prediction.
Experimental results demonstrate that our proposed framework achieves
comparable state-of-the-art performance with real-time inference speed in
public datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CromSS: Cross-modal pre-training with noisy labels for remote sensing
  image segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.01217v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.01217v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenying Liu, Conrad Albrecht, Yi Wang, Xiao Xiang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We explore the potential of large-scale noisily labeled data to enhance
feature learning by pretraining semantic segmentation models within a
multi-modal framework for geospatial applications. We propose a novel
Cross-modal Sample Selection (CromSS) method, a weakly supervised pretraining
strategy designed to improve feature representations through cross-modal
consistency and noise mitigation techniques. Unlike conventional pretraining
approaches, CromSS exploits massive amounts of noisy and easy-to-come-by labels
for improved feature learning beneficial to semantic segmentation tasks. We
investigate middle and late fusion strategies to optimize the multi-modal
pretraining architecture design. We also introduce a cross-modal sample
selection module to mitigate the adverse effects of label noise, which employs
a cross-modal entangling strategy to refine the estimated confidence masks
within each modality to guide the sampling process. Additionally, we introduce
a spatial-temporal label smoothing technique to counteract overconfidence for
enhanced robustness against noisy labels. To validate our approach, we
assembled the multi-modal dataset, NoLDO-S12, which consists of a large-scale
noisy label subset from Google's Dynamic World (DW) dataset for pretraining and
two downstream subsets with high-quality labels from Google DW and
OpenStreetMap (OSM) for transfer learning. Experimental results on two
downstream tasks and the publicly available DFC2020 dataset demonstrate that
when effectively utilized, the low-cost noisy labels can significantly enhance
feature learning for segmentation tasks. All data, code, and pretrained weights
will be made publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 1st short version was accepted as an oral presentation by ICLR
  2024 ML4RS workshop. The 2nd extended version is being under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Doracamom: Joint 3D Detection and Occupancy Prediction with <span class="highlight-title">Multi</span>-view
  4D Radars and Cameras for Omnidirectional Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15394v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15394v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lianqing Zheng, Jianan Liu, Runwei Guan, Long Yang, Shouyi Lu, Yuanzhe Li, Xiaokai Bai, Jie Bai, Zhixiong Ma, Hui-Liang Shen, Xichan Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D object detection and occupancy prediction are critical tasks in autonomous
driving, attracting significant attention. Despite the potential of recent
vision-based methods, they encounter challenges under adverse conditions. Thus,
integrating cameras with next-generation 4D imaging radar to achieve unified
multi-task perception is highly significant, though research in this domain
remains limited. In this paper, we propose Doracamom, the first framework that
fuses multi-view cameras and 4D radar for joint 3D object detection and
semantic occupancy prediction, enabling comprehensive environmental perception.
Specifically, we introduce a novel Coarse Voxel Queries Generator that
integrates geometric priors from 4D radar with semantic features from images to
initialize voxel queries, establishing a robust foundation for subsequent
Transformer-based refinement. To leverage temporal information, we design a
Dual-Branch Temporal Encoder that processes multi-modal temporal features in
parallel across BEV and voxel spaces, enabling comprehensive spatio-temporal
representation learning. Furthermore, we propose a Cross-Modal BEV-Voxel Fusion
module that adaptively fuses complementary features through attention
mechanisms while employing auxiliary tasks to enhance feature quality.
Extensive experiments on the OmniHD-Scenes, View-of-Delft (VoD), and TJ4DRadSet
datasets demonstrate that Doracamom achieves state-of-the-art performance in
both tasks, establishing new benchmarks for multi-modal 3D perception. Code and
models will be publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ADUGS-VINS: Generalized Visual-Inertial Odometry for Robust Navigation
  in Highly Dynamic and Complex Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.19289v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.19289v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Zhou, Jingbin Liu, Junbin Xie, Jianyu Zhang, Yingze Hu, Jiele Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual-inertial odometry (VIO) is widely used in various fields, such as
robots, drones, and autonomous vehicles. However, real-world scenes often
feature dynamic objects, compromising the accuracy of VIO. The diversity and
partial occlusion of these objects present a tough challenge for existing
dynamic VIO methods. To tackle this challenge, we introduce ADUGS-VINS, which
integrates an enhanced SORT algorithm along with a promptable foundation model
into VIO, thereby improving pose estimation accuracy in environments with
diverse dynamic objects and frequent occlusions. We evaluated our proposed
method using multiple public datasets representing various scenes, as well as
in a real-world scenario involving diverse dynamic objects. The experimental
results demonstrate that our proposed method performs impressively in multiple
scenarios, outperforming other state-of-the-art methods. This highlights its
remarkable generalization and adaptability in diverse dynamic environments,
showcasing its potential to handle various dynamic objects in practical
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Locality-aware Gaussian Compression for Fast and High-quality Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.05757v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.05757v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seungjoo Shin, Jaesik Park, Sunghyun Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LocoGS, a locality-aware 3D Gaussian Splatting (3DGS) framework
that exploits the spatial coherence of 3D Gaussians for compact modeling of
volumetric scenes. To this end, we first analyze the local coherence of 3D
Gaussian attributes, and propose a novel locality-aware 3D Gaussian
representation that effectively encodes locally-coherent Gaussian attributes
using a neural field representation with a minimal storage requirement. On top
of the novel representation, LocoGS is carefully designed with additional
components such as dense initialization, an adaptive spherical harmonics
bandwidth scheme and different encoding schemes for different Gaussian
attributes to maximize compression performance. Experimental results
demonstrate that our approach outperforms the rendering quality of existing
compact Gaussian representations for representative real-world 3D datasets
while achieving from 54.6$\times$ to 96.6$\times$ compressed storage size and
from 2.1$\times$ to 2.4$\times$ rendering speed than 3DGS. Even our approach
also demonstrates an averaged 2.4$\times$ higher rendering speed than the
state-of-the-art compression method with comparable compression performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025. Project page:
  https://seungjooshin.github.io/LocoGS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RALAD: Bridging the Real-to-Sim Domain Gap in Autonomous Driving with
  Retrieval-Augmented Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12296v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12296v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiacheng Zuo, Haibo Hu, Zikang Zhou, Yufei Cui, Ziquan Liu, Jianping Wang, Nan Guan, Jin Wang, Chun Jason Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the pursuit of robust autonomous driving systems, models trained on
real-world datasets often struggle to adapt to new environments, particularly
when confronted with corner cases such as extreme weather conditions.
Collecting these corner cases in the real world is non-trivial, which
necessitates the use of simulators for validation. However,the high
computational cost and the domain gap in data distribution have hindered the
seamless transition between real and simulated driving scenarios. To tackle
this challenge, we propose Retrieval-Augmented Learning for Autonomous Driving
(RALAD), a novel framework designed to bridge the real-to-sim gap at a low
cost. RALAD features three primary designs, including (1) domain adaptation via
an enhanced Optimal Transport (OT) method that accounts for both individual and
grouped image distances, (2) a simple and unified framework that can be applied
to various models, and (3) efficient fine-tuning techniques that freeze the
computationally expensive layers while maintaining robustness. Experimental
results demonstrate that RALAD compensates for the performance degradation in
simulated environments while maintaining accuracy in real-world scenarios
across three different models. Taking Cross View as an example, the mIOU and
mAP metrics in real-world scenarios remain stable before and after RALAD
fine-tuning, while in simulated environments,the mIOU and mAP metrics are
improved by 10.30% and 12.29%, respectively. Moreover, the re-training cost of
our approach is reduced by approximately 88.1%. Our code is available at
https://github.com/JiachengZuo/RALAD.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-Spectral Vision Transformer for Biometric Authentication using
  Forehead Subcutaneous Vein Pattern and Periocular Pattern 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19160v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19160v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arun K. Sharma, Shubhobrata Bhattacharya, Motahar Reza, Bishakh Bhattacharya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional biometric systems have encountered significant setbacks due to
various unavoidable factors, for example, face recognition-based biometrics
fails due to the wearing of face masks and fingerprints create hygiene
concerns. This paper proposes a novel lightweight cross-spectral vision
transformer (CS-ViT) for biometric authentication using forehead subcutaneous
vein patterns and periocular patterns, offering a promising alternative to
traditional methods, capable of performing well even with the face masks and
without any physical touch. The proposed framework comprises a cross-spectral
dual-channel architecture designed to handle two distinct biometric traits and
to capture inter-dependencies in terms of relative spectral patterns. Each
channel consists of a Phase-Only Correlation Cross-Spectral Attention (POC-CSA)
that captures their individual as well as correlated patterns. The computation
of cross-spectral attention using POC extracts the phase correlation in the
spatial features. Therefore, it is robust against the resolution/intensity
variations and illumination of the input images, assuming both biometric traits
are from the same person. The lightweight model is suitable for edge device
deployment. The performance of the proposed algorithm was rigorously evaluated
using the Forehead Subcutaneous Vein Pattern and Periocular Biometric Pattern
(FSVP-PBP) database. The results demonstrated the superiority of the algorithm
over state-of-the-art methods, achieving a remarkable classification accuracy
of 98.8% with the combined vein and periocular patterns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE TPAMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 3D-Affordance<span class="highlight-title">LLM</span>: Harnessing Large Language Models for Open-Vocabulary
  Affordance Detection in 3D Wo<span class="highlight-title">rl</span>ds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20041v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20041v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengshuo Chu, Xiang Deng, Qi Lv, Xiaoyang Chen, Yinchuan Li, Jianye Hao, Liqiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Affordance detection is a challenging problem with broad applications on
various robotic tasks. Existing methods typically formulate the detection
paradigm as a label-based semantic segmentation task. This paradigm relies on
predefined labels and lacks the ability to comprehend complex natural language,
resulting in limited generalization in open-world scene. To address these
limitations, we reformulate the traditional affordance detection paradigm into
\textit{Instruction Reasoning Affordance Segmentation} (IRAS) task. This task
is designed to output a affordance mask region given a query reasoning text,
which avoids fixed categories of input labels. We accordingly propose the
\textit{3D-AffordanceLLM} (3D-ADLLM), a framework designed for reasoning
affordance detection in 3D open-scene. Specifically, 3D-ADLLM introduces large
language models (LLMs) to 3D affordance perception with a custom-designed
decoder for generating affordance masks, thus achieving open-world reasoning
affordance detection. In addition, given the scarcity of 3D affordance datasets
for training large models, we seek to extract knowledge from general
segmentation data and transfer it to affordance detection. Thus, we propose a
multi-stage training strategy that begins with a novel pre-training task, i.e.,
\textit{Referring Object Part Segmentation}~(ROPS). This stage is designed to
equip the model with general recognition and segmentation capabilities at the
object-part level. Then followed by fine-tuning with the IRAS task, 3D-ADLLM
obtains the reasoning ability for affordance detection. In summary, 3D-ADLLM
leverages the rich world knowledge and human-object interaction reasoning
ability of LLMs, achieving approximately an 8\% improvement in mIoU on
open-vocabulary affordance detection tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representation Engineering: A Top-Down Approach to AI Transparency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01405v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01405v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, J. Zico Kolter, Dan Hendrycks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we identify and characterize the emerging area of
representation engineering (RepE), an approach to enhancing the transparency of
AI systems that draws on insights from cognitive neuroscience. RepE places
population-level representations, rather than neurons or circuits, at the
center of analysis, equipping us with novel methods for monitoring and
manipulating high-level cognitive phenomena in deep neural networks (DNNs). We
provide baselines and an initial analysis of RepE techniques, showing that they
offer simple yet effective solutions for improving our understanding and
control of large language models. We showcase how these methods can provide
traction on a wide range of safety-relevant problems, including honesty,
harmlessness, power-seeking, and more, demonstrating the promise of top-down
transparency research. We hope that this work catalyzes further exploration of
RepE and fosters advancements in the transparency and safety of AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at
  https://github.com/andyzoujm/representation-engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Low-Biased General Annotated <span class="highlight-title">Dataset</span> Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.10831v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.10831v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dengyang Jiang, Haoyu Wang, Lei Zhang, Wei Wei, Guang Dai, Mengmeng Wang, Jingdong Wang, Yanning Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training backbone networks on a general annotated dataset (e.g.,
ImageNet) that comprises numerous manually collected images with category
annotations has proven to be indispensable for enhancing the generalization
capacity of downstream visual tasks. However, those manually collected images
often exhibit bias, which is non-transferable across either categories or
domains, thus causing the model's generalization capacity degeneration. To
mitigate this problem, we present an low-biased general annotated dataset
generation framework (lbGen). Instead of expensive manual collection, we aim at
directly generating low-biased images with category annotations. To achieve
this goal, we propose to leverage the advantage of a multimodal foundation
model (e.g., CLIP), in terms of aligning images in an low-biased semantic space
defined by language. Specifically, we develop a bi-level semantic alignment
loss, which not only forces all generated images to be consistent with the
semantic distribution of all categories belonging to the target dataset in an
adversarial learning manner, but also requires each generated image to match
the semantic description of its category name. In addition, we further cast an
existing image quality scoring model into a quality assurance loss to preserve
the quality of the generated image. By leveraging these two loss functions, we
can obtain an low-biased image generation model by simply fine-tuning a
pre-trained diffusion model using only all category names in the target dataset
as input. Experimental results confirm that, compared with the manually labeled
dataset or other synthetic datasets, the utilization of our generated
low-biased datasets leads to stable generalization capacity enhancement of
different backbone networks across various tasks, especially in tasks where the
manually labeled samples are scarce.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structural-Entropy-Based Sample Selection for Efficient and Effective
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02268v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02268v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianchi Xie, Jiangning Zhu, Guozu Ma, Minzhi Lin, Wei Chen, Weikai Yang, Shixia Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sample selection improves the efficiency and effectiveness of machine
learning models by providing informative and representative samples. Typically,
samples can be modeled as a sample graph, where nodes are samples and edges
represent their similarities. Most existing methods are based on local
information, such as the training difficulty of samples, thereby overlooking
global information, such as connectivity patterns. This oversight can result in
suboptimal selection because global information is crucial for ensuring that
the selected samples well represent the structural properties of the graph. To
address this issue, we employ structural entropy to quantify global information
and losslessly decompose it from the whole graph to individual nodes using the
Shapley value. Based on the decomposition, we present
$\textbf{S}$tructural-$\textbf{E}$ntropy-based sample $\textbf{S}$election
($\textbf{SES}$), a method that integrates both global and local information to
select informative and representative samples. SES begins by constructing a
$k$NN-graph among samples based on their similarities. It then measures sample
importance by combining structural entropy (global metric) with training
difficulty (local metric). Finally, SES applies importance-biased blue noise
sampling to select a set of diverse and representative samples. Comprehensive
experiments on three learning scenarios -- supervised learning, active
learning, and continual learning -- clearly demonstrate the effectiveness of
our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dynamic Gaussians Mesh: Consistent Mesh Reconstruction from Dynamic
  Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12379v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12379v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isabella Liu, Hao Su, Xiaolong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern 3D engines and graphics pipelines require mesh as a memory-efficient
representation, which allows efficient rendering, geometry processing, texture
editing, and many other downstream operations. However, it is still highly
difficult to obtain high-quality mesh in terms of detailed structure and time
consistency from dynamic observations. To this end, we introduce Dynamic
Gaussians Mesh (DG-Mesh), a framework to reconstruct a high-fidelity and
time-consistent mesh from dynamic input. Our work leverages the recent
advancement in 3D Gaussian Splatting to construct the mesh sequence with
temporal consistency from dynamic observations. Building on top of this
representation, DG-Mesh recovers high-quality meshes from the Gaussian points
and can track the mesh vertices over time, which enables applications such as
texture editing on dynamic objects. We introduce the Gaussian-Mesh Anchoring,
which encourages evenly distributed Gaussians, resulting better mesh
reconstruction through mesh-guided densification and pruning on the deformed
Gaussians. By applying cycle-consistent deformation between the canonical and
the deformed space, we can project the anchored Gaussian back to the canonical
space and optimize Gaussians across all time frames. During the evaluation on
different datasets, DG-Mesh provides significantly better mesh reconstruction
and rendering than baselines. Project page: https://www.liuisabella.com/DG-Mesh
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://www.liuisabella.com/DG-Mesh</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> ESVO2: Direct Visual-Inertial Odometry with Stereo Event Cameras 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09374v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09374v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junkai Niu, Sheng Zhong, Xiuyuan Lu, <span class="highlight-author">Shaojie Shen</span>, Guillermo Gallego, Yi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event-based visual odometry is a specific branch of visual Simultaneous
Localization and Mapping (SLAM) techniques, which aims at solving tracking and
mapping subproblems (typically in parallel), by exploiting the special working
principles of neuromorphic (i.e., event-based) cameras. Due to the
motion-dependent nature of event data, explicit data association (i.e., feature
matching) under large-baseline view-point changes is difficult to establish,
making direct methods a more rational choice. However, state-of-the-art direct
methods are limited by the high computational complexity of the mapping
sub-problem and the degeneracy of camera pose tracking in certain degrees of
freedom (DoF) in rotation. In this paper, we tackle these issues by building an
event-based stereo visual-inertial odometry system on top of a direct pipeline.
Specifically, to speed up the mapping operation, we propose an efficient
strategy for sampling contour points according to the local dynamics of events.
The mapping performance is also improved in terms of structure completeness and
local smoothness by merging the temporal stereo and static stereo results. To
circumvent the degeneracy of camera pose tracking in recovering the pitch and
yaw components of general 6-DoF motion, we introduce IMU measurements as motion
priors via pre-integration. To this end, a compact back-end is proposed for
continuously updating the IMU bias and predicting the linear velocity, enabling
an accurate motion prediction for camera pose tracking. The resulting system
scales well with modern high-resolution event cameras and leads to better
global positioning accuracy in large-scale outdoor environments. Extensive
evaluations on five publicly available datasets featuring different resolutions
and scenarios justify the superior performance of the proposed system against
five state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OMG: Opacity Matters in Material Modeling with Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10988v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10988v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Silong Yong, Venkata Nagarjun Pudureddiyur Manivannan, Bernhard Kerbl, Zifu Wan, Simon Stepputtis, Katia Sycara, Yaqi Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decomposing geometry, materials and lighting from a set of images, namely
inverse rendering, has been a long-standing problem in computer vision and
graphics. Recent advances in neural rendering enable photo-realistic and
plausible inverse rendering results. The emergence of 3D Gaussian Splatting has
boosted it to the next level by showing real-time rendering potentials. An
intuitive finding is that the models used for inverse rendering do not take
into account the dependency of opacity w.r.t. material properties, namely cross
section, as suggested by optics. Therefore, we develop a novel approach that
adds this dependency to the modeling itself. Inspired by radiative transfer, we
augment the opacity term by introducing a neural network that takes as input
material properties to provide modeling of cross section and a physically
correct activation function. The gradients for material properties are
therefore not only from color but also from opacity, facilitating a constraint
for their optimization. Therefore, the proposed method incorporates more
accurate physical properties compared to previous works. We implement our
method into 3 different baselines that use Gaussian Splatting for inverse
rendering and achieve significant improvements universally in terms of novel
view synthesis and material modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PATCH: a deep learning method to assess heterogeneity of artistic
  practice in historical paintings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01912v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01912v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Van Horn, Lauryn Smith, Mahamad Mahmoud, Michael McMaster, Clara Pinchbeck, Ina Martin, Andrew Lininger, Anthony Ingrisano, Adam Lowe, Carlos Bayod, Elizabeth Bolman, Kenneth Singer, Michael Hinczewski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The history of art has seen significant shifts in the manner in which
artworks are created, making understanding of creative processes a central
question in technical art history. In the Renaissance and Early Modern period,
paintings were largely produced by master painters directing workshops of
apprentices who often contributed to projects. The masters varied significantly
in artistic and managerial styles, meaning different combinations of artists
and implements might be seen both between masters and within workshops or even
individual canvases. Information on how different workshops were managed and
the processes by which artworks were created remains elusive. Machine learning
methods have potential to unearth new information about artists' creative
processes by extending the analysis of brushwork to a microscopic scale.
Analysis of workshop paintings, however, presents a challenge in that
documentation of the artists and materials involved is sparse, meaning external
examples are not available to train networks to recognize their contributions.
Here we present a novel machine learning approach we call pairwise assignment
training for classifying heterogeneity (PATCH) that is capable of identifying
individual artistic practice regimes with no external training data, or "ground
truth." The method achieves unsupervised results by supervised means, and
outperforms both simple statistical procedures and unsupervised machine
learning methods. We apply this method to two historical paintings by the
Spanish Renaissance master, El Greco: The Baptism of Christ and Christ on the
Cross with Landscape, and our findings regarding the former potentially
challenge previous work that has assigned the painting to workshop members.
Further, the results of our analyses create a measure of heterogeneity of
artistic practice that can be used to characterize artworks across time and
space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>main text: 16 pages, 6 figures; SI: 7 pages, 3 figures; v2: minor
  typo corrections, higher resolution figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ S-NeRF++: Autonomous Driving Simulation via Neural Reconstruction and
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02112v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02112v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yurui Chen, Junge Zhang, Ziyang Xie, Wenye Li, Feihu Zhang, Jiachen Lu, Li Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous driving simulation system plays a crucial role in enhancing
self-driving data and simulating complex and rare traffic scenarios, ensuring
navigation safety. However, traditional simulation systems, which often heavily
rely on manual modeling and 2D image editing, struggled with scaling to
extensive scenes and generating realistic simulation data. In this study, we
present S-NeRF++, an innovative autonomous driving simulation system based on
neural reconstruction. Trained on widely-used self-driving datasets such as
nuScenes and Waymo, S-NeRF++ can generate a large number of realistic street
scenes and foreground objects with high rendering quality as well as offering
considerable flexibility in manipulation and simulation. Specifically, S-NeRF++
is an enhanced neural radiance field for synthesizing large-scale scenes and
moving vehicles, with improved scene parameterization and camera pose learning.
The system effectively utilizes noisy and sparse LiDAR data to refine training
and address depth outliers, ensuring high-quality reconstruction and novel-view
rendering. It also provides a diverse foreground asset bank by reconstructing
and generating different foreground vehicles to support comprehensive scenario
creation.Moreover, we have developed an advanced foreground-background fusion
pipeline that skillfully integrates illumination and shadow effects, further
enhancing the realism of our simulations. With the high-quality simulated data
provided by our S-NeRF++, we found the perception methods enjoy performance
boosts on several autonomous driving downstream tasks, further demonstrating
our proposed simulator's effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE TPAMI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AdvLogo: Adversarial Patch Attack against Object Detectors based on
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07002v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07002v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boming Miao, Chunxiao Li, Yao Zhu, Weixiang Sun, Zizhe Wang, Xiaoyi Wang, Chuanlong Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of deep learning, object detectors have
demonstrated impressive performance; however, vulnerabilities still exist in
certain scenarios. Current research exploring the vulnerabilities using
adversarial patches often struggles to balance the trade-off between attack
effectiveness and visual quality. To address this problem, we propose a novel
framework of patch attack from semantic perspective, which we refer to as
AdvLogo. Based on the hypothesis that every semantic space contains an
adversarial subspace where images can cause detectors to fail in recognizing
objects, we leverage the semantic understanding of the diffusion denoising
process and drive the process to adversarial subareas by perturbing the latent
and unconditional embeddings at the last timestep. To mitigate the distribution
shift that exposes a negative impact on image quality, we apply perturbation to
the latent in frequency domain with the Fourier Transform. Experimental results
demonstrate that AdvLogo achieves strong attack performance while maintaining
high visual quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DynamicCity: Large-Scale 4D Occupancy Generation from Dynamic Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.18084v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.18084v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengwei Bian, Lingdong Kong, Haozhe Xie, Liang Pan, Yu Qiao, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Urban scene generation has been developing rapidly recently. However,
existing methods primarily focus on generating static and single-frame scenes,
overlooking the inherently dynamic nature of real-world driving environments.
In this work, we introduce DynamicCity, a novel 4D occupancy generation
framework capable of generating large-scale, high-quality dynamic 4D scenes
with semantics. DynamicCity mainly consists of two key models. 1) A VAE model
for learning HexPlane as the compact 4D representation. Instead of using naive
averaging operations, DynamicCity employs a novel Projection Module to
effectively compress 4D features into six 2D feature maps for HexPlane
construction, which significantly enhances HexPlane fitting quality (up to
12.56 mIoU gain). Furthermore, we utilize an Expansion & Squeeze Strategy to
reconstruct 3D feature volumes in parallel, which improves both network
training efficiency and reconstruction accuracy than naively querying each 3D
point (up to 7.05 mIoU gain, 2.06x training speedup, and 70.84% memory
reduction). 2) A DiT-based diffusion model for HexPlane generation. To make
HexPlane feasible for DiT generation, a Padded Rollout Operation is proposed to
reorganize all six feature planes of the HexPlane as a squared 2D feature map.
In particular, various conditions could be introduced in the diffusion or
sampling process, supporting versatile 4D generation applications, such as
trajectory- and command-driven generation, inpainting, and layout-conditioned
generation. Extensive experiments on the CarlaSC and Waymo datasets demonstrate
that DynamicCity significantly outperforms existing state-of-the-art 4D
occupancy generation methods across multiple metrics. The code and models have
been released to facilitate future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 Spotlight; 35 pages, 18 figures, 15 tables; Project Page at
  https://dynamic-city.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Calib3D: Calibrating Model Preferences for Reliable 3D Scene
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17010v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17010v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingdong Kong, Xiang Xu, Jun Cen, Wenwei Zhang, Liang Pan, Kai Chen, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety-critical 3D scene understanding tasks necessitate not only accurate
but also confident predictions from 3D perception models. This study introduces
Calib3D, a pioneering effort to benchmark and scrutinize the reliability of 3D
scene understanding models from an uncertainty estimation viewpoint. We
comprehensively evaluate 28 state-of-the-art models across 10 diverse 3D
datasets, uncovering insightful phenomena that cope with both the aleatoric and
epistemic uncertainties in 3D scene understanding. We discover that despite
achieving impressive levels of accuracy, existing models frequently fail to
provide reliable uncertainty estimates -- a pitfall that critically undermines
their applicability in safety-sensitive contexts. Through extensive analysis of
key factors such as network capacity, LiDAR representations, rasterization
resolutions, and 3D data augmentation techniques, we correlate these aspects
directly with the model calibration efficacy. Furthermore, we introduce DeptS,
a novel depth-aware scaling approach aimed at enhancing 3D model calibration.
Extensive experiments across a wide range of configurations validate the
superiority of our method. We hope this work could serve as a cornerstone for
fostering reliable 3D scene understanding. Code and benchmark toolkit are
publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV 2025 Oral; 26 pages, 8 figures, 12 tables; Code at
  https://github.com/ldkong1205/Calib3D</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tuning Timestep-Distilled Diffusion Model Using Pairwise Sample
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03190v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03190v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichen Miao, Zhengyuan Yang, Kevin Lin, Ze Wang, Zicheng Liu, Lijuan Wang, Qiang Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in timestep-distilled diffusion models have enabled
high-quality image generation that rivals non-distilled multi-step models, but
with significantly fewer inference steps. While such models are attractive for
applications due to the low inference cost and latency, fine-tuning them with a
naive diffusion objective would result in degraded and blurry outputs. An
intuitive alternative is to repeat the diffusion distillation process with a
fine-tuned teacher model, which produces good results but is cumbersome and
computationally intensive; the distillation training usually requires magnitude
higher of training compute compared to fine-tuning for specific image styles.
In this paper, we present an algorithm named pairwise sample optimization
(PSO), which enables the direct fine-tuning of an arbitrary timestep-distilled
diffusion model. PSO introduces additional reference images sampled from the
current time-step distilled model, and increases the relative likelihood margin
between the training images and reference images. This enables the model to
retain its few-step generation ability, while allowing for fine-tuning of its
output distribution. We also demonstrate that PSO is a generalized formulation
which can be flexibly extended to both offline-sampled and online-sampled
pairwise data, covering various popular objectives for diffusion model
preference optimization. We evaluate PSO in both preference optimization and
other fine-tuning tasks, including style transfer and concept customization. We
show that PSO can directly adapt distilled models to human-preferred generation
with both offline and online-generated pairwise preference image data. PSO also
demonstrates effectiveness in style transfer and concept customization by
directly tuning timestep-distilled diffusion models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FlexDrive: Toward Trajectory Flexibility in Driving Scene Reconstruction
  and Rendering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21093v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21093v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingqiu Zhou, Lue Fan, Linjiang Huang, Xiaoyu Shi, Si Liu, Zhaoxiang Zhang, Hongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Driving scene reconstruction and rendering have advanced significantly using
the 3D Gaussian Splatting. However, most prior research has focused on the
rendering quality along a pre-recorded vehicle path and struggles to generalize
to out-of-path viewpoints, which is caused by the lack of high-quality
supervision in those out-of-path views. To address this issue, we introduce an
Inverse View Warping technique to create compact and high-quality images as
supervision for the reconstruction of the out-of-path views, enabling
high-quality rendering results for those views. For accurate and robust inverse
view warping, a depth bootstrap strategy is proposed to obtain on-the-fly dense
depth maps during the optimization process, overcoming the sparsity and
incompleteness of LiDAR depth data. Our method achieves superior in-path and
out-of-path reconstruction and rendering performance on the widely used Waymo
Open dataset. In addition, a simulator-based benchmark is proposed to obtain
the out-of-path ground truth and quantitatively evaluate the performance of
out-of-path rendering, where our method outperforms previous methods by a
significant margin.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semantically Structured Image Compression via Irregular Group-Based
  Decoupling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.02586v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.02586v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruoyu Feng, Yixin Gao, Xin Jin, Runsen Feng, Zhibo Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image compression techniques typically focus on compressing rectangular
images for human consumption, however, resulting in transmitting redundant
content for downstream applications. To overcome this limitation, some previous
works propose to semantically structure the bitstream, which can meet specific
application requirements by selective transmission and reconstruction.
Nevertheless, they divide the input image into multiple rectangular regions
according to semantics and ignore avoiding information interaction among them,
causing waste of bitrate and distorted reconstruction of region boundaries. In
this paper, we propose to decouple an image into multiple groups with irregular
shapes based on a customized group mask and compress them independently. Our
group mask describes the image at a finer granularity, enabling significant
bitrate saving by reducing the transmission of redundant content. Moreover, to
ensure the fidelity of selective reconstruction, this paper proposes the
concept of group-independent transform that maintain the independence among
distinct groups. And we instantiate it by the proposed Group-Independent
Swin-Block (GI Swin-Block). Experimental results demonstrate that our framework
structures the bitstream with negligible cost, and exhibits superior
performance on both visual quality and intelligent task supporting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept by ICCV2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Learn Weight Generation via Trajectory Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01117v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01117v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunchuan Guan, Yu Liu, Ke Zhou, Zhiqi Shen, Serge Belongie, Jenq-Neng Hwang, Lei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based algorithms have emerged as promising techniques for weight
generation, particularly in scenarios like multi-task learning that require
frequent weight updates. However, existing solutions suffer from limited
cross-task transferability. In addition, they only utilize optimal weights as
training samples, ignoring the value of other weights in the optimization
process. To address these issues, we propose Lt-Di, which integrates the
diffusion algorithm with meta-learning to generate weights for unseen tasks.
Furthermore, we extend the vanilla diffusion algorithm into a trajectory
diffusion algorithm to utilize other weights along the optimization trajectory.
Trajectory diffusion decomposes the entire diffusion chain into multiple
shorter ones, improving training and inference efficiency. We analyze the
convergence properties of the weight generation paradigm and improve
convergence efficiency without additional time overhead. Our experiments
demonstrate Lt-Di's higher accuracy while reducing computational overhead
across various tasks, including zero-shot and few-shot learning, multi-domain
generalization, and large-scale language model fine-tuning.Our code is released
at https://anonymous.4open.science/r/Lt-Di-0E51.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Multi</span>-modal AI for comprehensive breast cancer prognostication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21256v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21256v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Witowski, Ken G. Zeng, Joseph Cappadona, Jailan Elayoubi, Khalil Choucair, Elena Diana Chiru, Nancy Chan, Young-Joon Kang, Frederick Howard, Irina Ostrovnaya, Carlos Fernandez-Granda, Freya Schnabel, Zoe Steinsnyder, Ugur Ozerdem, Kangning Liu, Waleed Abdulsattar, Yu Zong, Lina Daoud, Rafic Beydoun, Anas Saad, Nitya Thakore, Mohammad Sadic, Frank Yeung, Elisa Liu, Theodore Hill, Benjamin Swett, Danielle Rigau, Andrew Clayburn, Valerie Speirs, Marcus Vetter, Lina Sojak, Simone Soysal, Daniel Baumhoer, Jia-Wern Pan, Haslina Makmur, Soo-Hwang Teo, Linda Ma Pak, Victor Angel, Dovile Zilenaite-Petrulaitiene, Arvydas Laurinavicius, Natalie Klar, Brian D. Piening, Carlo Bifulco, Sun-Young Jun, Jae Pak Yi, Su Hyun Lim, Adam Brufsky, Francisco J. Esteva, Lajos Pusztai, Yann LeCun, Krzysztof J. Geras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Treatment selection in breast cancer is guided by molecular subtypes and
clinical characteristics. However, current tools including genomic assays lack
the accuracy required for optimal clinical decision-making. We developed a
novel artificial intelligence (AI)-based approach that integrates digital
pathology images with clinical data, providing a more robust and effective
method for predicting the risk of cancer recurrence in breast cancer patients.
Specifically, we utilized a vision transformer pan-cancer foundation model
trained with self-supervised learning to extract features from digitized
H&E-stained slides. These features were integrated with clinical data to form a
multi-modal AI test predicting cancer recurrence and death. The test was
developed and evaluated using data from a total of 8,161 female breast cancer
patients across 15 cohorts originating from seven countries. Of these, 3,502
patients from five cohorts were used exclusively for evaluation, while the
remaining patients were used for training. Our test accurately predicted our
primary endpoint, disease-free interval, in the five evaluation cohorts
(C-index: 0.71 [0.68-0.75], HR: 3.63 [3.02-4.37, p<0.001]). In a direct
comparison (n=858), the AI test was more accurate than Oncotype DX, the
standard-of-care 21-gene assay, achieving a C-index of 0.67 [0.61-0.74] versus
0.61 [0.49-0.73], respectively. Additionally, the AI test added independent
prognostic information to Oncotype DX in a multivariate analysis (HR: 3.11
[1.91-5.09, p<0.001)]). The test demonstrated robust accuracy across major
molecular breast cancer subtypes, including TNBC (C-index: 0.71 [0.62-0.81],
HR: 3.81 [2.35-6.17, p=0.02]), where no diagnostic tools are currently
recommended by clinical guidelines. These results suggest that our AI test
improves upon the accuracy of existing prognostic tests, while being applicable
to a wider range of patients.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Vision-Language-Action Models for Embodied AI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14093v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14093v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yueen Ma, Zixing Song, Yuzheng Zhuang, Jianye Hao, Irwin King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Embodied AI is widely recognized as a key element of artificial general
intelligence because it involves controlling embodied agents to perform tasks
in the physical world. Building on the success of large language models and
vision-language models, a new category of multimodal models -- referred to as
vision-language-action models (VLAs) -- has emerged to address
language-conditioned robotic tasks in embodied AI by leveraging their distinct
ability to generate actions. In recent years, a myriad of VLAs have been
developed, making it imperative to capture the rapidly evolving landscape
through a comprehensive survey. To this end, we present the first survey on
VLAs for embodied AI. This work provides a detailed taxonomy of VLAs, organized
into three major lines of research. The first line focuses on individual
components of VLAs. The second line is dedicated to developing control policies
adept at predicting low-level actions. The third line comprises high-level task
planners capable of decomposing long-horizon tasks into a sequence of subtasks,
thereby guiding VLAs to follow more general user instructions. Furthermore, we
provide an extensive summary of relevant resources, including datasets,
simulators, and benchmarks. Finally, we discuss the challenges faced by VLAs
and outline promising future directions in embodied AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, a survey of vision-language-action models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GAMED-Snake: Gradient-aware Adaptive Momentum Evolution Deep Snake Model
  for <span class="highlight-title">Multi</span>-organ Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.12844v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.12844v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruicheng Zhang, Haowei Guo, Zeyu Zhang, Puxin Yan, Shen Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-organ segmentation is a critical yet challenging task due to complex
anatomical backgrounds, blurred boundaries, and diverse morphologies. This
study introduces the Gradient-aware Adaptive Momentum Evolution Deep Snake
(GAMED-Snake) model, which establishes a novel paradigm for contour-based
segmentation by integrating gradient-based learning with adaptive momentum
evolution mechanisms. The GAMED-Snake model incorporates three major
innovations: First, the Distance Energy Map Prior (DEMP) generates a
pixel-level force field that effectively attracts contour points towards the
true boundaries, even in scenarios with complex backgrounds and blurred edges.
Second, the Differential Convolution Inception Module (DCIM) precisely extracts
comprehensive energy gradients, significantly enhancing segmentation accuracy.
Third, the Adaptive Momentum Evolution Mechanism (AMEM) employs cross-attention
to establish dynamic features across different iterations of evolution,
enabling precise boundary alignment for diverse morphologies. Experimental
results on four challenging multi-organ segmentation datasets demonstrate that
GAMED-Snake improves the mDice metric by approximately 2% compared to
state-of-the-art methods. Code will be available at
https://github.com/SYSUzrc/GAMED-Snake.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Noise2Score3D:Unsupervised Tweedie's Approach for Point Cloud Denoising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16826v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16826v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangbin Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building on recent advances in Bayesian statistics and image denoising, we
propose Noise2Score3D, a fully unsupervised framework for point cloud denoising
that addresses the critical challenge of limited availability of clean data.
Noise2Score3D learns the gradient of the underlying point cloud distribution
directly from noisy data, eliminating the need for clean data during training.
By leveraging Tweedie's formula, our method performs inference in a single
step, avoiding the iterative processes used in existing unsupervised methods,
thereby improving both performance and efficiency. Experimental results
demonstrate that Noise2Score3D achieves state-of-the-art performance on
standard benchmarks, outperforming other unsupervised methods in Chamfer
distance and point-to-mesh metrics, and rivaling some supervised approaches.
Furthermore, Noise2Score3D demonstrates strong generalization ability beyond
training datasets. Additionally, we introduce Total Variation for Point Cloud,
a criterion that allows for the estimation of unknown noise parameters, which
further enhances the method's versatility and real-world utility.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMed-RAG: Versatile <span class="highlight-title">Multi</span>modal RAG System for Medical Vision Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13085v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13085v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Xia, Kangyu Zhu, Haoran Li, Tianze Wang, Weijia Shi, Sheng Wang, Linjun Zhang, James Zou, Huaxiu Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence (AI) has demonstrated significant potential in
healthcare, particularly in disease diagnosis and treatment planning. Recent
progress in Medical Large Vision-Language Models (Med-LVLMs) has opened up new
possibilities for interactive diagnostic tools. However, these models often
suffer from factual hallucination, which can lead to incorrect diagnoses.
Fine-tuning and retrieval-augmented generation (RAG) have emerged as methods to
address these issues. However, the amount of high-quality data and distribution
shifts between training data and deployment data limit the application of
fine-tuning methods. Although RAG is lightweight and effective, existing
RAG-based approaches are not sufficiently general to different medical domains
and can potentially cause misalignment issues, both between modalities and
between the model and the ground truth. In this paper, we propose a versatile
multimodal RAG system, MMed-RAG, designed to enhance the factuality of
Med-LVLMs. Our approach introduces a domain-aware retrieval mechanism, an
adaptive retrieved contexts selection method, and a provable RAG-based
preference fine-tuning strategy. These innovations make the RAG process
sufficiently general and reliable, significantly improving alignment when
introducing retrieved contexts. Experimental results across five medical
datasets (involving radiology, ophthalmology, pathology) on medical VQA and
report generation demonstrate that MMed-RAG can achieve an average improvement
of 43.8% in the factual accuracy of Med-LVLMs. Our data and code are available
in https://github.com/richard-peng-xia/MMed-RAG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Generalizable Scene Change Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06214v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06214v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaewoo Kim, Uehwan Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While current state-of-the-art Scene Change Detection (SCD) approaches
achieve impressive results in well-trained research data, they become
unreliable under unseen environments and different temporal conditions;
in-domain performance drops from 77.6\% to 8.0\% in a previously unseen
environment and to 4.6\% under a different temporal condition -- calling for
generalizable SCD and benchmark. In this work, we propose the Generalizable
Scene Change Detection Framework (GeSCF), which addresses unseen domain
performance and temporal consistency -- to meet the growing demand for anything
SCD. Our method leverages the pre-trained Segment Anything Model (SAM) in a
zero-shot manner. For this, we design Initial Pseudo-mask Generation and
Geometric-Semantic Mask Matching -- seamlessly turning user-guided prompt and
single-image based segmentation into scene change detection for a pair of
inputs without guidance. Furthermore, we define the Generalizable Scene Change
Detection (GeSCD) benchmark along with novel metrics and an evaluation protocol
to facilitate SCD research in generalizability. In the process, we introduce
the ChangeVPR dataset, a collection of challenging image pairs with diverse
environmental scenarios -- including urban, suburban, and rural settings.
Extensive experiments across various datasets demonstrate that GeSCF achieves
an average performance gain of 19.2\% on existing SCD datasets and 30.0\% on
the ChangeVPR dataset, nearly doubling the prior art performance. We believe
our work can lay a solid foundation for robust and generalizable SCD research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Manuscript. Accepted to CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAA: Meticulous Adversarial Attack against Vision-Language Pre-trained
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08079v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08079v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng-Fei Zhang, Guangdong Bai, Zi Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current adversarial attacks for evaluating the robustness of vision-language
pre-trained (VLP) models in multi-modal tasks suffer from limited
transferability, where attacks crafted for a specific model often struggle to
generalize effectively across different models, limiting their utility in
assessing robustness more broadly. This is mainly attributed to the
over-reliance on model-specific features and regions, particularly in the image
modality. In this paper, we propose an elegant yet highly effective method
termed Meticulous Adversarial Attack (MAA) to fully exploit model-independent
characteristics and vulnerabilities of individual samples, achieving enhanced
generalizability and reduced model dependence. MAA emphasizes fine-grained
optimization of adversarial images by developing a novel resizing and sliding
crop (RScrop) technique, incorporating a multi-granularity similarity
disruption (MGSD) strategy. Extensive experiments across diverse VLP models,
multiple benchmark datasets, and a variety of downstream tasks demonstrate that
MAA significantly enhances the effectiveness and transferability of adversarial
attacks. A large cohort of performance studies is conducted to generate
insights into the effectiveness of various model configurations, guiding future
advancements in this domain.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RobotFingerPrint: Unified Gripper Coordinate Space for <span class="highlight-title">Multi</span>-Gripper
  Grasp Synthesis and Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14519v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14519v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ninad Khargonkar, Luis Felipe Casas, Balakrishnan Prabhakaran, Yu Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel grasp representation named the Unified Gripper
Coordinate Space (UGCS) for grasp synthesis and grasp transfer. Our
representation leverages spherical coordinates to create a shared coordinate
space across different robot grippers, enabling it to synthesize and transfer
grasps for both novel objects and previously unseen grippers. The strength of
this representation lies in the ability to map palm and fingers of a gripper
and the unified coordinate space. Grasp synthesis is formulated as predicting
the unified spherical coordinates on object surface points via a conditional
variational autoencoder. The predicted unified gripper coordinates establish
exact correspondences between the gripper and object points, which is used to
optimize grasp pose and joint values. Grasp transfer is facilitated through the
point-to-point correspondence between any two (potentially unseen) grippers and
solved via a similar optimization. Extensive simulation and real-world
experiments showcase the efficacy of the unified grasp representation for grasp
synthesis in generating stable and diverse grasps. Similarly, we showcase
real-world grasp transfer from human demonstrations across different objects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 11 figures, 3 tables. Project page available at
  https://irvlutd.github.io/RobotFingerPrint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Labyrinth of Links: Navigating the Associative Maze of <span class="highlight-title">Multi</span>-modal
  <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01417v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01417v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong Li, Nanxi Li, Yuanjie Chen, Jianbin Zhu, Qinlu Guo, Cewu Lu, Yong-Lu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal Large Language Models (MLLMs) have exhibited impressive
capability. However, recently many deficiencies of MLLMs have been found
compared to human intelligence, $\textit{e.g.}$, hallucination. To drive the
MLLMs study, the community dedicated efforts to building larger benchmarks with
complex tasks. In this paper, we propose benchmarking an essential but usually
overlooked intelligence: $\textbf{association}$, a human's basic capability to
link observation and prior practice memory. To comprehensively investigate
MLLM's performance on the association, we formulate the association task and
devise a standard benchmark based on adjective and verb semantic concepts.
Instead of costly data annotation and curation, we propose a convenient
$\textbf{annotation-free}$ construction method transforming the general dataset
for our association tasks. Simultaneously, we devise a rigorous data refinement
process to eliminate confusion in the raw dataset. Building on this database,
we establish three levels of association tasks: single-step, synchronous, and
asynchronous associations. Moreover, we conduct a comprehensive investigation
into the MLLMs' zero-shot association capabilities, addressing multiple
dimensions, including three distinct memory strategies, both open-source and
closed-source MLLMs, cutting-edge Mixture-of-Experts (MoE) models, and the
involvement of human experts. Our systematic investigation shows that current
open-source MLLMs consistently exhibit poor capability in our association
tasks, even the currently state-of-the-art GPT-4V(vision) also has a
significant gap compared to humans. We believe our benchmark would pave the way
for future MLLM studies. $\textit{Our data and code are available at:}$
https://mvig-rhos.com/llm_inception.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025. Project page:
  https://mvig-rhos.com/llm_inception</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Supervised Contrastive Learning for Videos using Differentiable
  Local Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.04607v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.04607v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keyne Oei, Amr Gomaa, Anna Maria Feit, João Belo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust frame-wise embeddings are essential to perform video analysis and
understanding tasks. We present a self-supervised method for representation
learning based on aligning temporal video sequences. Our framework uses a
transformer-based encoder to extract frame-level features and leverages them to
find the optimal alignment path between video sequences. We introduce the novel
Local-Alignment Contrastive (LAC) loss, which combines a differentiable local
alignment loss to capture local temporal dependencies with a contrastive loss
to enhance discriminative learning. Prior works on video alignment have focused
on using global temporal ordering across sequence pairs, whereas our loss
encourages identifying the best-scoring subsequence alignment. LAC uses the
differentiable Smith-Waterman (SW) affine method, which features a flexible
parameterization learned through the training phase, enabling the model to
adjust the temporal gap penalty length dynamically. Evaluations show that our
learned representations outperform existing state-of-the-art approaches on
action recognition tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in 2nd Workshop on Video Understanding and its Applications,
  held in conjunction with the British Machine Vision Conference (BMVC) 2024</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">2</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Intelligence via Trial and Error 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18858v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18858v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingtao Zhan, Jiahao Zhao, Jiayu Li, Yiqun Liu, Bo Zhang, Qingyao Ai, Jiaxin Mao, Hongning Wang, Min Zhang, Shaoping Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intelligence is a crucial trait for species to find solutions within a
limited number of trial-and-error attempts. Building on this idea, we introduce
Survival Game as a framework to evaluate intelligence based on the number of
failed attempts in a trial-and-error process. Fewer failures indicate higher
intelligence. When the expectation and variance of failure counts are both
finite, it signals the ability to consistently find solutions to new
challenges, which we define as the Autonomous Level of intelligence. Using
Survival Game, we comprehensively evaluate existing AI systems. Our results
show that while AI systems achieve the Autonomous Level in simple tasks, they
are still far from it in more complex tasks, such as vision, search,
recommendation, and language. While scaling current AI technologies might help,
this would come at an astronomical cost. Projections suggest that achieving the
Autonomous Level for general tasks would require $10^{26}$ parameters. To put
this into perspective, loading such a massive model requires so many H100 GPUs
that their total value is $10^{7}$ times that of Apple Inc.'s market value.
Even with Moore's Law, supporting such a parameter scale would take $70$ years.
This staggering cost highlights the complexity of human tasks and the
inadequacies of current AI technologies. To further investigate this
phenomenon, we conduct a theoretical analysis of Survival Game and its
experimental results. Our findings suggest that human tasks possess a
criticality property. As a result, Autonomous Level requires a deep
understanding of the task's underlying mechanisms. Current AI systems, however,
do not fully grasp these mechanisms and instead rely on superficial mimicry,
making it difficult for them to reach an autonomous level. We believe Survival
Game can not only guide the future development of AI but also offer profound
insights into human intelligence.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimize Incompatible Parameters through Compatibility-aware Knowledge
  Integration <span class="chip">AAAI'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07596v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07596v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheqi Lv, Keming Ye, Zishu Wei, Qi Tian, Shengyu Zhang, Wenqiao Zhang, Wenjie Wang, Kun Kuang, Tat-Seng Chua, Fei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have become foundational to advancements in multiple
domains, including recommendation systems, natural language processing, and so
on. Despite their successes, these models often contain incompatible parameters
that can be underutilized or detrimental to model performance, particularly
when faced with specific, varying data distributions. Existing research excels
in removing such parameters or merging the outputs of multiple different
pretrained models. However, the former focuses on efficiency rather than
performance, while the latter requires several times more computing and storage
resources to support inference. In this paper, we set the goal to explicitly
improve these incompatible parameters by leveraging the complementary strengths
of different models, thereby directly enhancing the models without any
additional parameters. Specifically, we propose Compatibility-aware Knowledge
Integration (CKI), which consists of Parameter Compatibility Assessment and
Parameter Splicing, which are used to evaluate the knowledge content of
multiple models and integrate the knowledge into one model, respectively. The
integrated model can be used directly for inference or for further fine-tuning.
We conduct extensive experiments on various datasets for recommendation and
language tasks, and the results show that Compatibility-aware Knowledge
Integration can effectively optimize incompatible parameters under multiple
tasks and settings to break through the training limit of the original model
without increasing the inference cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published on AAAI'25(Oral): The Annual AAAI Conference on Artificial
  Intelligence</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">105</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Matryoshka Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06786v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06786v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pranav Nair, Puranjay Datta, Jeff Dean, Prateek Jain, Aditya Kusupati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantizing model weights is critical for reducing the communication and
inference costs of large models. However, quantizing models -- especially to
low precisions like int4 or int2 -- requires a trade-off in model quality;
int2, in particular, is known to severely degrade model quality. Consequently,
practitioners are often forced to maintain multiple models with different
quantization levels or serve a single model that best satisfies the
quality-latency trade-off. On the other hand, integer data types, such as int8,
inherently possess a nested (Matryoshka) structure where smaller bit-width
integers, like int4 or int2, are nested within the most significant bits.
Leveraging this insight, in this paper, we propose Matryoshka Quantization
(MatQuant), a novel multi-scale quantization technique that alleviates the
aforementioned challenge. This technique allows us to train and maintain a
single quantized model but serve it with the precision demanded by the
deployment. Furthermore, leveraging MatQuant's co-training and co-distillation
regularization, int2 precision models extracted by MatQuant outperform standard
int2 quantization by up to to 4% and 7% with OmniQuant and QAT as base
algorithms respectively. Finally, we demonstrate that by using an extra bit to
represent outliers, a model with an effective precision of 2.05-bit gives an
additional 6% improvement with OmniQuant as the base algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InductionBench: <span class="highlight-title">LLM</span>s Fail in the Simplest Complexity Class 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15823v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15823v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenyue Hua, Tyler Wong, Sun Fei, Liangming Pan, Adam Jardine, William Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown remarkable improvements in reasoning
and many existing benchmarks have been addressed by models such as o1 and o3
either fully or partially. However, a majority of these benchmarks emphasize
deductive reasoning, including mathematical and coding tasks in which rules
such as mathematical axioms or programming syntax are clearly defined, based on
which LLMs can plan and apply these rules to arrive at a solution. In contrast,
inductive reasoning, where one infers the underlying rules from observed data,
remains less explored. Such inductive processes lie at the heart of scientific
discovery, as they enable researchers to extract general principles from
empirical observations. To assess whether LLMs possess this capacity, we
introduce InductionBench, a new benchmark designed to evaluate the inductive
reasoning ability of LLMs. Our experimental findings reveal that even the most
advanced models available struggle to master the simplest complexity classes
within the subregular hierarchy of functions, highlighting a notable deficiency
in current LLMs' inductive reasoning capabilities. Coda and data are available
https://github.com/Wenyueh/inductive_reasoning_benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Error Bounds for Physics-Informed Neural Networks in Fokker-Planck PDEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22371v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22371v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chun-Wei Kong, Luca Laurenti, Jay McMahon, Morteza Lahijanian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic differential equations are commonly used to describe the evolution
of stochastic processes. The state uncertainty of such processes is best
represented by the probability density function (PDF), whose evolution is
governed by the Fokker-Planck partial differential equation (FP-PDE). However,
it is generally infeasible to solve the FP-PDE in closed form. In this work, we
show that physics-informed neural networks (PINNs) can be trained to
approximate the solution PDF. Our main contribution is the analysis of PINN
approximation error: we develop a theoretical framework to construct tight
error bounds using PINNs. In addition, we derive a practical error bound that
can be efficiently constructed with standard training methods. We discuss that
this error-bound framework generalizes to approximate solutions of other linear
PDEs. Empirical results on nonlinear, high-dimensional, and chaotic systems
validate the correctness of our error bounds while demonstrating the
scalability of PINNs and their significant computational speedup in obtaining
accurate PDF solutions compared to the Monte Carlo approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>paper under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CAMEx: Curvature-aware Merging of Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18821v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18821v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dung V. Nguyen, Minh H. Nguyen, Luc Q. Nguyen, Rachel S. Y. Teo, Tan M. Nguyen, Linh Duy Tran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing methods for merging experts during model training and fine-tuning
predominantly rely on Euclidean geometry, which assumes a flat parameter space.
This assumption can limit the model's generalization ability, especially during
the pre-training phase, where the parameter manifold might exhibit more complex
curvature. Curvature-aware merging methods typically require additional
information and computational resources to approximate the Fisher Information
Matrix, adding memory overhead. In this paper, we introduce CAMEx
(Curvature-Aware Merging of Experts), a novel expert merging protocol that
incorporates natural gradients to account for the non-Euclidean curvature of
the parameter manifold. By leveraging natural gradients, CAMEx adapts more
effectively to the structure of the parameter space, improving alignment
between model updates and the manifold's geometry. This approach enhances both
pre-training and fine-tuning, resulting in better optimization trajectories and
improved generalization without the substantial memory overhead typically
associated with curvature-aware methods. Our contributions are threefold: (1)
CAMEx significantly outperforms traditional Euclidean-based expert merging
techniques across various natural language processing tasks, leading to
enhanced performance during pre-training and fine-tuning; (2) we introduce a
dynamic merging architecture that optimizes resource utilization, achieving
high performance while reducing computational costs, facilitating efficient
scaling of large language models; and (3) we provide both theoretical and
empirical evidence to demonstrate the efficiency of our proposed method. The
code is publicly available at: https://github.com/kpup1710/CAMEx.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 5 Figures, 7 Tables. Published at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Disparate Model Performance and Stability in Machine Learning Clinical
  Support for Diabetes and Heart Diseases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19495v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19495v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ioannis Bilionis, Ricardo C. Berrios, Luis Fernandez-Luque, Carlos Castillo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Learning (ML) algorithms are vital for supporting clinical
decision-making in biomedical informatics. However, their predictive
performance can vary across demographic groups, often due to the
underrepresentation of historically marginalized populations in training
datasets. The investigation reveals widespread sex- and age-related inequities
in chronic disease datasets and their derived ML models. Thus, a novel
analytical framework is introduced, combining systematic arbitrariness with
traditional metrics like accuracy and data complexity. The analysis of data
from over 25,000 individuals with chronic diseases revealed mild sex-related
disparities, favoring predictive accuracy for males, and significant
age-related differences, with better accuracy for younger patients. Notably,
older patients showed inconsistent predictive accuracy across seven datasets,
linked to higher data complexity and lower model performance. This highlights
that representativeness in training data alone does not guarantee equitable
outcomes, and model arbitrariness must be addressed before deploying models in
clinical settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper will be presented in American Medical Informatics
  Association (AMIA) Informatics Summit Conference 2025 (Pittsburgh, PA). 10
  pages, 2 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ "FRAME: Forward Recursive Adaptive Model Extraction-A Technique for
  Advance Feature Selection" 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.11972v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.11972v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nachiket Kapure, Harsh Joshi, Parul Kumari, Rajeshwari Mistri, Manasi Mali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The challenges in feature selection, particularly in balancing model
accuracy, interpretability, and computational efficiency, remain a critical
issue in advancing machine learning methodologies. To address these
complexities, this study introduces a novel hybrid approach, the Forward
Recursive Adaptive Model Extraction Technique (FRAME), which combines Forward
Selection and Recursive Feature Elimination (RFE) to enhance feature selection
across diverse datasets. By combining the exploratory capabilities of Forward
Selection with the refinement strengths of RFE, FRAME systematically identifies
optimal feature subsets, striking a harmonious trade-off between
experimentation and precision. A comprehensive evaluation of FRAME is conducted
against traditional methods such as SelectKBest and Lasso Regression, using
high-dimensional, noisy, and heterogeneous datasets. The results demonstrate
that FRAME consistently delivers superior predictive performance based on
downstream machine learning evaluation metrics. It efficiently performs
dimensionality reduction with strong model performance, thus being especially
useful for applications that need interpretable and accurate predictions, e.g.,
biomedical diagnostics.
  This research emphasizes the need to evaluate feature selection techniques on
diverse datasets to test their robustness and generalizability. The results
indicate that FRAME has great potential for further development, especially by
incorporating deep learning frameworks for adaptive and real-time feature
selection in dynamic settings. By advancing feature selection methodologies,
FRAME offers a practical and effective solution to improve machine learning
applications across multiple domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated version with refinements before JMLR submission. Improved
  clarity, expanded literature review, refined methodology, updated
  experimental results, and enhanced conclusion. FRAME's scalability, deep
  learning integration, and real-world applications are further highlighted</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IOHunter: Graph Foundation Model to Uncover Online Information
  Operations <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14663v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14663v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Minici, Luca Luceri, Francesco Fabbri, Emilio Ferrara
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social media platforms have become vital spaces for public discourse, serving
as modern agor\`as where a wide range of voices influence societal narratives.
However, their open nature also makes them vulnerable to exploitation by
malicious actors, including state-sponsored entities, who can conduct
information operations (IOs) to manipulate public opinion. The spread of
misinformation, false news, and misleading claims threatens democratic
processes and societal cohesion, making it crucial to develop methods for the
timely detection of inauthentic activity to protect the integrity of online
discourse. In this work, we introduce a methodology designed to identify users
orchestrating information operations, a.k.a. IO drivers, across various
influence campaigns. Our framework, named IOHunter, leverages the combined
strengths of Language Models and Graph Neural Networks to improve
generalization in supervised, scarcely-supervised, and cross-IO contexts. Our
approach achieves state-of-the-art performance across multiple sets of IOs
originating from six countries, significantly surpassing existing approaches.
This research marks a step toward developing Graph Foundation Models
specifically tailored for the task of IO detection on social media platforms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Langevin <span class="highlight-title">Multi</span>plicative Weights Update with Applications in Polynomial
  Portfolio Management <span class="chip">AAAI-2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19210v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19210v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Feng, Xiao Wang, Tian Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider nonconvex optimization problem over simplex, and more generally,
a product of simplices. We provide an algorithm, Langevin Multiplicative
Weights Update (LMWU) for solving global optimization problems by adding a
noise scaling with the non-Euclidean geometry in the simplex. Non-convex
optimization has been extensively studied by machine learning community due to
its application in various scenarios such as neural network approximation and
finding Nash equilibrium. Despite recent progresses on provable guarantee of
escaping and avoiding saddle point (convergence to local minima) and global
convergence of Langevin gradient based method without constraints, the global
optimization with constraints is less studied. We show that LMWU algorithm is
provably convergent to interior global minima with a non-asymptotic convergence
analysis. We verify the efficiency of the proposed algorithm in real data set
from polynomial portfolio management, where optimization of a highly non-linear
objective function plays a crucial role.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for AAAI-2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting the Test-Time Scaling of o1-like Models: Do they Truly
  Possess Test-Time Scaling Capabilities? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12215v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12215v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyuan Zeng, Qinyuan Cheng, Zhangyue Yin, Yunhua Zhou, Xipeng Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of test-time scaling in large language models (LLMs), exemplified
by OpenAI's o1 series, has advanced reasoning capabilities by scaling
computational resource allocation during inference. While successors like QwQ,
Deepseek-R1 (R1) and LIMO replicate these advancements, whether these models
truly possess test-time scaling capabilities remains underexplored. This study
found that longer CoTs of these o1-like models do not consistently enhance
accuracy; in fact, correct solutions are often shorter than incorrect ones for
the same questions. Further investigation shows this phenomenon is closely
related to models' self-revision capabilities - longer CoTs contain more
self-revisions, which often lead to performance degradation. We then compare
sequential and parallel scaling strategies on QwQ, R1 and LIMO, finding that
parallel scaling achieves better coverage and scalability. Based on these
insights, we propose Shortest Majority Vote, a method that combines parallel
scaling strategies with CoT length characteristics, significantly improving
models' test-time scalability compared to conventional majority voting
approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Add the github link</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ <span class="highlight-title">★</span> Gradient-Based <span class="highlight-title">Multi</span>-Objective Deep Learning: Algorithms, Theories,
  Applications, and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10945v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10945v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiyu Chen, Xiaoyuan Zhang, Baijiong Lin, Xi Lin, Han Zhao, Qing<span class="highlight-author">fu Zhang</span>, James T. Kwok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-objective optimization (MOO) in deep learning aims to simultaneously
optimize multiple conflicting objectives, a challenge frequently encountered in
areas like multi-task learning and multi-criteria learning. Recent advancements
in gradient-based MOO methods have enabled the discovery of diverse types of
solutions, ranging from a single balanced solution to finite or even infinite
Pareto sets, tailored to user needs. These developments have broad applications
across domains such as reinforcement learning, computer vision, recommendation
systems, and large language models. This survey provides the first
comprehensive review of gradient-based MOO in deep learning, covering
algorithms, theories, and practical applications. By unifying various
approaches and identifying critical challenges, it serves as a foundational
resource for driving innovation in this evolving field. A comprehensive list of
MOO algorithms in deep learning is available at
https://github.com/Baijiong-Lin/Awesome-Multi-Objective-Deep-Learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Supervised Iterative Refinement for Anomaly Detection in Industrial
  Quality Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11561v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11561v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Aqeel, Shakiba Sharifi, Marco Cristani, Francesco Setti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces the Iterative Refinement Process (IRP), a robust
anomaly detection methodology designed for high-stakes industrial quality
control. The IRP enhances defect detection accuracy through a cyclic data
refinement strategy, iteratively removing misleading data points to improve
model performance and robustness. We validate the IRP's effectiveness using two
benchmark datasets, Kolektor SDD2 (KSDD2) and MVTec AD, covering a wide range
of industrial products and defect types. Our experimental results demonstrate
that the IRP consistently outperforms traditional anomaly detection models,
particularly in environments with high noise levels. This study highlights the
IRP's potential to significantly enhance anomaly detection processes in
industrial settings, effectively managing the challenges of sparse and noisy
data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to VISAPP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preconditioned Inexact Stochastic ADMM for Deep Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10784v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10784v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenglong Zhou, Ouya Wang, Ziyan Luo, Yongxu Zhu, Geoffrey Ye Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent advancement of foundation models (FMs) has brought about a
paradigm shift, revolutionizing various sectors worldwide. The popular
optimizers used to train these models are stochastic gradient descent-based
algorithms, which face inherent limitations, such as slow convergence and
stringent assumptions for convergence. In particular, data heterogeneity
arising from distributed settings poses significant challenges to their
theoretical and numerical performance. This paper develops an algorithm, PISA
({P}reconditioned {I}nexact {S}tochastic {A}lternating Direction Method of
Multipliers), which enables scalable parallel computing and supports various
second-moment schemes. Grounded in rigorous theoretical guarantees, the
algorithm converges under the sole assumption of Lipschitz continuity of the
gradient, thereby removing the need for other conditions commonly imposed by
stochastic methods. This capability enables PISA to tackle the challenge of
data heterogeneity effectively. Comprehensive experimental evaluations for
training or fine-tuning diverse FMs, including vision models, large language
models, reinforcement learning models, generative adversarial networks, and
recurrent neural networks, demonstrate its superior numerical performance
compared to various state-of-the-art optimizers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gumbel Counterfactual Generation From Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07180v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07180v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shauli Ravfogel, Anej Svete, Vésteinn Snæbjarnarson, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding and manipulating the causal generation mechanisms in language
models is essential for controlling their behavior. Previous work has primarily
relied on techniques such as representation surgery -- e.g., model ablations or
manipulation of linear subspaces tied to specific concepts -- to
\emph{intervene} on these models. To understand the impact of interventions
precisely, it is useful to examine \emph{counterfactuals} -- e.g., how a given
sentence would have appeared had it been generated by the model following a
specific intervention. We highlight that counterfactual reasoning is
conceptually distinct from interventions, as articulated in Pearl's causal
hierarchy. Based on this observation, we propose a framework for generating
true string counterfactuals by reformulating language models as a structural
equation model using the Gumbel-max trick, which we called Gumbel
counterfactual generation. This reformulation allows us to model the joint
distribution over original strings and their counterfactuals resulting from the
same instantiation of the sampling noise. We develop an algorithm based on
hindsight Gumbel sampling that allows us to infer the latent noise variables
and generate counterfactuals of observed strings. Our experiments demonstrate
that the approach produces meaningful counterfactuals while at the same time
showing that commonly used intervention techniques have considerable undesired
side effects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Using High-Level Patterns to Estimate How Humans Predict a Robot will
  Behave 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13533v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13533v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sagar Parekh, Lauren Bramblett, Nicola Bezzo, Dylan P. Losey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans interacting with robots often form predictions of what the robot will
do next. For instance, based on the recent behavior of an autonomous car, a
nearby human driver might predict that the car is going to remain in the same
lane. It is important for the robot to understand the human's prediction for
safe and seamless interaction: e.g., if the autonomous car knows the human
thinks it is not merging -- but the autonomous car actually intends to merge --
then the car can adjust its behavior to prevent an accident. Prior works
typically assume that humans make precise predictions of robot behavior.
However, recent research on human-human prediction suggests the opposite:
humans tend to approximate other agents by predicting their high-level
behaviors. We apply this finding to develop a second-order theory of mind
approach that enables robots to estimate how humans predict they will behave.
To extract these high-level predictions directly from data, we embed the recent
human and robot trajectories into a discrete latent space. Each element of this
latent space captures a different type of behavior (e.g., merging in front of
the human, remaining in the same lane) and decodes into a vector field across
the state space that is consistent with the underlying behavior type. We
hypothesize that our resulting high-level and course predictions of robot
behavior will correspond to actual human predictions. We provide initial
evidence in support of this hypothesis through proof-of-concept simulations,
testing our method's predictions against those of real users, and experiments
on a real-world interactive driving dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kinetix: Investigating the Training of General Agents through Open-Ended
  Physics-Based Control Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23208v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23208v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Matthews, Michael Beukman, Chris Lu, Jakob Foerster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large models trained with self-supervised learning on offline datasets
have shown remarkable capabilities in text and image domains, achieving the
same generalisation for agents that act in sequential decision problems remains
an open challenge. In this work, we take a step towards this goal by
procedurally generating tens of millions of 2D physics-based tasks and using
these to train a general reinforcement learning (RL) agent for physical
control. To this end, we introduce Kinetix: an open-ended space of
physics-based RL environments that can represent tasks ranging from robotic
locomotion and grasping to video games and classic RL environments, all within
a unified framework. Kinetix makes use of our novel hardware-accelerated
physics engine Jax2D that allows us to cheaply simulate billions of environment
steps during training. Our trained agent exhibits strong physical reasoning
capabilities in 2D space, being able to zero-shot solve unseen human-designed
environments. Furthermore, fine-tuning this general agent on tasks of interest
shows significantly stronger performance than training an RL agent *tabula
rasa*. This includes solving some environments that standard RL training
completely fails at. We believe this demonstrates the feasibility of large
scale, mixed-quality pre-training for online RL and we hope that Kinetix will
serve as a useful framework to investigate this further.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 Oral. The first two authors contributed equally. Project
  page located at: https://kinetix-env.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Multi</span>-Modal and <span class="highlight-title">Multi</span>-Attribute Generation of Single Cells with CFGen 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11734v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11734v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Palma, Till Richter, Hanyi Zhang, Manuel Lubetzki, Alexander Tong, Andrea Dittadi, Fabian Theis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative modeling of single-cell RNA-seq data is crucial for tasks like
trajectory inference, batch effect removal, and simulation of realistic
cellular data. However, recent deep generative models simulating synthetic
single cells from noise operate on pre-processed continuous gene expression
approximations, overlooking the discrete nature of single-cell data, which
limits their effectiveness and hinders the incorporation of robust noise
models. Additionally, aspects like controllable multi-modal and multi-label
generation of cellular data remain underexplored. This work introduces CellFlow
for Generation (CFGen), a flow-based conditional generative model that
preserves the inherent discreteness of single-cell data. CFGen generates
whole-genome multi-modal single-cell data reliably, improving the recovery of
crucial biological data characteristics while tackling relevant generative
tasks such as rare cell type augmentation and batch correction. We also
introduce a novel framework for compositional data generation using Flow
Matching. By showcasing CFGen on a diverse set of biological datasets and
settings, we provide evidence of its value to the fields of computational
biology and deep generative models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages, 22 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Fairness in Unsupervised Graph Anomaly Detection through
  Disentanglement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00987v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00987v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjing Chang, Kay Liu, Philip S. Yu, Jianjun Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph anomaly detection (GAD) is increasingly crucial in various
applications, ranging from financial fraud detection to fake news detection.
However, current GAD methods largely overlook the fairness problem, which might
result in discriminatory decisions skewed toward certain demographic groups
defined on sensitive attributes (e.g., gender, religion, ethnicity, etc.). This
greatly limits the applicability of these methods in real-world scenarios in
light of societal and ethical restrictions. To address this critical gap, we
make the first attempt to integrate fairness with utility in GAD
decision-making. Specifically, we devise a novel DisEntangle-based
FairnEss-aware aNomaly Detection framework on the attributed graph, named
DEFEND. DEFEND first introduces disentanglement in GNNs to capture informative
yet sensitive-irrelevant node representations, effectively reducing societal
bias inherent in graph representation learning. Besides, to alleviate
discriminatory bias in evaluating anomalous nodes, DEFEND adopts a
reconstruction-based anomaly detection, which concentrates solely on node
attributes without incorporating any graph structure. Additionally, given the
inherent association between input and sensitive attributes, DEFEND constrains
the correlation between the reconstruction error and the predicted sensitive
attributes. Our empirical evaluations on real-world datasets reveal that DEFEND
performs effectively in GAD and significantly enhances fairness compared to
state-of-the-art baselines. To foster reproducibility, our code is available at
https://github.com/AhaChang/DEFEND.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TMLR. Code available at
  https://github.com/AhaChang/DEFEND</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimizing Backward Policies in GFlowNets via Trajectory Likelihood
  Maximization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15474v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15474v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timofei Gritsaev, Nikita Morozov, Sergey Samsonov, Daniil Tiapkin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Flow Networks (GFlowNets) are a family of generative models that
learn to sample objects with probabilities proportional to a given reward
function. The key concept behind GFlowNets is the use of two stochastic
policies: a forward policy, which incrementally constructs compositional
objects, and a backward policy, which sequentially deconstructs them. Recent
results show a close relationship between GFlowNet training and
entropy-regularized reinforcement learning (RL) problems with a particular
reward design. However, this connection applies only in the setting of a fixed
backward policy, which might be a significant limitation. As a remedy to this
problem, we introduce a simple backward policy optimization algorithm that
involves direct maximization of the value function in an entropy-regularized
Markov Decision Process (MDP) over intermediate rewards. We provide an
extensive experimental evaluation of the proposed approach across various
benchmarks in combination with both RL and GFlowNet algorithms and demonstrate
its faster convergence and mode discovery in complex environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimize Incompatible Parameters through Compatibility-aware Knowledge
  Integration <span class="chip">AAAI'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.07596v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.07596v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheqi Lv, Keming Ye, Zishu Wei, Qi Tian, Shengyu Zhang, Wenqiao Zhang, Wenjie Wang, Kun Kuang, Tat-Seng Chua, Fei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have become foundational to advancements in multiple
domains, including recommendation systems, natural language processing, and so
on. Despite their successes, these models often contain incompatible parameters
that can be underutilized or detrimental to model performance, particularly
when faced with specific, varying data distributions. Existing research excels
in removing such parameters or merging the outputs of multiple different
pretrained models. However, the former focuses on efficiency rather than
performance, while the latter requires several times more computing and storage
resources to support inference. In this paper, we set the goal to explicitly
improve these incompatible parameters by leveraging the complementary strengths
of different models, thereby directly enhancing the models without any
additional parameters. Specifically, we propose Compatibility-aware Knowledge
Integration (CKI), which consists of Parameter Compatibility Assessment and
Parameter Splicing, which are used to evaluate the knowledge content of
multiple models and integrate the knowledge into one model, respectively. The
integrated model can be used directly for inference or for further fine-tuning.
We conduct extensive experiments on various datasets for recommendation and
language tasks, and the results show that Compatibility-aware Knowledge
Integration can effectively optimize incompatible parameters under multiple
tasks and settings to break through the training limit of the original model
without increasing the inference cost.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published on AAAI'25(Oral): The Annual AAAI Conference on Artificial
  Intelligence</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AnyECG: Foundational Models for <span class="highlight-title">Multi</span>task Cardiac Analysis in Real-Wo<span class="highlight-title">rl</span>d
  Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17711v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17711v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Wang, Xu Cao, Yaojun Hu, Haochao Ying, Hongxia Xu, Ruijia Wu, James Matthew Rehg, Jimeng Sun, Jian Wu, Jintai Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Electrocardiogram (ECG), a non-invasive and affordable tool for cardiac
monitoring, is highly sensitive in detecting acute heart attacks. However, due
to the lengthy nature of ECG recordings, numerous machine learning methods have
been developed for automated heart disease detection to reduce human workload.
Despite these efforts, performance remains suboptimal. A key obstacle is the
inherent complexity of ECG data, which includes heterogeneity (e.g., varying
sampling rates), high levels of noise, demographic-related pattern shifts, and
intricate rhythm-event associations. To overcome these challenges, this paper
introduces AnyECG, a foundational model designed to extract robust
representations from any real-world ECG data. Specifically, a tailored ECG
Tokenizer encodes each fixed-duration ECG fragment into a token and, guided by
proxy tasks, converts noisy, continuous ECG features into discrete, compact,
and clinically meaningful local rhythm codes. These codes encapsulate basic
morphological, frequency, and demographic information (e.g., sex), effectively
mitigating signal noise. We further pre-train the AnyECG to learn rhythmic
pattern associations across ECG tokens, enabling the capture of cardiac event
semantics. By being jointly pre-trained on diverse ECG data sources, AnyECG is
capable of generalizing across a wide range of downstream tasks where ECG
signals are recorded from various devices and scenarios. The experimental
results show that AnyECG achieves an average performance improvement of 6%
across four critical tasks-anomaly detection, arrhythmia classification,
corrupted lead generation, and ultra-long ECG recognition. AnyECG learns common
ECG rhythm from data and significantly outperforms state-of-the-art methods in
each of these tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nonasymptotic Analysis of Stochastic Gradient Descent with the
  Richardson-Romberg Extrapolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05106v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05106v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marina Sheshukova, Denis Belomestny, Alain Durmus, Eric Moulines, Alexey Naumov, Sergey Samsonov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the problem of solving strongly convex and smooth minimization
problems using stochastic gradient descent (SGD) algorithm with a constant step
size. Previous works suggested to combine the Polyak-Ruppert averaging
procedure with the Richardson-Romberg extrapolation to reduce the asymptotic
bias of SGD at the expense of a mild increase of the variance. We significantly
extend previous results by providing an expansion of the mean-squared error of
the resulting estimator with respect to the number of iterations $n$. We show
that the root mean-squared error can be decomposed into the sum of two terms: a
leading one of order $\mathcal{O}(n^{-1/2})$ with explicit dependence on a
minimax-optimal asymptotic covariance matrix, and a second-order term of order
$\mathcal{O}(n^{-3/4})$, where the power $3/4$ is best known. We also extend
this result to the higher-order moment bounds. Our analysis relies on the
properties of the SGD iterates viewed as a time-homogeneous Markov chain. In
particular, we establish that this chain is geometrically ergodic with respect
to a suitably defined weighted Wasserstein semimetric.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR-2025, camera-ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry
  Scientific Hypotheses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07076v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07076v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific discovery contributes largely to human society's prosperity, and
recent progress shows that LLMs could potentially catalyze this process.
However, it is still unclear whether LLMs can discover novel and valid
hypotheses in chemistry. In this work, we investigate this central research
question: Can LLMs automatically discover novel and valid chemistry research
hypotheses given only a chemistry research background (consisting of a research
question and/or a background survey), without limitation on the domain of the
research question? After extensive discussions with chemistry experts, we
propose an assumption that a majority of chemistry hypotheses can be resulted
from a research background and several inspirations. With this key insight, we
break the central question into three smaller fundamental questions. In brief,
they are: (1) given a background question, whether LLMs can retrieve good
inspirations; (2) with background and inspirations, whether LLMs can lead to
hypothesis; and (3) whether LLMs can identify good hypotheses to rank them
higher. To investigate these questions, we construct a benchmark consisting of
51 chemistry papers published in Nature, Science, or a similar level in 2024
(all papers are only available online since 2024). Every paper is divided by
chemistry PhD students into three components: background, inspirations, and
hypothesis. The goal is to rediscover the hypothesis, given only the background
and a large randomly selected chemistry literature corpus consisting the ground
truth inspiration papers, with LLMs trained with data up to 2023. We also
develop an LLM-based multi-agent framework that leverages the assumption,
consisting of three stages reflecting the three smaller questions. The proposed
method can rediscover many hypotheses with very high similarity with the ground
truth ones, covering the main innovations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards a General Time Series Anomaly Detector with Adaptive Bottlenecks
  and Dual Adversarial Decoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15273v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15273v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qichao Shentu, Beibu Li, Kai Zhao, Yang Shu, Zhongwen Rao, Lujia Pan, Bin Yang, Chenjuan Guo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Time series anomaly detection plays a vital role in a wide range of
applications. Existing methods require training one specific model for each
dataset, which exhibits limited generalization capability across different
target datasets, hindering anomaly detection performance in various scenarios
with scarce training data. Aiming at this problem, we propose constructing a
general time series anomaly detection model, which is pre-trained on extensive
multi-domain datasets and can subsequently apply to a multitude of downstream
scenarios. The significant divergence of time series data across different
domains presents two primary challenges in building such a general model: (1)
meeting the diverse requirements of appropriate information bottlenecks
tailored to different datasets in one unified model, and (2) enabling
distinguishment between multiple normal and abnormal patterns, both are crucial
for effective anomaly detection in various target scenarios. To tackle these
two challenges, we propose a General time series anomaly Detector with Adaptive
Bottlenecks and Dual Adversarial Decoders (DADA), which enables flexible
selection of bottlenecks based on different data and explicitly enhances clear
differentiation between normal and abnormal series. We conduct extensive
experiments on nine target datasets from different domains. After pre-training
on multi-domain data, DADA, serving as a zero-shot anomaly detector for these
datasets, still achieves competitive or even superior results compared to those
models tailored to each specific dataset. The code is made available at
https://github.com/decisionintelligence/DADA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the 13th International Conference on Learning
  Representations (ICLR 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Learning With Sine-Activated Low-rank Matrices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.19243v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.19243v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiping Ji, Hemanth Saratchandran, Cameron Gordon, Zeyu Zhang, Simon Lucey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-rank decomposition has emerged as a vital tool for enhancing parameter
efficiency in neural network architectures, gaining traction across diverse
applications in machine learning. These techniques significantly lower the
number of parameters, striking a balance between compactness and performance.
However, a common challenge has been the compromise between parameter
efficiency and the accuracy of the model, where reduced parameters often lead
to diminished accuracy compared to their full-rank counterparts. In this work,
we propose a novel theoretical framework that integrates a sinusoidal function
within the low-rank decomposition process. This approach not only preserves the
benefits of the parameter efficiency characteristic of low-rank methods but
also increases the decomposition's rank, thereby enhancing model performance.
Our method proves to be a plug in enhancement for existing low-rank models, as
evidenced by its successful application in Vision Transformers (ViT), Large
Language Models (LLMs), Neural Radiance Fields (NeRF) and 3D shape modelling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally. Paper accepted at ICLR
  2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Meta-Learning Approach to Bayesian Causal Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.16577v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.16577v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anish Dhir, Matthew Ashman, James Requeima, Mark van der Wilk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discovering a unique causal structure is difficult due to both inherent
identifiability issues, and the consequences of finite data. As such,
uncertainty over causal structures, such as those obtained from a Bayesian
posterior, are often necessary for downstream tasks. Finding an accurate
approximation to this posterior is challenging, due to the large number of
possible causal graphs, as well as the difficulty in the subproblem of finding
posteriors over the functional relationships of the causal edges. Recent works
have used meta-learning to view the problem of estimating the maximum
a-posteriori causal graph as supervised learning. Yet, these methods are
limited when estimating the full posterior as they fail to encode key
properties of the posterior, such as correlation between edges and permutation
equivariance with respect to nodes. Further, these methods also cannot reliably
sample from the posterior over causal structures. To address these limitations,
we propose a Bayesian meta learning model that allows for sampling causal
structures from the posterior and encodes these key properties. We compare our
meta-Bayesian causal discovery against existing Bayesian causal discovery
methods, demonstrating the advantages of directly learning a posterior over
causal structure.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Poison-splat: Computation Cost Attack on 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08190v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08190v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiahao Lu, Yifan Zhang, Qiuhong Shen, Xinchao Wang, Shuicheng Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian splatting (3DGS), known for its groundbreaking performance and
efficiency, has become a dominant 3D representation and brought progress to
many 3D vision tasks. However, in this work, we reveal a significant security
vulnerability that has been largely overlooked in 3DGS: the computation cost of
training 3DGS could be maliciously tampered by poisoning the input data. By
developing an attack named Poison-splat, we reveal a novel attack surface where
the adversary can poison the input images to drastically increase the
computation memory and time needed for 3DGS training, pushing the algorithm
towards its worst computation complexity. In extreme cases, the attack can even
consume all allocable memory, leading to a Denial-of-Service (DoS) that
disrupts servers, resulting in practical damages to real-world 3DGS service
vendors. Such a computation cost attack is achieved by addressing a bi-level
optimization problem through three tailored strategies: attack objective
approximation, proxy model rendering, and optional constrained optimization.
These strategies not only ensure the effectiveness of our attack but also make
it difficult to defend with simple defensive measures. We hope the revelation
of this novel attack surface can spark attention to this crucial yet overlooked
vulnerability of 3DGS systems. Our code is available at
https://github.com/jiahaolu97/poison-splat .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025 as a spotlight paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Geometry and Optimization of Polynomial Convolutional Networks <span class="chip">AISTATS 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00722v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00722v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vahid Shahverdi, Giovanni Luca Marchetti, Kathlén Kohn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study convolutional neural networks with monomial activation functions.
Specifically, we prove that their parameterization map is regular and is an
isomorphism almost everywhere, up to rescaling the filters. By leveraging on
tools from algebraic geometry, we explore the geometric properties of the image
in function space of this map - typically referred to as neuromanifold. In
particular, we compute the dimension and the degree of the neuromanifold, which
measure the expressivity of the model, and describe its singularities.
Moreover, for a generic large dataset, we derive an explicit formula that
quantifies the number of critical points arising in the optimization of a
regression loss.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AISTATS 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Federated Temporal Graph Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12343v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12343v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Zhou, Yang Liu, Xianghong Xu, Qian Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal graph clustering is a complex task that involves discovering
meaningful structures in dynamic graphs where relationships and entities change
over time. Existing methods typically require centralized data collection,
which poses significant privacy and communication challenges. In this work, we
introduce a novel Federated Temporal Graph Clustering (FTGC) framework that
enables decentralized training of graph neural networks (GNNs) across multiple
clients, ensuring data privacy throughout the process. Our approach
incorporates a temporal aggregation mechanism to effectively capture the
evolution of graph structures over time and a federated optimization strategy
to collaboratively learn high-quality clustering representations. By preserving
data privacy and reducing communication overhead, our framework achieves
competitive performance on temporal graph datasets, making it a promising
solution for privacy-sensitive, real-world applications involving dynamic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MLOmics: Benchmark for Machine Learning on Cancer <span class="highlight-title">Multi</span>-Omics Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02143v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02143v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziwei Yang, Rikuto Kotoge, Xihao Piao, Zheng Chen, Lingwei Zhu, Peng Gao, Yasuko Matsubara, Yasushi Sakurai, Jimeng Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Framing the investigation of diverse cancers as a machine learning problem
has recently shown significant potential in multi-omics analysis and cancer
research. Empowering these successful machine learning models are the
high-quality training datasets with sufficient data volume and adequate
preprocessing. However, while there exist several public data portals including
The Cancer Genome Atlas (TCGA) multi-omics initiative or open-bases such as the
LinkedOmics, these databases are not off-the-shelf for existing machine
learning models. In this paper we propose MLOmics, an open cancer multi-omics
benchmark aiming at serving better the development and evaluation of
bioinformatics and machine learning models. MLOmics contains 8,314 patient
samples covering all 32 cancer types with four omics types, stratified
features, and extensive baselines. Complementary support for downstream
analysis and bio-knowledge linking are also included to support
interdisciplinary analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Decade's Battle on <span class="highlight-title">Dataset</span> Bias: Are We There Yet? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08632v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08632v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuang Liu, Kaiming He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We revisit the "dataset classification" experiment suggested by Torralba &
Efros (2011) a decade ago, in the new era with large-scale, diverse, and
hopefully less biased datasets as well as more capable neural network
architectures. Surprisingly, we observe that modern neural networks can achieve
excellent accuracy in classifying which dataset an image is from: e.g., we
report 84.7% accuracy on held-out validation data for the three-way
classification problem consisting of the YFCC, CC, and DataComp datasets. Our
further experiments show that such a dataset classifier could learn semantic
features that are generalizable and transferable, which cannot be explained by
memorization. We hope our discovery will inspire the community to rethink
issues involving dataset bias.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ICLR 2025 (Oral Presentation)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal Brain Apoptosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17941v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17941v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyuan Sun, Zheng Fang, Jiaxu Wang, Junjie Jiang, Delei Kong, Chenming Hu, Yuetong Fang, Renjing Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing complexity and parameter count of Convolutional Neural
Networks (CNNs) and Transformers pose challenges in terms of computational
efficiency and resource demands. Pruning has been identified as an effective
strategy to address these challenges by removing redundant elements such as
neurons, channels, or connections, thereby enhancing computational efficiency
without heavily compromising performance. This paper builds on the foundational
work of Optimal Brain Damage (OBD) by advancing the methodology of parameter
importance estimation using the Hessian matrix. Unlike previous approaches that
rely on approximations, we introduce Optimal Brain Apoptosis (OBA), a novel
pruning method that calculates the Hessian-vector product value directly for
each parameter. By decomposing the Hessian matrix across network layers and
identifying conditions under which inter-layer Hessian submatrices are
non-zero, we propose a highly efficient technique for computing the
second-order Taylor expansion of parameters. This approach allows for a more
precise pruning process, particularly in the context of CNNs and Transformers,
as validated in our experiments including VGG19, ResNet32, ResNet50, and
ViT-B/16 on CIFAR10, CIFAR100 and Imagenet datasets. Our code is available at
https://github.com/NEU-REAL/OBA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Iterated $Q$-Network: Beyond One-Step Be<span class="highlight-title">llm</span>an Updates in Deep
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.02107v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.02107v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Théo Vincent, Daniel Palenicek, Boris Belousov, Jan Peters, Carlo D'Eramo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The vast majority of Reinforcement Learning methods is largely impacted by
the computation effort and data requirements needed to obtain effective
estimates of action-value functions, which in turn determine the quality of the
overall performance and the sample-efficiency of the learning procedure.
Typically, action-value functions are estimated through an iterative scheme
that alternates the application of an empirical approximation of the Bellman
operator and a subsequent projection step onto a considered function space. It
has been observed that this scheme can be potentially generalized to carry out
multiple iterations of the Bellman operator at once, benefiting the underlying
learning algorithm. However, till now, it has been challenging to effectively
implement this idea, especially in high-dimensional problems. In this paper, we
introduce iterated $Q$-Network (i-QN), a novel principled approach that enables
multiple consecutive Bellman updates by learning a tailored sequence of
action-value functions where each serves as the target for the next. We show
that i-QN is theoretically grounded and that it can be seamlessly used in
value-based and actor-critic methods. We empirically demonstrate the advantages
of i-QN in Atari $2600$ games and MuJoCo continuous control problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at TMLR: https://openreview.net/forum?id=Lt2H8Bd8jF</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring the Effectiveness of Object-Centric Representations in Visual
  Question Answering: Comparative Insights with Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.15589v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.15589v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Mohammad Karimi Mamaghan, Samuele Papa, Karl Henrik Johansson, Stefan Bauer, Andrea Dittadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object-centric (OC) representations, which model visual scenes as
compositions of discrete objects, have the potential to be used in various
downstream tasks to achieve systematic compositional generalization and
facilitate reasoning. However, these claims have yet to be thoroughly validated
empirically. Recently, foundation models have demonstrated unparalleled
capabilities across diverse domains, from language to computer vision,
positioning them as a potential cornerstone of future research for a wide range
of computational tasks. In this paper, we conduct an extensive empirical study
on representation learning for downstream Visual Question Answering (VQA),
which requires an accurate compositional understanding of the scene. We
thoroughly investigate the benefits and trade-offs of OC models and alternative
approaches including large pre-trained foundation models on both synthetic and
real-world data, ultimately identifying a promising path to leverage the
strengths of both paradigms. The extensiveness of our study, encompassing over
600 downstream VQA models and 15 different types of upstream representations,
also provides several additional insights that we believe will be of interest
to the community at large.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive $Q$-Network: On-the-fly Target Selection for Deep Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16195v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16195v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Théo Vincent, Fabian Wahren, Jan Peters, Boris Belousov, Carlo D'Eramo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Reinforcement Learning (RL) is well known for being highly sensitive to
hyperparameters, requiring practitioners substantial efforts to optimize them
for the problem at hand. This also limits the applicability of RL in real-world
scenarios. In recent years, the field of automated Reinforcement Learning
(AutoRL) has grown in popularity by trying to address this issue. However,
these approaches typically hinge on additional samples to select
well-performing hyperparameters, hindering sample-efficiency and practicality.
Furthermore, most AutoRL methods are heavily based on already existing AutoML
methods, which were originally developed neglecting the additional challenges
inherent to RL due to its non-stationarities. In this work, we propose a new
approach for AutoRL, called Adaptive $Q$-Network (AdaQN), that is tailored to
RL to take into account the non-stationarity of the optimization procedure
without requiring additional samples. AdaQN learns several $Q$-functions, each
one trained with different hyperparameters, which are updated online using the
$Q$-function with the smallest approximation error as a shared target. Our
selection scheme simultaneously handles different hyperparameters while coping
with the non-stationarity induced by the RL optimization procedure and being
orthogonal to any critic-based RL algorithm. We demonstrate that AdaQN is
theoretically sound and empirically validate it in MuJoCo control problems and
Atari $2600$ games, showing benefits in sample-efficiency, overall performance,
robustness to stochasticity and training stability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR https://iclr.cc/virtual/2025/poster/28508</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Offline Model-Based Optimization by Learning to Rank 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11502v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11502v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rong-Xi Tan, Ke Xue, Shen-Huan Lyu, Haopu Shang, Yao Wang, Yaoyuan Wang, Sheng Fu, Chao Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline model-based optimization (MBO) aims to identify a design that
maximizes a black-box function using only a fixed, pre-collected dataset of
designs and their corresponding scores. A common approach in offline MBO is to
train a regression-based surrogate model by minimizing mean squared error (MSE)
and then find the best design within this surrogate model by different
optimizers (e.g., gradient ascent). However, a critical challenge is the risk
of out-of-distribution errors, i.e., the surrogate model may typically
overestimate the scores and mislead the optimizers into suboptimal regions.
Prior works have attempted to address this issue in various ways, such as using
regularization techniques and ensemble learning to enhance the robustness of
the model, but it still remains. In this paper, we argue that regression models
trained with MSE are not well-aligned with the primary goal of offline MBO,
which is to select promising designs rather than to predict their scores
precisely. Notably, if a surrogate model can maintain the order of candidate
designs based on their relative score relationships, it can produce the best
designs even without precise predictions. To validate it, we conduct
experiments to compare the relationship between the quality of the final
designs and MSE, finding that the correlation is really very weak. In contrast,
a metric that measures order-maintaining quality shows a significantly stronger
correlation. Based on this observation, we propose learning a ranking-based
model that leverages learning to rank techniques to prioritize promising
designs based on their relative scores. We show that the generalization error
on ranking loss can be well bounded. Empirical results across diverse tasks
demonstrate the superior performance of our proposed ranking-based models than
twenty existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AI, Meet Human: Learning Paradigms for Hybrid Decision Making Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.06287v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.06287v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clara Punzi, Roberto Pellungrini, Mattia Setzu, Fosca Giannotti, Dino Pedreschi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Everyday we increasingly rely on machine learning models to automate and
support high-stake tasks and decisions. This growing presence means that humans
are now constantly interacting with machine learning-based systems, training
and using models everyday. Several different techniques in computer science
literature account for the human interaction with machine learning systems, but
their classification is sparse and the goals varied. This survey proposes a
taxonomy of Hybrid Decision Making Systems, providing both a conceptual and
technical framework for understanding how current computer science literature
models interaction between humans and machines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Signature Kernel Conditional Independence Tests in Causal Discovery for
  Stochastic Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.18477v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.18477v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Georg Manten, Cecilia Casolo, Emilio Ferrucci, Søren Wengel Mogensen, Cristopher Salvi, Niki Kilbertus
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inferring the causal structure underlying stochastic dynamical systems from
observational data holds great promise in domains ranging from science and
health to finance. Such processes can often be accurately modeled via
stochastic differential equations (SDEs), which naturally imply causal
relationships via "which variables enter the differential of which other
variables". In this paper, we develop conditional independence (CI) constraints
on coordinate processes over selected intervals that are Markov with respect to
the acyclic dependence graph (allowing self-loops) induced by a general SDE
model. We then provide a sound and complete causal discovery algorithm, capable
of handling both fully and partially observed data, and uniquely recovering the
underlying or induced ancestral graph by exploiting time directionality
assuming a CI oracle. Finally, to make our algorithm practically usable, we
also propose a flexible, consistent signature kernel-based CI test to infer
these constraints from data. We extensively benchmark the CI test in isolation
and as part of our causal discovery algorithms, outperforming existing
approaches in SDE models and beyond.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variational Best-of-N Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.06057v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.06057v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Afra Amini, Tim Vieira, Elliott Ash, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Best-of-N (BoN) is a popular and effective algorithm for aligning language
models to human preferences. The algorithm works as follows: at inference time,
N samples are drawn from the language model, and the sample with the highest
reward, as judged by a reward model, is returned as the output. Despite its
effectiveness, BoN is computationally expensive; it reduces sampling throughput
by a factor of N. To make BoN more efficient at inference time, one strategy is
to fine-tune the language model to mimic what BoN does during inference. To
achieve this, we derive the distribution induced by the BoN algorithm. We then
propose to fine-tune the language model to minimize backward KL divergence to
the BoN distribution. Our approach is analogous to mean-field variational
inference and, thus, we term it variational BoN (vBoN). To the extent this
fine-tuning is successful and we end up with a good approximation, we have
reduced the inference cost by a factor of N. Our experiments on controlled
generation and summarization tasks show that BoN is the most effective
alignment method, and our variational approximation to BoN achieves the closest
performance to BoN and surpasses models fine-tuned using the standard
KL-constrained RL objective. In the controlled generation task, vBoN appears
more frequently on the Pareto frontier of reward and KL divergence compared to
other alignment methods. In the summarization task, vBoN achieves high reward
values across various sampling temperatures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FLEXtime: Filterbank learning to explain time series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.05841v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.05841v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thea Brüsch, Kristoffer K. Wickstrøm, Mikkel N. Schmidt, Robert Jenssen, Tommy S. Alstrøm
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art methods for explaining predictions from time series involve
learning an instance-wise saliency mask for each time step; however, many types
of time series are difficult to interpret in the time domain, due to the
inherently complex nature of the data. Instead, we propose to view time series
explainability as saliency maps over interpretable parts, leaning on
established signal processing methodology on signal decomposition.
Specifically, we propose a new method called FLEXtime that uses a bank of
bandpass filters to split the time series into frequency bands. Then, we learn
the combination of these bands that optimally explains the model's prediction.
Our extensive evaluation shows that, on average, FLEXtime outperforms
state-of-the-art explainability methods across a range of datasets. FLEXtime
fills an important gap in the current time series explainability methodology
and is a valuable tool for a wide range of time series such as EEG and audio.
Code will be made available at https://github.com/theabrusch/FLEXtime.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Prompt: Unlocking the Power of Visual Prompt Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18936v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18936v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minh Le, Anh Nguyen, Huy Nguyen, Chau Nguyen, Nhat Ho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Prompt Tuning (VPT) has recently emerged as a powerful method for
adapting pre-trained vision models to downstream tasks. By introducing
learnable prompt tokens as task-specific instructions, VPT effectively guides
pre-trained transformer models with minimal overhead. Despite its empirical
success, a comprehensive theoretical understanding of VPT remains an active
area of research. Building on recent insights into the connection between
mixture of experts and prompt-based approaches, we identify a key limitation in
VPT: the restricted functional expressiveness in prompt formulation. To address
this limitation, we propose Visual Adaptive Prompt Tuning (VAPT), a new
generation of prompts that redefines prompts as adaptive functions of the
input. Our theoretical analysis shows that this simple yet intuitive approach
achieves optimal sample efficiency. Empirical results on VTAB-1K and FGVC
further demonstrate VAPT's effectiveness, with performance gains of 7.34% and
1.04% over fully fine-tuning baselines, respectively. Notably, VAPT also
surpasses VPT by a substantial margin while using fewer parameters. These
results highlight both the effectiveness and efficiency of our method and pave
the way for future research to explore the potential of adaptive prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>57 pages, 10 figures, 18 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HOPE: A Reinforcement Learning-based Hybrid Policy Path Planner for
  Diverse Parking Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20579v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20579v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyang Jiang, Yueyuan Li, Songan Zhang, Siyuan Chen, Chunxiang Wang, Ming Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated parking stands as a highly anticipated application of autonomous
driving technology. However, existing path planning methodologies fall short of
addressing this need due to their incapability to handle the diverse and
complex parking scenarios in reality. While non-learning methods provide
reliable planning results, they are vulnerable to intricate occasions, whereas
learning-based ones are good at exploration but unstable in converging to
feasible solutions. To leverage the strengths of both approaches, we introduce
Hybrid pOlicy Path plannEr (HOPE). This novel solution integrates a
reinforcement learning agent with Reeds-Shepp curves, enabling effective
planning across diverse scenarios. HOPE guides the exploration of the
reinforcement learning agent by applying an action mask mechanism and employs a
transformer to integrate the perceived environmental information with the mask.
To facilitate the training and evaluation of the proposed planner, we propose a
criterion for categorizing the difficulty level of parking scenarios based on
space and obstacle distribution. Experimental results demonstrate that our
approach outperforms typical rule-based algorithms and traditional
reinforcement learning methods, showing higher planning success rates and
generalization across various scenarios. We also conduct real-world experiments
to verify the practicability of HOPE. The code for our solution is openly
available on https://github.com/jiamiya/HOPE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by T-ITS. 11 pages, 5 tables, 6 figures, 2 page appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PnP-Flow: Plug-and-Play Image Restoration with Flow Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02423v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02423v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ségolène Martin, Anne Gagneux, Paul Hagemann, Gabriele Steidl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce Plug-and-Play (PnP) Flow Matching, an algorithm
for solving imaging inverse problems. PnP methods leverage the strength of
pre-trained denoisers, often deep neural networks, by integrating them in
optimization schemes. While they achieve state-of-the-art performance on
various inverse problems in imaging, PnP approaches face inherent limitations
on more generative tasks like inpainting. On the other hand, generative models
such as Flow Matching pushed the boundary in image sampling yet lack a clear
method for efficient use in image restoration. We propose to combine the PnP
framework with Flow Matching (FM) by defining a time-dependent denoiser using a
pre-trained FM model. Our algorithm alternates between gradient descent steps
on the data-fidelity term, reprojections onto the learned FM path, and
denoising. Notably, our method is computationally efficient and
memory-friendly, as it avoids backpropagation through ODEs and trace
computations. We evaluate its performance on denoising, super-resolution,
deblurring, and inpainting tasks, demonstrating superior results compared to
existing PnP algorithms and Flow Matching based state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Meta Curvature-Aware Minimization for Domain Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.11542v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.11542v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyang Chen, Yiwen Ye, Feilong Tang, Yongsheng Pan, Yong Xia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain generalization (DG) aims to enhance the ability of models trained on
source domains to generalize effectively to unseen domains. Recently,
Sharpness-Aware Minimization (SAM) has shown promise in this area by reducing
the sharpness of the loss landscape to obtain more generalized models. However,
SAM and its variants sometimes fail to guide the model toward a flat minimum,
and their training processes exhibit limitations, hindering further
improvements in model generalization. In this paper, we first propose an
improved model training process aimed at encouraging the model to converge to a
flat minima. To achieve this, we design a curvature metric that has a minimal
effect when the model is far from convergence but becomes increasingly
influential in indicating the curvature of the minima as the model approaches a
local minimum. Then we derive a novel algorithm from this metric, called Meta
Curvature-Aware Minimization (MeCAM), to minimize the curvature around the
local minima. Specifically, the optimization objective of MeCAM simultaneously
minimizes the regular training loss, the surrogate gap of SAM, and the
surrogate gap of meta-learning. We provide theoretical analysis on MeCAM's
generalization error and convergence rate, and demonstrate its superiority over
existing DG methods through extensive experiments on five benchmark DG
datasets, including PACS, VLCS, OfficeHome, TerraIncognita, and DomainNet. Code
will be available on GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 5 figures, 17 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Training One-Step Diffusion Models Without Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08005v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08005v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingtian Zhang, Jiajun He, Wenlin Chen, Zijing Ou, José Miguel Hernández-Lobato, Bernhard Schölkopf, David Barber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in one-step generative models typically follow a two-stage
process: first training a teacher diffusion model and then distilling it into a
one-step student model. This distillation process traditionally relies on both
the teacher model's score function to compute the distillation loss and its
weights for student initialization. In this paper, we explore whether one-step
generative models can be trained directly without this distillation process.
First, we show that the teacher's score function is not essential and propose a
family of distillation methods that achieve competitive results without relying
on score estimation. Next, we demonstrate that initialization from teacher
weights is indispensable in successful training. Surprisingly, we find that
this benefit is not due to improved ``input-output" mapping but rather the
learned feature representations, which dominate distillation quality. Our
findings provide a better understanding of the role of initialization in
one-step model training and its impact on distillation quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TAG: A <span class="highlight-title">Decentralized</span> Framework for <span class="highlight-title">Multi</span>-Agent Hierarchical
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.15425v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.15425v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giuseppe Paolo, Abdelhakim Benechehab, Hamza Cherkaoui, Albert Thomas, Balázs Kégl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hierarchical organization is fundamental to biological systems and human
societies, yet artificial intelligence systems often rely on monolithic
architectures that limit adaptability and scalability. Current hierarchical
reinforcement learning (HRL) approaches typically restrict hierarchies to two
levels or require centralized training, which limits their practical
applicability. We introduce TAME Agent Framework (TAG), a framework for
constructing fully decentralized hierarchical multi-agent systems.TAG enables
hierarchies of arbitrary depth through a novel LevelEnv concept, which
abstracts each hierarchy level as the environment for the agents above it. This
approach standardizes information flow between levels while preserving loose
coupling, allowing for seamless integration of diverse agent types. We
demonstrate the effectiveness of TAG by implementing hierarchical architectures
that combine different RL agents across multiple levels, achieving improved
performance over classical multi-agent RL baselines on standard benchmarks. Our
results show that decentralized hierarchical organization enhances both
learning speed and final performance, positioning TAG as a promising direction
for scalable multi-agent systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Slowing Down Forgetting in Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06916v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06916v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pascal Janetzky, Tobias Schlagenhauf, Stefan Feuerriegel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A common challenge in continual learning (CL) is catastrophic forgetting,
where the performance on old tasks drops after new, additional tasks are
learned. In this paper, we propose a novel framework called ReCL to slow down
forgetting in CL. Our framework exploits an implicit bias of gradient-based
neural networks due to which these converge to margin maximization points. Such
convergence points allow us to reconstruct old data from previous tasks, which
we then combine with the current training data. Our framework is flexible and
can be applied on top of existing, state-of-the-art CL methods. We further
demonstrate the performance gain from our framework across a large series of
experiments, including two challenging CL scenarios (class incremental and
domain incremental learning), different datasets (MNIST, CIFAR10,
TinyImagenet), and different network architectures. Across all experiments, we
find large performance gains through ReCL. To the best of our knowledge, our
framework is the first to address catastrophic forgetting by leveraging models
in CL as their own memory buffers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causality Is Key to Understand and Balance <span class="highlight-title">Multi</span>ple Goals in Trustworthy
  ML and Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21123v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21123v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruta Binkyte, Ivaxi Sheth, Zhijing Jin, Mohammad Havaei, Bernhard Schölkopf, Mario Fritz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring trustworthiness in machine learning (ML) systems is crucial as they
become increasingly embedded in high-stakes domains. This paper advocates for
integrating causal methods into machine learning to navigate the trade-offs
among key principles of trustworthy ML, including fairness, privacy,
robustness, accuracy, and explainability. While these objectives should ideally
be satisfied simultaneously, they are often addressed in isolation, leading to
conflicts and suboptimal solutions. Drawing on existing applications of
causality in ML that successfully align goals such as fairness and accuracy or
privacy and robustness, this paper argues that a causal approach is essential
for balancing multiple competing objectives in both trustworthy ML and
foundation models. Beyond highlighting these trade-offs, we examine how
causality can be practically integrated into ML and foundation models, offering
solutions to enhance their reliability and interpretability. Finally, we
discuss the challenges, limitations, and opportunities in adopting causal
frameworks, paving the way for more accountable and ethically sound AI systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MANTRA: The Manifold Triangulations Assemblage 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02392v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02392v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rubén Ballester, Ernst Röell, Daniel Bīn Schmid, Mathieu Alain, Sergio Escalera, Carles Casacuberta, Bastian Rieck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rising interest in leveraging higher-order interactions present in
complex systems has led to a surge in more expressive models exploiting
higher-order structures in the data, especially in topological deep learning
(TDL), which designs neural networks on higher-order domains such as simplicial
complexes. However, progress in this field is hindered by the scarcity of
datasets for benchmarking these architectures. To address this gap, we
introduce MANTRA, the first large-scale, diverse, and intrinsically
higher-order dataset for benchmarking higher-order models, comprising over
43,000 and 250,000 triangulations of surfaces and three-dimensional manifolds,
respectively. With MANTRA, we assess several graph- and simplicial
complex-based models on three topological classification tasks. We demonstrate
that while simplicial complex-based neural networks generally outperform their
graph-based counterparts in capturing simple topological invariants, they also
struggle, suggesting a rethink of TDL. Thus, MANTRA serves as a benchmark for
assessing and advancing topological methods, leading the way for more effective
higher-order models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2025 (https://openreview.net/forum?id=X6y5CC44HM)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attacking Large Language Models with Projected Gradient Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09154v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09154v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Geisler, Tom Wollschläger, M. H. I. Abdalla, Johannes Gasteiger, Stephan Günnemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current LLM alignment methods are readily broken through specifically crafted
adversarial prompts. While crafting adversarial prompts using discrete
optimization is highly effective, such attacks typically use more than 100,000
LLM calls. This high computational cost makes them unsuitable for, e.g.,
quantitative analyses and adversarial training. To remedy this, we revisit
Projected Gradient Descent (PGD) on the continuously relaxed input prompt.
Although previous attempts with ordinary gradient-based attacks largely failed,
we show that carefully controlling the error introduced by the continuous
relaxation tremendously boosts their efficacy. Our PGD for LLMs is up to one
order of magnitude faster than state-of-the-art discrete optimization to
achieve the same devastating attack results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EXACFS -- A CIL Method to mitigate Catastrophic Forgetting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23751v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23751v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S Balasubramanian, M Sai Subramaniam, Sai Sriram Talasu, Yedu Krishna P, Manepalli Pranav Phanindra Sai, Ravi Mukkamala, Darshan Gera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNS) excel at learning from static datasets but
struggle with continual learning, where data arrives sequentially. Catastrophic
forgetting, the phenomenon of forgetting previously learned knowledge, is a
primary challenge. This paper introduces EXponentially Averaged Class-wise
Feature Significance (EXACFS) to mitigate this issue in the class incremental
learning (CIL) setting. By estimating the significance of model features for
each learned class using loss gradients, gradually aging the significance
through the incremental tasks and preserving the significant features through a
distillation loss, EXACFS effectively balances remembering old knowledge
(stability) and learning new knowledge (plasticity). Extensive experiments on
CIFAR-100 and ImageNet-100 demonstrate EXACFS's superior performance in
preserving stability while acquiring plasticity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exact Certification of (Graph) Neural Networks Against Label Poisoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.00537v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.00537v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahalakshmi Sabanayagam, Lukas Gosch, Stephan Günnemann, Debarghya Ghoshdastidar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models are highly vulnerable to label flipping, i.e., the
adversarial modification (poisoning) of training labels to compromise
performance. Thus, deriving robustness certificates is important to guarantee
that test predictions remain unaffected and to understand worst-case robustness
behavior. However, for Graph Neural Networks (GNNs), the problem of certifying
label flipping has so far been unsolved. We change this by introducing an exact
certification method, deriving both sample-wise and collective certificates.
Our method leverages the Neural Tangent Kernel (NTK) to capture the training
dynamics of wide networks enabling us to reformulate the bilevel optimization
problem representing label flipping into a Mixed-Integer Linear Program (MILP).
We apply our method to certify a broad range of GNN architectures in node
classification tasks. Thereby, concerning the worst-case robustness to label
flipping: $(i)$ we establish hierarchies of GNNs on different benchmark graphs;
$(ii)$ quantify the effect of architectural choices such as activations, depth
and skip-connections; and surprisingly, $(iii)$ uncover a novel phenomenon of
the robustness plateauing for intermediate perturbation budgets across all
investigated datasets and architectures. While we focus on GNNs, our
certificates are applicable to sufficiently wide NNs in general through their
NTK. Thus, our work presents the first exact certificate to a poisoning attack
ever derived for neural networks, which could be of independent interest. The
code is available at https://github.com/saper0/qpcert.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a spotlight presentation at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ReFocus: Reinforcing Mid-Frequency and Key-Frequency Modeling for
  <span class="highlight-title">Multi</span>variate Time Series Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16890v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16890v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guoqi Yu, Yaoming Li, Juncheng Wang, Xiaoyu Guo, Angelica I. Aviles-Rivero, Tong Yang, Shujun Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements have progressively incorporated frequency-based
techniques into deep learning models, leading to notable improvements in
accuracy and efficiency for time series analysis tasks. However, the
Mid-Frequency Spectrum Gap in the real-world time series, where the energy is
concentrated at the low-frequency region while the middle-frequency band is
negligible, hinders the ability of existing deep learning models to extract the
crucial frequency information. Additionally, the shared Key-Frequency in
multivariate time series, where different time series share indistinguishable
frequency patterns, is rarely exploited by existing literature. This work
introduces a novel module, Adaptive Mid-Frequency Energy Optimizer, based on
convolution and residual learning, to emphasize the significance of
mid-frequency bands. We also propose an Energy-based Key-Frequency Picking
Block to capture shared Key-Frequency, which achieves superior inter-series
modeling performance with fewer parameters. A novel Key-Frequency Enhanced
Training strategy is employed to further enhance Key-Frequency modeling, where
spectral information from other channels is randomly introduced into each
channel. Our approach advanced multivariate time series forecasting on the
challenging Traffic, ECL, and Solar benchmarks, reducing MSE by 4%, 6%, and 5%
compared to the previous SOTA iTransformer. Code is available at this GitHub
Repository: https://github.com/Levi-Ackman/ReFocus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning-Driven Malware Classification with API Call Sequence
  Analysis and Concept Drift Handling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08679v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08679v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bishwajit Prasad Gond, Durga Prasad Mohapatra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Malware classification in dynamic environments presents a significant
challenge due to concept drift, where the statistical properties of malware
data evolve over time, complicating detection efforts. To address this issue,
we propose a deep learning framework enhanced with a genetic algorithm to
improve malware classification accuracy and adaptability. Our approach
incorporates mutation operations and fitness score evaluations within genetic
algorithms to continuously refine the deep learning model, ensuring robustness
against evolving malware threats. Experimental results demonstrate that this
hybrid method significantly enhances classification performance and
adaptability, outperforming traditional static models. Our proposed approach
offers a promising solution for real-time malware classification in
ever-changing cybersecurity landscapes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Q-Adapter: Customizing Pre-trained <span class="highlight-title">LLM</span>s to New Preferences with
  Forgetting Mitigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.03856v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.03856v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Chen Li, Fuxiang Zhang, Wenjie Qiu, Lei Yuan, Chengxing Jia, Zongzhang Zhang, Yang Yu, Bo An
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs), trained on a large amount of corpus, have
demonstrated remarkable abilities. However, it may not be sufficient to
directly apply open-source LLMs like Llama to certain real-world scenarios,
since most of them are trained for \emph{general} purposes. Thus, the demands
for customizing publicly available LLMs emerge, but are currently
under-studied. In this work, we consider customizing pre-trained LLMs with new
human preferences. Specifically, the LLM should not only meet the new
preference but also preserve its original capabilities after customization.
Drawing inspiration from the observation that human preference can be expressed
as a reward model, we propose to cast LLM customization as optimizing the sum
of two reward functions, one of which (denoted as $r_1$) was used to pre-train
the LLM while the other (denoted as $r_2$) characterizes the new human
preference. The obstacle here is that both reward functions are unknown, making
the application of modern reinforcement learning methods infeasible. Thanks to
the residual Q-learning framework, we can restore the customized LLM with the
pre-trained LLM and the \emph{residual Q-function} without the reward function
$r_1$. Moreover, we find that for a fixed pre-trained LLM, the reward function
$r_2$ can be derived from the residual Q-function, enabling us to directly
learn the residual Q-function from the new human preference data upon the
Bradley-Terry model. We name our method Q-Adapter as it introduces an adapter
module to approximate the residual Q-function for customizing the pre-trained
LLM towards the new preference. Experiments based on the Llama-3.1 model on the
DSP dataset and HH-RLHF dataset illustrate the superior effectiveness of
Q-Adapter on both retaining existing knowledge and learning new preferences.
Code is available at https://github.com/mansicer/Q-Adapter.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera ready version of ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scintillation pulse characterization with spectrum-inspired temporal
  neural networks: case studies on particle detector signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07267v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07267v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengcheng Ai, Xiangming Sun, Zhi Deng, Xinchi Ran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Particle detectors based on scintillators are widely used in high-energy
physics and astroparticle physics experiments, nuclear medicine imaging,
industrial and environmental detection, etc. Precisely extracting scintillation
signal characteristics at the event level is important for these applications,
not only in respect of understanding the scintillator itself, but also kinds
and physical property of incident particles. Recent researches demonstrate
data-driven neural networks surpass traditional statistical methods, especially
when the analytical form of signals is hard to obtain, or noise is significant.
However, most densely connected or convolution-based networks fail to fully
exploit the spectral and temporal structure of scintillation signals, leaving
large space for performance improvement. In this paper, we propose a network
architecture specially tailored for scintillation pulse characterization based
on previous works on time series analysis. The core insight is that, by
directly applying Fast Fourier Transform on original signals and utilizing
different frequency components, the proposed network architecture can serve as
a lightweight and enhanced representation learning backbone. We prove our idea
in two case studies: (a) simulation data generated with the setting of the LUX
dark matter detector, and (b) experimental electrical signals with fast
electronics to emulate scintillation variations for the NICA/MPD calorimeter.
The proposed model achieves significantly better results than the reference
model in literature and densely connected models, and demonstrates higher
cost-efficiency than conventional machine learning methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 14 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SURGE: On the Potential of Large Language Models as General-Purpose
  Surrogate Code Executors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11167v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11167v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bohan Lyu, Siqiao Huang, Zichen Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural surrogate models have emerged as powerful and efficient tools in data
mining. Meanwhile, large language models (LLMs) have demonstrated remarkable
capabilities in code-related tasks. We investigate a novel application: using
LLMs as surrogate models for code execution prediction. Given LLMs' unique
ability to understand and process diverse programs, they present a promising
direction for building general-purpose surrogate models. To systematically
investigate this capability, we introduce SURGE, a comprehensive benchmark with
$1160$ problems covering $8$ key aspects: multi-language programming tasks,
competition-level programming problems, repository-level code analysis,
high-cost scientific computing, time-complexity-intensive algorithms, buggy
code analysis, programs dependent on specific compilers or execution
environments, and formal mathematical proof verification. Through extensive
empirical analysis of $21$ open-source and proprietary LLMs, we examine scaling
laws, data efficiency, and predictive accuracy. Our findings reveal important
insights about the feasibility of LLMs as efficient surrogates for
computational processes, with implications for automated software testing,
program analysis, and computational resource optimization in data mining
applications. Code and dataset are released at
https://github.com/Imbernoulli/SURGE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Preference Optimization through Reward Model Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19316v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19316v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adam Fisch, Jacob Eisenstein, Vicky Zayats, Alekh Agarwal, Ahmad Beirami, Chirag Nagpal, Pete Shaw, Jonathan Berant
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language model (LM) post-training (or alignment) involves maximizing a reward
function that is derived from preference annotations. Direct Preference
Optimization (DPO) is a popular offline alignment method that trains a policy
directly on preference data without the need to train a reward model or apply
reinforcement learning. However, the empirical evidence suggests that DPO
typically assigns implicit rewards that overfit, and trend towards infinite
magnitude. This frequently leads to degenerate policies, sometimes causing even
the probabilities of the preferred generations to go to zero. In this work, we
analyze this phenomenon and use distillation to get a better proxy for the true
preference distribution over generation pairs: we train the LM such that its
induced implicit reward, i.e., the scaled log-likelihood ratio of the model to
the reference model, matches an explicit reward model trained on the preference
data. Moreover, to account for uncertainty in the reward model we are
distilling from, we optimize against a family of reward models that, as a
whole, is likely to include at least one reasonable proxy for the preference
distribution. Our results show that distilling from such a family of reward
models leads to improved robustness to distribution shift in preference
annotations, while preserving the simple supervised nature of DPO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Graph Foundation Models: A Study on the Generalization of
  Positional and Structural Encodings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07407v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07407v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Billy Joe Franks, Moshe Eliasof, Semih Cantürk, Guy Wolf, Carola-Bibiane Schönlieb, Sophie Fellenz, Marius Kloft
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in integrating positional and structural encodings (PSEs)
into graph neural networks (GNNs) have significantly enhanced their performance
across various graph learning tasks. However, the general applicability of
these encodings and their potential to serve as foundational representations
for graphs remain uncertain. This paper investigates the fine-tuning
efficiency, scalability with sample size, and generalization capability of
learnable PSEs across diverse graph datasets. Specifically, we evaluate their
potential as universal pre-trained models that can be easily adapted to new
tasks with minimal fine-tuning and limited data. Furthermore, we assess the
expressivity of the learned representations, particularly, when used to augment
downstream GNNs. We demonstrate through extensive benchmarking and empirical
analysis that PSEs generally enhance downstream models. However, some datasets
may require specific PSE-augmentations to achieve optimal performance.
Nevertheless, our findings highlight their significant potential to become
integral components of future graph foundation models. We provide new insights
into the strengths and limitations of PSEs, contributing to the broader
discourse on foundation models in graph learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at TMLR (https://openreview.net/forum?id=mSoDRZXsqj)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DailyDilemmas: Revealing Value Preferences of <span class="highlight-title">LLM</span>s with Quandaries of
  Daily Life 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02683v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02683v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Ying Chiu, Liwei Jiang, Yejin Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As users increasingly seek guidance from LLMs for decision-making in daily
life, many of these decisions are not clear-cut and depend significantly on the
personal values and ethical standards of people. We present DailyDilemmas, a
dataset of 1,360 moral dilemmas encountered in everyday life. Each dilemma
presents two possible actions, along with affected parties and relevant human
values for each action. Based on these dilemmas, we gather a repository of
human values covering diverse everyday topics, such as interpersonal
relationships, workplace, and environmental issues. With DailyDilemmas, we
evaluate LLMs on these dilemmas to determine what action they will choose and
the values represented by these action choices. Then, we analyze values through
the lens of five theoretical frameworks inspired by sociology, psychology, and
philosophy, including the World Values Survey, Moral Foundations Theory,
Maslow's Hierarchy of Needs, Aristotle's Virtues, and Plutchik's Wheel of
Emotions. For instance, we find LLMs are most aligned with self-expression over
survival in World Values Survey and care over loyalty in Moral Foundations
Theory. Interestingly, we find substantial preference differences in models for
some core values. For example, for truthfulness, Mixtral-8x7B neglects it by
9.7% while GPT-4-turbo selects it by 9.4%. We also study the recent guidance
released by OpenAI (ModelSpec), and Anthropic (Constitutional AI) to understand
how their designated principles reflect their models' actual value
prioritization when facing nuanced moral reasoning in daily-life settings.
Finally, we find that end users cannot effectively steer such prioritization
using system prompts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted into ICLR 2025 (spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Test-Time Compute: from System-1 Thinking to System-2 Thinking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.02497v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.02497v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixin Ji, Juntao Li, Hai Ye, Kaixin Wu, Kai Yao, Jia Xu, Linjian Mo, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable performance of the o1 model in complex reasoning demonstrates
that test-time compute scaling can further unlock the model's potential,
enabling powerful System-2 thinking. However, there is still a lack of
comprehensive surveys for test-time compute scaling. We trace the concept of
test-time compute back to System-1 models. In System-1 models, test-time
compute addresses distribution shifts and improves robustness and
generalization through parameter updating, input modification, representation
editing, and output calibration. In System-2 models, it enhances the model's
reasoning ability to solve complex problems through repeated sampling,
self-correction, and tree search. We organize this survey according to the
trend of System-1 to System-2 thinking, highlighting the key role of test-time
compute in the transition from System-1 models to weak System-2 models, and
then to strong System-2 models. We also point out a few possible future
directions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SheetAgent: Towards A Generalist Agent for Spreadsheet Reasoning and
  Manipulation via Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.03636v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.03636v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibin Chen, Yifu Yuan, Zeyu Zhang, Yan Zheng, Jinyi Liu, Fei Ni, Jianye Hao, Hangyu Mao, Fuzheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spreadsheets are ubiquitous across the World Wide Web, playing a critical
role in enhancing work efficiency across various domains. Large language model
(LLM) has been recently attempted for automatic spreadsheet manipulation but
has not yet been investigated in complicated and realistic tasks where
reasoning challenges exist (e.g., long horizon manipulation with multi-step
reasoning and ambiguous requirements). To bridge the gap with the real-world
requirements, we introduce SheetRM, a benchmark featuring long-horizon and
multi-category tasks with reasoning-dependent manipulation caused by real-life
challenges. To mitigate the above challenges, we further propose SheetAgent, a
novel autonomous agent that utilizes the power of LLMs. SheetAgent consists of
three collaborative modules: Planner, Informer, and Retriever, achieving both
advanced reasoning and accurate manipulation over spreadsheets without human
interaction through iterative task reasoning and reflection. Extensive
experiments demonstrate that SheetAgent delivers 20--40\% pass rate
improvements on multiple benchmarks over baselines, achieving enhanced
precision in spreadsheet manipulation and demonstrating superior table
reasoning abilities. More details and visualizations are available at the
project website: https://sheetagent.github.io/. The datasets and source code
are available at https://anonymous.4open.science/r/SheetAgent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by International World Wide Web Conference (WWW) 2025 (oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Spike<span class="highlight-title">LLM</span>: Scaling up Spiking Neural Network to Large Language Models via
  Saliency-based Spiking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04752v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04752v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingrun Xing, Boyan Gao, Zheng Zhang, David A. Clifton, Shitao Xiao, Li Du, Guoqi Li, Jiajun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) with billions of
parameters have improved performance in various applications, but their
inference processes demand significant energy and computational resources. In
contrast, the human brain, with approximately 86 billion neurons, is much more
energy-efficient than LLMs with similar parameters. Inspired by this, we
redesign 7$\sim$70 billion parameter LLMs using bio-plausible spiking
mechanisms, emulating the efficient behavior of the human brain. We propose the
first spiking large language model, SpikeLLM. Coupled with the proposed model,
two essential approaches are proposed to improve spike training efficiency:
Generalized Integrate-and-Fire (GIF) neurons to compress spike length from $T$
to $\frac{T}{L} \log_2 L$ bits, and an Optimal Brain Spiking framework to
divide outlier channels and allocate different $T$ for GIF neurons, which
further compresses spike length to approximate $log_2T$ bits. The necessity of
spike-driven LLM is proved by comparison with quantized LLMs with similar
operations. In the OmniQuant pipeline, SpikeLLM reduces 11.01% WikiText2
perplexity and improves 2.55% accuracy of common scene reasoning on a LLAMA-7B
W4A4 model. In the GPTQ pipeline, SpikeLLM achieves direct additive in linear
layers, significantly exceeding PB-LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Learning Under Density Shift in Incremental Settings Using
  Cramér-Rao-Based Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12949v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12949v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Behraj Khan, Behroz Mirza, Nouman Durrani, Tahir Syed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The continuous surge in data volume and velocity is often dealt with using
data orchestration and distributed processing approaches, abstracting away the
machine learning challenges that exist at the algorithmic level. With growing
interest in automating the learning loop, training with data that arrive in a
sequence rather than in the classical in-memory training data form will face a
machine learning challenge because of evolving feature distributions across
batches of training data biasing the cross-validation step
(\cite{sugiyama2012machine}). This work takes a distributed density estimation
angle to the problem where data are temporally distributed. It processes data
in batches and allows a neural network to treat a batch as training data. The
method accumulates knowledge about the data density via posterior probability
absorption using the Fisher Information Matrix, which contains information
about the local optimization gradients for the batch. This is then used as a
regularizer for the loss in the following batch, and therefore the density
estimate for the entire dataset constructively gets more robust to the non-iid
distribution shift. This needs the presence of a pair of batches in memory at a
time, so the space cost is not a function of the size of the complete,
distributed dataset. We proposed a novel regularization-based approach
Covariate Shift Correction $C^{2}A$ that leverages Fisher information and
Kullback-Leibler divergence to adapt to both natural and sequential covariate
shift caused by dataset fragmentation. $C^{2}A$ achieves $19\%$ accuracy at
maximum against state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>It is the older version of our this paper arXiv:2502.15756. So this
  is the duplicate older version mistakenly uploaded. There are mistakes in the
  method part of this paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Long-Term EEG Partitioning for Seizure Onset Detection <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.15598v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.15598v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Chen, Yasuko Matsubara, Yasushi Sakurai, Jimeng Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models have recently shown great success in classifying
epileptic patients using EEG recordings. Unfortunately, classification-based
methods lack a sound mechanism to detect the onset of seizure events. In this
work, we propose a two-stage framework, SODor, that explicitly models seizure
onset through a novel task formulation of subsequence clustering. Given an EEG
sequence, the framework first learns a set of second-level embeddings with
label supervision. It then employs model-based clustering to explicitly capture
long-term temporal dependencies in EEG sequences and identify meaningful
subsequences. Epochs within a subsequence share a common cluster assignment
(normal or seizure), with cluster or state transitions representing successful
onset detections. Extensive experiments on three datasets demonstrate that our
method can correct misclassifications, achieving 5\%-11\% classification
improvements over other baselines and accurately detecting seizure onsets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Channel Dimensions to Isolate Outliers for Low-bit Weight
  Quantization of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.15531v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.15531v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jung Hwan Heo, Jeonghoon Kim, Beomseok Kwon, Byeongwook Kim, Se Jung Kwon, Dongsoo Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have recently demonstrated remarkable success
across various tasks. However, efficiently serving LLMs has been a challenge
due to the large memory bottleneck, specifically in small batch inference
settings (e.g. mobile devices). Weight-only quantization can be a promising
approach, but sub-4 bit quantization remains a challenge due to large-magnitude
activation outliers. To mitigate the undesirable outlier effect, we first
propose per-IC quantization, a simple yet effective method that creates
quantization groups within each input channel (IC) rather than the conventional
per-output-channel (per-OC). Our method is motivated by the observation that
activation outliers affect the input dimension of the weight matrix, so
similarly grouping the weights in the IC direction can isolate outliers within
a group. We also find that activation outliers do not dictate quantization
difficulty, and inherent weight sensitivities also exist. With per-IC
quantization as a new outlier-friendly scheme, we propose Adaptive Dimensions
(AdaDim), a versatile quantization framework that can adapt to various weight
sensitivity patterns. We demonstrate the effectiveness of AdaDim by augmenting
prior methods such as Round-To-Nearest and GPTQ, showing significant
improvements across various language modeling benchmarks for both base (up to
+4.7% on MMLU) and instruction-tuned (up to +10% on HumanEval) LLMs. Code is
available at https://github.com/johnheo/adadim-llm
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cross-Spectral Vision Transformer for Biometric Authentication using
  Forehead Subcutaneous Vein Pattern and Periocular Pattern 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.19160v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.19160v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arun K. Sharma, Shubhobrata Bhattacharya, Motahar Reza, Bishakh Bhattacharya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional biometric systems have encountered significant setbacks due to
various unavoidable factors, for example, face recognition-based biometrics
fails due to the wearing of face masks and fingerprints create hygiene
concerns. This paper proposes a novel lightweight cross-spectral vision
transformer (CS-ViT) for biometric authentication using forehead subcutaneous
vein patterns and periocular patterns, offering a promising alternative to
traditional methods, capable of performing well even with the face masks and
without any physical touch. The proposed framework comprises a cross-spectral
dual-channel architecture designed to handle two distinct biometric traits and
to capture inter-dependencies in terms of relative spectral patterns. Each
channel consists of a Phase-Only Correlation Cross-Spectral Attention (POC-CSA)
that captures their individual as well as correlated patterns. The computation
of cross-spectral attention using POC extracts the phase correlation in the
spatial features. Therefore, it is robust against the resolution/intensity
variations and illumination of the input images, assuming both biometric traits
are from the same person. The lightweight model is suitable for edge device
deployment. The performance of the proposed algorithm was rigorously evaluated
using the Forehead Subcutaneous Vein Pattern and Periocular Biometric Pattern
(FSVP-PBP) database. The results demonstrated the superiority of the algorithm
over state-of-the-art methods, achieving a remarkable classification accuracy
of 98.8% with the combined vein and periocular patterns.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE TPAMI</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leray-Schauder Mappings for Operator Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01746v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01746v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emanuele Zappala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an algorithm for learning operators between Banach spaces, based
on the use of Leray-Schauder mappings to learn a finite-dimensional
approximation of compact subspaces. We show that the resulting method is a
universal approximator of (possibly nonlinear) operators. We demonstrate the
efficiency of the approach on two benchmark datasets showing it achieves
results comparable to state of the art models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 2 figures, 1 table. Comments are welcome! v2: Theoretical
  analysis expanded, several explanations regarding the experiments have been
  added for improved clarity</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Representation Engineering: A Top-Down Approach to AI Transparency 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.01405v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.01405v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andy Zou, Long Phan, Sarah Chen, James Campbell, Phillip Guo, Richard Ren, Alexander Pan, Xuwang Yin, Mantas Mazeika, Ann-Kathrin Dombrowski, Shashwat Goel, Nathaniel Li, Michael J. Byun, Zifan Wang, Alex Mallen, Steven Basart, Sanmi Koyejo, Dawn Song, Matt Fredrikson, J. Zico Kolter, Dan Hendrycks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we identify and characterize the emerging area of
representation engineering (RepE), an approach to enhancing the transparency of
AI systems that draws on insights from cognitive neuroscience. RepE places
population-level representations, rather than neurons or circuits, at the
center of analysis, equipping us with novel methods for monitoring and
manipulating high-level cognitive phenomena in deep neural networks (DNNs). We
provide baselines and an initial analysis of RepE techniques, showing that they
offer simple yet effective solutions for improving our understanding and
control of large language models. We showcase how these methods can provide
traction on a wide range of safety-relevant problems, including honesty,
harmlessness, power-seeking, and more, demonstrating the promise of top-down
transparency research. We hope that this work catalyzes further exploration of
RepE and fosters advancements in the transparency and safety of AI systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at
  https://github.com/andyzoujm/representation-engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Geometry-Aware Approaches for Balancing Performance and Theoretical
  Guarantees in Linear Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.14872v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.14872v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuwei Luo, Mohsen Bayati
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper is motivated by recent research in the $d$-dimensional stochastic
linear bandit literature, which has revealed an unsettling discrepancy:
algorithms like Thompson sampling and Greedy demonstrate promising empirical
performance, yet this contrasts with their pessimistic theoretical regret
bounds. The challenge arises from the fact that while these algorithms may
perform poorly in certain problem instances, they generally excel in typical
instances. To address this, we propose a new data-driven technique that tracks
the geometric properties of the uncertainty ellipsoid around the main problem
parameter. This methodology enables us to formulate a data-driven frequentist
regret bound, which incorporates the geometric information, for a broad class
of base algorithms, including Greedy, OFUL, and Thompson sampling. This result
allows us to identify and ``course-correct" problem instances in which the base
algorithms perform poorly. The course-corrected algorithms achieve the minimax
optimal regret of order $\tilde{\mathcal{O}}(d\sqrt{T})$ for a $T$-period
decision-making scenario, effectively maintaining the desirable attributes of
the base algorithms, including their empirical efficacy. We present simulation
results to validate our findings using synthetic and real data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TokenSelect: Efficient Long-Context Inference and Length Extrapolation
  for <span class="highlight-title">LLM</span>s via Dynamic Token-Level KV Cache Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02886v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02886v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Wu, Zhuoshi Pan, Chao Wang, Liyi Chen, Yunchu Bai, Tianfu Wang, Kun Fu, Zheng Wang, Hui Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of Large Language Models (LLMs) has driven growing
demand for processing extended context sequences in contemporary applications.
However, this progress faces two major challenges: performance degradation due
to sequence lengths out-of-distribution, and excessively long inference times
caused by the quadratic computational complexity of attention. These issues
hinder the application of LLMs in long-context scenarios. In this paper, we
propose Dynamic Token-Level KV Cache Selection (TokenSelect), a training-free
method for efficient and accurate long-context inference. TokenSelect builds
upon the observation of non-contiguous attention sparsity, using Query-Key dot
products to measure per-head KV Cache criticality at token-level. By per-head
soft voting mechanism, TokenSelect selectively involves a few critical KV cache
tokens in attention calculation without sacrificing accuracy. To further
accelerate TokenSelect, we design the Selection Cache based on observations of
consecutive Query similarity and implemented efficient dot product kernel,
significantly reducing the overhead. A comprehensive evaluation of TokenSelect
demonstrates up to 23.84x speedup in attention computation and up to 2.28x
acceleration in end-to-end latency, while providing superior performance
compared to state-of-the-art long-context inference methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Speed-accuracy relations for the diffusion models: Wisdom from
  nonequilibrium thermodynamics and optimal transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04495v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04495v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kotaro Ikeda, Tomoya Uda, Daisuke Okanohara, Sosuke Ito
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We discuss a connection between a generative model, called the diffusion
model, and nonequilibrium thermodynamics for the Fokker-Planck equation, called
stochastic thermodynamics. Based on the techniques of stochastic
thermodynamics, we derive the speed-accuracy relations for the diffusion
models, which are inequalities that relate the accuracy of data generation to
the entropy production rate, which can be interpreted as the speed of the
diffusion dynamics in the absence of the non-conservative force. From a
stochastic thermodynamic perspective, our results provide a quantitative
insight into how best to generate data in diffusion models. The optimal
learning protocol is introduced by the geodesic of space of the 2-Wasserstein
distance in optimal transport theory. We numerically illustrate the validity of
the speed-accuracy relations for the diffusion models with different noise
schedules and the different data. We numerically discuss our results for the
optimal and suboptimal learning protocols. We also show the inaccurate data
generation due to the non-conservative force, and the applicability of our
results to data generation from the real-world image datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structural-Entropy-Based Sample Selection for Efficient and Effective
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02268v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02268v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianchi Xie, Jiangning Zhu, Guozu Ma, Minzhi Lin, Wei Chen, Weikai Yang, Shixia Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sample selection improves the efficiency and effectiveness of machine
learning models by providing informative and representative samples. Typically,
samples can be modeled as a sample graph, where nodes are samples and edges
represent their similarities. Most existing methods are based on local
information, such as the training difficulty of samples, thereby overlooking
global information, such as connectivity patterns. This oversight can result in
suboptimal selection because global information is crucial for ensuring that
the selected samples well represent the structural properties of the graph. To
address this issue, we employ structural entropy to quantify global information
and losslessly decompose it from the whole graph to individual nodes using the
Shapley value. Based on the decomposition, we present
$\textbf{S}$tructural-$\textbf{E}$ntropy-based sample $\textbf{S}$election
($\textbf{SES}$), a method that integrates both global and local information to
select informative and representative samples. SES begins by constructing a
$k$NN-graph among samples based on their similarities. It then measures sample
importance by combining structural entropy (global metric) with training
difficulty (local metric). Finally, SES applies importance-biased blue noise
sampling to select a set of diverse and representative samples. Comprehensive
experiments on three learning scenarios -- supervised learning, active
learning, and continual learning -- clearly demonstrate the effectiveness of
our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PATCH: a deep learning method to assess heterogeneity of artistic
  practice in historical paintings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01912v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01912v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Van Horn, Lauryn Smith, Mahamad Mahmoud, Michael McMaster, Clara Pinchbeck, Ina Martin, Andrew Lininger, Anthony Ingrisano, Adam Lowe, Carlos Bayod, Elizabeth Bolman, Kenneth Singer, Michael Hinczewski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The history of art has seen significant shifts in the manner in which
artworks are created, making understanding of creative processes a central
question in technical art history. In the Renaissance and Early Modern period,
paintings were largely produced by master painters directing workshops of
apprentices who often contributed to projects. The masters varied significantly
in artistic and managerial styles, meaning different combinations of artists
and implements might be seen both between masters and within workshops or even
individual canvases. Information on how different workshops were managed and
the processes by which artworks were created remains elusive. Machine learning
methods have potential to unearth new information about artists' creative
processes by extending the analysis of brushwork to a microscopic scale.
Analysis of workshop paintings, however, presents a challenge in that
documentation of the artists and materials involved is sparse, meaning external
examples are not available to train networks to recognize their contributions.
Here we present a novel machine learning approach we call pairwise assignment
training for classifying heterogeneity (PATCH) that is capable of identifying
individual artistic practice regimes with no external training data, or "ground
truth." The method achieves unsupervised results by supervised means, and
outperforms both simple statistical procedures and unsupervised machine
learning methods. We apply this method to two historical paintings by the
Spanish Renaissance master, El Greco: The Baptism of Christ and Christ on the
Cross with Landscape, and our findings regarding the former potentially
challenge previous work that has assigned the painting to workshop members.
Further, the results of our analyses create a measure of heterogeneity of
artistic practice that can be used to characterize artworks across time and
space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>main text: 16 pages, 6 figures; SI: 7 pages, 3 figures; v2: minor
  typo corrections, higher resolution figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Node-Time Conditional Prompt Learning In Dynamic Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13937v8">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13937v8.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingtong Yu, Zhenghao Liu, Xinming Zhang, Yuan Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic graphs capture evolving interactions between entities, such as in
social networks, online learning platforms, and crowdsourcing projects. For
dynamic graph modeling, dynamic graph neural networks (DGNNs) have emerged as a
mainstream technique. However, they are generally pre-trained on the link
prediction task, leaving a significant gap from the objectives of downstream
tasks such as node classification. To bridge the gap, prompt-based learning has
gained traction on graphs, but most existing efforts focus on static graphs,
neglecting the evolution of dynamic graphs. In this paper, we propose
DYGPROMPT, a novel pre-training and prompt learning framework for dynamic graph
modeling. First, we design dual prompts to address the gap in both task
objectives and temporal variations across pre-training and downstream tasks.
Second, we recognize that node and time features mutually characterize each
other, and propose dual condition-nets to model the evolving node-time patterns
in downstream tasks. Finally, we thoroughly evaluate and analyze DYGPROMPT
through extensive experiments on four public datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AdvLogo: Adversarial Patch Attack against Object Detectors based on
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.07002v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.07002v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boming Miao, Chunxiao Li, Yao Zhu, Weixiang Sun, Zizhe Wang, Xiaoyi Wang, Chuanlong Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of deep learning, object detectors have
demonstrated impressive performance; however, vulnerabilities still exist in
certain scenarios. Current research exploring the vulnerabilities using
adversarial patches often struggles to balance the trade-off between attack
effectiveness and visual quality. To address this problem, we propose a novel
framework of patch attack from semantic perspective, which we refer to as
AdvLogo. Based on the hypothesis that every semantic space contains an
adversarial subspace where images can cause detectors to fail in recognizing
objects, we leverage the semantic understanding of the diffusion denoising
process and drive the process to adversarial subareas by perturbing the latent
and unconditional embeddings at the last timestep. To mitigate the distribution
shift that exposes a negative impact on image quality, we apply perturbation to
the latent in frequency domain with the Fourier Transform. Experimental results
demonstrate that AdvLogo achieves strong attack performance while maintaining
high visual quality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative Representational Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.09906v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.09906v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Muennighoff, Hongjin Su, Liang Wang, Nan Yang, Furu Wei, Tao Yu, Amanpreet Singh, Douwe Kiela
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  All text-based language problems can be reduced to either generation or
embedding. Current models only perform well at one or the other. We introduce
generative representational instruction tuning (GRIT) whereby a large language
model is trained to handle both generative and embedding tasks by
distinguishing between them through instructions. Compared to other open
models, our resulting GritLM 7B sets a new state of the art on the Massive Text
Embedding Benchmark (MTEB) and outperforms all models up to its size on a range
of generative tasks. By scaling up further, GritLM 8x7B outperforms all open
generative language models that we tried while still being among the best
embedding models. Notably, we find that GRIT matches training on only
generative or embedding data, thus we can unify both at no performance loss.
Among other benefits, the unification via GRIT speeds up Retrieval-Augmented
Generation (RAG) by > 60% for long documents, by no longer requiring separate
retrieval and generation models. Models, code, etc. are freely available at
https://github.com/ContextualAI/gritlm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>67 pages (16 main), 25 figures, 34 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Calib3D: Calibrating Model Preferences for Reliable 3D Scene
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17010v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17010v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingdong Kong, Xiang Xu, Jun Cen, Wenwei Zhang, Liang Pan, Kai Chen, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safety-critical 3D scene understanding tasks necessitate not only accurate
but also confident predictions from 3D perception models. This study introduces
Calib3D, a pioneering effort to benchmark and scrutinize the reliability of 3D
scene understanding models from an uncertainty estimation viewpoint. We
comprehensively evaluate 28 state-of-the-art models across 10 diverse 3D
datasets, uncovering insightful phenomena that cope with both the aleatoric and
epistemic uncertainties in 3D scene understanding. We discover that despite
achieving impressive levels of accuracy, existing models frequently fail to
provide reliable uncertainty estimates -- a pitfall that critically undermines
their applicability in safety-sensitive contexts. Through extensive analysis of
key factors such as network capacity, LiDAR representations, rasterization
resolutions, and 3D data augmentation techniques, we correlate these aspects
directly with the model calibration efficacy. Furthermore, we introduce DeptS,
a novel depth-aware scaling approach aimed at enhancing 3D model calibration.
Extensive experiments across a wide range of configurations validate the
superiority of our method. We hope this work could serve as a cornerstone for
fostering reliable 3D scene understanding. Code and benchmark toolkit are
publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WACV 2025 Oral; 26 pages, 8 figures, 12 tables; Code at
  https://github.com/ldkong1205/Calib3D</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Federated Learning in Practice: Reflections and Projections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08892v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08892v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katharine Daly, Hubert Eichner, Peter Kairouz, H. Brendan McMahan, Daniel Ramage, Zheng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is a machine learning technique that enables multiple
entities to collaboratively learn a shared model without exchanging their local
data. Over the past decade, FL systems have achieved substantial progress,
scaling to millions of devices across various learning domains while offering
meaningful differential privacy (DP) guarantees. Production systems from
organizations like Google, Apple, and Meta demonstrate the real-world
applicability of FL. However, key challenges remain, including verifying
server-side DP guarantees and coordinating training across heterogeneous
devices, limiting broader adoption. Additionally, emerging trends such as large
(multi-modal) models and blurred lines between training, inference, and
personalization challenge traditional FL frameworks. In response, we propose a
redefined FL framework that prioritizes privacy principles rather than rigid
definitions. We also chart a path forward by leveraging trusted execution
environments and open-source ecosystems to address these challenges and
facilitate future advancements in FL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at 2024 IEEE 6th International Conference on Trust, Privacy
  and Security in Intelligent Systems, and Applications (TPS-ISA)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compositional simulation-based inference for time series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02728v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02728v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Gloeckler, Shoji Toyota, Kenji Fukumizu, Jakob H. Macke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Amortized simulation-based inference (SBI) methods train neural networks on
simulated data to perform Bayesian inference. While this strategy avoids the
need for tractable likelihoods, it often requires a large number of simulations
and has been challenging to scale to time series data. Scientific simulators
frequently emulate real-world dynamics through thousands of single-state
transitions over time. We propose an SBI approach that can exploit such
Markovian simulators by locally identifying parameters consistent with
individual state transitions. We then compose these local results to obtain a
posterior over parameters that align with the entire time series observation.
We focus on applying this approach to neural posterior score estimation but
also show how it can be applied, e.g., to neural likelihood (ratio) estimation.
We demonstrate that our approach is more simulation-efficient than directly
estimating the global posterior on several synthetic benchmark tasks and
simulators used in ecology and epidemiology. Finally, we validate scalability
and simulation efficiency of our approach by applying it to a high-dimensional
Kolmogorov flow simulator with around one million data dimensions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in the proceedings of the Thirteenth International
  Conference on Learning Representations (ICLR 2025), Singapore, 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Direct Distributional Optimization for Provable Alignment of Diffusion
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02954v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02954v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryotaro Kawata, Kazusato Oko, Atsushi Nitanda, Taiji Suzuki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel alignment method for diffusion models from distribution
optimization perspectives while providing rigorous convergence guarantees. We
first formulate the problem as a generic regularized loss minimization over
probability distributions and directly optimize the distribution using the Dual
Averaging method. Next, we enable sampling from the learned distribution by
approximating its score function via Doob's $h$-transform technique. The
proposed framework is supported by rigorous convergence guarantees and an
end-to-end bound on the sampling error, which imply that when the original
distribution's score is known accurately, the complexity of sampling from
shifted distributions is independent of isoperimetric conditions. This
framework is broadly applicable to general distribution optimization problems,
including alignment tasks in Reinforcement Learning with Human Feedback (RLHF),
Direct Preference Optimization (DPO), and Kahneman-Tversky Optimization (KTO).
We empirically validate its performance on synthetic and image datasets using
the DPO objective.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Iterative Nash Policy Optimization: Aligning <span class="highlight-title">LLM</span>s with General
  Preferences via No-Regret Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00617v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00617v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuheng Zhang, Dian Yu, Baolin Peng, Linfeng Song, Ye Tian, Mingyue Huo, Nan Jiang, Haitao Mi, Dong Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning with Human Feedback (RLHF) has achieved great success
in aligning large language models (LLMs) with human preferences. Prevalent RLHF
approaches are reward-based, following the Bradley-Terry (BT) model assumption,
which may not fully capture the complexity of human preferences. In this paper,
we explore RLHF under a general preference framework and approach it from a
game-theoretic perspective. Specifically, we formulate the problem as a
two-player game and propose a novel online algorithm, iterative Nash policy
optimization (INPO). The key idea is to let the policy play against itself via
no-regret learning, thereby approximating the Nash policy. Unlike previous
methods, INPO bypasses the need for estimating the expected win rate for
individual responses, which typically incurs high computational or annotation
costs. Instead, we introduce a new loss objective that is directly minimized
over a preference dataset. We provide theoretical analysis for our approach and
demonstrate its effectiveness through experiments on various representative
benchmarks. With an LLaMA-3-8B-based SFT model, INPO achieves a 42.6%
length-controlled win rate on AlpacaEval 2.0 and a 37.8% win rate on
Arena-Hard, showing substantial improvement over the state-of-the-art online
RLHF algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discovering physical laws with parallel combinatorial tree search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.04405v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.04405v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Ruan, Yilong Xu, Ze-Feng Gao, Yike Guo, Hao Sun, Ji-Rong Wen, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Symbolic regression plays a crucial role in modern scientific research thanks
to its capability of discovering concise and interpretable mathematical
expressions from data. A grand challenge lies in the arduous search for
parsimonious and generalizable mathematical formulas, in an infinite search
space, while intending to fit the training data. Existing algorithms have faced
a critical bottleneck of accuracy and efficiency over a decade when handling
problems of complexity, which essentially hinders the pace of applying symbolic
regression for scientific exploration across interdisciplinary domains. To this
end, we introduce a parallel combinatorial tree search (PCTS) model to
efficiently distill generic mathematical expressions from limited data. Through
a series of extensive experiments, we demonstrate the superior accuracy and
efficiency of PCTS for equation discovery, which greatly outperforms the
state-of-the-art baseline models on over 200 synthetic and experimental
datasets (e.g., lifting its performance by up to 99% accuracy improvement and
one-order of magnitude speed up). PCTS represents a key advance in accurate and
efficient data-driven discovery of symbolic, interpretable models (e.g.,
underlying physical laws) and marks a pivotal transition towards scalable
symbolic learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Learn Weight Generation via Trajectory Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01117v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01117v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunchuan Guan, Yu Liu, Ke Zhou, Zhiqi Shen, Serge Belongie, Jenq-Neng Hwang, Lei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion-based algorithms have emerged as promising techniques for weight
generation, particularly in scenarios like multi-task learning that require
frequent weight updates. However, existing solutions suffer from limited
cross-task transferability. In addition, they only utilize optimal weights as
training samples, ignoring the value of other weights in the optimization
process. To address these issues, we propose Lt-Di, which integrates the
diffusion algorithm with meta-learning to generate weights for unseen tasks.
Furthermore, we extend the vanilla diffusion algorithm into a trajectory
diffusion algorithm to utilize other weights along the optimization trajectory.
Trajectory diffusion decomposes the entire diffusion chain into multiple
shorter ones, improving training and inference efficiency. We analyze the
convergence properties of the weight generation paradigm and improve
convergence efficiency without additional time overhead. Our experiments
demonstrate Lt-Di's higher accuracy while reducing computational overhead
across various tasks, including zero-shot and few-shot learning, multi-domain
generalization, and large-scale language model fine-tuning.Our code is released
at https://anonymous.4open.science/r/Lt-Di-0E51.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Struc2mapGAN: improving synthetic cryo-EM density maps with generative
  adversarial networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.17674v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.17674v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenwei Zhang, Anne Condon, Khanh Dao Duc
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating synthetic cryogenic electron microscopy 3D density maps from
molecular structures has potential important applications in structural
biology. Yet existing simulation-based methods cannot mimic all the complex
features present in experimental maps, such as secondary structure elements. As
an alternative, we propose struc2mapGAN, a novel data-driven method that
employs a generative adversarial network to produce improved experimental-like
density maps from molecular structures. More specifically, struc2mapGAN uses a
nested U-Net architecture as the generator, with an additional L1 loss term and
further processing of raw training experimental maps to enhance learning
efficiency. While struc2mapGAN can promptly generate maps after training, we
demonstrate that it outperforms existing simulation-based methods for a wide
array of tested maps and across various evaluation metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">LLM</span>OPT: Learning to Define and Solve General Optimization Problems from
  Scratch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13213v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13213v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caigao Jiang, Xiang Shu, Hong Qian, Xingyu Lu, Jun Zhou, Aimin Zhou, Yang Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimization problems are prevalent across various scenarios. Formulating and
then solving optimization problems described by natural language often requires
highly specialized human expertise, which could block the widespread
application of optimization-based decision making. To automate problem
formulation and solving, leveraging large language models (LLMs) has emerged as
a potential way. However, this kind of approach suffers from the issue of
optimization generalization. Namely, the accuracy of most current LLM-based
methods and the generality of optimization problem types that they can model
are still limited. In this paper, we propose a unified learning-based framework
called LLMOPT to boost optimization generalization. Starting from the natural
language descriptions of optimization problems and a pre-trained LLM, LLMOPT
constructs the introduced five-element formulation as a universal model for
learning to define diverse optimization problem types. Then, LLMOPT employs the
multi-instruction tuning to enhance both problem formalization and solver code
generation accuracy and generality. After that, to prevent hallucinations in
LLMs, such as sacrificing solving accuracy to avoid execution errors, the model
alignment and self-correction mechanism are adopted in LLMOPT. We evaluate the
optimization generalization ability of LLMOPT and compared methods across six
real-world datasets covering roughly 20 fields such as health, environment,
energy and manufacturing, etc. Extensive experiment results show that LLMOPT is
able to model various optimization problem types such as linear/nonlinear
programming, mixed integer programming, and combinatorial optimization, and
achieves a notable 11.08% average solving accuracy improvement compared with
the state-of-the-art methods. The code is available at
https://github.com/caigaojiang/LLMOPT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TSVD: Bridging Theory and Practice in Continual Learning with
  Pre-trained Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00645v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00645v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangzu Peng, Juan Elenter, Joshua Agterberg, Alejandro Ribeiro, René Vidal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of continual learning (CL) is to train a model that can solve
multiple tasks presented sequentially. Recent CL approaches have achieved
strong performance by leveraging large pre-trained models that generalize well
to downstream tasks. However, such methods lack theoretical guarantees, making
them prone to unexpected failures. Conversely, principled CL approaches often
fail to achieve competitive performance. In this work, we aim to bridge this
gap between theory and practice by designing a simple CL method that is
theoretically sound and highly performant. Specifically, we lift pre-trained
features into a higher dimensional space and formulate an over-parametrized
minimum-norm least-squares problem. We find that the lifted features are highly
ill-conditioned, potentially leading to large training errors (numerical
instability) and increased generalization errors. We address these challenges
by continually truncating the singular value decomposition (SVD) of the lifted
features. Our approach, termed TSVD, is stable with respect to the choice of
hyperparameters, can handle hundreds of tasks, and outperforms state-of-the-art
CL methods on multiple datasets. Importantly, our method satisfies a recurrence
relation throughout its continual learning process, which allows us to prove it
maintains small training and generalization errors by appropriately truncating
a fraction of SVD factors. This results in a stable continual learning method
with strong empirical performance and theoretical guarantees. Code available:
https://github.com/liangzu/tsvd.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>47 pages, 18 figures, 16 tables (v2, accepted to ICLR 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMed-RAG: Versatile <span class="highlight-title">Multi</span>modal RAG System for Medical Vision Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13085v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13085v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Xia, Kangyu Zhu, Haoran Li, Tianze Wang, Weijia Shi, Sheng Wang, Linjun Zhang, James Zou, Huaxiu Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial Intelligence (AI) has demonstrated significant potential in
healthcare, particularly in disease diagnosis and treatment planning. Recent
progress in Medical Large Vision-Language Models (Med-LVLMs) has opened up new
possibilities for interactive diagnostic tools. However, these models often
suffer from factual hallucination, which can lead to incorrect diagnoses.
Fine-tuning and retrieval-augmented generation (RAG) have emerged as methods to
address these issues. However, the amount of high-quality data and distribution
shifts between training data and deployment data limit the application of
fine-tuning methods. Although RAG is lightweight and effective, existing
RAG-based approaches are not sufficiently general to different medical domains
and can potentially cause misalignment issues, both between modalities and
between the model and the ground truth. In this paper, we propose a versatile
multimodal RAG system, MMed-RAG, designed to enhance the factuality of
Med-LVLMs. Our approach introduces a domain-aware retrieval mechanism, an
adaptive retrieved contexts selection method, and a provable RAG-based
preference fine-tuning strategy. These innovations make the RAG process
sufficiently general and reliable, significantly improving alignment when
introducing retrieved contexts. Experimental results across five medical
datasets (involving radiology, ophthalmology, pathology) on medical VQA and
report generation demonstrate that MMed-RAG can achieve an average improvement
of 43.8% in the factual accuracy of Med-LVLMs. Our data and code are available
in https://github.com/richard-peng-xia/MMed-RAG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HORAE: A Domain-Agnostic Modeling Language for Automating <span class="highlight-title">Multi</span>modal
  Service Regulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06600v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06600v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yutao Sun, Mingshuai Chen, Kangjia Zhao, Jintao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial intelligence is rapidly encroaching on the field of service
regulation. This work-in-progress article presents the design principles behind
HORAE, a unified specification language to model multimodal regulation rules
across a diverse set of domains. We show how HORAE facilitates an intelligent
service regulation pipeline by further exploiting a fine-tuned large language
model named HORAE that automates the HORAE modeling process, thereby yielding
an end-to-end framework for fully automated intelligent service regulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling Offline Model-Based <span class="highlight-title">RL</span> via Jointly-Optimized Wo<span class="highlight-title">rl</span>d-Action Model
  Pretraining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00564v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00564v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Cheng, Ruixi Qiao, Yingwei Ma, Binhua Li, Gang Xiong, Qinghai Miao, Yongbin Li, Yisheng Lv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A significant aspiration of offline reinforcement learning (RL) is to develop
a generalist agent with high capabilities from large and heterogeneous
datasets. However, prior approaches that scale offline RL either rely heavily
on expert trajectories or struggle to generalize to diverse unseen tasks.
Inspired by the excellent generalization of world model in conditional video
generation, we explore the potential of image observation-based world model for
scaling offline RL and enhancing generalization on novel tasks. In this paper,
we introduce JOWA: Jointly-Optimized World-Action model, an offline model-based
RL agent pretrained on multiple Atari games with 6 billion tokens data to learn
general-purpose representation and decision-making ability. Our method jointly
optimizes a world-action model through a shared transformer backbone, which
stabilize temporal difference learning with large models during pretraining.
Moreover, we propose a provably efficient and parallelizable planning algorithm
to compensate for the Q-value estimation error and thus search out better
policies. Experimental results indicate that our largest agent, with 150
million parameters, achieves 78.9% human-level performance on pretrained games
using only 10% subsampled offline data, outperforming existing state-of-the-art
large-scale offline RL baselines by 31.6% on averange. Furthermore, JOWA scales
favorably with model capacity and can sample-efficiently transfer to novel
games using only 5k offline fine-tuning data (approximately 4 trajectories) per
game, demonstrating superior generalization. We will release codes and model
weights at https://github.com/CJReinforce/JOWA
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PhyMPGN: Physics-encoded Message Passing Graph Network for
  spatiotemporal PDE systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01337v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01337v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bocheng Zeng, Qi Wang, Mengtao Yan, Yang Liu, Ruizhi Chengze, Yi Zhang, Hongsheng Liu, Zidong Wang, Hao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Solving partial differential equations (PDEs) serves as a cornerstone for
modeling complex dynamical systems. Recent progresses have demonstrated grand
benefits of data-driven neural-based models for predicting spatiotemporal
dynamics (e.g., tremendous speedup gain compared with classical numerical
methods). However, most existing neural models rely on rich training data, have
limited extrapolation and generalization abilities, and suffer to produce
precise or reliable physical prediction under intricate conditions (e.g.,
irregular mesh or geometry, complex boundary conditions, diverse PDE
parameters, etc.). To this end, we propose a new graph learning approach,
namely, Physics-encoded Message Passing Graph Network (PhyMPGN), to model
spatiotemporal PDE systems on irregular meshes given small training datasets.
Specifically, we incorporate a GNN into a numerical integrator to approximate
the temporal marching of spatiotemporal dynamics for a given PDE system.
Considering that many physical phenomena are governed by diffusion processes,
we further design a learnable Laplace block, which encodes the discrete
Laplace-Beltrami operator, to aid and guide the GNN learning in a physically
feasible solution space. A boundary condition padding strategy is also designed
to improve the model convergence and accuracy. Extensive experiments
demonstrate that PhyMPGN is capable of accurately predicting various types of
spatiotemporal dynamics on coarse unstructured meshes, consistently achieves
the state-of-the-art results, and outperforms other baselines with considerable
gains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Closer Look at Machine Unlearning for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.08109v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.08109v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaojian Yuan, Tianyu Pang, Chao Du, Kejiang Chen, Weiming Zhang, Min Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) may memorize sensitive or copyrighted content,
raising privacy and legal concerns. Due to the high cost of retraining from
scratch, researchers attempt to employ machine unlearning to remove specific
content from LLMs while preserving the overall performance. In this paper, we
discuss several issues in machine unlearning for LLMs and provide our insights
on possible approaches. To address the issue of inadequate evaluation of model
outputs after unlearning, we introduce three additional metrics to evaluate
token diversity, sentence semantics, and factual correctness. We then
categorize unlearning methods into untargeted and targeted, and discuss their
issues respectively. Specifically, the behavior that untargeted unlearning
attempts to approximate is unpredictable and may involve hallucinations, and
existing regularization is insufficient for targeted unlearning. To alleviate
these issues, we propose using the objective of maximizing entropy (ME) for
untargeted unlearning and incorporate answer preservation (AP) loss as
regularization for targeted unlearning. Experimental results across three
scenarios, i.e., fictitious unlearning, continual unlearning, and real-world
unlearning, demonstrate the effectiveness of our approaches. The code is
available at https://github.com/sail-sg/closer-look-LLM-unlearning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Lean <span class="highlight-title">Dataset</span> for International Math Olympiad: Small Steps towards
  Writing Math Proofs for Hard Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18872v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18872v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roozbeh Yousefzadeh, Xuenan Cao, Azim Ospanov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using AI to write formal proofs for mathematical problems is a challenging
task that has seen some advancements in recent years. Automated systems such as
Lean can verify the correctness of proofs written in formal language, yet
writing the proofs in formal language can be challenging for humans and
machines. The miniF2F benchmark has 20 IMO problems in its test set, yet formal
proofs are available only for 6 of these problems (3 of which are only written
by mathematicians). The model with best accuracy can only prove 2 of these 20
IMO problems, from 1950s and 60s, while its training set is a secret. In this
work, we write complete, original formal proofs for the remaining IMO problems
in Lean along with 3 extra problems from IMO 2022 and 2023. This effort expands
the availability of proof currently in the public domain by creating 5,880
lines of Lean proof. The goal of the paper is to pave the way for developing AI
models that can automatically write the formal proofs for all the IMO problems
in miniF2F and beyond by providing an evaluation benchmark. In this pursuit, we
devise a method to decompose the proofs of these problems into their building
blocks, constructing a dataset of 1,329 lemmas with more than 40k lines of Lean
code. These lemmas are not trivial, yet they are approachable, providing the
opportunity to evaluate and diagnose the failures and successes of AI models.
We evaluate the ability of the SOTA LLMs on our dataset and analyze their
success and failure modes from different perspectives. Our dataset and code is
available at: https://github.com/roozbeh-yz/IMO-Steps.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Decision-Making in Stochastic Environments through Learned
  Temporal Abstraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21186v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21186v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baiting Luo, Ava Pettet, Aron Laszka, Abhishek Dubey, Ayan Mukhopadhyay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential decision-making in high-dimensional continuous action spaces,
particularly in stochastic environments, faces significant computational
challenges. We explore this challenge in the traditional offline RL setting,
where an agent must learn how to make decisions based on data collected through
a stochastic behavior policy. We present Latent Macro Action Planner (L-MAP),
which addresses this challenge by learning a set of temporally extended
macro-actions through a state-conditional Vector Quantized Variational
Autoencoder (VQ-VAE), effectively reducing action dimensionality. L-MAP employs
a (separate) learned prior model that acts as a latent transition model and
allows efficient sampling of plausible actions. During planning, our approach
accounts for stochasticity in both the environment and the behavior policy by
using Monte Carlo tree search (MCTS). In offline RL settings, including
stochastic continuous control tasks, L-MAP efficiently searches over discrete
latent actions to yield high expected returns. Empirical results demonstrate
that L-MAP maintains low decision latency despite increased action
dimensionality. Notably, across tasks ranging from continuous control with
inherently stochastic dynamics to high-dimensional robotic hand manipulation,
L-MAP significantly outperforms existing model-based methods and performs
on-par with strong model-free actor-critic baselines, highlighting the
effectiveness of the proposed approach in planning in complex and stochastic
environments with high-dimensional action spaces.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR2025. Code would be available at
  https://github.com/BaitingLuo/L-MAP.git</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Feature Learning in Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.01021v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.01021v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andi Han, Wei Huang, Yuan Cao, Difan Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The predominant success of diffusion models in generative modeling has
spurred significant interest in understanding their theoretical foundations. In
this work, we propose a feature learning framework aimed at analyzing and
comparing the training dynamics of diffusion models with those of traditional
classification models. Our theoretical analysis demonstrates that diffusion
models, due to the denoising objective, are encouraged to learn more balanced
and comprehensive representations of the data. In contrast, neural networks
with a similar architecture trained for classification tend to prioritize
learning specific patterns in the data, often focusing on easy-to-learn
components. To support these theoretical insights, we conduct several
experiments on both synthetic and real-world datasets, which empirically
validate our findings and highlight the distinct feature learning dynamics in
diffusion models compared to classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Weighted Point Set Embedding for <span class="highlight-title">Multi</span>modal Contrastive Learning Toward
  Optimal Similarity Metric 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.19228v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.19228v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toshimitsu Uesaka, Taiji Suzuki, Yuhta Takida, Chieh-Hsin Lai, Naoki Murata, Yuki Mitsufuji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In typical multimodal contrastive learning, such as CLIP, encoders produce
one point in the latent representation space for each input. However, one-point
representation has difficulty in capturing the relationship and the similarity
structure of a huge amount of instances in the real world. For richer classes
of the similarity, we propose the use of weighted point sets, namely, sets of
pairs of weight and vector, as representations of instances. In this work, we
theoretically show the benefit of our proposed method through a new
understanding of the contrastive loss of CLIP, which we call symmetric InfoNCE.
We clarify that the optimal similarity that minimizes symmetric InfoNCE is the
pointwise mutual information, and show an upper bound of excess risk on
downstream classification tasks of representations that achieve the optimal
similarity. In addition, we show that our proposed similarity based on weighted
point sets consistently achieves the optimal similarity. To verify the
effectiveness of our proposed method, we demonstrate pretraining of text-image
representation models and classification tasks on common benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Penalized Principal Component Analysis Using Smoothing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2309.13838v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2309.13838v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rebecca M. Hurwitz, Georg Hahn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Principal components computed via PCA (principal component analysis) are
traditionally used to reduce dimensionality in genomic data or to correct for
population stratification. In this paper, we explore the penalized eigenvalue
problem (PEP) which reformulates the computation of the first eigenvector as an
optimization problem and adds an $L_1$ penalty constraint to enforce sparseness
of the solution. The contribution of our article is threefold. First, we extend
PEP by applying smoothing to the original LASSO-type $L_1$ penalty. This allows
one to compute analytical gradients which enable faster and more efficient
minimization of the objective function associated with the optimization
problem. Second, we demonstrate how higher order eigenvectors can be calculated
with PEP using established results from singular value decomposition (SVD).
Third, we present four experimental studies to demonstrate the usefulness of
the smoothed penalized eigenvectors. Using data from the 1000 Genomes Project
dataset, we empirically demonstrate that our proposed smoothed PEP allows one
to increase numerical stability and obtain meaningful eigenvectors. We also
employ the penalized eigenvector approach in two additional real data
applications (computation of a polygenic risk score and clustering),
demonstrating that exchanging the penalized eigenvectors for their smoothed
counterparts can increase prediction accuracy in polygenic risk scores and
enhance discernibility of clusterings. Moreover, we compare our proposed
smoothed PEP to seven state-of-the-art algorithms for sparse PCA and evaluate
the accuracy of the obtained eigenvectors, their support recovery, and their
runtime.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OLMoE: Open Mixture-of-Experts Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.02060v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.02060v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Muennighoff, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Jacob Morrison, Sewon Min, Weijia Shi, Pete Walsh, Oyvind Tafjord, Nathan Lambert, Yuling Gu, Shane Arora, Akshita Bhagia, Dustin Schwenk, David Wadden, Alexander Wettig, Binyuan Hui, Tim Dettmers, Douwe Kiela, Ali Farhadi, Noah A. Smith, Pang Wei Koh, Amanpreet Singh, Hannaneh Hajishirzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce OLMoE, a fully open, state-of-the-art language model leveraging
sparse Mixture-of-Experts (MoE). OLMoE-1B-7B has 7 billion (B) parameters but
uses only 1B per input token. We pretrain it on 5 trillion tokens and further
adapt it to create OLMoE-1B-7B-Instruct. Our models outperform all available
models with similar active parameters, even surpassing larger ones like
Llama2-13B-Chat and DeepSeekMoE-16B. We present various experiments on MoE
training, analyze routing in our model showing high specialization, and
open-source all aspects of our work: model weights, training data, code, and
logs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>63 pages (24 main), 36 figures, 17 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Large Language Model Continual Unlearning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10223v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10223v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chongyang Gao, Lixu Wang, Kaize Ding, Chenkai Weng, Xiao Wang, Qi Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While large language models have demonstrated impressive performance across
various domains and tasks, their security issues have become increasingly
severe. Machine unlearning has emerged as a representative approach for model
safety and security by removing the influence of undesired data on the target
model. However, these methods do not sufficiently consider that unlearning
requests in real-world scenarios are continuously emerging, especially in the
context of LLMs, which may lead to accumulated model utility loss that
eventually becomes unacceptable. Moreover, existing LLM unlearning methods
often ignore previous data access limitations due to privacy concerns and
copyright protection. Without previous data, the utility preservation during
unlearning is much harder. To overcome these challenges, we propose the OOO
framework that includes an Orthogonal low-rank adapter (LoRA) for continually
unlearning requested data and an Out-Of-Distribution (OOD) detector to measure
the similarity between input and unlearning data. The orthogonal LoRA achieves
parameter disentanglement among continual unlearning requests. The OOD detector
is trained with a novel contrastive entropy loss and utilizes a glocal-aware
scoring mechanism. During inference, our OOO framework can decide whether and
to what extent to load the unlearning LoRA based on the OOD detector's
predicted similarity between the input and the unlearned knowledge. Notably,
OOO's effectiveness does not rely on any retained data. We conducted extensive
experiments on OOO and state-of-the-art LLM unlearning methods across three
tasks and seven datasets. The results indicate that OOO consistently achieves
the best unlearning effectiveness and utility preservation, especially when
facing continuous unlearning requests. The source codes can be found at
https://github.com/GCYZSL/O3-LLM-UNLEARNING.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by ICLR 2025. The first two authors
  contribute equally and they are ordered alphabetically</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BECAUSE: Bilinear Causal Representation for Generalizable Offline
  Model-based Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.10967v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.10967v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haohong Lin, Wenhao Ding, Jian Chen, Laixi Shi, Jiacheng Zhu, Bo Li, Ding Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline model-based reinforcement learning (MBRL) enhances data efficiency by
utilizing pre-collected datasets to learn models and policies, especially in
scenarios where exploration is costly or infeasible. Nevertheless, its
performance often suffers from the objective mismatch between model and policy
learning, resulting in inferior performance despite accurate model predictions.
This paper first identifies the primary source of this mismatch comes from the
underlying confounders present in offline data for MBRL. Subsequently, we
introduce \textbf{B}ilin\textbf{E}ar \textbf{CAUS}al
r\textbf{E}presentation~(BECAUSE), an algorithm to capture causal
representation for both states and actions to reduce the influence of the
distribution shift, thus mitigating the objective mismatch problem.
Comprehensive evaluations on 18 tasks that vary in data quality and environment
context demonstrate the superior performance of BECAUSE over existing offline
RL algorithms. We show the generalizability and robustness of BECAUSE under
fewer samples or larger numbers of confounders. Additionally, we offer
theoretical analysis of BECAUSE to prove its error bound and sample efficiency
when integrating causal representation into offline MBRL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RobotFingerPrint: Unified Gripper Coordinate Space for <span class="highlight-title">Multi</span>-Gripper
  Grasp Synthesis and Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14519v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14519v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ninad Khargonkar, Luis Felipe Casas, Balakrishnan Prabhakaran, Yu Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel grasp representation named the Unified Gripper
Coordinate Space (UGCS) for grasp synthesis and grasp transfer. Our
representation leverages spherical coordinates to create a shared coordinate
space across different robot grippers, enabling it to synthesize and transfer
grasps for both novel objects and previously unseen grippers. The strength of
this representation lies in the ability to map palm and fingers of a gripper
and the unified coordinate space. Grasp synthesis is formulated as predicting
the unified spherical coordinates on object surface points via a conditional
variational autoencoder. The predicted unified gripper coordinates establish
exact correspondences between the gripper and object points, which is used to
optimize grasp pose and joint values. Grasp transfer is facilitated through the
point-to-point correspondence between any two (potentially unseen) grippers and
solved via a similar optimization. Extensive simulation and real-world
experiments showcase the efficacy of the unified grasp representation for grasp
synthesis in generating stable and diverse grasps. Similarly, we showcase
real-world grasp transfer from human demonstrations across different objects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 11 figures, 3 tables. Project page available at
  https://irvlutd.github.io/RobotFingerPrint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Labyrinth of Links: Navigating the Associative Maze of <span class="highlight-title">Multi</span>-modal
  <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01417v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01417v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong Li, Nanxi Li, Yuanjie Chen, Jianbin Zhu, Qinlu Guo, Cewu Lu, Yong-Lu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal Large Language Models (MLLMs) have exhibited impressive
capability. However, recently many deficiencies of MLLMs have been found
compared to human intelligence, $\textit{e.g.}$, hallucination. To drive the
MLLMs study, the community dedicated efforts to building larger benchmarks with
complex tasks. In this paper, we propose benchmarking an essential but usually
overlooked intelligence: $\textbf{association}$, a human's basic capability to
link observation and prior practice memory. To comprehensively investigate
MLLM's performance on the association, we formulate the association task and
devise a standard benchmark based on adjective and verb semantic concepts.
Instead of costly data annotation and curation, we propose a convenient
$\textbf{annotation-free}$ construction method transforming the general dataset
for our association tasks. Simultaneously, we devise a rigorous data refinement
process to eliminate confusion in the raw dataset. Building on this database,
we establish three levels of association tasks: single-step, synchronous, and
asynchronous associations. Moreover, we conduct a comprehensive investigation
into the MLLMs' zero-shot association capabilities, addressing multiple
dimensions, including three distinct memory strategies, both open-source and
closed-source MLLMs, cutting-edge Mixture-of-Experts (MoE) models, and the
involvement of human experts. Our systematic investigation shows that current
open-source MLLMs consistently exhibit poor capability in our association
tasks, even the currently state-of-the-art GPT-4V(vision) also has a
significant gap compared to humans. We believe our benchmark would pave the way
for future MLLM studies. $\textit{Our data and code are available at:}$
https://mvig-rhos.com/llm_inception.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025. Project page:
  https://mvig-rhos.com/llm_inception</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NL2FOL: Translating Natural Language to First-Order Logic for Logical
  Fallacy Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.02318v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.02318v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhinav Lalwani, Tasha Kim, Lovish Chopra, Christopher Hahn, Zhijing Jin, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Translating natural language into formal language such as First-Order Logic
(FOL) is a foundational challenge in NLP with wide-ranging applications in
automated reasoning, misinformation tracking, and knowledge validation. In this
paper, we introduce Natural Language to First-Order Logic (NL2FOL), a framework
to autoformalize natural language to FOL step by step using Large Language
Models (LLMs). Our approach addresses key challenges in this translation
process, including the integration of implicit background knowledge. By
leveraging structured representations generated by NL2FOL, we use
Satisfiability Modulo Theory (SMT) solvers to reason about the logical validity
of natural language statements. We present logical fallacy detection as a case
study to evaluate the efficacy of NL2FOL. Being neurosymbolic, our approach
also provides interpretable insights into the reasoning process and
demonstrates robustness without requiring model fine-tuning or labeled training
data. Our framework achieves strong performance on multiple datasets. On the
LOGIC dataset, NL2FOL achieves an F1-score of 78%, while generalizing
effectively to the LOGICCLIMATE dataset with an F1-score of 80%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Identifying Drift, Diffusion, and Causal Structure from Temporal
  Snapshots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.22729v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.22729v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Guan, Joseph Janssen, Hossein Rahmani, Andrew Warren, Stephen Zhang, Elina Robeva, Geoffrey Schiebinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic differential equations (SDEs) are a fundamental tool for modelling
dynamic processes, including gene regulatory networks (GRNs), contaminant
transport, financial markets, and image generation. However, learning the
underlying SDE from data is a challenging task, especially if individual
trajectories are not observable. Motivated by burgeoning research in
single-cell datasets, we present the first comprehensive approach for jointly
identifying the drift and diffusion of an SDE from its temporal marginals.
Assuming linear drift and additive diffusion, we prove that these parameters
are identifiable from marginals if and only if the initial distribution lacks
any generalized rotational symmetries. We further prove that the causal graph
of any SDE with additive diffusion can be recovered from the SDE parameters. To
complement this theory, we adapt entropy-regularized optimal transport to
handle anisotropic diffusion, and introduce APPEX (Alternating Projection
Parameter Estimation from $X_0$), an iterative algorithm designed to estimate
the drift, diffusion, and causal graph of an additive noise SDE, solely from
temporal marginals. We show that APPEX iteratively decreases Kullback-Leibler
divergence to the true solution, and demonstrate its effectiveness on simulated
data from linear additive noise SDEs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Does SGD really happen in tiny subspaces? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16002v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16002v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minhak Song, Kwangjun Ahn, Chulhee Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the training dynamics of deep neural networks is challenging
due to their high-dimensional nature and intricate loss landscapes. Recent
studies have revealed that, along the training trajectory, the gradient
approximately aligns with a low-rank top eigenspace of the training loss
Hessian, referred to as the dominant subspace. Given this alignment, this paper
explores whether neural networks can be trained within the dominant subspace,
which, if feasible, could lead to more efficient training methods. Our primary
observation is that when the SGD update is projected onto the dominant
subspace, the training loss does not decrease further. This suggests that the
observed alignment between the gradient and the dominant subspace is spurious.
Surprisingly, projecting out the dominant subspace proves to be just as
effective as the original update, despite removing the majority of the original
update component. We observe similar behavior across practical setups,
including the large learning rate regime (also known as Edge of Stability),
Sharpness-Aware Minimization, momentum, and adaptive optimizers. We discuss the
main causes and implications of this spurious alignment, shedding light on the
dynamics of neural network training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asymptotic Behavior of Adversarial Training Estimator under
  $\ell_\infty$-Perturbation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.15262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.15262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiling Xie, Xiaoming Huo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial training has been proposed to protect machine learning models
against adversarial attacks. This paper focuses on adversarial training under
$\ell_\infty$-perturbation, which has recently attracted much research
attention. The asymptotic behavior of the adversarial training estimator is
investigated in the generalized linear model. The results imply that the
asymptotic distribution of the adversarial training estimator under
$\ell_\infty$-perturbation could put a positive probability mass at $0$ when
the true parameter is $0$, providing a theoretical guarantee of the associated
sparsity-recovery ability. Alternatively, a two-step procedure is proposed --
adaptive adversarial training, which could further improve the performance of
adversarial training under $\ell_\infty$-perturbation. Specifically, the
proposed procedure could achieve asymptotic variable-selection consistency and
unbiasedness. Numerical experiments are conducted to show the sparsity-recovery
ability of adversarial training under $\ell_\infty$-perturbation and to compare
the empirical performance between classic adversarial training and adaptive
adversarial training.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FoodM<span class="highlight-title">LLM</span>-JP: Leveraging <span class="highlight-title">Multi</span>modal Large Language Models for Japanese
  Recipe Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18459v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18459v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuki Imajuku, Yoko Yamakata, Kiyoharu Aizawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research on food image understanding using recipe data has been a
long-standing focus due to the diversity and complexity of the data. Moreover,
food is inextricably linked to people's lives, making it a vital research area
for practical applications such as dietary management. Recent advancements in
Multimodal Large Language Models (MLLMs) have demonstrated remarkable
capabilities, not only in their vast knowledge but also in their ability to
handle languages naturally. While English is predominantly used, they can also
support multiple languages including Japanese. This suggests that MLLMs are
expected to significantly improve performance in food image understanding
tasks. We fine-tuned open MLLMs LLaVA-1.5 and Phi-3 Vision on a Japanese recipe
dataset and benchmarked their performance against the closed model GPT-4o. We
then evaluated the content of generated recipes, including ingredients and
cooking procedures, using 5,000 evaluation samples that comprehensively cover
Japanese food culture. Our evaluation demonstrates that the open models trained
on recipe data outperform GPT-4o, the current state-of-the-art model, in
ingredient generation. Our model achieved F1 score of 0.531, surpassing
GPT-4o's F1 score of 0.481, indicating a higher level of accuracy. Furthermore,
our model exhibited comparable performance to GPT-4o in generating cooking
procedure text.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 5 figures. We found errors in the calculation of evaluation
  metrics, which were corrected in this version with
  $\color{blue}{\text{modifications highlighted in blue}}$. Please also see the
  Appendix</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Robotics <span class="chip" style="font-size: 60%">67</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NavG: Risk-Aware Navigation in Crowded Environments Based on
  Reinforcement Learning with Guidance Points 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02111v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02111v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianyi Zhang, Wentao Luo, Boyi Liu, Ziyang Zhang, Yaoyuan Wang, Jingtai Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motion planning in navigation systems is highly susceptible to upstream
perceptual errors, particularly in human detection and tracking. To mitigate
this issue, the concept of guidance points--a novel directional cue within a
reinforcement learning-based framework--is introduced. A structured method for
identifying guidance points is developed, consisting of obstacle boundary
extraction, potential guidance point detection, and redundancy elimination. To
integrate guidance points into the navigation pipeline, a
perception-to-planning mapping strategy is proposed, unifying guidance points
with other perceptual inputs and enabling the RL agent to effectively leverage
the complementary relationships among raw laser data, human detection and
tracking, and guidance points. Qualitative and quantitative simulations
demonstrate that the proposed approach achieves the highest success rate and
near-optimal travel times, greatly improving both safety and efficiency.
Furthermore, real-world experiments in dynamic corridors and lobbies validate
the robot's ability to confidently navigate around obstacles and robustly avoid
pedestrians.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Balancing Act: Trading Off Doppler Odometry and Map Registration for
  Efficient Lidar Localization <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02107v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02107v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katya M. Papais, Daniil Lisus, David J. Yoon, Andrew Lambert, Keith Y. K. Leung, Timothy D. Barfoot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most autonomous vehicles rely on accurate and efficient localization, which
is achieved by comparing live sensor data to a preexisting map, to navigate
their environment. Balancing the accuracy of localization with computational
efficiency remains a significant challenge, as high-accuracy methods often come
with higher computational costs. In this paper, we present two ways of
improving lidar localization efficiency and study their impact on performance.
First, we integrate a lightweight Doppler-based odometry method into a
topometric localization pipeline and compare its performance against an
iterative closest point (ICP)-based method. We highlight the trade-offs between
these approaches: the Doppler estimator offers faster, lightweight updates,
while ICP provides higher accuracy at the cost of increased computational load.
Second, by controlling the frequency of localization updates and leveraging
odometry estimates between them, we demonstrate that accurate localization can
be maintained while optimizing for computational efficiency using either
odometry method. Our experimental results show that localizing every 10 lidar
frames strikes a favourable balance, achieving a localization accuracy below
0.05 meters in translation and below 0.1 degrees in orientation while reducing
computational effort by over 30% in an ICP-based pipeline. We quantify the
trade-off of accuracy to computational effort using over 100 kilometers of
real-world driving data in different on-road environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures, 2 tables, submitted to IEEE/RSJ International
  Conference on Intelligent Robots and Systems (IROS) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OVAMOS: A Framework for Open-Vocabulary <span class="highlight-title">Multi</span>-Object Search in Unknown
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02106v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02106v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianwei Wang, Yifan Xu, Vineet Kamat, Carol Menassa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Object search is a fundamental task for robots deployed in indoor building
environments, yet challenges arise due to observation instability, especially
for open-vocabulary models. While foundation models (LLMs/VLMs) enable
reasoning about object locations even without direct visibility, the ability to
recover from failures and replan remains crucial. The Multi-Object Search (MOS)
problem further increases complexity, requiring the tracking multiple objects
and thorough exploration in novel environments, making observation uncertainty
a significant obstacle. To address these challenges, we propose a framework
integrating VLM-based reasoning, frontier-based exploration, and a Partially
Observable Markov Decision Process (POMDP) framework to solve the MOS problem
in novel environments. VLM enhances search efficiency by inferring
object-environment relationships, frontier-based exploration guides navigation
in unknown spaces, and POMDP models observation uncertainty, allowing recovery
from failures in occlusion and cluttered environments. We evaluate our
framework on 120 simulated scenarios across several Habitat-Matterport3D (HM3D)
scenes and a real-world robot experiment in a 50-square-meter office,
demonstrating significant improvements in both efficiency and success rate over
baseline methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 4 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Augmentation for NeRFs in the Low Data Limit <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02092v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02092v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayush Gaggar, Todd D. Murphey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current methods based on Neural Radiance Fields fail in the low data limit,
particularly when training on incomplete scene data. Prior works augment
training data only in next-best-view applications, which lead to hallucinations
and model collapse with sparse data. In contrast, we propose adding a set of
views during training by rejection sampling from a posterior uncertainty
distribution, generated by combining a volumetric uncertainty estimator with
spatial coverage. We validate our results on partially observed scenes; on
average, our method performs 39.9% better with 87.5% less variability across
established scene reconstruction benchmarks, as compared to state of the art
baselines. We further demonstrate that augmenting the training set by sampling
from any distribution leads to better, more consistent scene reconstruction in
sparse environments. This work is foundational for robotic tasks where
augmenting a dataset with informative data is critical in resource-constrained,
a priori unknown environments. Videos and source code are available at
https://murpheylab.github.io/low-data-nerf/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in 2025 IEEE International Conference on Robotics and
  Automation (ICRA 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncertainty Representation in a SOTIF-Related Use Case with
  Dempster-Shafer Theory for LiDAR Sensor-Based Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milin Patel, Rolf Jung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncertainty in LiDAR sensor-based object detection arises from environmental
variability and sensor performance limitations. Representing these
uncertainties is essential for ensuring the Safety of the Intended
Functionality (SOTIF), which focuses on preventing hazards in automated driving
scenarios. This paper presents a systematic approach to identifying,
classifying, and representing uncertainties in LiDAR-based object detection
within a SOTIF-related scenario. Dempster-Shafer Theory (DST) is employed to
construct a Frame of Discernment (FoD) to represent detection outcomes.
Conditional Basic Probability Assignments (BPAs) are applied based on
dependencies among identified uncertainty sources. Yager's Rule of Combination
is used to resolve conflicting evidence from multiple sources, providing a
structured framework to evaluate uncertainties' effects on detection accuracy.
The study applies variance-based sensitivity analysis (VBSA) to quantify and
prioritize uncertainties, detailing their specific impact on detection
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted as extended paper of Vehicle Technology and Intelligent
  Transport Systems (VEHITS)2024 conference and will be published by Springer
  in a CCIS Series book later in 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CorrA: Leveraging Large Language Models for Dynamic Obstacle Avoidance
  of Autonomous Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02076v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02076v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shanting Wang, Panagiotis Typaldos, Andreas A. Malikopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present Corridor-Agent (CorrA), a framework that integrates
large language models (LLMs) with model predictive control (MPC) to address the
challenges of dynamic obstacle avoidance in autonomous vehicles. Our approach
leverages LLM reasoning ability to generate appropriate parameters for
sigmoid-based boundary functions that define safe corridors around obstacles,
effectively reducing the state-space of the controlled vehicle. The proposed
framework adjusts these boundaries dynamically based on real-time vehicle data
that guarantees collision-free trajectories while also ensuring both
computational efficiency and trajectory optimality. The problem is formulated
as an optimal control problem and solved with differential dynamic programming
(DDP) for constrained optimization, and the proposed approach is embedded
within an MPC framework. Extensive simulation and real-world experiments
demonstrate that the proposed framework achieves superior performance in
maintaining safety and efficiency in complex, dynamic environments compared to
a baseline MPC approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Active Alignments of Lens Systems with Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthias Burkhardt, Tobias Schmähling, Michael Layh, Tobias Windisch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning a lens system relative to an imager is a critical challenge in
camera manufacturing. While optimal alignment can be mathematically computed
under ideal conditions, real-world deviations caused by manufacturing
tolerances often render this approach impractical. Measuring these tolerances
can be costly or even infeasible, and neglecting them may result in suboptimal
alignments. We propose a reinforcement learning (RL) approach that learns
exclusively in the pixel space of the sensor output, eliminating the need to
develop expert-designed alignment concepts. We conduct an extensive benchmark
study and show that our approach surpasses other methods in speed, precision,
and robustness. We further introduce relign, a realistic, freely explorable,
open-source simulation utilizing physically based rendering that models optical
systems with non-deterministic manufacturing tolerances and noise in robotic
alignment movement. It provides an interface to popular machine learning
frameworks, enabling seamless experimentation and development. Our work
highlights the potential of RL in a manufacturing environment to enhance
efficiency of optical alignments while minimizing the need for manual
intervention.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Constraint-Based Modeling of Dynamic Entities in 3D Scene Graphs for
  Robust <span class="highlight-title">SLAM</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Giberna, Muhammad Shaheer, Hriday Bavle, Jose Andres Millan-Romera, Jose Luis Sanchez-Lopez, Holger Voos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous robots depend crucially on their ability to perceive and process
information from dynamic, ever-changing environments. Traditional simultaneous
localization and mapping (SLAM) approaches struggle to maintain consistent
scene representations because of numerous moving objects, often treating
dynamic elements as outliers rather than explicitly modeling them in the scene
representation. In this paper, we present a novel hierarchical 3D scene
graph-based SLAM framework that addresses the challenge of modeling and
estimating the pose of dynamic objects and agents. We use fiducial markers to
detect dynamic entities and to extract their attributes while improving
keyframe selection and implementing new capabilities for dynamic entity
mapping. We maintain a hierarchical representation where dynamic objects are
registered in the SLAM graph and are constrained with robot keyframes and the
floor level of the building with our novel entity-keyframe constraints and
intra-entity constraints. By combining semantic and geometric constraints
between dynamic entities and the environment, our system jointly optimizes the
SLAM graph to estimate the pose of the robot and various dynamic agents and
objects while maintaining an accurate map. Experimental evaluation demonstrates
that our approach achieves a 27.57% reduction in pose estimation error compared
to traditional methods and enables higher-level reasoning about scene dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FRMD: Fast Robot Motion Diffusion with Consistency-Distilled Movement
  Primitives for Smooth Action Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02048v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02048v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xirui Shi, Jun Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of using diffusion models to generate fast, smooth,
and temporally consistent robot motions. Although diffusion models have
demonstrated superior performance in robot learning due to their task
scalability and multi-modal flexibility, they suffer from two fundamental
limitations: (1) they often produce non-smooth, jerky motions due to their
inability to capture temporally consistent movement dynamics, and (2) their
iterative sampling process incurs prohibitive latency for many robotic tasks.
Inspired by classic robot motion generation methods such as DMPs and ProMPs,
which capture temporally and spatially consistent dynamic of trajectories using
low-dimensional vectors -- and by recent advances in diffusion-based image
generation that use consistency models with probability flow ODEs to accelerate
the denoising process, we propose Fast Robot Motion Diffusion (FRMD). FRMD
uniquely integrates Movement Primitives (MPs) with Consistency Models to enable
efficient, single-step trajectory generation. By leveraging probabilistic flow
ODEs and consistency distillation, our method models trajectory distributions
while learning a compact, time-continuous motion representation within an
encoder-decoder architecture. This unified approach eliminates the slow,
multi-step denoising process of conventional diffusion models, enabling
efficient one-step inference and smooth robot motion generation. We extensively
evaluated our FRMD on the well-recognized Meta-World and ManiSkills Benchmarks,
ranging from simple to more complex manipulation tasks, comparing its
performance against state-of-the-art baselines. Our results show that FRMD
generates significantly faster, smoother trajectories while achieving higher
success rates.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2406.01586 by other authors</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Robot Programming: Mixed Reality Gripper Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02042v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02042v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Rettinger, Leander Hacker, Philipp Wolters, Gerhard Rigoll
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional robot programming methods are complex and time-consuming for
users. In recent years, alternative approaches such as mixed reality have been
explored to address these challenges and optimize robot programming. While the
findings of the mixed reality robot programming methods are convincing, most
existing methods rely on gesture interaction for robot programming. Since
controller-based interactions have proven to be more reliable, this paper
examines three controller-based programming methods within a mixed reality
scenario: 1) Classical Jogging, where the user positions the robot's end
effector using the controller's thumbsticks, 2) Direct Control, where the
controller's position and orientation directly corresponds to the end
effector's, and 3) Gripper Control, where the controller is enhanced with a
3D-printed gripper attachment to grasp and release objects. A within-subjects
study (n = 30) was conducted to compare these methods. The findings indicate
that the Gripper Control condition outperforms the others in terms of task
completion time, user experience, mental demand, and task performance, while
also being the preferred method. Therefore, it demonstrates promising potential
as an effective and efficient approach for future robot programming. Video
available at https://youtu.be/83kWr8zUFIQ.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Pretrained Embeddings as a Behavior Specification Mechanism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02012v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02012v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parv Kapoor, Abigail Hammer, Ashish Kapoor, Karen Leung, Eunsuk Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose an approach to formally specifying the behavioral properties of
systems that rely on a perception model for interactions with the physical
world. The key idea is to introduce embeddings -- mathematical representations
of a real-world concept -- as a first-class construct in a specification
language, where properties are expressed in terms of distances between a pair
of ideal and observed embeddings. To realize this approach, we propose a new
type of temporal logic called Embedding Temporal Logic (ETL), and describe how
it can be used to express a wider range of properties about AI-enabled systems
than previously possible. We demonstrate the applicability of ETL through a
preliminary evaluation involving planning tasks in robots that are driven by
foundation models; the results are promising, showing that embedding-based
specifications can be used to steer a system towards desirable behaviors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Minimum-Length Coordinated Motions For Two Convex Centrally-Symmetric
  Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.02010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.02010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Kirkpatrick, Paul Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of determining coordinated motions, of minimum total
length, for two arbitrary convex centrally-symmetric (CCS) robots in an
otherwise obstacle-free plane. Using the total path length traced by the two
robot centres as a measure of distance, we give an exact characterization of a
(not necessarily unique) shortest collision-avoiding motion for all initial and
goal configurations of the robots. The individual paths are composed of at most
six convex pieces, and their total length can be expressed as a simple integral
with a closed form solution depending only on the initial and goal
configuration of the robots. The path pieces are either straight segments or
segments of the boundary of the Minkowski sum of the two robots (circular arcs,
in the special case of disc robots). Furthermore, the paths can be
parameterized in such a way that (i) only one robot is moving at any given time
(decoupled motion), or (ii) the orientation of the robot configuration changes
monotonically.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 32 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Discrete-Time Hybrid Automata Learning: Legged Locomotion Meets
  Skateboarding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01842v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01842v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Liu, Sangli Teng, Ben Liu, Wei Zhang, Maani Ghaffari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces Discrete-time Hybrid Automata Learning (DHAL), a
framework using on-policy Reinforcement Learning to identify and execute
mode-switching without trajectory segmentation or event function learning.
Hybrid dynamical systems, which include continuous flow and discrete mode
switching, can model robotics tasks like legged robot locomotion. Model-based
methods usually depend on predefined gaits, while model-free approaches lack
explicit mode-switching knowledge. Current methods identify discrete modes via
segmentation before regressing continuous flow, but learning high-dimensional
complex rigid body dynamics without trajectory labels or segmentation is a
challenging open problem. Our approach incorporates a beta policy distribution
and a multi-critic architecture to model contact-guided motions, exemplified by
a challenging quadrupedal robot skateboard task. We validate our method through
simulations and real-world tests, demonstrating robust performance in hybrid
dynamical systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Multi</span>-Stage Manipulation with Demonstration-Augmented Reward, Policy,
  and Wo<span class="highlight-title">rl</span>d Model Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01837v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01837v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrià López Escoriza, Nicklas Hansen, Stone Tao, Tongzhou Mu, Hao Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-horizon tasks in robotic manipulation present significant challenges in
reinforcement learning (RL) due to the difficulty of designing dense reward
functions and effectively exploring the expansive state-action space. However,
despite a lack of dense rewards, these tasks often have a multi-stage
structure, which can be leveraged to decompose the overall objective into
manageable subgoals. In this work, we propose DEMO3, a framework that exploits
this structure for efficient learning from visual inputs. Specifically, our
approach incorporates multi-stage dense reward learning, a bi-phasic training
scheme, and world model learning into a carefully designed
demonstration-augmented RL framework that strongly mitigates the challenge of
exploration in long-horizon tasks. Our evaluations demonstrate that our method
improves data-efficiency by an average of 40% and by 70% on particularly
difficult tasks compared to state-of-the-art approaches. We validate this
across 16 sparse-reward tasks spanning four domains, including challenging
humanoid visual control tasks using as few as five demonstrations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page can be found at
  https://adrialopezescoriza.github.io/demo3/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TacCap: A Wearable FBG-Based Tactile Sensor for Seamless Human-to-Robot
  Skill Transfer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01789v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01789v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengyi Xing, Hao Li, Yi-Lin Wei, Tian-Ao Ren, Tianyu Tu, Yuhao Lin, Elizabeth Schumann, Wei-Shi Zheng, Mark R. Cutkosky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tactile sensing is essential for dexterous manipulation, yet large-scale
human demonstration datasets lack tactile feedback, limiting their
effectiveness in skill transfer to robots. To address this, we introduce
TacCap, a wearable Fiber Bragg Grating (FBG)-based tactile sensor designed for
seamless human-to-robot transfer. TacCap is lightweight, durable, and immune to
electromagnetic interference, making it ideal for real-world data collection.
We detail its design and fabrication, evaluate its sensitivity, repeatability,
and cross-sensor consistency, and assess its effectiveness through grasp
stability prediction and ablation studies. Our results demonstrate that TacCap
enables transferable tactile data collection, bridging the gap between human
demonstrations and robotic execution. To support further research and
development, we open-source our hardware design and software.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ vS-Graphs: Integrating Visual <span class="highlight-title">SLAM</span> and Situational Graphs through
  <span class="highlight-title">Multi</span>-level Scene Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Tourani, Saad Ejaz, Hriday Bavle, David Morilla-Cabello, Jose Luis Sanchez-Lopez, Holger Voos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current Visual Simultaneous Localization and Mapping (VSLAM) systems often
struggle to create maps that are both semantically rich and easily
interpretable. While incorporating semantic scene knowledge aids in building
richer maps with contextual associations among mapped objects, representing
them in structured formats like scene graphs has not been widely addressed,
encountering complex map comprehension and limited scalability. This paper
introduces visual S-Graphs (vS-Graphs), a novel real-time VSLAM framework that
integrates vision-based scene understanding with map reconstruction and
comprehensible graph-based representation. The framework infers structural
elements (i.e., rooms and corridors) from detected building components (i.e.,
walls and ground surfaces) and incorporates them into optimizable 3D scene
graphs. This solution enhances the reconstructed map's semantic richness,
comprehensibility, and localization accuracy. Extensive experiments on standard
benchmarks and real-world datasets demonstrate that vS-Graphs outperforms
state-of-the-art VSLAM methods, reducing trajectory error by an average of
3.38% and up to 9.58% on real-world data. Furthermore, the proposed framework
achieves environment-driven semantic entity detection accuracy comparable to
precise LiDAR-based frameworks using only visual features. A web page
containing more media and evaluation outcomes is available on
https://snt-arg.github.io/vsgraphs-results/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 8 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ No Plan but Everything Under Control: Robustly Solving Sequential Tasks
  with Dynamically Composed Gradient Descent <span class="chip">ICRA25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vito Mengers, Oliver Brock
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel gradient-based approach for solving sequential tasks by
dynamically adjusting the underlying myopic potential field in response to
feedback and the world's regularities. This adjustment implicitly considers
subgoals encoded in these regularities, enabling the solution of long
sequential tasks, as demonstrated by solving the traditional planning domain of
Blocks World - without any planning. Unlike conventional planning methods, our
feedback-driven approach adapts to uncertain and dynamic environments, as
demonstrated by one hundred real-world trials involving drawer manipulation.
These experiments highlight the robustness of our method compared to planning
and show how interactive perception and error recovery naturally emerge from
gradient descent without explicitly implementing them. This offers a
computationally efficient alternative to planning for a variety of sequential
tasks, while aligning with observations on biological problem-solving
strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICRA25; 7 pages + 6 figures; Supplementary Material under
  https://www.tu.berlin/robotics/papers/noplan</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FLAME: A Federated Learning Benchmark for Robotic Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Santiago Bou Betran, Alberta Longhini, Miguel Vasco, Yuchong Zhang, Danica Kragic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in robotic manipulation has been fueled by large-scale
datasets collected across diverse environments. Training robotic manipulation
policies on these datasets is traditionally performed in a centralized manner,
raising concerns regarding scalability, adaptability, and data privacy. While
federated learning enables decentralized, privacy-preserving training, its
application to robotic manipulation remains largely unexplored. We introduce
FLAME (Federated Learning Across Manipulation Environments), the first
benchmark designed for federated learning in robotic manipulation. FLAME
consists of: (i) a set of large-scale datasets of over 160,000 expert
demonstrations of multiple manipulation tasks, collected across a wide range of
simulated environments; (ii) a training and evaluation framework for robotic
policy learning in a federated setting. We evaluate standard federated learning
algorithms in FLAME, showing their potential for distributed policy learning
and highlighting key challenges. Our benchmark establishes a foundation for
scalable, adaptive, and privacy-aware robotic learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Code-as-Symbolic-Planner: Foundation Model-Based Robot Planning via
  Symbolic Code Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchao Chen, Yilun Hao, Yang Zhang, Chuchu Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent works have shown great potentials of Large Language Models (LLMs) in
robot task and motion planning (TAMP). Current LLM approaches generate text- or
code-based reasoning chains with sub-goals and action plans. However, they do
not fully leverage LLMs' symbolic computing and code generation capabilities.
Many robot TAMP tasks involve complex optimization under multiple constraints,
where pure textual reasoning is insufficient. While augmenting LLMs with
predefined solvers and planners improves performance, it lacks generalization
across tasks. Given LLMs' growing coding proficiency, we enhance their TAMP
capabilities by steering them to generate code as symbolic planners for
optimization and constraint verification. Unlike prior work that uses code to
interface with robot action modules, we steer LLMs to generate code as solvers,
planners, and checkers for TAMP tasks requiring symbolic computing, while still
leveraging textual reasoning to incorporate common sense. With a multi-round
guidance and answer evolution framework, the proposed Code-as-Symbolic-Planner
improves success rates by average 24.1\% over best baseline methods across
seven typical TAMP tasks and three popular LLMs. Code-as-Symbolic-Planner shows
strong effectiveness and generalizability across discrete and continuous
environments, 2D/3D simulations and real-world settings, as well as single- and
multi-robot tasks with diverse requirements. See our project website
https://yongchao98.github.io/Code-Symbol-Planner/ for prompts, videos, and
code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAPS: Context-Aware Priority Sampling for Enhanced Imitation Learning in
  Autonomous Driving 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamidreza Mirkhani, Behzad Khamidehi, Ehsan Ahmadi, Fazel Arasteh, Mohammed Elmahgiubi, Weize Zhang, Umar Rajguru, Kasra Rezaee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce CAPS (Context-Aware Priority Sampling), a novel
method designed to enhance data efficiency in learning-based autonomous driving
systems. CAPS addresses the challenge of imbalanced training datasets in
imitation learning by leveraging Vector Quantized Variational Autoencoders
(VQ-VAEs). The use of VQ-VAE provides a structured and interpretable data
representation, which helps reveal meaningful patterns in the data. These
patterns are used to group the data into clusters, with each sample being
assigned a cluster ID. The cluster IDs are then used to re-balance the dataset,
ensuring that rare yet valuable samples receive higher priority during
training. By ensuring a more diverse and informative training set, CAPS
improves the generalization of the trained planner across a wide range of
driving scenarios. We evaluate our method through closed-loop simulations in
the CARLA environment. The results on Bench2Drive scenarios demonstrate that
our framework outperforms state-of-the-art methods, leading to notable
improvements in model performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Note on the Time Complexity of Using Subdivision Methods for the
  Approximation of Fibers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael M. Bilevich, Dan Halperin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Subdivision methods such as quadtrees, octrees, and higher-dimensional
orthrees are standard practice in different domains of computer science. We can
use these methods to represent given geometries, such as curves, meshes, or
surfaces. This representation is achieved by splitting some bounding voxel
recursively while further splitting only sub-voxels that intersect with the
given geometry. It is fairly known that subdivision methods are more efficient
than traversing a fine-grained voxel grid. In this short note, we propose
another outlook on analyzing the construction time complexity of orthrees to
represent implicitly defined geometries that are fibers (preimages) of some
function. This complexity is indeed asymptotically better than traversing dense
voxel grids, under certain conditions, which we specify in the note. In fact,
the complexity is output sensitive, and is closely related to the Hausdorff
measure and Hausdorff dimension of the resulting geometry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RoboDexVLM: Visual Language Model-Enabled Task Planning and Motion
  Control for Dexterous Robot Manipulation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01616v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01616v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haichao Liu, Sikai Guo, Pengfei Mai, Jiahang Cao, Haoang Li, Jun Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces RoboDexVLM, an innovative framework for robot task
planning and grasp detection tailored for a collaborative manipulator equipped
with a dexterous hand. Previous methods focus on simplified and limited
manipulation tasks, which often neglect the complexities associated with
grasping a diverse array of objects in a long-horizon manner. In contrast, our
proposed framework utilizes a dexterous hand capable of grasping objects of
varying shapes and sizes while executing tasks based on natural language
commands. The proposed approach has the following core components: First, a
robust task planner with a task-level recovery mechanism that leverages
vision-language models (VLMs) is designed, which enables the system to
interpret and execute open-vocabulary commands for long sequence tasks. Second,
a language-guided dexterous grasp perception algorithm is presented based on
robot kinematics and formal methods, tailored for zero-shot dexterous
manipulation with diverse objects and commands. Comprehensive experimental
results validate the effectiveness, adaptability, and robustness of RoboDexVLM
in handling long-horizon scenarios and performing dexterous grasping. These
results highlight the framework's ability to operate in complex environments,
showcasing its potential for open-vocabulary dexterous manipulation. Our
open-source project page can be found at
https://henryhcliu.github.io/robodexvlm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Soft Everting Prosthetic Hand and Comparison with Existing Body-Powered
  Terminal Devices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01585v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01585v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gayoung Park, Katalin Schäffer, Margaret M. Coad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we explore the use of a soft gripper, specifically a soft
inverting-everting toroidal hydrostat, as a prosthetic hand. We present a
design of the gripper integrated into a body-powered elbow-driven system and
evaluate its performance compared to similar body-powered terminal devices: the
Kwawu 3D-printed hand and the Hosmer hook. Our experiments highlight advantages
of the Everting hand, such as low required cable tension for operation (1.6 N
for Everting, 30.0 N for Kwawu, 28.1 N for Hosmer), limited restriction on the
elbow angle range, and secure grasping capability (peak pulling force required
to remove an object: 15.8 N for Everting, 6.9 N for Kwawu, 4.0 N for Hosmer).
In our pilot user study, six able-bodied participants performed standardized
hand dexterity tests. With the Everting hand compared to the Kwawu hand, users
transferred more blocks in one minute and completed three tasks (moving small
common objects, simulated feeding with a spoon, and moving large empty cans)
faster (p~$\leq$~0.05). With the Everting hand compared to the Hosmer hook,
users moved large empty cans faster (p~$\leq$~0.05) and achieved similar
performance on all other tasks. Overall, user preference leaned toward the
Everting hand for its adaptable grip and ease of use, although its abilities
could be improved in tasks requiring high precision such as writing with a pen,
and in handling heavier objects such as large heavy cans.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper was accepted to the 8th IEEE-RAS International Conference
  on Soft Robotics (RoboSoft 2025). The corresponding video attachment is
  available at: https://youtu.be/zO_Some_HxY</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MLINE-VINS: Robust Monocular Visual-Inertial <span class="highlight-title">SLAM</span> With Flow Manhattan
  and Line Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01571v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01571v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Ye, Haoyuan Li, Weiyang Lin, Xianqiang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we introduce MLINE-VINS, a novel monocular visual-inertial
odometry (VIO) system that leverages line features and Manhattan Word
assumption. Specifically, for line matching process, we propose a novel
geometric line optical flow algorithm that efficiently tracks line features
with varying lengths, whitch is do not require detections and descriptors in
every frame. To address the instability of Manhattan estimation from line
features, we propose a tracking-by-detection module that consistently tracks
and optimizes Manhattan framse in consecutive images. By aligning the Manhattan
World with the VIO world frame, the tracking could restart using the latest
pose from back-end, simplifying the coordinate transformations within the
system. Furthermore, we implement a mechanism to validate Manhattan frames and
a novel global structural constraints back-end optimization. Extensive
experiments results on vairous datasets, including benchmark and self-collected
datasets, show that the proposed approach outperforms existing methods in terms
of accuracy and long-range robustness. The source code of our method is
available at: https://github.com/LiHaoy-ux/MLINE-VINS.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ VF-Plan: Bridging the Art Gallery Problem and Static LiDAR Scanning with
  Visibility Field Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01562v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01562v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Biao Xionga, Longjun Zhanga, Ruiqi Huanga, Junwei Zhoua, Bojian Wub, Fashuai Lic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Viewpoint planning is crucial for 3D data collection and autonomous
navigation, yet existing methods often miss key optimization objectives for
static LiDAR, resulting in suboptimal network designs. The Viewpoint Planning
Problem (VPP), which builds upon the Art Gallery Problem (AGP), requires not
only full coverage but also robust registrability and connectivity under
limited sensor views. We introduce a greedy optimization algorithm that tackles
these VPP and AGP challenges through a novel Visibility Field (VF) approach.
The VF captures visibility characteristics unique to static LiDAR, enabling a
reduction from 2D to 1D by focusing on medial axis and joints. This leads to a
minimal, fully connected viewpoint network with comprehensive coverage and
minimal redundancy. Experiments across diverse environments show that our
method achieves high efficiency and scalability, matching or surpassing expert
designs. Compared to state-of-the-art methods, our approach achieves comparable
viewpoint counts (VC) while reducing Weighted Average Path Length (WAPL) by
approximately 95\%, indicating a much more compact and connected network.
Dataset and source code will be released upon acceptance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MapEx<span class="highlight-title">RL</span>: Human-Inspired Indoor Exploration with Predicted Environment
  Context and Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01548v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01548v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Narek Harutyunyan, Brady Moon, Seungchan Kim, Cherie Ho, Adam Hung, Sebastian Scherer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Path planning for robotic exploration is challenging, requiring reasoning
over unknown spaces and anticipating future observations. Efficient exploration
requires selecting budget-constrained paths that maximize information gain.
Despite advances in autonomous exploration, existing algorithms still fall
short of human performance, particularly in structured environments where
predictive cues exist but are underutilized. Guided by insights from our user
study, we introduce MapExRL, which improves robot exploration efficiency in
structured indoor environments by enabling longer-horizon planning through
reinforcement learning (RL) and global map predictions. Unlike many RL-based
exploration methods that use motion primitives as the action space, our
approach leverages frontiers for more efficient model learning and longer
horizon reasoning. Our framework generates global map predictions from the
observed map, which our policy utilizes, along with the prediction uncertainty,
estimated sensor coverage, frontier distance, and remaining distance budget, to
assess the strategic long-term value of frontiers. By leveraging multiple
frontier scoring methods and additional context, our policy makes more informed
decisions at each stage of the exploration. We evaluate our framework on a
real-world indoor map dataset, achieving up to an 18.8% improvement over the
strongest state-of-the-art baseline, with even greater gains compared to
conventional frontier-based algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exo-ViHa: A Cross-Platform Exoskeleton System with Visual and Haptic
  Feedback for Efficient Dexterous Skill Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01543v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01543v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xintao Chao, Shilong Mu, Yushan Liu, Shoujie Li, Chuqiao Lyu, Xiao-Ping Zhang, Wenbo Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning has emerged as a powerful paradigm for robot skills
learning. However, traditional data collection systems for dexterous
manipulation face challenges, including a lack of balance between acquisition
efficiency, consistency, and accuracy. To address these issues, we introduce
Exo-ViHa, an innovative 3D-printed exoskeleton system that enables users to
collect data from a first-person perspective while providing real-time haptic
feedback. This system combines a 3D-printed modular structure with a slam
camera, a motion capture glove, and a wrist-mounted camera. Various dexterous
hands can be installed at the end, enabling it to simultaneously collect the
posture of the end effector, hand movements, and visual data. By leveraging the
first-person perspective and direct interaction, the exoskeleton enhances the
task realism and haptic feedback, improving the consistency between
demonstrations and actual robot deployments. In addition, it has cross-platform
compatibility with various robotic arms and dexterous hands. Experiments show
that the system can significantly improve the success rate and efficiency of
data collection for dexterous manipulation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Origami-Inspired Soft Gripper with Tunable Constant Force Output 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01481v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01481v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenwei Ni, Chang Xu, Zhihang Qin, Ceng Zhang, Zhiqiang Tang, Peiyi Wang, Cecilia Laschi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soft robotic grippers gently and safely manipulate delicate objects due to
their inherent adaptability and softness. Limited by insufficient stiffness and
imprecise force control, conventional soft grippers are not suitable for
applications that require stable grasping force. In this work, we propose a
soft gripper that utilizes an origami-inspired structure to achieve tunable
constant force output over a wide strain range. The geometry of each taper
panel is established to provide necessary parameters such as protrusion
distance, taper angle, and crease thickness required for 3D modeling and FEA
analysis. Simulations and experiments show that by optimizing these parameters,
our design can achieve a tunable constant force output. Moreover, the
origami-inspired soft gripper dynamically adapts to different shapes while
preventing excessive forces, with potential applications in logistics,
manufacturing, and other industrial settings that require stable and adaptive
operations
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 8 figures, conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trajectory Planning with Signal Temporal Logic Costs using Deterministic
  Path Integral Optimization <span class="chip">ICRA25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Halder, Hannes Homburger, Lothar Kiltz, Johannes Reuter, Matthias Althoff
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Formulating the intended behavior of a dynamic system can be challenging.
Signal temporal logic (STL) is frequently used for this purpose due to its
suitability in formalizing comprehensible, modular, and versatile
spatiotemporal specifications. Due to scaling issues with respect to the
complexity of the specifications and the potential occurrence of
non-differentiable terms, classical optimization methods often solve STL-based
problems inefficiently. Smoothing and approximation techniques can alleviate
these issues but require changing the optimization problem. This paper proposes
a novel sampling-based method based on model predictive path integral control
to solve optimal control problems with STL cost functions. We demonstrate the
effectiveness of our method on benchmark motion planning problems and compare
its performance with state-of-the-art methods. The results show that our method
efficiently solves optimal control problems with STL costs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6+2 pages, 3 figures, P. Halder and H. Homburger contributed equally
  to the paper, accepted to the 2025 IEEE International Conference on Robotics
  & Automation (ICRA25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interactive Navigation for Legged Manipulators with Learned Arm-Pushing
  Controller 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01474v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01474v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihai Bi, Kai Chen, Chunxin Zheng, Yulin Li, Haoang Li, Jun Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactive navigation is crucial in scenarios where proactively interacting
with objects can yield shorter paths, thus significantly improving traversal
efficiency. Existing methods primarily focus on using the robot body to
relocate large obstacles (which could be comparable to the size of a robot).
However, they prove ineffective in narrow or constrained spaces where the
robot's dimensions restrict its manipulation capabilities. This paper
introduces a novel interactive navigation framework for legged manipulators,
featuring an active arm-pushing mechanism that enables the robot to reposition
movable obstacles in space-constrained environments. To this end, we develop a
reinforcement learning-based arm-pushing controller with a two-stage reward
strategy for large-object manipulation. Specifically, this strategy first
directs the manipulator to a designated pushing zone to achieve a kinematically
feasible contact configuration. Then, the end effector is guided to maintain
its position at appropriate contact points for stable object displacement while
preventing toppling. The simulations validate the robustness of the arm-pushing
controller, showing that the two-stage reward strategy improves policy
convergence and long-term performance. Real-world experiments further
demonstrate the effectiveness of the proposed navigation framework, which
achieves shorter paths and reduced traversal time. The open-source project can
be found at
https://github.com/Zhihaibi/Interactive-Navigation-for-legged-manipulator.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aerial Gym Simulator: A Framework for Highly Parallelized Simulation of
  Aerial Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01471v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01471v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mihir Kulkarni, Welf Rehberg, Kostas Alexis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper contributes the Aerial Gym Simulator, a highly parallelized,
modular framework for simulation and rendering of arbitrary multirotor
platforms based on NVIDIA Isaac Gym. Aerial Gym supports the simulation of
under-, fully- and over-actuated multirotors offering parallelized geometric
controllers, alongside a custom GPU-accelerated rendering framework for
ray-casting capable of capturing depth, segmentation and vertex-level
annotations from the environment. Multiple examples for key tasks, such as
depth-based navigation through reinforcement learning are provided. The
comprehensive set of tools developed within the framework makes it a powerful
resource for research on learning for control, planning, and navigation using
state information as well as exteroceptive sensor observations. Extensive
simulation studies are conducted and successful sim2real transfer of trained
policies is demonstrated. The Aerial Gym Simulator is open-sourced at:
https://github.com/ntnu-arl/aerial_gym_simulator.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in IEEE Robotics and Automation Letters
  (RA-L)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AVR: Active Vision-Driven Robotic Precision Manipulation with Viewpoint
  and Focal Length Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01439v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01439v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushan Liu, Shilong Mu, Xintao Chao, Zizhen Li, Yao Mu, Tianxing Chen, Shoujie Li, Chuqiao Lyu, Xiao-ping Zhang, Wenbo Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robotic manipulation within dynamic environments presents challenges to
precise control and adaptability. Traditional fixed-view camera systems face
challenges adapting to change viewpoints and scale variations, limiting
perception and manipulation precision. To tackle these issues, we propose the
Active Vision-driven Robotic (AVR) framework, a teleoperation hardware solution
that supports dynamic viewpoint and dynamic focal length adjustments to
continuously center targets and maintain optimal scale, accompanied by a
corresponding algorithm that effectively enhances the success rates of various
operational tasks. Using the RoboTwin platform with a real-time image
processing plugin, AVR framework improves task success rates by 5%-16% on five
manipulation tasks. Physical deployment on a dual-arm system demonstrates in
collaborative tasks and 36% precision in screwdriver insertion, outperforming
baselines by over 25%. Experimental results confirm that AVR framework enhances
environmental perception, manipulation repeatability (40% $\le $1 cm error),
and robustness in complex scenarios, paving the way for future robotic
precision manipulation methods in the pursuit of human-level robot dexterity
and precision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CAO-RONet: A Robust 4D Radar Odometry with Exploring More Information
  from Low-Quality Points 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01438v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01438v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiheng Li, Yubo Cui, Ningyuan Huang, Chenglin Pang, Zheng Fang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, 4D millimetre-wave radar exhibits more stable perception ability
than LiDAR and camera under adverse conditions (e.g. rain and fog). However,
low-quality radar points hinder its application, especially the odometry task
that requires a dense and accurate matching. To fully explore the potential of
4D radar, we introduce a learning-based odometry framework, enabling robust
ego-motion estimation from finite and uncertain geometry information. First,
for sparse radar points, we propose a local completion to supplement missing
structures and provide denser guideline for aligning two frames. Then, a
context-aware association with a hierarchical structure flexibly matches points
of different scales aided by feature similarity, and improves local matching
consistency through correlation balancing. Finally, we present a window-based
optimizer that uses historical priors to establish a coupling state estimation
and correct errors of inter-frame matching. The superiority of our algorithm is
confirmed on View-of-Delft dataset, achieving around a 50% performance
improvement over previous approaches and delivering accuracy on par with LiDAR
odometry. Our code will be available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RUSSO: Robust Underwater <span class="highlight-title">SLAM</span> with Sonar Optimization against Visual
  Degradation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01434v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01434v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shu Pan, Ziyang Hong, Zhangrui Hu, Xiandong Xu, Wenjie Lu, Liang Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual degradation in underwater environments poses unique and significant
challenges, which distinguishes underwater SLAM from popular vision-based SLAM
on the ground. In this paper, we propose RUSSO, a robust underwater SLAM system
which fuses stereo camera, inertial measurement unit (IMU), and imaging sonar
to achieve robust and accurate localization in challenging underwater
environments for 6 degrees of freedom (DoF) estimation. During visual
degradation, the system is reduced to a sonar-inertial system estimating 3-DoF
poses. The sonar pose estimation serves as a strong prior for IMU propagation,
thereby enhancing the reliability of pose estimation with IMU propagation.
Additionally, we propose a SLAM initialization method that leverages the
imaging sonar to counteract the lack of visual features during the
initialization stage of SLAM. We extensively validate RUSSO through experiments
in simulator, pool, and sea scenarios. The results demonstrate that RUSSO
achieves better robustness and localization accuracy compared to the
state-of-the-art visual-inertial SLAM systems, especially in visually
challenging scenarios. To the best of our knowledge, this is the first time
fusing stereo camera, IMU, and imaging sonar to realize robust underwater SLAM
against visual degradation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CognitiveDrone: A VLA Model and Evaluation Benchmark for Real-Time
  Cognitive Task Solving and Reasoning in UAVs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2503.01378v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2503.01378v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Artem Lykov, Valerii Serpiva, Muhammad Haris Khan, Oleg Sautenkov, Artyom Myshlyaev, Grik Tadevosyan, Yasheerah Yaqoot, Dzmitry Tsetserukou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces CognitiveDrone, a novel Vision-Language-Action (VLA)
model tailored for complex Unmanned Aerial Vehicles (UAVs) tasks that demand
advanced cognitive abilities. Trained on a dataset comprising over 8,000
simulated flight trajectories across three key categories-Human Recognition,
Symbol Understanding, and Reasoning-the model generates real-time 4D action
commands based on first-person visual inputs and textual instructions. To
further enhance performance in intricate scenarios, we propose
CognitiveDrone-R1, which integrates an additional Vision-Language Model (VLM)
reasoning module to simplify task directives prior to high-frequency control.
Experimental evaluations using our open-source benchmark, CognitiveDroneBench,
reveal that while a racing-oriented model (RaceVLA) achieves an overall success
rate of 31.3%, the base CognitiveDrone model reaches 59.6%, and
CognitiveDrone-R1 attains a success rate of 77.2%. These results demonstrate
improvements of up to 30% in critical cognitive tasks, underscoring the
effectiveness of incorporating advanced reasoning capabilities into UAV control
systems. Our contributions include the development of a state-of-the-art VLA
model for UAV control and the introduction of the first dedicated benchmark for
assessing cognitive tasks in drone operations. The complete repository is
available at cognitivedrone.github.io
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper submitted to the IEEE conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiffTORI: Differentiable Trajectory Optimization for Deep Reinforcement
  and Imitation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.05421v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.05421v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weikang Wan, Ziyu Wang, Yufei Wang, Zackory Erickson, David Held
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces DiffTORI, which utilizes Differentiable Trajectory
Optimization as the policy representation to generate actions for deep
Reinforcement and Imitation learning. Trajectory optimization is a powerful and
widely used algorithm in control, parameterized by a cost and a dynamics
function. The key to our approach is to leverage the recent progress in
differentiable trajectory optimization, which enables computing the gradients
of the loss with respect to the parameters of trajectory optimization. As a
result, the cost and dynamics functions of trajectory optimization can be
learned end-to-end. DiffTORI addresses the ``objective mismatch'' issue of
prior model-based RL algorithms, as the dynamics model in DiffTORI is learned
to directly maximize task performance by differentiating the policy gradient
loss through the trajectory optimization process. We further benchmark DiffTORI
for imitation learning on standard robotic manipulation task suites with
high-dimensional sensory observations and compare our method to feed-forward
policy classes as well as Energy-Based Models (EBM) and Diffusion. Across 15
model-based RL tasks and 35 imitation learning tasks with high-dimensional
image and point cloud inputs, DiffTORI outperforms prior state-of-the-art
methods in both domains. Our code is available at
https://github.com/wkwan7/DiffTORI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2024 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Feasibility of Fingerprinting Collaborative Robot Traffic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.06802v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.06802v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Tang, Diogo Barradas, Urs Hengartner, Yue Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study examines privacy risks in collaborative robotics, focusing on the
potential for traffic analysis in encrypted robot communications. While
previous research has explored low-level command recovery in teleoperation
setups, our work investigates high-level motion recovery from script-based
control interfaces. We evaluate the efficacy of prominent website
fingerprinting techniques (e.g., Tik-Tok, RF) and their limitations in
accurately identifying robotic actions due to their inability to capture
detailed temporal relationships. To address this, we introduce a traffic
classification approach using signal processing techniques, demonstrating high
accuracy in action identification and highlighting the vulnerability of
encrypted communications to privacy breaches. Additionally, we explore defenses
such as packet padding and timing manipulation, revealing the challenges in
balancing traffic analysis resistance with network efficiency. Our findings
emphasize the need for continued development of practical defenses in robotic
privacy and security.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robots that Suggest Safe Alternatives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09883v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09883v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyun Joe Jeong, Rosy Chen, Andrea Bajcsy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Goal-conditioned policies, such as those learned via imitation learning,
provide an easy way for humans to influence what tasks robots accomplish.
However, these robot policies are not guaranteed to execute safely or to
succeed when faced with out-of-distribution requests. In this work, we enable
robots to know when they can confidently execute a user's desired goal, and
automatically suggest safe alternatives when they cannot. Our approach is
inspired by control-theoretic safety filtering, wherein a safety filter
minimally adjusts a robot's candidate action to be safe. Our key idea is to
pose alternative suggestion as a safe control problem in goal space, rather
than in action space. Offline, we use reachability analysis to compute a
goal-parameterized reach-avoid value network which quantifies the safety and
liveness of the robot's pre-trained policy. Online, our robot uses the
reach-avoid value network as a safety filter, monitoring the human's given goal
and actively suggesting alternatives that are similar but meet the safety
specification. We demonstrate our Safe ALTernatives (SALT) framework in
simulation experiments with indoor navigation and Franka Panda tabletop
manipulation, and with both discrete and continuous goal representations. We
find that SALT is able to learn to predict successful and failed closed-loop
executions, is a less pessimistic monitor than open-loop uncertainty
quantification, and proposes alternatives that consistently align with those
people find acceptable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Coordinated <span class="highlight-title">Multi</span>-Robot Navigation with Formation Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.01618v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.01618v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Deng, Peng Gao, Williard Joshua Jose, Christopher Reardon, Maggie Wigness, John Rogers, Hao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coordinated multi-robot navigation is an essential ability for a team of
robots operating in diverse environments. Robot teams often need to maintain
specific formations, such as wedge formations, to enhance visibility,
positioning, and efficiency during fast movement. However, complex environments
such as narrow corridors challenge rigid team formations, which makes effective
formation control difficult in real-world environments. To address this
challenge, we introduce a novel Adaptive Formation with Oscillation Reduction
(AFOR) approach to improve coordinated multi-robot navigation. We develop AFOR
under the theoretical framework of hierarchical learning and integrate a
spring-damper model with hierarchical learning to enable both team coordination
and individual robot control. At the upper level, a graph neural network
facilitates formation adaptation and information sharing among the robots. At
the lower level, reinforcement learning enables each robot to navigate and
avoid obstacles while maintaining the formations. We conducted extensive
experiments using Gazebo in the Robot Operating System (ROS), a high-fidelity
Unity3D simulator with ROS, and real robot teams. Results demonstrate that AFOR
enables smooth navigation with formation adaptation in complex scenarios and
outperforms previous methods. More details of this work are provided on the
project website: https://hcrlab.gitlab.io/project/afor.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rao-Blackwellized POMDP Planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.16392v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.16392v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiho Lee, Nisar R. Ahmed, Kyle H. Wray, Zachary N. Sunberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partially Observable Markov Decision Processes (POMDPs) provide a structured
framework for decision-making under uncertainty, but their application requires
efficient belief updates. Sequential Importance Resampling Particle Filters
(SIRPF), also known as Bootstrap Particle Filters, are commonly used as belief
updaters in large approximate POMDP solvers, but they face challenges such as
particle deprivation and high computational costs as the system's state
dimension grows. To address these issues, this study introduces
Rao-Blackwellized POMDP (RB-POMDP) approximate solvers and outlines generic
methods to apply Rao-Blackwellization in both belief updates and online
planning. We compare the performance of SIRPF and Rao-Blackwellized Particle
Filters (RBPF) in a simulated localization problem where an agent navigates
toward a target in a GPS-denied environment using POMCPOW and RB-POMCPOW
planners. Our results not only confirm that RBPFs maintain accurate belief
approximations over time with fewer particles, but, more surprisingly, RBPFs
combined with quadrature-based integration improve planning quality
significantly compared to SIRPF-based planning under the same computational
limits.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Planning and Reasoning with 3D Deformable Objects for Hierarchical
  Text-to-3D Robotic Shaping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.01765v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.01765v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alison Bartsch, Amir Barati Farimani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deformable object manipulation remains a key challenge in developing
autonomous robotic systems that can be successfully deployed in real-world
scenarios. In this work, we explore the challenges of deformable object
manipulation through the task of sculpting clay into 3D shapes. We propose the
first coarse-to-fine autonomous sculpting system in which the sculpting agent
first selects how many and where to place discrete chunks of clay into the
workspace to create a coarse shape, and then iteratively refines the shape with
sequences of deformation actions. We leverage large language models for
sub-goal generation, and train a point cloud region-based action model to
predict robot actions from the desired point cloud sub-goals. Additionally, our
method is the first autonomous sculpting system that is a real-world text-to-3D
shaping pipeline without any explicit 3D goals or sub-goals provided to the
system. We demonstrate our method is able to successfully create a set of
simple shapes solely from text-based prompting. Furthermore, we explore
rigorously how to best quantify success for the text-to-3D sculpting task, and
compare existing text-image and text-point cloud similarity metrics to human
evaluations for this task. For experimental videos, human evaluation details,
and full prompts, please see our project website:
https://sites.google.com/andrew.cmu.edu/hierarchicalsculpting
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discrete GCBF Proximal Policy Optimization for <span class="highlight-title">Multi</span>-agent Safe Optimal
  Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03640v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03640v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songyuan Zhang, Oswin So, Mitchell Black, Chuchu Fan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Control policies that can achieve high task performance and satisfy safety
constraints are desirable for any system, including multi-agent systems (MAS).
One promising technique for ensuring the safety of MAS is distributed control
barrier functions (CBF). However, it is difficult to design distributed
CBF-based policies for MAS that can tackle unknown discrete-time dynamics,
partial observability, changing neighborhoods, and input constraints,
especially when a distributed high-performance nominal policy that can achieve
the task is unavailable. To tackle these challenges, we propose DGPPO, a new
framework that simultaneously learns both a discrete graph CBF which handles
neighborhood changes and input constraints, and a distributed high-performance
safe policy for MAS with unknown discrete-time dynamics. We empirically
validate our claims on a suite of multi-agent tasks spanning three different
simulation engines. The results suggest that, compared with existing methods,
our DGPPO framework obtains policies that achieve high task performance
(matching baselines that ignore the safety constraints), and high safety rates
(matching the most conservative baselines), with a constant set of
hyperparameters across all environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 15 figures; Accepted by the thirteenth International
  Conference on Learning Representations (ICLR 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Societal Attitudes Toward Service Robots: Adore, Abhor, Ignore, or
  Unsure? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.01231v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.01231v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        V. Yoganathan, V. -S. Osburg, A. Fronzetti Colladon, V. Charles, W. Toporowski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Societal or population-level attitudes are aggregated patterns of different
individual attitudes, representing collective general predispositions. As
service robots become ubiquitous, understanding attitudes towards them at the
population (vs. individual) level enables firms to expand robot services to a
broad (vs. niche) market. Targeting population-level attitudes would benefit
service firms because: (1) they are more persistent, thus, stronger predictors
of behavioral patterns and (2) this approach is less reliant on personal data,
whereas individualized services are vulnerable to AI-related privacy risks. As
for service theory, ignoring broad unobserved differences in attitudes produces
biased conclusions, and our systematic review of previous research highlights a
poor understanding of potential heterogeneity in attitudes toward service
robots. We present five diverse studies (S1-S5), utilizing multinational and
"real world" data (Ntotal = 89,541; years: 2012-2024). Results reveal a stable
structure comprising four distinct attitude profiles (S1-S5): positive
("adore"), negative ("abhor"), indifferent ("ignore"), and ambivalent
("unsure"). The psychological need for interacting with service staff, and for
autonomy and relatedness in technology use, function as attitude profile
antecedents (S2). Importantly, the attitude profiles predict differences in
post-interaction discomfort and anxiety (S3), satisfaction ratings and service
evaluations (S4), and perceived sociability and uncanniness based on a robot's
humanlikeness (S5).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One Map to Find Them All: Real-time Open-Vocabulary Mapping for
  Zero-shot <span class="highlight-title">Multi</span>-Object Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11764v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11764v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Finn Lukas Busch, Timon Homberger, Jesús Ortega-Peimbert, Quantao Yang, Olov Andersson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The capability to efficiently search for objects in complex environments is
fundamental for many real-world robot applications. Recent advances in
open-vocabulary vision models have resulted in semantically-informed object
navigation methods that allow a robot to search for an arbitrary object without
prior training. However, these zero-shot methods have so far treated the
environment as unknown for each consecutive query. In this paper we introduce a
new benchmark for zero-shot multi-object navigation, allowing the robot to
leverage information gathered from previous searches to more efficiently find
new objects. To address this problem we build a reusable open-vocabulary
feature map tailored for real-time object search. We further propose a
probabilistic-semantic map update that mitigates common sources of errors in
semantic feature extraction and leverage this semantic uncertainty for informed
multi-object exploration. We evaluate our method on a set of object navigation
tasks in both simulation as well as with a real robot, running in real-time on
a Jetson Orin AGX. We demonstrate that it outperforms existing state-of-the-art
approaches both on single and multi-object navigation tasks. Additional videos,
code and the multi-object navigation benchmark will be available on
https://finnbsch.github.io/OneMap.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ConfigBot: Adaptive Resource Allocation for Robot Applications in
  Dynamic Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.10513v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.10513v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rohit Dwivedula, Sadanand Modak, Aditya Akella, Joydeep Biswas, Daehyeok Kim, Christopher J. Rossbach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The growing use of service robots in dynamic environments requires flexible
management of on-board compute resources to optimize the performance of diverse
tasks such as navigation, localization, and perception. Current robot
deployments often rely on static OS configurations and system
over-provisioning. However, they are suboptimal because they do not account for
variations in resource usage. This results in poor system-wide behavior such as
robot instability or inefficient resource use. This paper presents ConifgBot, a
novel system designed to adaptively reconfigure robot applications to meet a
predefined performance specification by leveraging \emph{runtime profiling} and
\emph{automated configuration tuning}. Through experiments on multiple real
robots, each running a different stack with diverse performance requirements,
which could be \emph{context}-dependent, we illustrate ConifgBot's efficacy in
maintaining system stability and optimizing resource allocation. Our findings
highlight the promise of automatic system configuration tuning for robot
deployments, including adaptation to dynamic changes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 8 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Autonomous Exploration and Semantic Updating of Large-Scale Indoor
  Environments with Mobile Robots 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15493v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15493v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sai Haneesh Allu, Itay Kadosh, Tyler Summers, Yu Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new robotic system that enables a mobile robot to autonomously
explore an unknown environment, build a semantic map of the environment, and
subsequently update the semantic map to reflect environment changes, such as
location changes of objects. Our system leverages a LiDAR scanner for 2D
occupancy grid mapping and an RGB-D camera for object perception. We introduce
a semantic map representation that combines a 2D occupancy grid map for
geometry with a topological map for object semantics. This map representation
enables us to effectively update the semantics by deleting or adding nodes to
the topological map. Our system has been tested on a Fetch robot, semantically
mapping a 93m x 90m and a 9m x 13m indoor environment and updating their
semantic maps once objects are moved in the environments
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures. Project page is available at
  https://irvlutd.github.io/SemanticMapping/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-shot Object-Centric Instruction Following: Integrating Foundation
  Models with Traditional Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.07848v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.07848v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sonia Raychaudhuri, Duy Ta, Katrina Ashton, Angel X. Chang, Jiuguang Wang, Bernadette Bucher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large scale scenes such as multifloor homes can be robustly and efficiently
mapped with a 3D graph of landmarks estimated jointly with robot poses in a
factor graph, a technique commonly used in commercial robots such as drones and
robot vacuums. In this work, we propose Language-Inferred Factor Graph for
Instruction Following (LIFGIF), a zero-shot method to ground natural language
instructions in such a map. LIFGIF also includes a policy for following natural
language navigation instructions in a novel environment while the map is
constructed, enabling robust navigation performance in the physical world. To
evaluate LIFGIF, we present a new dataset, Object-Centric VLN (OC-VLN), in
order to evaluate grounding of object-centric natural language navigation
instructions. We compare to two state-of-the-art zero-shot baselines from
related tasks, Object Goal Navigation and Vision Language Navigation, to
demonstrate that LIFGIF outperforms them across all our evaluation metrics on
OCVLN. Finally, we successfully demonstrate the effectiveness of LIFGIF for
performing zero-shot object-centric instruction following in the real world on
a Boston Dynamics Spot robot.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Object Compliance via Young's Modulus from Single Grasps using
  Camera-Based Tactile Sensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15304v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15304v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Burgess, Jialiang Zhao, Laurence Willemet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compliance is a useful parametrization of tactile information that humans
often utilize in manipulation tasks. It can be used to inform low-level
contact-rich actions or characterize objects at a high-level. In robotic
manipulation, existing approaches to estimate compliance have struggled to
generalize across both object shape and material. Using camera-based tactile
sensors, proprioception, and force measurements, we present a novel approach to
estimate object compliance as Young's modulus (E) from parallel grasps. We
evaluate our method over a novel dataset of 285 common objects, including a
wide array of shapes and materials with Young's moduli ranging from 5.0 kPa to
250 GPa. Combining analytical and data-driven approaches, we develop a hybrid
system using a multi-tower neural network to analyze a sequence of tactile
images from grasping. This system is shown to estimate the Young's modulus of
unseen objects within an order of magnitude at 74.2% accuracy across our
dataset. This is an improvement over purely analytical and data-driven
baselines which exhibit 28.9% and 65.0% accuracy respectively. Importantly,
this estimation system performs irrespective of object geometry and
demonstrates increased robustness across material types.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Open-Source Reproducible Chess Robot for Human-Robot Interaction
  Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18170v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18170v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Renchi Zhang, Joost de Winter, Dimitra Dodou, Harleigh Seyffert, Yke Bauke Eisma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in AI have accelerated the evolution of versatile robot
designs. Chess provides a standardized environment for evaluating the impact of
robot behavior on human behavior. This article presents an open-source chess
robot for human-robot interaction (HRI) research, specifically focusing on
verbal and non-verbal interactions. The OpenChessRobot recognizes chess pieces
using computer vision, executes moves, and interacts with the human player
through voice and robotic gestures. We detail the software design, provide
quantitative evaluations of the efficacy of the robot, and offer a guide for
its reproducibility. An online survey examining people's views of the robot in
three possible scenarios was conducted with 597 participants. The robot
received the highest ratings in the robotics education and the chess coach
scenarios, while the home entertainment scenario received the lowest scores.
The code is accessible on GitHub: https://github.com/renchizhhhh/OpenChessRobot
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visuotactile-Based Learning for Insertion with Compliant Hands 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.06408v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.06408v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Osher Azulay, Dhruv Metha Ramesh, Nimrod Curtis, Avishai Sintov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compared to rigid hands, underactuated compliant hands offer greater
adaptability to object shapes, provide stable grasps, and are often more
cost-effective. However, they introduce uncertainties in hand-object
interactions due to their inherent compliance and lack of precise finger
proprioception as in rigid hands. These limitations become particularly
significant when performing contact-rich tasks like insertion. To address these
challenges, additional sensing modalities are required to enable robust
insertion capabilities. This letter explores the essential sensing requirements
for successful insertion tasks with compliant hands, focusing on the role of
visuotactile perception (i.e., visual and tactile perception). We propose a
simulation-based multimodal policy learning framework that leverages all-around
tactile sensing and an extrinsic depth camera. A transformer-based policy,
trained through a teacher-student distillation process, is successfully
transferred to a real-world robotic system without further training. Our
results emphasize the crucial role of tactile sensing in conjunction with
visual perception for accurate object-socket pose estimation, successful
sim-to-real transfer and robust task execution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guaranteed Reach-Avoid for Black-Box Systems through Narrow Gaps via
  Neural Network Reachability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13195v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13195v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Kiu Chung, Wonsuhk Jung, Srivatsank Pullabhotla, Parth Shinde, Yadu Sunil, Saihari Kota, Luis Felipe Wolf Batista, Cédric Pradalier, Shreyas Kousik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the classical reach-avoid problem, autonomous mobile robots are tasked to
reach a goal while avoiding obstacles. However, it is difficult to provide
guarantees on the robot's performance when the obstacles form a narrow gap and
the robot is a black-box (i.e. the dynamics are not known analytically, but
interacting with the system is cheap). To address this challenge, this paper
presents NeuralPARC. The method extends the authors' prior Piecewise Affine
Reach-avoid Computation (PARC) method to systems modeled by rectified linear
unit (ReLU) neural networks, which are trained to represent parameterized
trajectory data demonstrated by the robot. NeuralPARC computes the reachable
set of the network while accounting for modeling error, and returns a set of
states and parameters with which the black-box system is guaranteed to reach
the goal and avoid obstacles. NeuralPARC is shown to outperform PARC,
generating provably-safe extreme vehicle drift parking maneuvers in simulations
and in real life on a model car, as well as enabling safety on an autonomous
surface vehicle (ASV) subjected to large disturbances and controlled by a deep
reinforcement learning (RL) policy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Constrained Bandwidth Observation Sharing for <span class="highlight-title">Multi</span>-Robot Navigation in
  Dynamic Environments via Intelligent Knapsack 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.09975v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.09975v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anirudh Chari, Rui Chen, Han Zheng, Changliu Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-robot navigation is increasingly crucial in various domains, including
disaster response, autonomous vehicles, and warehouse and manufacturing
automation. Robot teams often must operate in highly dynamic environments and
under strict bandwidth constraints imposed by communication infrastructure,
rendering effective observation sharing within the system a challenging
problem. This paper presents a novel optimal communication scheme, Intelligent
Knapsack (iKnap), for multi-robot navigation in dynamic environments under
bandwidth constraints. We model multi-robot communication as belief propagation
in a graph of inferential agents. We then formulate the combinatorial
optimization for observation sharing as a 0/1 knapsack problem, where each
potential pairwise communication between robots is assigned a decision-making
utility to be weighed against its bandwidth cost, and the system has some
cumulative bandwidth limit. We evaluate our approach in a simulated robotic
warehouse with human workers using ROS2 and the Open Robotics Middleware
Framework. Compared to state-of-the-art broadcast-based optimal communication
schemes, iKnap yields significant improvements in navigation performance with
respect to scenario complexity while maintaining a similar runtime.
Furthermore, iKnap utilizes allocated bandwidth and observational resources
more efficiently than existing approaches, especially in very low-resource and
high-uncertainty settings. Based on these results, we claim that the proposed
method enables more robust collaboration for multi-robot teams in real-world
navigation problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RECON: Reducing Causal Confusion with Human-Placed Markers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13607v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13607v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robert Ramirez Sanchez, Heramb Nemlekar, Shahabedin Sagheb, Cara M. Nunez, Dylan P. Losey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning enables robots to learn new tasks from human examples. One
fundamental limitation while learning from humans is causal confusion. Causal
confusion occurs when the robot's observations include both task-relevant and
extraneous information: for instance, a robot's camera might see not only the
intended goal, but also clutter and changes in lighting within its environment.
Because the robot does not know which aspects of its observations are important
a priori, it often misinterprets the human's examples and fails to learn the
desired task. To address this issue, we highlight that -- while the robot
learner may not know what to focus on -- the human teacher does. In this paper
we propose that the human proactively marks key parts of their task with small,
lightweight beacons. Under our framework (RECON) the human attaches these
beacons to task-relevant objects before providing demonstrations: as the human
shows examples of the task, beacons track the position of marked objects. We
then harness this offline beacon data to train a task-relevant state embedding.
Specifically, we embed the robot's observations to a latent state that is
correlated with the measured beacon readings: in practice, this causes the
robot to autonomously filter out extraneous observations and make decisions
based on features learned from the beacon data. Our simulations and a real
robot experiment suggest that this framework for human-placed beacons mitigates
causal confusion. Indeed, we find that using RECON significantly reduces the
number of demonstrations needed to convey the task, lowering the overall time
required for human teaching. See videos here: https://youtu.be/oy85xJvtLSU
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Online Tree Reconstruction and Forest Inventory on a Mobile Robotic
  System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17622v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17622v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonard Freißmuth, Matias Mattamala, Nived Chebrolu, Simon Schaefer, Stefan Leutenegger, Maurice Fallon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Terrestrial laser scanning (TLS) is the standard technique used to create
accurate point clouds for digital forest inventories. However, the measurement
process is demanding, requiring up to two days per hectare for data collection,
significant data storage, as well as resource-heavy post-processing of 3D data.
In this work, we present a real-time mapping and analysis system that enables
online generation of forest inventories using mobile laser scanners that can be
mounted e.g. on mobile robots. Given incrementally created and locally accurate
submaps-data payloads-our approach extracts tree candidates using a custom,
Voronoi-inspired clustering algorithm. Tree candidates are reconstructed using
an adapted Hough algorithm, which enables robust modeling of the tree stem.
Further, we explicitly incorporate the incremental nature of the data
collection by consistently updating the database using a pose graph LiDAR SLAM
system. This enables us to refine our estimates of the tree traits if an area
is revisited later during a mission. We demonstrate competitive accuracy to TLS
or manual measurements using laser scanners that we mounted on backpacks or
mobile robots operating in conifer, broad-leaf and mixed forests. Our results
achieve RMSE of 1.93 cm, a bias of 0.65 cm and a standard deviation of 1.81 cm
(averaged across these sequences)-with no post-processing required after the
mission is complete.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FlowNav: Combining Flow Matching and Depth Priors for Efficient
  Navigation <span class="chip">IROS'25</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09524v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09524v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samiran Gode, Abhijeet Nayak, Débora N. P. Oliveira, Michael Krawez, Cordelia Schmid, Wolfram Burgard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective robot navigation in unseen environments is a challenging task that
requires precise control actions at high frequencies. Recent advances have
framed it as an image-goal-conditioned control problem, where the robot
generates navigation actions using frontal RGB images. Current state-of-the-art
methods in this area use diffusion policies to generate these control actions.
Despite their promising results, these models are computationally expensive and
suffer from weak perception. To address these limitations, we present FlowNav,
a novel approach that uses a combination of Conditional Flow Matching (CFM) and
depth priors from off-the-shelf foundation models to learn action policies for
robot navigation. FlowNav is significantly more accurate at navigation and
exploration than state-of-the-art methods. We validate our contributions using
real robot experiments in multiple unseen environments, demonstrating improved
navigation reliability and accuracy. We make the code and trained models
publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IROS'25. Previous version accepted at CoRL 2024 workshop
  on Learning Effective Abstractions for Planning (LEAP) and workshop on
  Differentiable Optimization Everywhere: Simulation, Estimation, Learning, and
  Control</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMDVS-LF: <span class="highlight-title">Multi</span>-Modal Dynamic Vision Sensor and Eye-Tracking <span class="highlight-title">Dataset</span> for
  Line Following 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.18038v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.18038v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Resch, Mónika Farsang, Radu Grosu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dynamic Vision Sensors (DVS) offer a unique advantage in control applications
due to their high temporal resolution and asynchronous event-based data. Still,
their adoption in machine learning algorithms remains limited. To address this
gap and promote the development of models that leverage the specific
characteristics of DVS data, we introduce the MMDVS-LF: Multi-Modal Dynamic
Vision Sensor and Eye-Tracking Dataset for Line Following. This comprehensive
dataset is the first to integrate multiple sensor modalities, including DVS
recordings and eye-tracking data from a small-scale standardized vehicle.
Additionally, the dataset includes RGB video, odometry, Inertial Measurement
Unit (IMU) data, and demographic data of drivers performing a Line Following.
With its diverse range of data, MMDVS-LF opens new opportunities for developing
event-based deep learning algorithms just like the MNIST dataset did for
Convolutional Neural Networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Physically Realizable Adversarial Attacks in Embodied Vision
  Navigation <span class="chip">IROS</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.10071v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.10071v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Chen, Jiawei Tu, Chao Qi, Yonghao Dang, Feng Zhou, Wei Wei, Jianqin Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The significant advancements in embodied vision navigation have raised
concerns about its susceptibility to adversarial attacks exploiting deep neural
networks. Investigating the adversarial robustness of embodied vision
navigation is crucial, especially given the threat of 3D physical attacks that
could pose risks to human safety. However, existing attack methods for embodied
vision navigation often lack physical feasibility due to challenges in
transferring digital perturbations into the physical world. Moreover, current
physical attacks for object detection struggle to achieve both multi-view
effectiveness and visual naturalness in navigation scenarios. To address this,
we propose a practical attack method for embodied navigation by attaching
adversarial patches to objects, where both opacity and textures are learnable.
Specifically, to ensure effectiveness across varying viewpoints, we employ a
multi-view optimization strategy based on object-aware sampling, which
optimizes the patch's texture based on feedback from the vision-based
perception model used in navigation. To make the patch inconspicuous to human
observers, we introduce a two-stage opacity optimization mechanism, in which
opacity is fine-tuned after texture optimization. Experimental results
demonstrate that our adversarial patches decrease the navigation success rate
by an average of 22.39%, outperforming previous methods in practicality,
effectiveness, and naturalness. Code is available at:
https://github.com/chen37058/Physical-Attacks-in-Embodied-Nav
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 7 figures, submitted to IEEE/RSJ International Conference on
  Intelligent Robots and Systems (IROS) 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Using High-Level Patterns to Estimate How Humans Predict a Robot will
  Behave 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13533v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13533v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sagar Parekh, Lauren Bramblett, Nicola Bezzo, Dylan P. Losey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans interacting with robots often form predictions of what the robot will
do next. For instance, based on the recent behavior of an autonomous car, a
nearby human driver might predict that the car is going to remain in the same
lane. It is important for the robot to understand the human's prediction for
safe and seamless interaction: e.g., if the autonomous car knows the human
thinks it is not merging -- but the autonomous car actually intends to merge --
then the car can adjust its behavior to prevent an accident. Prior works
typically assume that humans make precise predictions of robot behavior.
However, recent research on human-human prediction suggests the opposite:
humans tend to approximate other agents by predicting their high-level
behaviors. We apply this finding to develop a second-order theory of mind
approach that enables robots to estimate how humans predict they will behave.
To extract these high-level predictions directly from data, we embed the recent
human and robot trajectories into a discrete latent space. Each element of this
latent space captures a different type of behavior (e.g., merging in front of
the human, remaining in the same lane) and decodes into a vector field across
the state space that is consistent with the underlying behavior type. We
hypothesize that our resulting high-level and course predictions of robot
behavior will correspond to actual human predictions. We provide initial
evidence in support of this hypothesis through proof-of-concept simulations,
testing our method's predictions against those of real users, and experiments
on a real-world interactive driving dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Designing Robots to Help Women 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.04123v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.04123v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Cooney, Lena Klasén, Fernando Alonso-Fernandez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robots are being designed to help people in an increasing variety of
settings--but seemingly little attention has been given so far to the specific
needs of women, who represent roughly half of the world's population but are
highly underrepresented in robotics. Here we used a speculative prototyping
approach to explore this expansive design space: First, we identified some
potential challenges of interest, including crimes and illnesses that
disproportionately affect women, as well as potential opportunities for
designers, which were visualized in five sketches. Then, one of the sketched
scenarios was further explored by developing a prototype, of a robotic helper
drone equipped with computer vision to detect hidden cameras that could be used
to spy on women. While object detection introduced some errors, hidden cameras
were identified with a reasonable accuracy of 80% (Intersection over Union
(IoU) score: 0.40). Our aim is that the identified challenges and opportunities
could help spark discussion and inspire designers, toward realizing a safer,
more inclusive future through responsible use of technology.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, Accepted Version of a Published Conference Paper in 14th
  Scandinavian Conference on Artificial Intelligence (SCAI 2024): AI for a
  better society, June 10-11, 2024, J\"onk\"oping, Sweden, Link\"oping
  Electronic Conference Proceedings (ECP) ISSN: 1650-3740 (CC BY 4.0 License)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Preference Aligned Diffusion Planner for Quadrupedal Locomotion Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13586v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13586v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Yuan, Zhiwei Shang, Zifan Wang, Chenkai Wang, Zhao Shan, Meixin Zhu, Chenjia Bai, Xuelong Li, Weiwei Wan, Kensuke Harada
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models demonstrate superior performance in capturing complex
distributions from large-scale datasets, providing a promising solution for
quadrupedal locomotion control. However, the robustness of the diffusion
planner is inherently dependent on the diversity of the pre-collected datasets.
To mitigate this issue, we propose a two-stage learning framework to enhance
the capability of the diffusion planner under limited dataset
(reward-agnostic). Through the offline stage, the diffusion planner learns the
joint distribution of state-action sequences from expert datasets without using
reward labels. Subsequently, we perform the online interaction in the
simulation environment based on the trained offline planner, which
significantly diversified the original behavior and thus improves the
robustness. Specifically, we propose a novel weak preference labeling method
without the ground-truth reward or human preferences. The proposed method
exhibits superior stability and velocity tracking accuracy in pacing, trotting,
and bounding gait under different speeds and can perform a zero-shot transfer
to the real Unitree Go1 robots. The project website for this paper is at
https://shangjaven.github.io/preference-aligned-diffusion-legged.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stereo Hand-Object Reconstruction for Human-to-Robot Handover 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07487v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07487v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yik Lung Pang, Alessio Xompero, Changjae Oh, Andrea Cavallaro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Jointly estimating hand and object shape facilitates the grasping task in
human-to-robot handovers. However, relying on hand-crafted prior knowledge
about the geometric structure of the object fails when generalising to unseen
objects, and depth sensors fail to detect transparent objects such as drinking
glasses. In this work, we propose a stereo-based method for hand-object
reconstruction that combines single-view reconstructions probabilistically to
form a coherent stereo reconstruction. We learn 3D shape priors from a large
synthetic hand-object dataset to ensure that our method is generalisable, and
use RGB inputs to better capture transparent objects. We show that our method
reduces the object Chamfer distance compared to existing RGB based hand-object
reconstruction methods on single view and stereo settings. We process the
reconstructed hand-object shape with a projection-based outlier removal step
and use the output to guide a human-to-robot handover pipeline with
wide-baseline stereo RGB cameras. Our hand-object reconstruction enables a
robot to successfully receive a diverse range of household objects from the
human.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 9 figures, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MATCH POLICY: A Simple Pipeline from Point Cloud Registration to
  Manipulation Policies 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.15517v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.15517v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haojie Huang, Haotian Liu, Dian Wang, Robin Walters, Robert Platt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many manipulation tasks require the robot to rearrange objects relative to
one another. Such tasks can be described as a sequence of relative poses
between parts of a set of rigid bodies. In this work, we propose MATCH POLICY,
a simple but novel pipeline for solving high-precision pick and place tasks.
Instead of predicting actions directly, our method registers the pick and place
targets to the stored demonstrations. This transfers action inference into a
point cloud registration task and enables us to realize nontrivial manipulation
policies without any training. MATCH POLICY is designed to solve high-precision
tasks with a key-frame setting. By leveraging the geometric interaction and the
symmetries of the task, it achieves extremely high sample efficiency and
generalizability to unseen configurations. We demonstrate its state-of-the-art
performance across various tasks on RLBench benchmark compared with several
strong baselines and test it on a real robot with six tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>project url: https://haojhuang.github.io/match_page/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ForceMimic: Force-Centric Imitation Learning with Force-Motion Capture
  System for Contact-Rich Manipulation <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07554v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07554v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhai Liu, Junbo Wang, Yiming Wang, Weiming Wang, Cewu Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In most contact-rich manipulation tasks, humans apply time-varying forces to
the target object, compensating for inaccuracies in the vision-guided hand
trajectory. However, current robot learning algorithms primarily focus on
trajectory-based policy, with limited attention given to learning force-related
skills. To address this limitation, we introduce ForceMimic, a force-centric
robot learning system, providing a natural, force-aware and robot-free robotic
demonstration collection system, along with a hybrid force-motion imitation
learning algorithm for robust contact-rich manipulation. Using the proposed
ForceCapture system, an operator can peel a zucchini in 5 minutes, while
force-feedback teleoperation takes over 13 minutes and struggles with task
completion. With the collected data, we propose HybridIL to train a
force-centric imitation learning model, equipped with hybrid force-position
control primitive to fit the predicted wrench-position parameters during robot
execution. Experiments demonstrate that our approach enables the model to learn
a more robust policy under the contact-rich task of vegetable peeling,
increasing the success rates by 54.5% relatively compared to state-ofthe-art
pure-vision-based imitation learning. Hardware, code, data and more results can
be found on the project website at https://forcemimic.github.io.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 7 figures, accepted by 2025 IEEE International Conference on
  Robotics and Automation (ICRA 2025), the first three authors contribute
  equally, project website at https://forcemimic.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Interpretable Data-Driven Ship Dynamics Model: Enhancing Physics-Based
  Motion Prediction with Parameter Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18696v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18696v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christos Papandreou, Michail Mathioudakis, Theodoros Stouraitis, Petros Iatropoulos, Antonios Nikitakis, Stavros Paschalakis, Konstantinos Kyriakopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deployment of autonomous navigation systems on ships necessitates
accurate motion prediction models tailored to individual vessels. Traditional
physics-based models, while grounded in hydrodynamic principles, often fail to
account for ship-specific behaviors under real-world conditions. Conversely,
purely data-driven models offer specificity but lack interpretability and
robustness in edge cases. This study proposes a data-driven physics-based model
that integrates physics-based equations with data-driven parameter
optimization, leveraging the strengths of both approaches to ensure
interpretability and adaptability. The model incorporates physics-based
components such as 3-DoF dynamics, rudder, and propeller forces, while
parameters such as resistance curve and rudder coefficients are optimized using
synthetic data. By embedding domain knowledge into the parameter optimization
process, the fitted model maintains physical consistency. Validation of the
approach is realized with two container ships by comparing, both qualitatively
and quantitatively, predictions against ground-truth trajectories. The results
demonstrate significant improvements, in predictive accuracy and reliability,
of the data-driven physics-based models over baseline physics-based models
tuned with traditional marine engineering practices. The fitted models capture
ship-specific behaviors in diverse conditions with their predictions being,
51.6% (ship A) and 57.8% (ship B) more accurate, 72.36% (ship A) and 89.67%
(ship B) more consistent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ephemerality meets LiDAR-based Lifelong Mapping <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyeonjae Gil, Dongjae Lee, Giseop Kim, Ayoung Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lifelong mapping is crucial for the long-term deployment of robots in dynamic
environments. In this paper, we present ELite, an ephemerality-aided
LiDAR-based lifelong mapping framework which can seamlessly align multiple
session data, remove dynamic objects, and update maps in an end-to-end fashion.
Map elements are typically classified as static or dynamic, but cases like
parked cars indicate the need for more detailed categories than binary. Central
to our approach is the probabilistic modeling of the world into two-stage
$\textit{ephemerality}$, which represent the transiency of points in the map
within two different time scales. By leveraging the spatiotemporal context
encoded in ephemeralities, ELite can accurately infer transient map elements,
maintain a reliable up-to-date static map, and improve robustness in aligning
the new data in a more fine-grained manner. Extensive real-world experiments on
long-term datasets demonstrate the robustness and effectiveness of our system.
The source code is publicly available for the robotics community:
https://github.com/dongjae0107/ELite.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6+2 pages, 11 figures, accepted at ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhanced Optimization Strategies to Design an Underactuated Hand
  Exoskeleton 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.07384v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.07384v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baris Akbas, Huseyin Taner Yuksel, Aleyna Soylemez, Mine Sarac, Fabio Stroppa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exoskeletons can boost human strength and provide assistance to individuals
with physical disabilities. However, ensuring safety and optimal performance in
their design poses substantial challenges. This study presents the design
process for an underactuated hand exoskeleton (U-HEx), first including a single
objective (maximizing force transmission), then expanding into multi objective
(also minimizing torque variance and actuator displacement). The optimization
relies on a Genetic Algorithm, the Big Bang-Big Crunch Algorithm, and their
versions for multi-objective optimization. Analyses revealed that using Big
Bang-Big Crunch provides high and more consistent results in terms of
optimality with lower convergence time. In addition, adding more objectives
offers a variety of trade-off solutions to the designers, who might later set
priorities for the objectives without repeating the process - at the cost of
complicating the optimization algorithm and computational burden. These
findings underline the importance of performing proper optimization while
designing exoskeletons, as well as providing a significant improvement to this
specific robotic design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures, 9 talbes</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HOPE: A Reinforcement Learning-based Hybrid Policy Path Planner for
  Diverse Parking Scenarios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20579v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20579v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyang Jiang, Yueyuan Li, Songan Zhang, Siyuan Chen, Chunxiang Wang, Ming Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated parking stands as a highly anticipated application of autonomous
driving technology. However, existing path planning methodologies fall short of
addressing this need due to their incapability to handle the diverse and
complex parking scenarios in reality. While non-learning methods provide
reliable planning results, they are vulnerable to intricate occasions, whereas
learning-based ones are good at exploration but unstable in converging to
feasible solutions. To leverage the strengths of both approaches, we introduce
Hybrid pOlicy Path plannEr (HOPE). This novel solution integrates a
reinforcement learning agent with Reeds-Shepp curves, enabling effective
planning across diverse scenarios. HOPE guides the exploration of the
reinforcement learning agent by applying an action mask mechanism and employs a
transformer to integrate the perceived environmental information with the mask.
To facilitate the training and evaluation of the proposed planner, we propose a
criterion for categorizing the difficulty level of parking scenarios based on
space and obstacle distribution. Experimental results demonstrate that our
approach outperforms typical rule-based algorithms and traditional
reinforcement learning methods, showing higher planning success rates and
generalization across various scenarios. We also conduct real-world experiments
to verify the practicability of HOPE. The code for our solution is openly
available on https://github.com/jiamiya/HOPE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by T-ITS. 11 pages, 5 tables, 6 figures, 2 page appendix</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-03-02T00:00:00Z">2025-03-02</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">48</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ConFit v2: Improving Resume-Job Matching using Hypothetical Resume
  Embedding and Runner-Up Hard-Negative Mining 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12361v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12361v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Yu, Ruize Xu, Chengyuan Xue, Jinzhong Zhang, Xu Ma, Zhou Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A reliable resume-job matching system helps a company recommend suitable
candidates from a pool of resumes and helps a job seeker find relevant jobs
from a list of job posts. However, since job seekers apply only to a few jobs,
interaction labels in resume-job datasets are sparse. We introduce ConFit v2,
an improvement over ConFit to tackle this sparsity problem. We propose two
techniques to enhance the encoder's contrastive training process: augmenting
job data with hypothetical reference resume generated by a large language
model; and creating high-quality hard negatives from unlabeled resume/job pairs
using a novel hard-negative mining strategy. We evaluate ConFit v2 on two
real-world datasets and demonstrate that it outperforms ConFit and prior
methods (including BM25 and OpenAI text-embedding-003), achieving an average
absolute improvement of 13.8% in recall and 17.5% in nDCG across job-ranking
and resume-ranking tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2401.16349</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inference to the Best Explanation in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.10767v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.10767v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dhairya Dalal, Marco Valentino, André Freitas, Paul Buitelaar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Large Language Models (LLMs) have found success in real-world
applications, their underlying explanatory process is still poorly understood.
This paper proposes IBE-Eval, a framework inspired by philosophical accounts on
Inference to the Best Explanation (IBE) to advance the interpretation and
evaluation of LLMs' explanations. IBE-Eval estimates the plausibility of
natural language explanations through a combination of explicit logical and
linguistic features including: consistency, parsimony, coherence, and
uncertainty. Extensive experiments are conducted on Causal Question Answering
(CQA), where \textit{IBE-Eval} is tasked to select the most plausible causal
explanation amongst competing ones generated by LLMs (i.e., GPT 3.5 and Llama
2). The experiments reveal that IBE-Eval can successfully identify the best
explanation with up to 77\% accuracy ($\approx 27\%$ above random), improving
upon a GPT 3.5-as-a-Judge baseline ($\approx+17\%$) while being intrinsically
more efficient and interpretable. Additional analyses suggest that, despite
model-specific variances, LLM-generated explanations tend to conform to IBE
criteria and that IBE-Eval is significantly correlated with human judgment,
opening up opportunities for future development of automated explanation
verification tools.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inference Scaling for Long-Context Retrieval Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04343v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04343v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenrui Yue, Honglei Zhuang, Aijun Bai, Kai Hui, Rolf Jagerman, Hansi Zeng, Zhen Qin, Dong Wang, Xuanhui Wang, Michael Bendersky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The scaling of inference computation has unlocked the potential of
long-context large language models (LLMs) across diverse settings. For
knowledge-intensive tasks, the increased compute is often allocated to
incorporate more external knowledge. However, without effectively utilizing
such knowledge, solely expanding context does not always enhance performance.
In this work, we investigate inference scaling for retrieval augmented
generation (RAG), exploring the combination of multiple strategies beyond
simply increasing the quantity of knowledge, including in-context learning and
iterative prompting. These strategies provide additional flexibility to scale
test-time computation (e.g., by increasing retrieved documents or generation
steps), thereby enhancing LLMs' ability to effectively acquire and utilize
contextual information. We address two key questions: (1) How does RAG
performance benefit from the scaling of inference computation when optimally
configured? (2) Can we predict the optimal test-time compute allocation for a
given budget by modeling the relationship between RAG performance and inference
parameters? Our observations reveal that increasing inference computation leads
to nearly linear gains in RAG performance when optimally allocated, a
relationship we describe as the inference scaling laws for RAG. Building on
this, we further develop the computation allocation model to estimate RAG
performance across different inference configurations. The model predicts
optimal inference parameters under various computation constraints, which align
closely with the experimental results. By applying these optimal
configurations, we demonstrate that scaling inference compute on long-context
LLMs achieves up to 58.9% gains on benchmark datasets compared to standard RAG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Distributed</span> Speculative Inference (DSI): Speculation Parallelism for
  Provably Faster Lossless Language Model Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14105v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14105v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nadav Timor, Jonathan Mamou, Daniel Korat, Moshe Berchansky, Oren Pereg, Moshe Wasserblat, Tomer Galanti, Michal Gordon, David Harel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces distributed speculative inference (DSI), a novel
inference algorithm that is provably faster than speculative inference (SI)
[leviathan2023, chen2023, miao2024, sun2025, timor2025] and standard
autoregressive inference (non-SI). Like other SI algorithms, DSI operates on
frozen language models (LMs), requiring no training or architectural
modifications, and it preserves the target distribution. Prior studies on SI
have demonstrated empirical speedups over non-SI--but rely on sufficiently fast
and accurate drafters, which are often unavailable in practice. We identify a
gap where SI can be slower than non-SI if drafters are too slow or inaccurate.
We close this gap by proving that DSI is faster than both SI and non-SI--given
any drafters. DSI is therefore not only faster than SI, but also unlocks the
acceleration of LMs for which SI fails. DSI leverages speculation parallelism
(SP), a novel type of task parallelism, to orchestrate target and drafter
instances that overlap in time, establishing a new foundational tradeoff
between computational resources and latency. Our simulations show that DSI is
1.29-1.92x faster than SI in single-node setups for various off-the-shelf LMs
and tasks. We open-source all our code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2025. (Link:
  https://openreview.net/forum?id=cJd1BgZ9CS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prompting Fairness: Integrating Causality to Debias Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08743v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08743v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingling Li, Zeyu Tang, Xiaoyu Liu, Peter Spirtes, Kun Zhang, Liu Leqi, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), despite their remarkable capabilities, are
susceptible to generating biased and discriminatory responses. As LLMs
increasingly influence high-stakes decision-making (e.g., hiring and
healthcare), mitigating these biases becomes critical. In this work, we propose
a causality-guided debiasing framework to tackle social biases, aiming to
reduce the objectionable dependence between LLMs' decisions and the social
information in the input. Our framework introduces a novel perspective to
identify how social information can affect an LLM's decision through different
causal pathways. Leveraging these causal insights, we outline principled
prompting strategies that regulate these pathways through selection mechanisms.
This framework not only unifies existing prompting-based debiasing techniques,
but also opens up new directions for reducing bias by encouraging the model to
prioritize fact-based reasoning over reliance on biased social cues. We
validate our framework through extensive experiments on real-world datasets
across multiple domains, demonstrating its effectiveness in debiasing LLM
decisions, even with only black-box access to the model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Dual Process Theory in Language Agent Framework for Real-time
  Simultaneous Human-AI Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11882v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11882v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shao Zhang, Xihuai Wang, Wenhao Zhang, Chaoran Li, Junru Song, Tingyu Li, Lin Qiu, Xuezhi Cao, Xunliang Cai, Wen Yao, Weinan Zhang, Xinbing Wang, Ying Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agents built on large language models (LLMs) have excelled in turn-by-turn
human-AI collaboration but struggle with simultaneous tasks requiring real-time
interaction. Latency issues and the challenge of inferring variable human
strategies hinder their ability to make autonomous decisions without explicit
instructions. Through experiments with current independent System 1 and System
2 methods, we validate the necessity of using Dual Process Theory (DPT) in
real-time tasks. We propose DPT-Agent, a novel language agent framework that
integrates System 1 and System 2 for efficient real-time simultaneous human-AI
collaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and
code-as-policy for fast, intuitive, and controllable decision-making.
DPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous
reflection to infer human intentions and perform reasoning-based autonomous
decisions. We demonstrate the effectiveness of DPT-Agent through further
experiments with rule-based agents and human collaborators, showing significant
improvements over mainstream LLM-based frameworks. DPT-Agent can effectively
help LLMs convert correct slow thinking and reasoning into executable actions,
thereby improving performance. To the best of our knowledge, DPT-Agent is the
first language agent framework that achieves successful real-time simultaneous
human-AI collaboration autonomously. Code of DPT-Agent can be found in
https://github.com/sjtu-marl/DPT-Agent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint under review. Update the experimental results of the
  DeepSeek-R1 series models, o3-mini-high and o3-mini-medium</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Harnessing <span class="highlight-title">Multi</span>ple Large Language Models: A <span class="highlight-title">Survey</span> on <span class="highlight-title">LLM</span> Ensemble 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18036v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18036v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijun Chen, Jingzheng Li, Pengpeng Chen, Zhuoran Li, Kai Sun, Yuankai Luo, Qianren Mao, Dingqi Yang, Hailong Sun, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  LLM Ensemble -- which involves the comprehensive use of multiple large
language models (LLMs), each aimed at handling user queries during downstream
inference, to benefit from their individual strengths -- has gained substantial
attention recently. The widespread availability of LLMs, coupled with their
varying strengths and out-of-the-box usability, has profoundly advanced the
field of LLM Ensemble. This paper presents the first systematic review of
recent developments in LLM Ensemble. First, we introduce our taxonomy of LLM
Ensemble and discuss several related research problems. Then, we provide a more
in-depth classification of the methods under the broad categories of
"ensemble-before-inference, ensemble-during-inference,
ensemble-after-inference'', and review all relevant methods. Finally, we
introduce related benchmarks and applications, summarize existing studies, and
suggest several future research directions. A curated list of papers on LLM
Ensemble is available at https://github.com/junchenzhi/Awesome-LLM-Ensemble.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 2 figures, codebase:
  https://github.com/junchenzhi/Awesome-LLM-Ensemble</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models Meet Symbolic Provers for Logical Reasoning
  Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.06563v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.06563v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengwen Qi, Ren Ma, Bowen Li, He Du, Binyuan Hui, Jinwang Wu, Yuanjun Laili, Conghui He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  First-order logic (FOL) reasoning, which involves sequential deduction, is
pivotal for intelligent systems and serves as a valuable task for evaluating
reasoning capabilities, particularly in chain-of-thought (CoT) contexts.
Existing benchmarks often rely on extensive human annotation or handcrafted
templates, making it difficult to achieve the necessary complexity,
scalability, and diversity for robust evaluation. To address these limitations,
we propose a novel framework called ProverGen that synergizes the generative
strengths of Large Language Models (LLMs) with the rigor and precision of
symbolic provers, enabling the creation of a scalable, diverse, and
high-quality FOL reasoning dataset, ProverQA. ProverQA is also distinguished by
its inclusion of accessible and logically coherent intermediate reasoning steps
for each problem. Our evaluation shows that state-of-the-art LLMs struggle to
solve ProverQA problems, even with CoT prompting, highlighting the dataset's
challenging nature. We also finetune Llama3.1-8B-Instruct on a separate
training set generated by our framework. The finetuned model demonstrates
consistent improvements on both in-distribution and out-of-distribution test
sets, suggesting the value of our proposed data generation framework. Code
available at: https://github.com/opendatalab/ProverGen
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generative causal testing to bridge data-driven models and scientific
  theories in language neuroscience 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.00812v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.00812v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Richard Antonello, Chandan Singh, Shailee Jain, Aliyah Hsu, Sihang Guo, Jianfeng Gao, Bin Yu, Alexander Huth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representations from large language models are highly effective at predicting
BOLD fMRI responses to language stimuli. However, these representations are
largely opaque: it is unclear what features of the language stimulus drive the
response in each brain area. We present generative causal testing (GCT), a
framework for generating concise explanations of language selectivity in the
brain from predictive models and then testing those explanations in follow-up
experiments using LLM-generated stimuli.This approach is successful at
explaining selectivity both in individual voxels and cortical regions of
interest (ROIs), including newly identified microROIs in prefrontal cortex. We
show that explanatory accuracy is closely related to the predictive power and
stability of the underlying predictive models. Finally, we show that GCT can
dissect fine-grained differences between brain areas with similar functional
selectivity. These results demonstrate that LLMs can be used to bridge the
widening gap between data-driven models and formal scientific theories.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Mighty ToRR: A Benchmark for Table Reasoning and Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19412v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19412v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shir Ashury-Tahan, Yifan Mai, Rajmohan C, Ariel Gera, Yotam Perlitz, Asaf Yehudai, Elron Bandel, Leshem Choshen, Eyal Shnarch, Percy Liang, Michal Shmueli-Scheuer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite its real-world significance, model performance on tabular data
remains underexplored, leaving uncertainty about which model to rely on and
which prompt configuration to adopt. To address this gap, we create ToRR, a
benchmark for Table Reasoning and Robustness, measuring model performance and
robustness on table-related tasks. The benchmark includes 10 datasets that
cover different types of table reasoning capabilities across varied domains.
ToRR goes beyond model performance rankings, and is designed to reflect whether
models can handle tabular data consistently and robustly, across a variety of
common table representation formats. We present a leaderboard as well as
comprehensive analyses of the results of leading models over ToRR. Our results
reveal a striking pattern of brittle model behavior, where even strong models
are unable to perform robustly on tabular data tasks. Although no specific
table format leads to consistently better performance, we show that testing
over multiple formats is crucial for reliably estimating model capabilities.
Moreover, we show that the reliability boost from testing multiple prompts can
be equivalent to adding more test examples. Overall, our findings show that
table understanding and reasoning tasks remain a significant challenge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLaVA-Mini: Efficient Image and Video Large <span class="highlight-title">Multi</span>modal Models with One
  Vision Token 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03895v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03895v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaolei Zhang, Qingkai Fang, Zhe Yang, Yang Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of real-time large multimodal models (LMMs) like GPT-4o has
sparked considerable interest in efficient LMMs. LMM frameworks typically
encode visual inputs into vision tokens (continuous representations) and
integrate them and textual instructions into the context of large language
models (LLMs), where large-scale parameters and numerous context tokens
(predominantly vision tokens) result in substantial computational overhead.
Previous efforts towards efficient LMMs always focus on replacing the LLM
backbone with smaller models, while neglecting the crucial issue of token
quantity. In this paper, we introduce LLaVA-Mini, an efficient LMM with minimal
vision tokens. To achieve a high compression ratio of vision tokens while
preserving visual information, we first analyze how LMMs understand vision
tokens and find that most vision tokens only play a crucial role in the early
layers of LLM backbone, where they mainly fuse visual information into text
tokens. Building on this finding, LLaVA-Mini introduces modality pre-fusion to
fuse visual information into text tokens in advance, thereby facilitating the
extreme compression of vision tokens fed to LLM backbone into one token.
LLaVA-Mini is a unified large multimodal model that can support the
understanding of images, high-resolution images, and videos in an efficient
manner. Experiments across 11 image-based and 7 video-based benchmarks
demonstrate that LLaVA-Mini outperforms LLaVA-v1.5 with just 1 vision token
instead of 576. Efficiency analyses reveal that LLaVA-Mini can reduce FLOPs by
77%, deliver low-latency responses within 40 milliseconds, and process over
10,000 frames of video on the GPU hardware with 24GB of memory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025. Code: https://github.com/ictnlp/LLaVA-Mini
  Model: https://huggingface.co/ICTNLP/llava-mini-llama-3.1-8b</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Steering Large Language Models between Code Execution and Textual
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03524v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03524v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongchao Chen, Harsh Jhamtani, Srinagesh Sharma, Chuchu Fan, Chi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While a lot of recent research focuses on enhancing the textual reasoning
capabilities of Large Language Models (LLMs) by optimizing the multi-agent
framework or reasoning chains, several benchmark tasks can be solved with 100\%
success through direct coding, which is more scalable and avoids the
computational overhead associated with textual iterating and searching. Textual
reasoning has inherent limitations in solving tasks with challenges in math,
logics, optimization, and searching, which is unlikely to be solved by simply
scaling up the model and data size. The recently released OpenAI GPT Code
Interpreter and multi-agent frameworks such as AutoGen have demonstrated
remarkable proficiency of integrating code generation and execution to solve
complex tasks using LLMs. However, based on our experiments on 7 existing
popular methods for steering code/text generation in both single- and
multi-turn settings with 14 tasks and 6 types of LLMs (including the new
O1-preview), currently there is no optimal method to correctly steer LLMs to
write code when needed. We discover some interesting patterns on when models
use code vs. textual reasoning with the evolution to task complexity and model
sizes, which even result in an astonishingly inverse scaling behavior. We also
discover that results from LLM written code are not always better than using
textual reasoning, even if the task could be solved through code. To mitigate
the above issues, we propose three methods to better steer LLM code/text
generation and achieve a notable improvement. The costs of token lengths and
runtime are thoroughly discussed for all the methods. We believe the problem of
steering LLM code/text generation is critical for future research and has much
space for further improvement. Project Page, Datasets, and Codes are available
at https://yongchao98.github.io/CodeSteer/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 12 figures, 12 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Attention Sink Emerges in Language Models: An Empirical View 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10781v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10781v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangming Gu, Tianyu Pang, Chao Du, Qian Liu, Fengzhuo Zhang, Cunxiao Du, Ye Wang, Min Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Models (LMs) assign significant attention to the first token, even
if it is not semantically important, which is known as attention sink. This
phenomenon has been widely adopted in applications such as streaming/long
context generation, KV cache optimization, inference acceleration, model
quantization, and others. Despite its widespread use, a deep understanding of
attention sink in LMs is still lacking. In this work, we first demonstrate that
attention sinks exist universally in LMs with various inputs, even in small
models. Furthermore, attention sink is observed to emerge during the LM
pre-training, motivating us to investigate how optimization, data distribution,
loss function, and model architecture in LM pre-training influence its
emergence. We highlight that attention sink emerges after effective
optimization on sufficient training data. The sink position is highly
correlated with the loss function and data distribution. Most importantly, we
find that attention sink acts more like key biases, storing extra attention
scores, which could be non-informative and not contribute to the value
computation. We also observe that this phenomenon (at least partially) stems
from tokens' inner dependence on attention scores as a result of softmax
normalization. After relaxing such dependence by replacing softmax attention
with other attention operations, such as sigmoid attention without
normalization, attention sinks do not emerge in LMs up to 1B parameters. The
code is available at https://github.com/sail-sg/Attention-Sink.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generating Visual Stories with Grounded and Coreferent Characters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13555v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13555v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danyang Liu, Mirella Lapata, Frank Keller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Characters are important in narratives. They move the plot forward, create
emotional connections, and embody the story's themes. Visual storytelling
methods focus more on the plot and events relating to it, without building the
narrative around specific characters. As a result, the generated stories feel
generic, with character mentions being absent, vague, or incorrect. To mitigate
these issues, we introduce the new task of character-centric story generation
and present the first model capable of predicting visual stories with
consistently grounded and coreferent character mentions. Our model is finetuned
on a new dataset which we build on top of the widely used VIST benchmark.
Specifically, we develop an automated pipeline to enrich VIST with visual and
textual character coreference chains. We also propose new evaluation metrics to
measure the richness of characters and coreference in stories. Experimental
results show that our model generates stories with recurring characters which
are consistent and coreferent to larger extent compared to baselines and
state-of-the-art systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cheating Automatic <span class="highlight-title">LLM</span> Benchmarks: Null Models Achieve High Win Rates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07137v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07137v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Jing Jiang, Min Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and
MT-Bench, have become popular for evaluating language models due to their
cost-effectiveness and scalability compared to human evaluation. Achieving high
win rates on these benchmarks can significantly boost the promotional impact of
newly released language models. This promotional benefit may motivate tricks,
such as manipulating model output length or style to game win rates, even
though several mechanisms have been developed to control length and disentangle
style to reduce gameability. Nonetheless, we show that even a "null model" that
always outputs a constant response (irrelevant to input instructions) can cheat
automatic benchmarks and achieve top-ranked win rates: an 86.5% LC win rate on
AlpacaEval 2.0; an 83.0 score on Arena-Hard-Auto; and a 9.55 score on MT-Bench.
Moreover, the crafted cheating outputs are transferable because we assume that
the instructions of these benchmarks (e.g., 805 samples of AlpacaEval 2.0) are
private and cannot be accessed. While our experiments are primarily
proof-of-concept, an adversary could use LLMs to generate more imperceptible
cheating responses, unethically benefiting from high win rates and promotional
impact. Our findings call for the development of anti-cheating mechanisms for
reliable automatic benchmarks. The code is available at
https://github.com/sail-sg/Cheating-LLM-Benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Does A Text Preprocessing Pipeline Affect Ontology Syntactic
  Matching? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03962v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03962v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangcheng Qiang, Kerry Taylor, Weiqing Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The generic text preprocessing pipeline, comprising Tokenisation,
Normalisation, Stop Words Removal, and Stemming/Lemmatisation, has been
implemented in many systems for syntactic ontology matching (OM). However, the
lack of standardisation in text preprocessing creates diversity in mapping
results. In this paper, we investigate the effect of the text preprocessing
pipeline on syntactic OM in 8 Ontology Alignment Evaluation Initiative (OAEI)
tracks with 49 distinct alignments. We find that Phase 1 text preprocessing
(Tokenisation and Normalisation) is currently more effective than Phase 2 text
preprocessing (Stop Words Removal and Stemming/Lemmatisation). To repair the
less effective Phase 2 text preprocessing caused by unwanted false mappings, we
propose a novel context-based pipeline repair approach that employs an ad hoc
check to find common words that cause false mappings. These words are stored in
a reserved word set and applied in text preprocessing. The experimental results
show that our approach improves the matching correctness and the overall
matching performance. We also discuss the integration of the classical text
preprocessing pipeline with modern large language models (LLMs). We recommend
that LLMs inject the text preprocessing pipeline via function calling to avoid
the tendency towards unstable true mappings produced by prompt-based LLM
approaches, and use LLMs to repair false mappings generated by the text
preprocessing pipeline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 11 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intrinsic Dimension Correlation: uncovering nonlinear connections in
  <span class="highlight-title">multi</span>modal representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15812v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15812v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Basile, Santiago Acevedo, Luca Bortolussi, Fabio Anselmi, Alex Rodriguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To gain insight into the mechanisms behind machine learning methods, it is
crucial to establish connections among the features describing data points.
However, these correlations often exhibit a high-dimensional and strongly
nonlinear nature, which makes them challenging to detect using standard
methods. This paper exploits the entanglement between intrinsic dimensionality
and correlation to propose a metric that quantifies the (potentially nonlinear)
correlation between high-dimensional manifolds. We first validate our method on
synthetic data in controlled environments, showcasing its advantages and
drawbacks compared to existing techniques. Subsequently, we extend our analysis
to large-scale applications in neural network representations. Specifically, we
focus on latent representations of multimodal data, uncovering clear
correlations between paired visual and textual embeddings, whereas existing
methods struggle significantly in detecting similarity. Our results indicate
the presence of highly nonlinear correlation patterns between latent manifolds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Boosting Jailbreak Attack with Momentum 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.01229v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.01229v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihao Zhang, Zeming Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved remarkable success across diverse
tasks, yet they remain vulnerable to adversarial attacks, notably the
well-known jailbreak attack. In particular, the Greedy Coordinate Gradient
(GCG) attack has demonstrated efficacy in exploiting this vulnerability by
optimizing adversarial prompts through a combination of gradient heuristics and
greedy search. However, the efficiency of this attack has become a bottleneck
in the attacking process. To mitigate this limitation, in this paper we rethink
the generation of the adversarial prompts through an optimization lens, aiming
to stabilize the optimization process and harness more heuristic insights from
previous optimization iterations. Specifically, we propose the
\textbf{M}omentum \textbf{A}ccelerated G\textbf{C}G (\textbf{MAC}) attack,
which integrates a momentum term into the gradient heuristic to boost and
stabilize the random search for tokens in adversarial prompts. Experimental
results showcase the notable enhancement achieved by MAC over baselines in
terms of attack success rate and optimization efficiency. Moreover, we
demonstrate that MAC can still exhibit superior performance for transfer
attacks and models under defense mechanisms. Our code is available at
https://github.com/weizeming/momentum-attack-llm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICASSP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Developing a <span class="highlight-title">Multi</span>lingual <span class="highlight-title">Dataset</span> and Evaluation Metrics for
  Code-Switching: A Focus on Hong Kong's Polylingual Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17953v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17953v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Xie, Kani Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The existing audio datasets are predominantly tailored towards single
languages, overlooking the complex linguistic behaviors of multilingual
communities that engage in code-switching. This practice, where individuals
frequently mix two or more languages in their daily interactions, is
particularly prevalent in multilingual regions such as Hong Kong, China. To
bridge this gap, we have developed a 34.8-hour dataset of Mixed Cantonese and
English (MCE) audio using our Multi-Agent Data Generation Framework (MADGF). We
fine-tuned the open-source multilingual Automatic Speech Recognition (ASR)
model, Whisper, with the MCE dataset, leading to impressive zero-shot
performance. The traditional metrics overlook important factors such as latency
in real-world applications and code-switching scenarios. We have introduced a
novel evaluation metric called Fidelity to the Original Audio, Accuracy, and
Latency (FAL). This metric aims to overcome the limitations of traditional
metrics used to assess ASR systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Derailer-Rerailer: Adaptive Verification for Efficient and Reliable
  Language Model Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.13940v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.13940v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangya Wan, Yuqi Wu, Hao Wang, Shengming Zhao, Jie Chen, Sheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown impressive reasoning capabilities,
yet existing prompting methods face a critical trade-off: simple approaches
often struggle with complex tasks and reasoning stability, while more
sophisticated methods require multiple inferences and substantial computational
resources, limiting their practical deployment. To address this challenge, we
propose Derailer-Rerailer, a novel framework that adaptively balances reasoning
accuracy and computational efficiency. At its core, our framework employs a
lightweight Derailer mechanism to assess reasoning stability and selectively
triggers an advanced Rerailer verification process only when necessary, thereby
optimizing computational resource usage. Extensive evaluation across both open
and closed-source models on more than 20 categories of mathematical, symbolic,
and commonsense reasoning tasks demonstrates our framework's effectiveness:
Derailer-Rerailer achieves significant accuracy improvements (8-11\% across
various reasoning tasks) while maintaining 2-3 times better efficiency than
existing verification methods, with particularly strong performance in
mathematical and symbolic reasoning, offering a practical solution for
enhancing LLM reasoning reliability while significantly reducing computational
overhead.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Taxonomy, Opportunities, and Challenges of Representation Engineering
  for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19649v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19649v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Wehner, Sahar Abdelnabi, Daniel Tan, David Krueger, Mario Fritz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representation Engineering (RepE) is a novel paradigm for controlling the
behavior of LLMs. Unlike traditional approaches that modify inputs or fine-tune
the model, RepE directly manipulates the model's internal representations. As a
result, it may offer more effective, interpretable, data-efficient, and
flexible control over models' behavior. We present the first comprehensive
survey of RepE for LLMs, reviewing the rapidly growing literature to address
key questions: What RepE methods exist and how do they differ? For what
concepts and problems has RepE been applied? What are the strengths and
weaknesses of RepE compared to other methods? To answer these, we propose a
unified framework describing RepE as a pipeline comprising representation
identification, operationalization, and control. We posit that while RepE
methods offer significant potential, challenges remain, including managing
multiple concepts, ensuring reliability, and preserving models' performance.
Towards improving RepE, we identify opportunities for experimental and
methodological improvements and construct a guide for best practices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DiscoGraMS: Enhancing Movie Screen-Play Summarization using Movie
  Character-Aware Discourse Graph 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14666v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14666v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maitreya Prafulla Chitale, Uday Bindal, Rajakrishnan Rajkumar, Rahul Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Summarizing movie screenplays presents a unique set of challenges compared to
standard document summarization. Screenplays are not only lengthy, but also
feature a complex interplay of characters, dialogues, and scenes, with numerous
direct and subtle relationships and contextual nuances that are difficult for
machine learning models to accurately capture and comprehend. Recent attempts
at screenplay summarization focus on fine-tuning transformer-based pre-trained
models, but these models often fall short in capturing long-term dependencies
and latent relationships, and frequently encounter the "lost in the middle"
issue. To address these challenges, we introduce DiscoGraMS, a novel resource
that represents movie scripts as a movie character-aware discourse graph (CaD
Graph). This approach is well-suited for various downstream tasks, such as
summarization, question-answering, and salience detection. The model aims to
preserve all salient information, offering a more comprehensive and faithful
representation of the screenplay's content. We further explore a baseline
method that combines the CaD Graph with the corresponding movie script through
a late fusion of graph and text modalities, and we present very initial
promising results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NAACL 2025 (Main)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Market-Derived Financial Sentiment Analysis: Context-Aware Language
  Models for Crypto Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.14897v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.14897v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamid Moradi-Kamali, Mohammad-Hossein Rajabi-Ghozlou, Mahdi Ghazavi, Ali Soltani, Amirreza Sattarzadeh, Reza Entezari-Maleki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Financial Sentiment Analysis (FSA) traditionally relies on human-annotated
sentiment labels to infer investor sentiment and forecast market movements.
However, inferring the potential market impact of words based on their
human-perceived intentions is inherently challenging. We hypothesize that the
historical market reactions to words, offer a more reliable indicator of their
potential impact on markets than subjective sentiment interpretations by human
annotators. To test this hypothesis, a market-derived labeling approach is
proposed to assign tweet labels based on ensuing short-term price trends,
enabling the language model to capture the relationship between textual signals
and market dynamics directly. A domain-specific language model was fine-tuned
on these labels, achieving up to an 11% improvement in short-term trend
prediction accuracy over traditional sentiment-based benchmarks. Moreover, by
incorporating market and temporal context through prompt-tuning, the proposed
context-aware language model demonstrated an accuracy of 89.6% on a curated
dataset of 227 impactful Bitcoin-related news events with significant market
impacts. Aggregating daily tweet predictions into trading signals, our method
outperformed traditional fusion models (which combine sentiment-based and
price-based predictions). It challenged the assumption that sentiment-based
signals are inferior to price-based predictions in forecasting market
movements. Backtesting these signals across three distinct market regimes
yielded robust Sharpe ratios of up to 5.07 in trending markets and 3.73 in
neutral markets. Our findings demonstrate that language models can serve as
effective short-term market predictors. This paradigm shift underscores the
untapped capabilities of language models in financial decision-making and opens
new avenues for market prediction applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Speech Representation Learning Revisited: The Necessity of Separate
  Learnable Parameters and Robust Data Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.10557v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.10557v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hemant Yadav, Sunayana Sitaram, Rajiv Ratn Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech modeling methods learn one embedding for a fixed segment of speech,
typically in between 10-25 ms. The information present in speech can be divided
into two categories: "what is being said" (content) and "how it is expressed"
(other) and these two are orthogonal in nature causing the optimization
algorithm to find a sub-optimal solution if forced to optimize together. This
leads to sub-optimal performance in one or all downstream tasks as shown by
previous studies. Current self-supervised learning (SSL) methods such as HuBERT
are very good at modeling the content information present in speech. Data
augmentation improves the performance on tasks which require effective modeling
of other information but this leads to a divided capacity of the model. In this
work, we conduct a preliminary study to understand the importance of modeling
other information using separate learnable parameters. We propose a modified
version of HuBERT, termed Other HuBERT (O-HuBERT), to test our hypothesis. Our
findings are twofold: first, the O-HuBERT method is able to utilize all layers
to build complex features to encode other information; second, a robust data
augmentation strategy is essential for learning the information required by
tasks that depend on other information and to achieve state-of-the-art (SOTA)
performance on the SUPERB benchmark with a similarly sized model (100 million
parameters) and pre-training data (960 hours).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CogCoM: A Visual Language Model with Chain-of-Manipulations Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04236v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04236v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ji Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv, Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao Dong, Jie Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) have demonstrated their broad effectiveness
thanks to extensive training in aligning visual instructions to responses.
However, such training of conclusive alignment leads models to ignore essential
visual reasoning, further resulting in failures in meticulous visual problems
and unfaithful responses. Drawing inspiration from human cognition in solving
visual problems (e.g., marking, zoom in), this paper introduces Chain of
Manipulations, a mechanism that enables VLMs to solve problems step-by-step
with evidence. After training, models can solve various visual problems by
eliciting intrinsic manipulations (e.g., grounding, zoom in) with results
(e.g., boxes, image) actively without involving external tools, while also
allowing users to trace error causes. We study the roadmap to implement this
mechanism, including (1) a flexible design of manipulations upon extensive
analysis, (2) an efficient automated data generation pipeline, (3) a compatible
VLM architecture capable of multi-turn multi-image, and (4) a model training
process for versatile capabilities. With the design, we also manually annotate
6K high-quality samples for the challenging graphical mathematical problems.
Our trained model, \textbf{CogCoM}, equipped with this mechanism with 17B
parameters achieves state-of-the-art performance across 9 benchmarks from 4
categories, demonstrating the effectiveness while preserving the
interpretability. Our code, model weights, and collected data are publicly
available at https://github.com/THUDM/CogCoM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Curriculum-style Data Augmentation for <span class="highlight-title">LLM</span>-based Metaphor Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.02956v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.02956v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaidi Jia, Yanxia Wu, Ming Liu, Rongsheng Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, utilizing large language models (LLMs) for metaphor detection has
achieved promising results. However, these methods heavily rely on the
capabilities of closed-source LLMs, which come with relatively high inference
costs and latency. To address this, we propose a method for metaphor detection
by fine-tuning open-source LLMs, effectively reducing inference costs and
latency with a single inference step. Furthermore, metaphor detection suffers
from a severe data scarcity problem, which hinders effective fine-tuning of
LLMs. To tackle this, we introduce Curriculum-style Data Augmentation (CDA).
Specifically, before fine-tuning, we evaluate the training data to identify
correctly predicted instances for fine-tuning, while incorrectly predicted
instances are used as seed data for data augmentation. This approach enables
the model to quickly learn simpler knowledge and progressively acquire more
complex knowledge, thereby improving performance incrementally. Experimental
results demonstrate that our method achieves state-of-the-art performance
across all baselines. Additionally, we provide detailed ablation studies to
validate the effectiveness of CDA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What is Wrong with Perplexity for Long-context Language Modeling? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.23771v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.23771v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lizhe Fang, Yifei Wang, Zhaoyang Liu, Chenheng Zhang, Stefanie Jegelka, Jinyang Gao, Bolin Ding, Yisen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Handling long-context inputs is crucial for large language models (LLMs) in
tasks such as extended conversations, document summarization, and many-shot
in-context learning. While recent approaches have extended the context windows
of LLMs and employed perplexity (PPL) as a standard evaluation metric, PPL has
proven unreliable for assessing long-context capabilities. The underlying cause
of this limitation has remained unclear. In this work, we provide a
comprehensive explanation for this issue. We find that PPL overlooks key
tokens, which are essential for long-context understanding, by averaging across
all tokens and thereby obscuring the true performance of models in long-context
scenarios. To address this, we propose \textbf{LongPPL}, a novel metric that
focuses on key tokens by employing a long-short context contrastive method to
identify them. Our experiments demonstrate that LongPPL strongly correlates
with performance on various long-context benchmarks (e.g., Pearson correlation
of -0.96), significantly outperforming traditional PPL in predictive accuracy.
Additionally, we introduce \textbf{LongCE} (Long-context Cross-Entropy) loss, a
re-weighting strategy for fine-tuning that prioritizes key tokens, leading to
consistent improvements across diverse benchmarks. In summary, these
contributions offer deeper insights into the limitations of PPL and present
effective solutions for accurately evaluating and enhancing the long-context
capabilities of LLMs. Code is available at https://github.com/PKU-ML/LongPPL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sylber: Syllabic Embedding Representation of Speech from Raw Audio 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07168v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07168v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheol Jun Cho, Nicholas Lee, Akshat Gupta, Dhruv Agarwal, Ethan Chen, Alan W Black, Gopala K. Anumanchipalli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Syllables are compositional units of spoken language that efficiently
structure human speech perception and production. However, current neural
speech representations lack such structure, resulting in dense token sequences
that are costly to process. To bridge this gap, we propose a new model, Sylber,
that produces speech representations with clean and robust syllabic structure.
Specifically, we propose a self-supervised learning (SSL) framework that
bootstraps syllabic embeddings by distilling from its own initial unsupervised
syllabic segmentation. This results in a highly structured representation of
speech features, offering three key benefits: 1) a fast, linear-time syllable
segmentation algorithm, 2) efficient syllabic tokenization with an average of
4.27 tokens per second, and 3) novel phonological units suited for efficient
spoken language modeling. Our proposed segmentation method is highly robust and
generalizes to out-of-domain data and unseen languages without any tuning. By
training token-to-speech generative models, fully intelligible speech can be
reconstructed from Sylber tokens with a significantly lower bitrate than
baseline SSL tokens. This suggests that our model effectively compresses speech
into a compact sequence of tokens with minimal information loss. Lastly, we
demonstrate that categorical perception-a linguistic phenomenon in speech
perception-emerges naturally in Sylber, making the embedding space more
categorical and sparse than previous speech features and thus supporting the
high efficiency of our tokenization. Together, we present a novel SSL approach
for representing speech as syllables, with significant potential for efficient
speech tokenization and spoken language modeling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Path-Consistency: Prefix Enhancement for Efficient Inference in <span class="highlight-title">LLM</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01281v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01281v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiace Zhu, Yingtao Shen, Jie Zhao, An Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To enhance the reasoning capabilities of large language models (LLMs),
self-consistency has gained significant popularity by combining multiple
sampling with majority voting. However, the state-of-the-art self-consistency
approaches consume substantial computational resources and lead to significant
additional time costs due to the multiple sampling. This prevents its full
potential from being realized in scenarios where computational resources are
critical. To improve the inference efficiency, this paper introduces
\textit{path-consistency}, a method that leverages the confidence of answers
generated in earlier branches to identify the prefix of the most promising
path. By dynamically guiding the generation of subsequent branches based on
this prefix, the \textit{path-consistency} mitigates both the errors and
redundancies from random or less useful sampling in self-consistency. As a
result, it can significantly accelerate the inference process by reducing the
number of tokens generated. Our extensive empirical evaluation shows that the
\textit{path-consistency} achieves significant acceleration in inference
latency ranging from $7.8\%$ to $40.5\%$, while maintaining or even improving
task accuracy across different datasets, including mathematical reasoning,
common sense reasoning, symbolic reasoning, and code generation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Padding Tone: A Mechanistic Analysis of Padding Tokens in T2I Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06751v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06751v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Toker, Ido Galil, Hadas Orgad, Rinon Gal, Yoad Tewel, Gal Chechik, Yonatan Belinkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image (T2I) diffusion models rely on encoded prompts to guide the
image generation process. Typically, these prompts are extended to a fixed
length by adding padding tokens before text encoding. Despite being a default
practice, the influence of padding tokens on the image generation process has
not been investigated. In this work, we conduct the first in-depth analysis of
the role padding tokens play in T2I models. We develop two causal techniques to
analyze how information is encoded in the representation of tokens across
different components of the T2I pipeline. Using these techniques, we
investigate when and how padding tokens impact the image generation process.
Our findings reveal three distinct scenarios: padding tokens may affect the
model's output during text encoding, during the diffusion process, or be
effectively ignored. Moreover, we identify key relationships between these
scenarios and the model's architecture (cross or self-attention) and its
training process (frozen or trained text encoder). These insights contribute to
a deeper understanding of the mechanisms of padding tokens, potentially
informing future model design and training practices in T2I systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in: NAACL 2025. Project webpage:
  https://padding-tone.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Empathy Level Alignment via Reinforcement Learning for Empathetic
  Response Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02976v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02976v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui Ma, Bo Zhang, Bo Xu, Jian Wang, Hongfei Lin, Xiao Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Empathetic response generation, aiming to understand the user's situation and
feelings and respond empathically, is crucial in building human-like dialogue
systems. Traditional approaches typically employ maximum likelihood estimation
as the optimization objective during training, yet fail to align the empathy
levels between generated and target responses. To this end, we propose an
empathetic response generation framework using reinforcement learning (EmpRL).
The framework develops an effective empathy reward function and generates
empathetic responses by maximizing the expected reward through reinforcement
learning. EmpRL utilizes the pre-trained T5 model as the generator and further
fine-tunes it to initialize the policy. To align the empathy levels between
generated and target responses within a given context, an empathy reward
function containing three empathy communication mechanisms -- emotional
reaction, interpretation, and exploration -- is constructed using pre-designed
and pre-trained empathy identifiers. During reinforcement learning training,
the proximal policy optimization algorithm is used to fine-tune the policy,
enabling the generation of empathetic responses. Both automatic and human
evaluations demonstrate that the proposed EmpRL framework significantly
improves the quality of generated responses, enhances the similarity in empathy
levels between generated and target responses, and produces empathetic
responses covering both affective and cognitive aspects.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Affective Computing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Automated Circuit Discovery in Transformers using Contextual
  Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.00886v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.00886v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aliyah R. Hsu, Georgia Zhou, Yeshwanth Cherapanamjeri, Yaxuan Huang, Anobel Y. Odisho, Peter R. Carroll, Bin Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automated mechanistic interpretation research has attracted great interest
due to its potential to scale explanations of neural network internals to large
models. Existing automated circuit discovery work relies on activation patching
or its approximations to identify subgraphs in models for specific tasks
(circuits). They often suffer from slow runtime, approximation errors, and
specific requirements of metrics, such as non-zero gradients. In this work, we
introduce contextual decomposition for transformers (CD-T) to build
interpretable circuits in large language models. CD-T can produce circuits of
arbitrary level of abstraction, and is the first able to produce circuits as
fine-grained as attention heads at specific sequence positions efficiently.
CD-T consists of a set of mathematical equations to isolate contribution of
model features. Through recursively computing contribution of all nodes in a
computational graph of a model using CD-T followed by pruning, we are able to
reduce circuit discovery runtime from hours to seconds compared to
state-of-the-art baselines. On three standard circuit evaluation datasets
(indirect object identification, greater-than comparisons, and docstring
completion), we demonstrate that CD-T outperforms ACDC and EAP by better
recovering the manual circuits with an average of 97% ROC AUC under low
runtimes. In addition, we provide evidence that faithfulness of CD-T circuits
is not due to random chance by showing our circuits are 80% more faithful than
random circuits of up to 60% of the original model size. Finally, we show CD-T
circuits are able to perfectly replicate original models' behavior
(faithfulness $ = 1$) using fewer nodes than the baselines for all tasks. Our
results underscore the great promise of CD-T for efficient automated
mechanistic interpretability, paving the way for new insights into the workings
of large language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Hybrid Transformer Model for Fake News Detection: Leveraging Bayesian
  Optimization and Bidirectional Recurrent Unit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.09097v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.09097v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Huang, Zeqiu Xu, Peiyang Yu, Jingyuan Yi, Xiaochuan Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose an optimized Transformer model that integrates
Bayesian algorithms with a Bidirectional Gated Recurrent Unit (BiGRU), and
apply it to fake news classification for the first time. First, we employ the
TF-IDF method to extract features from news texts and transform them into
numeric representations to facilitate subsequent machine learning tasks. Two
sets of experiments are then conducted for fake news detection and
classification: one using a Transformer model optimized only with BiGRU, and
the other incorporating Bayesian algorithms into the BiGRU-based Transformer.
Experimental results show that the BiGRU-optimized Transformer achieves 100%
accuracy on the training set and 99.67% on the test set, while the addition of
the Bayesian algorithm maintains 100% accuracy on the training set and slightly
improves test-set accuracy to 99.73%. This indicates that the Bayesian
algorithm boosts model accuracy by 0.06%, further enhancing the detection
capability for fake news. Moreover, the proposed algorithm converges rapidly at
around the 10th training epoch with accuracy nearing 100%, demonstrating both
its effectiveness and its fast classification ability. Overall, the optimized
Transformer model, enhanced by the Bayesian algorithm and BiGRU, exhibits
excellent continuous learning and detection performance, offering a robust
technical means to combat the spread of fake news in the current era of
information overload.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MobA: <span class="highlight-title">Multi</span>faceted Memory-Enhanced Adaptive Planning for Efficient
  Mobile Task Automation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.13757v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.13757v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zichen Zhu, Hao Tang, Yansi Li, Dingye Liu, Hongshen Xu, Kunyao Lan, Danyang Zhang, Yixuan Jiang, Hao Zhou, Chenrun Wang, Situo Zhang, Liangtai Sun, Yixiao Wang, Yuheng Sun, Lu Chen, Kai Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing Multimodal Large Language Model (MLLM)-based agents face significant
challenges in handling complex GUI (Graphical User Interface) interactions on
devices. These challenges arise from the dynamic and structured nature of GUI
environments, which integrate text, images, and spatial relationships, as well
as the variability in action spaces across different pages and tasks. To
address these limitations, we propose MobA, a novel MLLM-based mobile assistant
system. MobA introduces an adaptive planning module that incorporates a
reflection mechanism for error recovery and dynamically adjusts plans to align
with the real environment contexts and action module's execution capacity.
Additionally, a multifaceted memory module provides comprehensive memory
support to enhance adaptability and efficiency. We also present MobBench, a
dataset designed for complex mobile interactions. Experimental results on
MobBench and AndroidArena demonstrate MobA's ability to handle dynamic GUI
environments and perform complex mobile task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NAACL 2025 Demo Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Examining Alignment of Large Language Models through Representative
  Heuristics: The Case of Political Stereotypes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.14294v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.14294v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sullam Jeoung, Yubin Ge, Haohan Wang, Jana Diesner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Examining the alignment of large language models (LLMs) has become
increasingly important, e.g., when LLMs fail to operate as intended. This study
examines the alignment of LLMs with human values for the domain of politics.
Prior research has shown that LLM-generated outputs can include political
leanings and mimic the stances of political parties on various issues. However,
the extent and conditions under which LLMs deviate from empirical positions are
insufficiently examined. To address this gap, we analyze the factors that
contribute to LLMs' deviations from empirical positions on political issues,
aiming to quantify these deviations and identify the conditions that cause
them.
  Drawing on findings from cognitive science about representativeness
heuristics, i.e., situations where humans lean on representative attributes of
a target group in a way that leads to exaggerated beliefs, we scrutinize LLM
responses through this heuristics' lens. We conduct experiments to determine
how LLMs inflate predictions about political parties, which results in
stereotyping. We find that while LLMs can mimic certain political parties'
positions, they often exaggerate these positions more than human survey
respondents do. Also, LLMs tend to overemphasize representativeness more than
humans. This study highlights the susceptibility of LLMs to representativeness
heuristics, suggesting a potential vulnerability of LLMs that facilitates
political stereotyping. We also test prompt-based mitigation strategies,
finding that strategies that can mitigate representative heuristics in humans
are also effective in reducing the influence of representativeness on
LLM-generated responses.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FACT-AUDIT: An Adaptive <span class="highlight-title">Multi</span>-Agent Framework for Dynamic Fact-Checking
  Evaluation of Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17924v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17924v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongzhan Lin, Yang Deng, Yuxuan Gu, Wenxuan Zhang, Jing Ma, See-Kiong Ng, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have significantly advanced the fact-checking
studies. However, existing automated fact-checking evaluation methods rely on
static datasets and classification metrics, which fail to automatically
evaluate the justification production and uncover the nuanced limitations of
LLMs in fact-checking. In this work, we introduce FACT-AUDIT, an agent-driven
framework that adaptively and dynamically assesses LLMs' fact-checking
capabilities. Leveraging importance sampling principles and multi-agent
collaboration, FACT-AUDIT generates adaptive and scalable datasets, performs
iterative model-centric evaluations, and updates assessments based on
model-specific responses. By incorporating justification production alongside
verdict prediction, this framework provides a comprehensive and evolving audit
of LLMs' factual reasoning capabilities, to investigate their trustworthiness.
Extensive experiments demonstrate that FACT-AUDIT effectively differentiates
among state-of-the-art LLMs, providing valuable insights into model strengths
and limitations in model-centric fact-checking analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive In-conversation Team Building for Language Model Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.19425v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.19425v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linxin Song, Jiale Liu, Jieyu Zhang, Shaokun Zhang, Ao Luo, Shijian Wang, Qingyun Wu, Chi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging multiple large language model (LLM) agents has shown to be a
promising approach for tackling complex tasks, while the effective design of
multiple agents for a particular application remains an art. It is thus
intriguing to answer a critical question: Given a task, how can we build a team
of LLM agents to solve it effectively? Our new adaptive team-building paradigm
offers a flexible solution, realized through a novel agent design named Captain
Agent. It dynamically forms and manages teams for each step of a task-solving
process, utilizing nested group conversations and reflection to ensure diverse
expertise and prevent stereotypical outputs, allowing for a flexible yet
structured approach to problem-solving. A comprehensive evaluation across six
real-world scenarios demonstrates that Captain Agent significantly outperforms
existing multi-agent methods with 21.94% improvement in average accuracy,
providing outstanding performance without requiring task-specific prompt
engineering. Our exploration of different backbone LLM and cost analysis
further shows that Captain Agent can improve the conversation quality of weak
LLM and achieve competitive performance with extremely low cost, which
illuminates the application of multi-agent systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SeqAR: Jailbreak <span class="highlight-title">LLM</span>s with Sequential Auto-Generated Characters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01902v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01902v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Yang, Zeguan Xiao, Xin Lu, Hongru Wang, Xuetao Wei, Hailiang Huang, Guanhua Chen, Yun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread applications of large language models (LLMs) have brought
about concerns regarding their potential misuse. Although aligned with human
preference data before release, LLMs remain vulnerable to various malicious
attacks. In this paper, we adopt a red-teaming strategy to enhance LLM safety
and introduce SeqAR, a simple yet effective framework to design jailbreak
prompts automatically. The SeqAR framework generates and optimizes multiple
jailbreak characters and then applies sequential jailbreak characters in a
single query to bypass the guardrails of the target LLM. Different from
previous work which relies on proprietary LLMs or seed jailbreak templates
crafted by human expertise, SeqAR can generate and optimize the jailbreak
prompt in a cold-start scenario using open-sourced LLMs without any seed
jailbreak templates. Experimental results show that SeqAR achieves attack
success rates of 88% and 60% in bypassing the safety alignment of GPT-3.5-1106
and GPT-4, respectively. Furthermore, we extensively evaluate the
transferability of the generated templates across different LLMs and held-out
malicious requests, while also exploring defense strategies against the
jailbreak attack designed by SeqAR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NAACL 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MACPO: Weak-to-Strong Alignment via <span class="highlight-title">Multi</span>-Agent Contrastive Preference
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07672v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07672v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yougang Lyu, Lingyong Yan, Zihan Wang, Dawei Yin, Pengjie Ren, Maarten de Rijke, Zhaochun Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large language models (LLMs) are rapidly advancing and achieving
near-human capabilities on specific tasks, aligning them with human values is
becoming more urgent. In scenarios where LLMs outperform humans, we face a
weak-to-strong alignment problem where we need to effectively align strong
student LLMs through weak supervision generated by weak teachers. Existing
alignment methods mainly focus on strong-to-weak alignment and self-alignment
settings, and it is impractical to adapt them to the much harder weak-to-strong
alignment setting. To fill this gap, we propose a multi-agent contrastive
preference optimization (MACPO) framework. MACPO facilitates weak teachers and
strong students to learn from each other by iteratively reinforcing unfamiliar
positive behaviors while penalizing familiar negative ones. To get this, we
devise a mutual positive behavior augmentation strategy to encourage weak
teachers and strong students to learn from each other's positive behavior and
further provide higher quality positive behavior for the next iteration.
Additionally, we propose a hard negative behavior construction strategy to
induce weak teachers and strong students to generate familiar negative behavior
by fine-tuning on negative behavioral data. Experimental results on the HH-RLHF
and PKU-SafeRLHF datasets, evaluated using both automatic metrics and human
judgments, demonstrate that MACPO simultaneously improves the alignment
performance of strong students and weak teachers. Moreover, as the number of
weak teachers increases, MACPO achieves better weak-to-strong alignment
performance through more iteration optimization rounds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ X-ALMA: Plug & Play Modules and Adaptive Rejection for Quality
  Translation at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03115v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03115v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoran Xu, Kenton Murray, Philipp Koehn, Hieu Hoang, Akiko Eriguchi, Huda Khayrallah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have achieved remarkable success across various
NLP tasks with a focus on English due to English-centric pre-training and
limited multilingual data. In this work, we focus on the problem of
translation, and while some multilingual LLMs claim to support for hundreds of
languages, models often fail to provide high-quality responses for mid- and
low-resource languages, leading to imbalanced performance heavily skewed in
favor of high-resource languages. We introduce **X-ALMA**, a model designed to
ensure top-tier performance across 50 diverse languages, regardless of their
resource levels. X-ALMA surpasses state-of-the-art open-source multilingual
LLMs, such as Aya-101 and Aya-23, in every single translation direction on the
FLORES-200 and WMT'23 test datasets according to COMET-22. This is achieved by
plug-and-play language-specific module architecture to prevent language
conflicts during training and a carefully designed training regimen with novel
optimization methods to maximize the translation performance. After the final
stage of training regimen, our proposed **A**daptive **R**ejection
**P**reference **O**ptimization (**ARPO**) surpasses existing preference
optimization methods in translation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2025 (spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MiLoRA: Harnessing Minor Singular Components for Parameter-Efficient <span class="highlight-title">LLM</span>
  Finetuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.09044v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.09044v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanqing Wang, Yixia Li, Shuo Wang, Guanhua Chen, Yun Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient finetuning of large language models (LLMs) aims to adapt the LLMs
with reduced computational and memory cost. Previous LoRA-based approaches
initialize the low-rank matrices with Gaussian distribution and zero values
while keeping the original weight matrices frozen. However, the trainable model
parameters optimized in an unguided subspace might interfere with the
well-learned subspace of the pretrained weight matrices. In this paper, we
propose MiLoRA, a simple yet effective LLM finetuning approach that only
updates the minor singular components of the weight matrix while keeping the
principal singular components frozen. It is observed that the minor matrix
corresponds to the noisy or long-tail information, while the principal matrix
contains important knowledge. The MiLoRA initializes the low-rank matrices
within a subspace that is orthogonal to the principal matrix, thus the
pretrained knowledge is expected to be well preserved. During finetuning,
MiLoRA makes the most use of the less-optimized subspace for learning the
labeled dataset. Extensive experiments on commonsense reasoning, math
reasoning, instruction following and visual instruction following benchmarks
present the superior performance of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted at NAACL 2025. Code is available at:
  https://github.com/sufenlp/MiLoRA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ L3Ms -- Lagrange Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21533v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21533v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guneet S. Dhillon, Xingjian Shi, Yee Whye Teh, Alex Smola
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised fine-tuning (SFT) and alignment of large language models (LLMs)
are key steps in providing a good user experience. However, the concept of an
appropriate alignment is inherently application-dependent, and current methods
often rely on heuristic choices to drive optimization. In this work, we
formulate SFT and alignment as a constrained optimization problem: the LLM is
fine-tuned on a task while being required to meet application-specific
requirements, without resorting to heuristics. To solve this, we propose
Lagrange Large Language Models (L3Ms), which employ logarithmic barriers to
enforce the constraints. This approach allows for the customization of L3Ms
across diverse applications while avoiding heuristic-driven processes. We
experimentally demonstrate the versatility and efficacy of L3Ms in achieving
tailored alignments for various applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Learning Representations (ICLR), 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Empirical Analysis of Uncertainty in Large Language Model Evaluations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10709v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10709v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiujie Xie, Qingqiu Li, Zhuohao Yu, Yuejie Zhang, Yue Zhang, Linyi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As LLM-as-a-Judge emerges as a new paradigm for assessing large language
models (LLMs), concerns have been raised regarding the alignment, bias, and
stability of LLM evaluators. While substantial work has focused on alignment
and bias, little research has concentrated on the stability of LLM evaluators.
In this paper, we conduct extensive experiments involving 9 widely used LLM
evaluators across 2 different evaluation settings to investigate the
uncertainty in model-based LLM evaluations. We pinpoint that LLM evaluators
exhibit varying uncertainty based on model families and sizes. With careful
comparative analyses, we find that employing special prompting strategies,
whether during inference or post-training, can alleviate evaluation uncertainty
to some extent. By utilizing uncertainty to enhance LLM's reliability and
detection capability in Out-Of-Distribution (OOD) data, we further fine-tune an
uncertainty-aware LLM evaluator named ConfiLM using a human-annotated
fine-tuning set and assess ConfiLM's OOD evaluation ability on a manually
designed test set sourced from the 2024 Olympics. Experimental results
demonstrate that incorporating uncertainty as additional information during the
fine-tuning phase can largely improve the model's evaluation performance in OOD
scenarios. The code and data are released at:
https://github.com/hasakiXie123/LLM-Evaluator-Uncertainty.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> on Large Language Model based Autonomous Agents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.11432v7">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.11432v7.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, Wayne Xin Zhao, Zhewei Wei, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous agents have long been a prominent research focus in both academic
and industry communities. Previous research in this field often focuses on
training agents with limited knowledge within isolated environments, which
diverges significantly from human learning processes, and thus makes the agents
hard to achieve human-like decisions. Recently, through the acquisition of vast
amounts of web knowledge, large language models (LLMs) have demonstrated
remarkable potential in achieving human-level intelligence. This has sparked an
upsurge in studies investigating LLM-based autonomous agents. In this paper, we
present a comprehensive survey of these studies, delivering a systematic review
of the field of LLM-based autonomous agents from a holistic perspective. More
specifically, we first discuss the construction of LLM-based autonomous agents,
for which we propose a unified framework that encompasses a majority of the
previous work. Then, we present a comprehensive overview of the diverse
applications of LLM-based autonomous agents in the fields of social science,
natural science, and engineering. Finally, we delve into the evaluation
strategies commonly used for LLM-based autonomous agents. Based on the previous
studies, we also present several challenges and future directions in this
field. To keep track of this field and continuously update our survey, we
maintain a repository of relevant references at
https://github.com/Paitesanshi/LLM-Agent-Survey.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Correcting several typos, 35 pages, 5 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalization v.s. Memorization: Tracing Language Models' Capabilities
  Back to Pretraining Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14985v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14985v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyi Wang, Antonis Antoniades, Yanai Elazar, Alfonso Amayuelas, Alon Albalak, Kexun Zhang, William Yang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The impressive capabilities of large language models (LLMs) have sparked
debate over whether these models genuinely generalize to unseen tasks or
predominantly rely on memorizing vast amounts of pretraining data. To explore
this issue, we introduce an extended concept of memorization, distributional
memorization, which measures the correlation between the LLM output
probabilities and the pretraining data frequency. To effectively capture
task-specific pretraining data frequency, we propose a novel task-gram language
model, which is built by counting the co-occurrence of semantically related
$n$-gram pairs from task inputs and outputs in the pretraining corpus. Using
the Pythia models trained on the Pile dataset, we evaluate four distinct tasks:
machine translation, factual question answering, world knowledge understanding,
and math reasoning. Our findings reveal varying levels of memorization, with
the strongest effect observed in factual question answering. Furthermore, while
model performance improves across all tasks as LLM size increases, only factual
question answering shows an increase in memorization, whereas machine
translation and reasoning tasks exhibit greater generalization, producing more
novel outputs. This study demonstrates that memorization plays a larger role in
simpler, knowledge-intensive tasks, while generalization is the key for harder,
reasoning-based tasks, providing a scalable method for analyzing large
pretraining corpora in greater depth.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mutual Enhancement of Large Language and Reinforcement Learning Models
  through Bi-Directional Feedback Mechanisms: A Planning Case Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.06603v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.06603v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangding Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable capabilities for
reinforcement learning (RL) models, such as planning and reasoning
capabilities. However, the problems of LLMs and RL model collaboration still
need to be solved. In this study, we employ a teacher-student learning
framework to tackle these problems, specifically by offering feedback for LLMs
using RL models and providing high-level information for RL models with LLMs in
a cooperative multi-agent setting. Within this framework, the LLM acts as a
teacher, while the RL model acts as a student. The two agents cooperatively
assist each other through a process of recursive help, such as "I help you help
I help." The LLM agent supplies abstract information to the RL agent, enabling
efficient exploration and policy improvement. In turn, the RL agent offers
feedback to the LLM agent, providing valuable, real-time information that helps
generate more useful tokens. This bi-directional feedback loop promotes
optimization, exploration, and mutual improvement for both agents, enabling
them to accomplish increasingly challenging tasks. Remarkably, we propose a
practical algorithm to address the problem and conduct empirical experiments to
evaluate the effectiveness of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VisRAG: Vision-based Retrieval-augmented Generation on <span class="highlight-title">Multi</span>-modality
  Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10594v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10594v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) is an effective technique that enables
large language models (LLMs) to utilize external knowledge sources for
generation. However, current RAG systems are solely based on text, rendering it
impossible to utilize vision information like layout and images that play
crucial roles in real-world multi-modality documents. In this paper, we
introduce VisRAG, which tackles this issue by establishing a vision-language
model (VLM)-based RAG pipeline. In this pipeline, instead of first parsing the
document to obtain text, the document is directly embedded using a VLM as an
image and then retrieved to enhance the generation of a VLM. Compared to
traditional text-based RAG, VisRAG maximizes the retention and utilization of
the data information in the original documents, eliminating the information
loss introduced during the parsing process. We collect both open-source and
synthetic data to train the retriever in VisRAG and explore a variety of
generation methods. Experiments demonstrate that VisRAG outperforms traditional
RAG in both the retrieval and generation stages, achieving a 20--40% end-to-end
performance gain over traditional text-based RAG pipeline. Further analysis
reveals that VisRAG is efficient in utilizing training data and demonstrates
strong generalization capability, positioning it as a promising solution for
RAG on multi-modality documents. Our code and data are available at
https://github.com/openbmb/visrag.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InstructRAG: Instructing Retrieval-Augmented Generation via
  Self-Synthesized Rationales 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.13629v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.13629v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhepei Wei, Wei-Lin Chen, Yu Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) has shown promising potential to enhance
the accuracy and factuality of language models (LMs). However, imperfect
retrievers or noisy corpora can introduce misleading or even erroneous
information to the retrieved contents, posing a significant challenge to the
generation quality. Existing RAG methods typically address this challenge by
directly predicting final answers despite potentially noisy inputs, resulting
in an implicit denoising process that is difficult to interpret and verify. On
the other hand, the acquisition of explicit denoising supervision is often
costly, involving significant human efforts. In this work, we propose
InstructRAG, where LMs explicitly learn the denoising process through
self-synthesized rationales -- First, we instruct the LM to explain how the
ground-truth answer is derived from retrieved documents. Then, these rationales
can be used either as demonstrations for in-context learning of explicit
denoising or as supervised fine-tuning data to train the model. Compared to
standard RAG approaches, InstructRAG requires no additional supervision, allows
for easier verification of the predicted answers, and effectively improves
generation accuracy. Experiments show InstructRAG consistently outperforms
existing RAG methods in both training-free and trainable scenarios, achieving a
relative improvement of 8.3% over the best baseline method on average across
five knowledge-intensive benchmarks. Extensive analysis indicates that
InstructRAG scales well with increased numbers of retrieved documents and
consistently exhibits robust denoising ability even in out-of-domain datasets,
demonstrating strong generalizability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025. Code: https://github.com/weizhepei/InstructRAG</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">70</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Denoising for Signal-Dependent and Row-Correlated Imaging
  Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.07887v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.07887v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Salmon, Alexander Krull
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate analysis of microscopy images is hindered by the presence of noise.
This noise is usually signal-dependent and often additionally correlated along
rows or columns of pixels. Current self- and unsupervised denoisers can address
signal-dependent noise, but none can reliably remove noise that is also row- or
column-correlated. Here, we present the first fully unsupervised deep
learning-based denoiser capable of handling imaging noise that is
row-correlated as well as signal-dependent. Our approach uses a Variational
Autoencoder (VAE) with a specially designed autoregressive decoder. This
decoder is capable of modeling row-correlated and signal-dependent noise but is
incapable of independently modeling underlying clean signal. The VAE therefore
produces latent variables containing only clean signal information, and these
are mapped back into image space using a proposed second decoder network. Our
method does not require a pre-trained noise model and can be trained from
scratch using unpaired noisy data. We benchmark our approach on microscopy
datatsets from a range of imaging modalities and sensor types, each with row-
or column-correlated, signal-dependent noise, and show that it outperforms
existing self- and unsupervised denoisers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Eagle: Exploring The Design Space for <span class="highlight-title">Multi</span>modal <span class="highlight-title">LLM</span>s with Mixture of
  Encoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15998v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15998v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, Yilin Zhao, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, Bryan Catanzaro, Andrew Tao, Jan Kautz, Zhiding Yu, Guilin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to accurately interpret complex visual information is a crucial
topic of multimodal large language models (MLLMs). Recent work indicates that
enhanced visual perception significantly reduces hallucinations and improves
performance on resolution-sensitive tasks, such as optical character
recognition and document analysis. A number of recent MLLMs achieve this goal
using a mixture of vision encoders. Despite their success, there is a lack of
systematic comparisons and detailed ablation studies addressing critical
aspects, such as expert selection and the integration of multiple vision
experts. This study provides an extensive exploration of the design space for
MLLMs using a mixture of vision encoders and resolutions. Our findings reveal
several underlying principles common to various existing strategies, leading to
a streamlined yet effective design approach. We discover that simply
concatenating visual tokens from a set of complementary vision encoders is as
effective as more complex mixing architectures or strategies. We additionally
introduce Pre-Alignment to bridge the gap between vision-focused encoders and
language tokens, enhancing model coherence. The resulting family of MLLMs,
Eagle, surpasses other leading open-source models on major MLLM benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Github: https://github.com/NVlabs/Eagle, HuggingFace:
  https://huggingface.co/NVEagle</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SV-RAG: LoRA-Contextualizing Adaptation of M<span class="highlight-title">LLM</span>s for Long Document
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01106v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01106v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jian Chen, Ruiyi Zhang, Yufan Zhou, Tong Yu, Franck Dernoncourt, Jiuxiang Gu, Ryan A. Rossi, Changyou Chen, Tong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal large language models (MLLMs) have recently shown great progress
in text-rich image understanding, yet they still struggle with complex,
multi-page visually-rich documents. Traditional methods using document parsers
for retrieval-augmented generation suffer from performance and efficiency
limitations, while directly presenting all pages to MLLMs leads to
inefficiencies, especially with lengthy ones. In this work, we present a novel
framework named **S**elf-**V**isual **R**etrieval-**A**ugmented **G**eneration
(SV-RAG), which can broaden horizons of any MLLM to support long-document
understanding. We demonstrate that **MLLMs themselves can be an effective
multimodal retriever** to fetch relevant pages and then answer user questions
based on these pages. SV-RAG is implemented with two specific MLLM adapters,
one for evidence page retrieval and the other for question answering. Empirical
results show state-of-the-art performance on public benchmarks, demonstrating
the effectiveness of SV-RAG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ALBAR: Adversarial Learning approach to mitigate Biases in Action
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.00156v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.00156v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Fioresi, Ishan Rajendrakumar Dave, Mubarak Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bias in machine learning models can lead to unfair decision making, and while
it has been well-studied in the image and text domains, it remains
underexplored in action recognition. Action recognition models often suffer
from background bias (i.e., inferring actions based on background cues) and
foreground bias (i.e., relying on subject appearance), which can be detrimental
to real-life applications such as autonomous vehicles or assisted living
monitoring. While prior approaches have mainly focused on mitigating background
bias using specialized augmentations, we thoroughly study both foreground and
background bias. We propose ALBAR, a novel adversarial training method that
mitigates foreground and background biases without requiring specialized
knowledge of the bias attributes. Our framework applies an adversarial
cross-entropy loss to the sampled static clip (where all the frames are the
same) and aims to make its class probabilities uniform using a proposed entropy
maximization loss. Additionally, we introduce a gradient penalty loss for
regularization against the debiasing process. We evaluate our method on
established background and foreground bias protocols, setting a new
state-of-the-art and strongly improving combined debiasing performance by over
12% absolute on HMDB51. Furthermore, we identify an issue of background leakage
in the existing UCF101 protocol for bias evaluation which provides a shortcut
to predict actions and does not provide an accurate measure of the debiasing
capability of a model. We address this issue by proposing more fine-grained
segmentation boundaries for the actor, where our method also outperforms
existing approaches. Project Page:
https://joefioresi718.github.io/ALBAR_webpage/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fréchet Wavelet Distance: A Domain-Agnostic Metric for Image
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15289v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15289v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lokesh Veeramacheneni, Moritz Wolter, Hildegard Kuehne, Juergen Gall
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern metrics for generative learning like Fr\'echet Inception Distance
(FID) and DINOv2-Fr\'echet Distance (FD-DINOv2) demonstrate impressive
performance. However, they suffer from various shortcomings, like a bias
towards specific generators and datasets. To address this problem, we propose
the Fr\'echet Wavelet Distance (FWD) as a domain-agnostic metric based on the
Wavelet Packet Transform ($W_p$). FWD provides a sight across a broad spectrum
of frequencies in images with a high resolution, preserving both spatial and
textural aspects. Specifically, we use $W_p$ to project generated and real
images to the packet coefficient space. We then compute the Fr\'echet distance
with the resultant coefficients to evaluate the quality of a generator. This
metric is general-purpose and dataset-domain agnostic, as it does not rely on
any pre-trained network, while being more interpretable due to its ability to
compute Fr\'echet distance per packet, enhancing transparency. We conclude with
an extensive evaluation of a wide variety of generators across various datasets
that the proposed FWD can generalize and improve robustness to domain shifts
and various corruptions compared to other metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TESGNN: Temporal Equivariant Scene Graph Neural Networks for Efficient
  and Robust <span class="highlight-title">Multi</span>-View 3D Scene Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.10509v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.10509v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quang P. M. Pham, Khoi T. N. Nguyen, Lan C. Ngo, Truong Do, Dezhen Song, Truong-Son Hy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene graphs have proven to be highly effective for various scene
understanding tasks due to their compact and explicit representation of
relational information. However, current methods often overlook the critical
importance of preserving symmetry when generating scene graphs from 3D point
clouds, which can lead to reduced accuracy and robustness, particularly when
dealing with noisy, multi-view data. Furthermore, a major limitation of prior
approaches is the lack of temporal modeling to capture time-dependent
relationships among dynamically evolving entities in a scene. To address these
challenges, we propose Temporal Equivariant Scene Graph Neural Network
(TESGNN), consisting of two key components: (1) an Equivariant Scene Graph
Neural Network (ESGNN), which extracts information from 3D point clouds to
generate scene graph while preserving crucial symmetry properties, and (2) a
Temporal Graph Matching Network, which fuses scene graphs generated by ESGNN
across multiple time sequences into a unified global representation using an
approximate graph-matching algorithm. Our combined architecture TESGNN
outperforms current state-of-the-art methods in scene graph generation,
achieving higher accuracy and faster training convergence. Moreover, we show
that leveraging the symmetry-preserving property produces a more stable and
accurate global scene representation compared to existing approaches. Last but
not least, it is computationally efficient and easily implementable using
existing frameworks, making it well-suited for real-time applications in
robotics and computer vision. This approach paves the way for more robust and
scalable solutions to complex multi-view scene understanding challenges. Our
source code is publicly available at: https://github.com/HySonLab/TESGraph
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2407.00609</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning General-Purpose Biomedical Volume Representations using
  Randomized Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02372v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02372v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neel Dey, Benjamin Billot, Hallee E. Wong, Clinton J. Wang, Mengwei Ren, P. Ellen Grant, Adrian V. Dalca, Polina Golland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current volumetric biomedical foundation models struggle to generalize as
public 3D datasets are small and do not cover the broad diversity of medical
procedures, conditions, anatomical regions, and imaging protocols. We address
this by creating a representation learning method that instead anticipates
strong domain shifts at training time itself. We first propose a data engine
that synthesizes highly variable training samples that would enable
generalization to new biomedical contexts. To then train a single 3D network
for any voxel-level task, we develop a contrastive learning method that
pretrains the network to be stable against nuisance imaging variation simulated
by the data engine, a key inductive bias for generalization. This network's
features can be used as robust representations of input images for downstream
tasks and its weights provide a strong, dataset-agnostic initialization for
finetuning on new datasets. As a result, we set new standards across both
multimodality registration and few-shot segmentation, a first for any 3D
biomedical vision model, all without (pre-)training on any existing dataset of
real images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025: International Conference on Learning Representations. Code
  and model weights available at https://github.com/neel-dey/anatomix.
  Keywords: synthetic data, representation learning, medical image analysis,
  image registration, image segmentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tri-Clustering: A <span class="highlight-title">Multi</span>-views Tri-level Information Fusion Context
  Clustering Framework for Localization and Classification in Mammography 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.14876v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.14876v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shilong Yang, Chulong Zhang, Qi Zang, Juan Yu, Liang Zeng, Xiao Luo, Yexuan Xing, Xin Pan, Qi Li, Xiaokun Liang, Yaoqin Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Breast cancer is a significant global health issue, and the diagnosis of
breast imaging has always been challenging. Mammography images typically have
extremely high resolution, with lesions occupying only a very small area.
Down-sampling in neural networks can easily lead to the loss of
microcalcifications or subtle structures, making it difficult for traditional
neural network architectures to address these issues. To tackle these
challenges, we propose a Context Clustering Network with triple information
fusion. Firstly, compared to CNNs or transformers, we find that Context
clustering methods (1) are more computationally efficient and (2) can more
easily associate structural or pathological features, making them suitable for
the clinical tasks of mammography. Secondly, we propose a triple information
fusion mechanism that integrates global information, feature-based local
information, and patch-based local information. The proposed approach is
rigorously evaluated on two public datasets, Vindr-Mammo and CBIS-DDSM, using
five independent splits to ensure statistical robustness. Our method achieves
an AUC of 0.828 on Vindr-Mammo and 0.805 on CBIS-DDSM, outperforming the next
best method by 3.1% and 2.4%, respectively. These improvements are
statistically significant (p<0.05), underscoring the benefits of Context
Clustering Network with triple information fusion. Overall, our Context
Clustering framework demonstrates strong potential as a scalable and
cost-effective solution for large-scale mammography screening, enabling more
efficient and accurate breast cancer detection. Access to our method is
available at https://github.com/Sohyu1/Mammo_Clustering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedBiP: Heterogeneous One-Shot Federated Learning with Personalized
  Latent Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04810v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04810v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haokun Chen, Hang Li, Yao Zhang, Jinhe Bi, Gengyuan Zhang, Yueqi Zhang, Philip Torr, Jindong Gu, Denis Krompass, Volker Tresp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One-Shot Federated Learning (OSFL), a special decentralized machine learning
paradigm, has recently gained significant attention. OSFL requires only a
single round of client data or model upload, which reduces communication costs
and mitigates privacy threats compared to traditional FL. Despite these
promising prospects, existing methods face challenges due to client data
heterogeneity and limited data quantity when applied to real-world OSFL
systems. Recently, Latent Diffusion Models (LDM) have shown remarkable
advancements in synthesizing high-quality images through pretraining on
large-scale datasets, thereby presenting a potential solution to overcome these
issues. However, directly applying pretrained LDM to heterogeneous OSFL results
in significant distribution shifts in synthetic data, leading to performance
degradation in classification models trained on such data. This issue is
particularly pronounced in rare domains, such as medical imaging, which are
underrepresented in LDM's pretraining data. To address this challenge, we
propose Federated Bi-Level Personalization (FedBiP), which personalizes the
pretrained LDM at both instance-level and concept-level. Hereby, FedBiP
synthesizes images following the client's local data distribution without
compromising the privacy regulations. FedBiP is also the first approach to
simultaneously address feature space heterogeneity and client data scarcity in
OSFL. Our method is validated through extensive experiments on three OSFL
benchmarks with feature space heterogeneity, as well as on challenging medical
and satellite image datasets with label heterogeneity. The results demonstrate
the effectiveness of FedBiP, which substantially outperforms other OSFL
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Synthesizing Physically Plausible Human Motions in 3D Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.09036v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.09036v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Pan, Jingbo Wang, Buzhen Huang, Junyu Zhang, Haofan Wang, Xu Tang, Yangang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a physics-based character control framework for synthesizing
human-scene interactions. Recent advances adopt physics simulation to mitigate
artifacts produced by data-driven kinematic approaches. However, existing
physics-based methods mainly focus on single-object environments, resulting in
limited applicability in realistic 3D scenes with multi-objects. To address
such challenges, we propose a framework that enables physically simulated
characters to perform long-term interaction tasks in diverse, cluttered, and
unseen 3D scenes. The key idea is to decouple human-scene interactions into two
fundamental processes, Interacting and Navigating, which motivates us to
construct two reusable Controllers, namely InterCon and NavCon. Specifically,
InterCon uses two complementary policies to enable characters to enter or leave
the interacting state with a particular object (e.g., sitting on a chair or
getting up). To realize navigation in cluttered environments, we introduce
NavCon, where a trajectory following policy enables characters to track
pre-planned collision-free paths. Benefiting from the divide and conquer
strategy, we can train all policies in simple environments and directly apply
them in complex multi-object scenes through coordination from a rule-based
scheduler. Video and code are available at
https://github.com/liangpan99/InterScene.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>3DV 2024 version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unleashing the Potential of Vision-Language Pre-Training for 3D
  Zero-Shot Lesion Segmentation via Mask-Attribute Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.15744v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.15744v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yankai Jiang, Wenhui Lei, Xiaofan Zhang, Shaoting Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in medical vision-language pre-training models have
driven significant progress in zero-shot disease recognition. However,
transferring image-level knowledge to pixel-level tasks, such as lesion
segmentation in 3D CT scans, remains a critical challenge. Due to the
complexity and variability of pathological visual characteristics, existing
methods struggle to align fine-grained lesion features not encountered during
training with disease-related textual representations. In this paper, we
present Malenia, a novel multi-scale lesion-level mask-attribute alignment
framework, specifically designed for 3D zero-shot lesion segmentation. Malenia
improves the compatibility between mask representations and their associated
elemental attributes, explicitly linking the visual features of unseen lesions
with the extensible knowledge learned from previously seen ones. Furthermore,
we design a Cross-Modal Knowledge Injection module to enhance both visual and
textual features with mutually beneficial information, effectively guiding the
generation of segmentation results. Comprehensive experiments across three
datasets and 12 lesion categories validate the superior performance of Malenia.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as ICLR 2025 conference paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bidirectional Consistency Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18035v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18035v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangchen Li, Jiajun He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models (DMs) are capable of generating remarkably high-quality
samples by iteratively denoising a random vector, a process that corresponds to
moving along the probability flow ordinary differential equation (PF ODE).
Interestingly, DMs can also invert an input image to noise by moving backward
along the PF ODE, a key operation for downstream tasks such as interpolation
and image editing. However, the iterative nature of this process restricts its
speed, hindering its broader application. Recently, Consistency Models (CMs)
have emerged to address this challenge by approximating the integral of the PF
ODE, largely reducing the number of iterations. Yet, the absence of an explicit
ODE solver complicates the inversion process. To resolve this, we introduce
Bidirectional Consistency Model (BCM), which learns a single neural network
that enables both forward and backward traversal along the PF ODE, efficiently
unifying generation and inversion tasks within one framework. We can train BCM
from scratch or tune it using a pretrained consistency model, which reduces the
training cost and increases scalability. We demonstrate that BCM enables
one-step generation and inversion while also allowing the use of additional
steps to enhance generation quality or reduce reconstruction error. We further
showcase BCM's capability in downstream tasks, such as interpolation and
inpainting. Our code and weights are available at
https://github.com/Mosasaur5526/BCM-iCT-torch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, 27 figures; a shorter version of this paper was acceppted
  at the ICML 2024 Workshop on Structured Probabilistic Inference & Generative
  Modeling</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video-Foley: Two-Stage Video-To-Sound Generation via Temporal Event
  Condition For Foley Sound 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11915v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11915v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junwon Lee, Jaekwon Im, Dabin Kim, Juhan Nam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foley sound synthesis is crucial for multimedia production, enhancing user
experience by synchronizing audio and video both temporally and semantically.
Recent studies on automating this labor-intensive process through
video-to-sound generation face significant challenges. Systems lacking explicit
temporal features suffer from poor alignment and controllability, while
timestamp-based models require costly and subjective human annotation. We
propose Video-Foley, a video-to-sound system using Root Mean Square (RMS) as an
intuitive condition with semantic timbre prompts (audio or text). RMS, a
frame-level intensity envelope closely related to audio semantics, acts as a
temporal event feature to guide audio generation from video. The
annotation-free self-supervised learning framework consists of two stages,
Video2RMS and RMS2Sound, incorporating novel ideas including RMS discretization
and RMS-ControlNet with a pretrained text-to-audio model. Our extensive
evaluation shows that Video-Foley achieves state-of-the-art performance in
audio-visual alignment and controllability for sound timing, intensity, timbre,
and nuance. Source code, model weights and demos are available on our companion
website. (https://jnwnlee.github.io/video-foley-demo)
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLaVA-Mini: Efficient Image and Video Large <span class="highlight-title">Multi</span>modal Models with One
  Vision Token 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03895v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03895v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaolei Zhang, Qingkai Fang, Zhe Yang, Yang Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of real-time large multimodal models (LMMs) like GPT-4o has
sparked considerable interest in efficient LMMs. LMM frameworks typically
encode visual inputs into vision tokens (continuous representations) and
integrate them and textual instructions into the context of large language
models (LLMs), where large-scale parameters and numerous context tokens
(predominantly vision tokens) result in substantial computational overhead.
Previous efforts towards efficient LMMs always focus on replacing the LLM
backbone with smaller models, while neglecting the crucial issue of token
quantity. In this paper, we introduce LLaVA-Mini, an efficient LMM with minimal
vision tokens. To achieve a high compression ratio of vision tokens while
preserving visual information, we first analyze how LMMs understand vision
tokens and find that most vision tokens only play a crucial role in the early
layers of LLM backbone, where they mainly fuse visual information into text
tokens. Building on this finding, LLaVA-Mini introduces modality pre-fusion to
fuse visual information into text tokens in advance, thereby facilitating the
extreme compression of vision tokens fed to LLM backbone into one token.
LLaVA-Mini is a unified large multimodal model that can support the
understanding of images, high-resolution images, and videos in an efficient
manner. Experiments across 11 image-based and 7 video-based benchmarks
demonstrate that LLaVA-Mini outperforms LLaVA-v1.5 with just 1 vision token
instead of 576. Efficiency analyses reveal that LLaVA-Mini can reduce FLOPs by
77%, deliver low-latency responses within 40 milliseconds, and process over
10,000 frames of video on the GPU hardware with 24GB of memory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025. Code: https://github.com/ictnlp/LLaVA-Mini
  Model: https://huggingface.co/ICTNLP/llava-mini-llama-3.1-8b</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Drag Your Gaussian: Effective Drag-Based Editing with Score Distillation
  for 3D Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.18672v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.18672v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yansong Qu, Dian Chen, Xinyang Li, Xiaofan Li, Shengchuan Zhang, Liujuan Cao, Rongrong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in 3D scene editing have been propelled by the rapid
development of generative models. Existing methods typically utilize generative
models to perform text-guided editing on 3D representations, such as 3D
Gaussian Splatting (3DGS). However, these methods are often limited to texture
modifications and fail when addressing geometric changes, such as editing a
character's head to turn around. Moreover, such methods lack accurate control
over the spatial position of editing results, as language struggles to
precisely describe the extent of edits. To overcome these limitations, we
introduce DYG, an effective 3D drag-based editing method for 3D Gaussian
Splatting. It enables users to conveniently specify the desired editing region
and the desired dragging direction through the input of 3D masks and pairs of
control points, thereby enabling precise control over the extent of editing.
DYG integrates the strengths of the implicit triplane representation to
establish the geometric scaffold of the editing results, effectively overcoming
suboptimal editing outcomes caused by the sparsity of 3DGS in the desired
editing regions. Additionally, we incorporate a drag-based Latent Diffusion
Model into our method through the proposed Drag-SDS loss function, enabling
flexible, multi-view consistent, and fine-grained editing. Extensive
experiments demonstrate that DYG conducts effective drag-based editing guided
by control point prompts, surpassing other baselines in terms of editing effect
and quality, both qualitatively and quantitatively. Visit our project page at
https://quyans.github.io/Drag-Your-Gaussian.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Visit our project page at https://quyans.github.io/Drag-Your-Gaussian</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Audio-Visual Instance Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18709v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18709v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruohao Guo, Xianghua Ying, Yaru Chen, Dantong Niu, Guangyao Li, Liao Qu, Yanyu Qi, Jinxing Zhou, Bowei Xing, Wenzhen Yue, Ji Shi, Qixun Wang, Peiliang Zhang, Buwen Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a new multi-modal task, termed audio-visual
instance segmentation (AVIS), which aims to simultaneously identify, segment
and track individual sounding object instances in audible videos. To facilitate
this research, we introduce a high-quality benchmark named AVISeg, containing
over 90K instance masks from 26 semantic categories in 926 long videos.
Additionally, we propose a strong baseline model for this task. Our model first
localizes sound source within each frame, and condenses object-specific
contexts into concise tokens. Then it builds long-range audio-visual
dependencies between these tokens using window-based attention, and tracks
sounding objects among the entire video sequences. Extensive experiments reveal
that our method performs best on AVISeg, surpassing the existing methods from
related tasks. We further conduct the evaluation on several multi-modal large
models. Unfortunately, they exhibits subpar performance on instance-level sound
source localization and temporal perception. We expect that AVIS will inspire
the community towards a more comprehensive multi-modal understanding. Dataset
and code is available at https://github.com/ruohaoguo/avis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Low-Resource Lane Following Algorithms for
  Compute-Constrained Automated Vehicles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.03114v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.03114v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beñat Froemming-Aldanondo, Tatiana Rastoskueva, Michael Evans, Marcial Machado, Anna Vadella, Rickey Johnson, Luis Escamilla, Milan Jostes, Devson Butani, Ryan Kaddis, Chan-Jin Chung, Joshua Siegel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reliable lane-following is essential for automated and assisted driving, yet
existing solutions often rely on models that require extensive computational
resources, limiting their deployment in compute-constrained vehicles. We
evaluate five low-resource lane-following algorithms designed for real-time
operation on vehicles with limited computing resources. Performance was
assessed through simulation and deployment on real drive-by-wire electric
vehicles, with evaluation metrics including reliability, comfort, speed, and
adaptability. The top-performing methods used unsupervised learning to detect
and separate lane lines with processing time under 10 ms per frame,
outperforming compute-intensive and poor generalizing deep learning approaches.
These approaches demonstrated robustness across lighting conditions, road
textures, and lane geometries. The findings highlight the potential for
efficient lane detection approaches to enhance the accessibility and
reliability of autonomous vehicle technologies. Reducing computing requirements
enables lane keeping to be widely deployed in vehicles as part of lower-level
automation, including active safety systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Supported by the National Science Foundation under Grants No. 2150292
  and 2150096</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SPARTUN3D: Situated Spatial Understanding of 3D Wo<span class="highlight-title">rl</span>d in Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03878v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03878v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Zhang, Zhiyang Xu, Ying Shen, Parisa Kordjamshidi, Lifu Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating the 3D world into large language models (3D-based LLMs) has been
a promising research direction for 3D scene understanding. However, current
3D-based LLMs fall short in situated understanding due to two key limitations:
1) existing 3D datasets are constructed from a global perspective of the 3D
scenes and lack situated context. 2) the architectures of existing 3D-based
LLMs lack explicit alignment between the spatial representations of 3D scenes
and natural language, limiting their performance in tasks requiring precise
spatial reasoning. We address these issues by introducing a scalable situated
3D dataset, named Spartun3D, that incorporates various situated spatial
reasoning tasks. Furthermore, we propose Spartun3D-LLM, built on an existing
3D-based LLM but integrated with a novel situated spatial alignment module,
aiming to enhance the alignment between 3D visual representations and their
corresponding textual descriptions. Experimental results demonstrate that both
our proposed dataset and alignment module significantly enhance the situated
spatial understanding of 3D-based LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HMD^2: Environment-aware Motion Generation from Single Egocentric
  Head-Mounted Device 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.13426v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.13426v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vladimir Guzov, Yifeng Jiang, Fangzhou Hong, Gerard Pons-Moll, Richard Newcombe, C. Karen Liu, Yuting Ye, Lingni Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the generation of realistic full-body human motion
using a single head-mounted device with an outward-facing color camera and the
ability to perform visual SLAM. To address the ambiguity of this setup, we
present HMD^2, a novel system that balances motion reconstruction and
generation. From a reconstruction standpoint, it aims to maximally utilize the
camera streams to produce both analytical and learned features, including head
motion, SLAM point cloud, and image embeddings. On the generative front, HMD^2
employs a multi-modal conditional motion diffusion model with a Transformer
backbone to maintain temporal coherence of generated motions, and utilizes
autoregressive inpainting to facilitate online motion inference with minimal
latency (0.17 seconds). We show that our system provides an effective and
robust solution that scales to a diverse dataset of over 200 hours of motion in
complex indoor and outdoor environments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on 3D Vision 2025 (3DV 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Audio-Visual Adversarial Vulnerability from Temporal and
  Modality Perspectives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11858v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11858v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeliang Zhang, Susan Liang, Daiki Shimada, Chenliang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While audio-visual learning equips models with a richer understanding of the
real world by leveraging multiple sensory modalities, this integration also
introduces new vulnerabilities to adversarial attacks.
  In this paper, we present a comprehensive study of the adversarial robustness
of audio-visual models, considering both temporal and modality-specific
vulnerabilities. We propose two powerful adversarial attacks: 1) a temporal
invariance attack that exploits the inherent temporal redundancy across
consecutive time segments and 2) a modality misalignment attack that introduces
incongruence between the audio and visual modalities. These attacks are
designed to thoroughly assess the robustness of audio-visual models against
diverse threats. Furthermore, to defend against such attacks, we introduce a
novel audio-visual adversarial training framework. This framework addresses key
challenges in vanilla adversarial training by incorporating efficient
adversarial perturbation crafting tailored to multi-modal data and an
adversarial curriculum strategy. Extensive experiments in the Kinetics-Sounds
dataset demonstrate that our proposed temporal and modality-based attacks in
degrading model performance can achieve state-of-the-art performance, while our
adversarial training defense largely improves the adversarial robustness as
well as the adversarial training efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tracking objects that change in appearance with phase synchrony 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02094v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02094v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sabine Muzellec, Drew Linsley, Alekh K. Ashok, Ennio Mingolla, Girik Malik, Rufin VanRullen, Thomas Serre
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objects we encounter often change appearance as we interact with them.
Changes in illumination (shadows), object pose, or the movement of non-rigid
objects can drastically alter available image features. How do biological
visual systems track objects as they change? One plausible mechanism involves
attentional mechanisms for reasoning about the locations of objects
independently of their appearances -- a capability that prominent neuroscience
theories have associated with computing through neural synchrony. Here, we
describe a novel deep learning circuit that can learn to precisely control
attention to features separately from their location in the world through
neural synchrony: the complex-valued recurrent neural network (CV-RNN). Next,
we compare object tracking in humans, the CV-RNN, and other deep neural
networks (DNNs), using FeatureTracker: a large-scale challenge that asks
observers to track objects as their locations and appearances change in
precisely controlled ways. While humans effortlessly solved FeatureTracker,
state-of-the-art DNNs did not. In contrast, our CV-RNN behaved similarly to
humans on the challenge, providing a computational proof-of-concept for the
role of phase synchronization as a neural substrate for tracking
appearance-morphing objects as they move about.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Dual-Purpose Framework for Backdoor Defense and Backdoor Amplification
  in Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19047v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19047v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vu Tuan Truong, Long Bao Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have emerged as state-of-the-art generative frameworks,
excelling in producing high-quality multi-modal samples. However, recent
studies have revealed their vulnerability to backdoor attacks, where backdoored
models generate specific, undesirable outputs called backdoor target (e.g.,
harmful images) when a pre-defined trigger is embedded to their inputs. In this
paper, we propose PureDiffusion, a dual-purpose framework that simultaneously
serves two contrasting roles: backdoor defense and backdoor attack
amplification. For defense, we introduce two novel loss functions to invert
backdoor triggers embedded in diffusion models. The first leverages
trigger-induced distribution shifts across multiple timesteps of the diffusion
process, while the second exploits the denoising consistency effect when a
backdoor is activated. Once an accurate trigger inversion is achieved, we
develop a backdoor detection method that analyzes both the inverted trigger and
the generated backdoor targets to identify backdoor attacks. In terms of attack
amplification with the role of an attacker, we describe how our trigger
inversion algorithm can be used to reinforce the original trigger embedded in
the backdoored diffusion model. This significantly boosts attack performance
while reducing the required backdoor training time. Experimental results
demonstrate that PureDiffusion achieves near-perfect detection accuracy,
outperforming existing defenses by a large margin, particularly against complex
trigger patterns. Additionally, in an attack scenario, our attack amplification
approach elevates the attack success rate (ASR) of existing backdoor attacks to
nearly 100\% while reducing training time by up to 20x.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Test-Time Adaptation for Combating Missing Modalities in Egocentric
  Videos 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.15161v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.15161v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Merey Ramazanova, Alejandro Pardo, Bernard Ghanem, Motasem Alfarra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding videos that contain multiple modalities is crucial, especially
in egocentric videos, where combining various sensory inputs significantly
improves tasks like action recognition and moment localization. However,
real-world applications often face challenges with incomplete modalities due to
privacy concerns, efficiency needs, or hardware issues. Current methods, while
effective, often necessitate retraining the model entirely to handle missing
modalities, making them computationally intensive, particularly with large
training datasets. In this study, we propose a novel approach to address this
issue at test time without requiring retraining. We frame the problem as a
test-time adaptation task, where the model adjusts to the available unlabeled
data at test time. Our method, MiDl~(Mutual information with
self-Distillation), encourages the model to be insensitive to the specific
modality source present during testing by minimizing the mutual information
between the prediction and the available modality. Additionally, we incorporate
self-distillation to maintain the model's original performance when both
modalities are available. MiDl represents the first self-supervised, online
solution for handling missing modalities exclusively at test time. Through
experiments with various pretrained models and datasets, MiDl demonstrates
substantial performance improvement without the need for retraining.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DIPSER: A <span class="highlight-title">Dataset</span> for In-Person Student Engagement Recognition in the
  Wild 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20209v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20209v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luis Marquez-Carpintero, Sergio Suescun-Ferrandiz, Carolina Lorenzo Álvarez, Jorge Fernandez-Herrero, Diego Viejo, Rosabel Roig-Vila, Miguel Cazorla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, a novel dataset is introduced, designed to assess student
attention within in-person classroom settings. This dataset encompasses RGB
camera data, featuring multiple cameras per student to capture both posture and
facial expressions, in addition to smartwatch sensor data for each individual.
This dataset allows machine learning algorithms to be trained to predict
attention and correlate it with emotion. A comprehensive suite of attention and
emotion labels for each student is provided, generated through self-reporting
as well as evaluations by four different experts. Our dataset uniquely combines
facial and environmental camera data, smartwatch metrics, and includes
underrepresented ethnicities in similar datasets, all within in-the-wild,
in-person settings, making it the most comprehensive dataset of its kind
currently available.
  The dataset presented offers an extensive and diverse collection of data
pertaining to student interactions across different educational contexts,
augmented with additional metadata from other tools. This initiative addresses
existing deficiencies by offering a valuable resource for the analysis of
student attention and emotion in face-to-face lessons.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MOVE: Effective and Harmless Ownership Verification via Embedded
  External Features <span class="chip">AAAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.02820v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.02820v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiming Li, Linghui Zhu, Xiaojun Jia, Yang Bai, Yong Jiang, Shu-Tao Xia, Xiaochun Cao, Kui Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Currently, deep neural networks (DNNs) are widely adopted in different
applications. Despite its commercial values, training a well-performing DNN is
resource-consuming. Accordingly, the well-trained model is valuable
intellectual property for its owner. However, recent studies revealed the
threats of model stealing, where the adversaries can obtain a function-similar
copy of the victim model, even when they can only query the model. In this
paper, we propose an effective and harmless model ownership verification (MOVE)
to defend against different types of model stealing simultaneously, without
introducing new security risks. In general, we conduct the ownership
verification by verifying whether a suspicious model contains the knowledge of
defender-specified external features. Specifically, we embed the external
features by modifying a few training samples with style transfer. We then train
a meta-classifier to determine whether a model is stolen from the victim. This
approach is inspired by the understanding that the stolen models should contain
the knowledge of features learned by the victim model. In particular,
\revision{we develop our MOVE method under both white-box and black-box
settings and analyze its theoretical foundation to provide comprehensive model
protection.} Extensive experiments on benchmark datasets verify the
effectiveness of our method and its resistance to potential adaptive attacks.
The codes for reproducing the main experiments of our method are available at
https://github.com/THUYimingLi/MOVE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper has been accepted by IEEE TPAMI 2025. It is the journal
  extension of our conference paper in AAAI 2022
  (https://ojs.aaai.org/index.php/AAAI/article/view/20036). 18 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ K-LoRA: Unlocking Training-Free Fusion of Any Subject and Style LoRAs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18461v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18461v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziheng Ouyang, Zhen Li, Qibin Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have explored combining different LoRAs to jointly generate
learned style and content. However, existing methods either fail to effectively
preserve both the original subject and style simultaneously or require
additional training. In this paper, we argue that the intrinsic properties of
LoRA can effectively guide diffusion models in merging learned subject and
style. Building on this insight, we propose K-LoRA, a simple yet effective
training-free LoRA fusion approach. In each attention layer, K-LoRA compares
the Top-K elements in each LoRA to be fused, determining which LoRA to select
for optimal fusion. This selection mechanism ensures that the most
representative features of both subject and style are retained during the
fusion process, effectively balancing their contributions. Experimental results
demonstrate that the proposed method effectively integrates the subject and
style information learned by the original LoRAs, outperforming state-of-the-art
training-based approaches in both qualitative and quantitative results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025, Project page: https://k-lora.github.io/K-LoRA.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Intrinsic Dimension Correlation: uncovering nonlinear connections in
  <span class="highlight-title">multi</span>modal representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.15812v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.15812v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Basile, Santiago Acevedo, Luca Bortolussi, Fabio Anselmi, Alex Rodriguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To gain insight into the mechanisms behind machine learning methods, it is
crucial to establish connections among the features describing data points.
However, these correlations often exhibit a high-dimensional and strongly
nonlinear nature, which makes them challenging to detect using standard
methods. This paper exploits the entanglement between intrinsic dimensionality
and correlation to propose a metric that quantifies the (potentially nonlinear)
correlation between high-dimensional manifolds. We first validate our method on
synthetic data in controlled environments, showcasing its advantages and
drawbacks compared to existing techniques. Subsequently, we extend our analysis
to large-scale applications in neural network representations. Specifically, we
focus on latent representations of multimodal data, uncovering clear
correlations between paired visual and textual embeddings, whereas existing
methods struggle significantly in detecting similarity. Our results indicate
the presence of highly nonlinear correlation patterns between latent manifolds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Q-Bench-Video: Benchmarking the Video Quality Understanding of LMMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.20063v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.20063v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zicheng Zhang, Ziheng Jia, Haoning Wu, Chunyi Li, Zijian Chen, Yingjie Zhou, Wei Sun, Xiaohong Liu, Xiongkuo Min, Weisi Lin, Guangtao Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rising interest in research on Large Multi-modal Models (LMMs) for
video understanding, many studies have emphasized general video comprehension
capabilities, neglecting the systematic exploration into video quality
understanding. To address this oversight, we introduce Q-Bench-Video in this
paper, a new benchmark specifically designed to evaluate LMMs' proficiency in
discerning video quality. a) To ensure video source diversity, Q-Bench-Video
encompasses videos from natural scenes, AI-generated Content (AIGC), and
Computer Graphics (CG). b) Building on the traditional multiple-choice
questions format with the Yes-or-No and What-How categories, we include
Open-ended questions to better evaluate complex scenarios. Additionally, we
incorporate the video pair quality comparison question to enhance
comprehensiveness. c) Beyond the traditional Technical, Aesthetic, and Temporal
distortions, we have expanded our evaluation aspects to include the dimension
of AIGC distortions, which addresses the increasing demand for video
generation. Finally, we collect a total of 2,378 question-answer pairs and test
them on 12 open-source & 5 proprietary LMMs. Our findings indicate that while
LMMs have a foundational understanding of video quality, their performance
remains incomplete and imprecise, with a notable discrepancy compared to human
performance. Through Q-Bench-Video, we seek to catalyze community interest,
stimulate further research, and unlock the untapped potential of LMMs to close
the gap in video quality understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HyperFace: Generating Synthetic Face Recognition <span class="highlight-title">Dataset</span>s by Exploring
  Face Embedding Hypersphere 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.08470v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.08470v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hatef Otroshi Shahreza, Sébastien Marcel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face recognition datasets are often collected by crawling Internet and
without individuals' consents, raising ethical and privacy concerns. Generating
synthetic datasets for training face recognition models has emerged as a
promising alternative. However, the generation of synthetic datasets remains
challenging as it entails adequate inter-class and intra-class variations.
While advances in generative models have made it easier to increase intra-class
variations in face datasets (such as pose, illumination, etc.), generating
sufficient inter-class variation is still a difficult task. In this paper, we
formulate the dataset generation as a packing problem on the embedding space
(represented on a hypersphere) of a face recognition model and propose a new
synthetic dataset generation approach, called HyperFace. We formalize our
packing problem as an optimization problem and solve it with a gradient
descent-based approach. Then, we use a conditional face generator model to
synthesize face images from the optimized embeddings. We use our generated
datasets to train face recognition models and evaluate the trained models on
several benchmarking real datasets. Our experimental results show that models
trained with HyperFace achieve state-of-the-art performance in training face
recognition using synthetic datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improved Baselines with Synchronized Encoding for Universal Medical
  Image Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09886v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09886v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sihan Yang, Xuande Mi, Jiadong Feng, Haixia Bi, Hai Zhang, Jian Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large foundation models, known for their strong zero-shot generalization
capabilities, can be applied to a wide range of downstream tasks. However,
developing foundation models for medical image segmentation poses a significant
challenge due to the domain gap between natural and medical images. While
fine-tuning techniques based on the Segment Anything Model (SAM) have been
explored, they primarily focus on scaling up data or refining inference
strategies without incorporating domain-specific architectural designs,
limiting their zero-shot performance. To optimize segmentation performance
under standard inference settings and provide a strong baseline for future
research, we introduce SyncSAM, which employs a synchronized dual-branch
encoder that integrates convolution and Transformer features in a synchronized
manner to enhance medical image encoding, and a multi-scale dual-branch decoder
to preserve image details. SyncSAM is trained on two of the largest medical
image segmentation datasets, SA-Med2D-20M and IMed-361M, resulting in a series
of pre-trained models for universal medical image segmentation. Experimental
results demonstrate that SyncSAM not only achieves state-of-the-art performance
on test sets but also exhibits strong zero-shot capabilities on unseen
datasets. The code and model weights are available at
https://github.com/Hhankyangg/SyncSAM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ StochSync: Stochastic Diffusion Synchronization for Image Generation in
  Arbitrary Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.15445v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.15445v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyeongmin Yeo, Jaihoon Kim, Minhyuk Sung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a zero-shot method for generating images in arbitrary spaces
(e.g., a sphere for 360{\deg} panoramas and a mesh surface for texture) using a
pretrained image diffusion model. The zero-shot generation of various visual
content using a pretrained image diffusion model has been explored mainly in
two directions. First, Diffusion Synchronization-performing reverse diffusion
processes jointly across different projected spaces while synchronizing them in
the target space-generates high-quality outputs when enough conditioning is
provided, but it struggles in its absence. Second, Score Distillation
Sampling-gradually updating the target space data through gradient
descent-results in better coherence but often lacks detail. In this paper, we
reveal for the first time the interconnection between these two methods while
highlighting their differences. To this end, we propose StochSync, a novel
approach that combines the strengths of both, enabling effective performance
with weak conditioning. Our experiments demonstrate that StochSync provides the
best performance in 360{\deg} panorama generation (where image conditioning is
not given), outperforming previous finetuning-based methods, and also delivers
comparable results in 3D mesh texturing (where depth conditioning is provided)
with previous methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://stochsync.github.io/ (ICLR 2025)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DartControl: A Diffusion-Based Autoregressive Motion Model for Real-Time
  Text-Driven Motion Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05260v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05260v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaifeng Zhao, Gen Li, Siyu Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-conditioned human motion generation, which allows for user interaction
through natural language, has become increasingly popular. Existing methods
typically generate short, isolated motions based on a single input sentence.
However, human motions are continuous and can extend over long periods,
carrying rich semantics. Creating long, complex motions that precisely respond
to streams of text descriptions, particularly in an online and real-time
setting, remains a significant challenge. Furthermore, incorporating spatial
constraints into text-conditioned motion generation presents additional
challenges, as it requires aligning the motion semantics specified by text
descriptions with geometric information, such as goal locations and 3D scene
geometry. To address these limitations, we propose DartControl, in short DART,
a Diffusion-based Autoregressive motion primitive model for Real-time
Text-driven motion control. Our model effectively learns a compact motion
primitive space jointly conditioned on motion history and text inputs using
latent diffusion models. By autoregressively generating motion primitives based
on the preceding history and current text input, DART enables real-time,
sequential motion generation driven by natural language descriptions.
Additionally, the learned motion primitive space allows for precise spatial
motion control, which we formulate either as a latent noise optimization
problem or as a Markov decision process addressed through reinforcement
learning. We present effective algorithms for both approaches, demonstrating
our model's versatility and superior performance in various motion synthesis
tasks. Experiments show our method outperforms existing baselines in motion
realism, efficiency, and controllability. Video results are available on the
project page: https://zkf1997.github.io/DART/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated ICLR camera ready version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CogCoM: A Visual Language Model with Chain-of-Manipulations Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.04236v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.04236v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ji Qi, Ming Ding, Weihan Wang, Yushi Bai, Qingsong Lv, Wenyi Hong, Bin Xu, Lei Hou, Juanzi Li, Yuxiao Dong, Jie Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language Models (VLMs) have demonstrated their broad effectiveness
thanks to extensive training in aligning visual instructions to responses.
However, such training of conclusive alignment leads models to ignore essential
visual reasoning, further resulting in failures in meticulous visual problems
and unfaithful responses. Drawing inspiration from human cognition in solving
visual problems (e.g., marking, zoom in), this paper introduces Chain of
Manipulations, a mechanism that enables VLMs to solve problems step-by-step
with evidence. After training, models can solve various visual problems by
eliciting intrinsic manipulations (e.g., grounding, zoom in) with results
(e.g., boxes, image) actively without involving external tools, while also
allowing users to trace error causes. We study the roadmap to implement this
mechanism, including (1) a flexible design of manipulations upon extensive
analysis, (2) an efficient automated data generation pipeline, (3) a compatible
VLM architecture capable of multi-turn multi-image, and (4) a model training
process for versatile capabilities. With the design, we also manually annotate
6K high-quality samples for the challenging graphical mathematical problems.
Our trained model, \textbf{CogCoM}, equipped with this mechanism with 17B
parameters achieves state-of-the-art performance across 9 benchmarks from 4
categories, demonstrating the effectiveness while preserving the
interpretability. Our code, model weights, and collected data are publicly
available at https://github.com/THUDM/CogCoM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLIPure: Purification in Latent Space via CLIP for Adversarially Robust
  Zero-Shot Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18176v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18176v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingkun Zhang, Keping Bi, Wei Chen, Jiafeng Guo, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we aim to build an adversarially robust zero-shot image
classifier. We ground our work on CLIP, a vision-language pre-trained encoder
model that can perform zero-shot classification by matching an image with text
prompts ``a photo of a <class-name>.''. Purification is the path we choose
since it does not require adversarial training on specific attack types and
thus can cope with any foreseen attacks. We then formulate purification risk as
the KL divergence between the joint distributions of the purification process
of denoising the adversarial samples and the attack process of adding
perturbations to benign samples, through bidirectional Stochastic Differential
Equations (SDEs). The final derived results inspire us to explore purification
in the multi-modal latent space of CLIP. We propose two variants for our
CLIPure approach: CLIPure-Diff which models the likelihood of images' latent
vectors with the DiffusionPrior module in DaLLE-2 (modeling the generation
process of CLIP's latent vectors), and CLIPure-Cos which models the likelihood
with the cosine similarity between the embeddings of an image and ``a photo of
a.''. As far as we know, CLIPure is the first purification method in
multi-modal latent space and CLIPure-Cos is the first purification method that
is not based on generative models, which substantially improves defense
efficiency. We conducted extensive experiments on CIFAR-10, ImageNet, and 13
datasets that previous CLIP-based defense methods used for evaluating zero-shot
classification robustness. Results show that CLIPure boosts the SOTA robustness
by a large margin, e.g., from 71.7% to 91.1% on CIFAR10, from 59.6% to 72.6% on
ImageNet, and 108% relative improvements of average robustness on the 13
datasets over previous SOTA. The code is available at
https://github.com/TMLResearchGroup-CAS/CLIPure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ L-WISE: Boosting Human Visual Category Learning Through Model-Based
  Image Selection And Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.09765v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.09765v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Morgan B. Talbot, Gabriel Kreiman, James J. DiCarlo, Guy Gaziv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The currently leading artificial neural network models of the visual ventral
stream - which are derived from a combination of performance optimization and
robustification methods - have demonstrated a remarkable degree of behavioral
alignment with humans on visual categorization tasks. We show that image
perturbations generated by these models can enhance the ability of humans to
accurately report the ground truth class. Furthermore, we find that the same
models can also be used out-of-the-box to predict the proportion of correct
human responses to individual images, providing a simple, human-aligned
estimator of the relative difficulty of each image. Motivated by these
observations, we propose to augment visual learning in humans in a way that
improves human categorization accuracy at test time. Our learning augmentation
approach consists of (i) selecting images based on their model-estimated
recognition difficulty, and (ii) applying image perturbations that aid
recognition for novice learners. We find that combining these model-based
strategies leads to categorization accuracy gains of 33-72% relative to control
subjects without these interventions, on unmodified, randomly selected held-out
test images. Beyond the accuracy gain, the training time for the augmented
learning group was also shortened by 20-23%, despite both groups completing the
same number of training trials. We demonstrate the efficacy of our approach in
a fine-grained categorization task with natural images, as well as two tasks in
clinically relevant image domains - histology and dermoscopy - where visual
learning is notoriously challenging. To the best of our knowledge, our work is
the first application of artificial neural networks to increase visual learning
performance in humans by enhancing category-specific image features.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Padding Tone: A Mechanistic Analysis of Padding Tokens in T2I Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.06751v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.06751v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Toker, Ido Galil, Hadas Orgad, Rinon Gal, Yoad Tewel, Gal Chechik, Yonatan Belinkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image (T2I) diffusion models rely on encoded prompts to guide the
image generation process. Typically, these prompts are extended to a fixed
length by adding padding tokens before text encoding. Despite being a default
practice, the influence of padding tokens on the image generation process has
not been investigated. In this work, we conduct the first in-depth analysis of
the role padding tokens play in T2I models. We develop two causal techniques to
analyze how information is encoded in the representation of tokens across
different components of the T2I pipeline. Using these techniques, we
investigate when and how padding tokens impact the image generation process.
Our findings reveal three distinct scenarios: padding tokens may affect the
model's output during text encoding, during the diffusion process, or be
effectively ignored. Moreover, we identify key relationships between these
scenarios and the model's architecture (cross or self-attention) and its
training process (frozen or trained text encoder). These insights contribute to
a deeper understanding of the mechanisms of padding tokens, potentially
informing future model design and training practices in T2I systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in: NAACL 2025. Project webpage:
  https://padding-tone.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Pair-VPR: Place-Aware Pre-training and Contrastive Pair Classification
  for Visual Place Recognition with Vision Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06614v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06614v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephen Hausler, Peyman Moghadam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we propose a novel joint training method for Visual Place
Recognition (VPR), which simultaneously learns a global descriptor and a pair
classifier for re-ranking. The pair classifier can predict whether a given pair
of images are from the same place or not. The network only comprises Vision
Transformer components for both the encoder and the pair classifier, and both
components are trained using their respective class tokens. In existing VPR
methods, typically the network is initialized using pre-trained weights from a
generic image dataset such as ImageNet. In this work we propose an alternative
pre-training strategy, by using Siamese Masked Image Modelling as a
pre-training task. We propose a Place-aware image sampling procedure from a
collection of large VPR datasets for pre-training our model, to learn visual
features tuned specifically for VPR. By re-using the Mask Image Modelling
encoder and decoder weights in the second stage of training, Pair-VPR can
achieve state-of-the-art VPR performance across five benchmark datasets with a
ViT-B encoder, along with further improvements in localization recall with
larger encoders. The Pair-VPR website is:
https://csiro-robotics.github.io/Pair-VPR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ WalnutData: A UAV Remote Sensing <span class="highlight-title">Dataset</span> of Green Walnuts and Model
  Evaluation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20092v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20092v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingjie Wu, Chenggui Yang, Huihua Wang, Chen Xue, Yibo Wang, Haoyu Wang, Yansong Wang, Can Peng, Yuqi Han, Ruoyu Li, Lijun Yun, Zaiqing Chen, Songfan Shi, Luhao Fang, Shuyi Wan, Tingfeng Li, Shuangyao Liu, Haotian Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The UAV technology is gradually maturing and can provide extremely powerful
support for smart agriculture and precise monitoring. Currently, there is no
dataset related to green walnuts in the field of agricultural computer vision.
Thus, in order to promote the algorithm design in the field of agricultural
computer vision, we used UAV to collect remote-sensing data from 8 walnut
sample plots. Considering that green walnuts are subject to various lighting
conditions and occlusion, we constructed a large-scale dataset with a
higher-granularity of target features - WalnutData. This dataset contains a
total of 30,240 images and 706,208 instances, and there are 4 target
categories: being illuminated by frontal light and unoccluded (A1), being
backlit and unoccluded (A2), being illuminated by frontal light and occluded
(B1), and being backlit and occluded (B2). Subsequently, we evaluated many
mainstream algorithms on WalnutData and used these evaluation results as the
baseline standard. The dataset and all evaluation results can be obtained at
https://github.com/1wuming/WalnutData.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ High-Resolution Image Synthesis via Next-Token Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.14808v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.14808v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dengsheng Chen, Jie Hu, Tiezhu Yue, Xiaoming Wei, Enhua Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, autoregressive models have demonstrated remarkable performance in
class-conditional image generation. However, the application of next-token
prediction to high-resolution text-to-image generation remains largely
unexplored. In this paper, we introduce \textbf{D-JEPA$\cdot$T2I}, an
autoregressive model based on continuous tokens that incorporates innovations
in both architecture and training strategy to generate high-quality,
photorealistic images at arbitrary resolutions, up to 4K. Architecturally, we
adopt the denoising joint embedding predictive architecture (D-JEPA) while
leveraging a multimodal visual transformer to effectively integrate textual and
visual features. Additionally, we introduce flow matching loss alongside the
proposed Visual Rotary Positional Embedding (VoPE) to enable continuous
resolution learning. In terms of training strategy, we propose a data feedback
mechanism that dynamically adjusts the sampling procedure based on statistical
analysis and an online learning critic model. This encourages the model to move
beyond its comfort zone, reducing redundant training on well-mastered scenarios
and compelling it to address more challenging cases with suboptimal generation
quality. For the first time, we achieve state-of-the-art high-resolution image
synthesis via next-token prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ET-SEED: Efficient Trajectory-Level SE(3) Equivariant Diffusion Policy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.03990v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.03990v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenrui Tie, Yue Chen, Ruihai Wu, Boxuan Dong, Zeyi Li, Chongkai Gao, Hao Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitation learning, e.g., diffusion policy, has been proven effective in
various robotic manipulation tasks. However, extensive demonstrations are
required for policy robustness and generalization. To reduce the demonstration
reliance, we leverage spatial symmetry and propose ET-SEED, an efficient
trajectory-level SE(3) equivariant diffusion model for generating action
sequences in complex robot manipulation tasks. Further, previous equivariant
diffusion models require the per-step equivariance in the Markov process,
making it difficult to learn policy under such strong constraints. We
theoretically extend equivariant Markov kernels and simplify the condition of
equivariant diffusion process, thereby significantly improving training
efficiency for trajectory-level SE(3) equivariant diffusion policy in an
end-to-end manner. We evaluate ET-SEED on representative robotic manipulation
tasks, involving rigid body, articulated and deformable object. Experiments
demonstrate superior data efficiency and manipulation proficiency of our
proposed method, as well as its ability to generalize to unseen configurations
with only a few demonstrations. Website: https://et-seed.github.io/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Autoregressive Video Generation without Vector Quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.14169v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.14169v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoge Deng, Ting Pan, Haiwen Diao, Zhengxiong Luo, Yufeng Cui, Huchuan Lu, Shiguang Shan, Yonggang Qi, Xinlong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach that enables autoregressive video
generation with high efficiency. We propose to reformulate the video generation
problem as a non-quantized autoregressive modeling of temporal frame-by-frame
prediction and spatial set-by-set prediction. Unlike raster-scan prediction in
prior autoregressive models or joint distribution modeling of fixed-length
tokens in diffusion models, our approach maintains the causal property of
GPT-style models for flexible in-context capabilities, while leveraging
bidirectional modeling within individual frames for efficiency. With the
proposed approach, we train a novel video autoregressive model without vector
quantization, termed NOVA. Our results demonstrate that NOVA surpasses prior
autoregressive video models in data efficiency, inference speed, visual
fidelity, and video fluency, even with a much smaller model capacity, i.e.,
0.6B parameters. NOVA also outperforms state-of-the-art image diffusion models
in text-to-image generation tasks, with a significantly lower training cost.
Additionally, NOVA generalizes well across extended video durations and enables
diverse zero-shot applications in one unified model. Code and models are
publicly available at https://github.com/baaivision/NOVA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025. Project page at
  https://github.com/baaivision/NOVA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SEED-X: <span class="highlight-title">Multi</span>modal Models with Unified <span class="highlight-title">Multi</span>-granularity Comprehension
  and Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.14396v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.14396v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuying Ge, Sijie Zhao, Jinguo Zhu, Yixiao Ge, Kun Yi, Lin Song, Chen Li, Xiaohan Ding, Ying Shan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid evolution of multimodal foundation model has demonstrated
significant progresses in vision-language understanding and generation, e.g.,
our previous work SEED-LLaMA. However, there remains a gap between its
capability and the real-world applicability, primarily due to the model's
limited capacity to effectively respond to various user instructions and
interact with diverse visual data. In this work, we focus on bridging this gap
through integrating two enhanced features: (1) comprehending images of
arbitrary sizes and ratios, and (2) enabling multi-granularity image
generation. We present a unified and versatile foundation model, namely,
SEED-X, which is able to model multi-granularity visual semantics for
comprehension and generation tasks. Besides the competitive results on public
benchmarks, SEED-X demonstrates its effectiveness in handling real-world
applications across various domains after instruction tuning. We hope that our
work will inspire future research into what can be achieved by versatile
multimodal foundation models in real-world applications. The models, codes, and
datasets are released in https://github.com/AILab-CVC/SEED-X.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>We added benchmark results (without updating models) and ablation
  study in this version. Project released at:
  https://github.com/AILab-CVC/SEED-X</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predictive Uncertainty Quantification for Bird's Eye View Segmentation:
  A Benchmark and Novel Loss Function 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.20986v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.20986v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linlin Yu, Bowen Yang, Tianhao Wang, Kangshuo Li, Feng Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The fusion of raw sensor data to create a Bird's Eye View (BEV)
representation is critical for autonomous vehicle planning and control. Despite
the growing interest in using deep learning models for BEV semantic
segmentation, anticipating segmentation errors and enhancing the explainability
of these models remain underexplored. This paper introduces a comprehensive
benchmark for predictive uncertainty quantification in BEV segmentation,
evaluating multiple uncertainty quantification methods across three popular
datasets with three representative network architectures. Our study focuses on
the effectiveness of quantified uncertainty in detecting misclassified and
out-of-distribution (OOD) pixels while also improving model calibration.
Through empirical analysis, we uncover challenges in existing uncertainty
quantification methods and demonstrate the potential of evidential deep
learning techniques, which capture both aleatoric and epistemic uncertainty. To
address these challenges, we propose a novel loss function,
Uncertainty-Focal-Cross-Entropy (UFCE), specifically designed for highly
imbalanced data, along with a simple uncertainty-scaling regularization term
that improves both uncertainty quantification and model calibration for BEV
segmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LANTERN: Accelerating Visual Autoregressive Models with Relaxed
  Speculative Decoding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03355v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03355v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Doohyuk Jang, Sihwan Park, June Yong Yang, Yeonsung Jung, Jihun Yun, Souvik Kundu, Sung-Yub Kim, Eunho Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Auto-Regressive (AR) models have recently gained prominence in image
generation, often matching or even surpassing the performance of diffusion
models. However, one major limitation of AR models is their sequential nature,
which processes tokens one at a time, slowing down generation compared to
models like GANs or diffusion-based methods that operate more efficiently.
While speculative decoding has proven effective for accelerating LLMs by
generating multiple tokens in a single forward, its application in visual AR
models remains largely unexplored. In this work, we identify a challenge in
this setting, which we term \textit{token selection ambiguity}, wherein visual
AR models frequently assign uniformly low probabilities to tokens, hampering
the performance of speculative decoding. To overcome this challenge, we propose
a relaxed acceptance condition referred to as LANTERN that leverages the
interchangeability of tokens in latent space. This relaxation restores the
effectiveness of speculative decoding in visual AR models by enabling more
flexible use of candidate tokens that would otherwise be prematurely rejected.
Furthermore, by incorporating a total variation distance bound, we ensure that
these speed gains are achieved without significantly compromising image quality
or semantic coherence. Experimental results demonstrate the efficacy of our
method in providing a substantial speed-up over speculative decoding. In
specific, compared to a na\"ive application of the state-of-the-art speculative
decoding, LANTERN increases speed-ups by $\mathbf{1.75}\times$ and
$\mathbf{1.82}\times$, as compared to greedy decoding and random sampling,
respectively, when applied to LlamaGen, a contemporary visual AR model. The
code is publicly available at https://github.com/jadohu/LANTERN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 13 figures, Accepted to ICLR 2025 (poster)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ InterMask: 3D Human Interaction Generation via Collaborative Masked
  Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10010v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10010v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Gohar Javed, Chuan Guo, Li Cheng, Xingyu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating realistic 3D human-human interactions from textual descriptions
remains a challenging task. Existing approaches, typically based on diffusion
models, often produce results lacking realism and fidelity. In this work, we
introduce InterMask, a novel framework for generating human interactions using
collaborative masked modeling in discrete space. InterMask first employs a
VQ-VAE to transform each motion sequence into a 2D discrete motion token map.
Unlike traditional 1D VQ token maps, it better preserves fine-grained
spatio-temporal details and promotes spatial awareness within each token.
Building on this representation, InterMask utilizes a generative masked
modeling framework to collaboratively model the tokens of two interacting
individuals. This is achieved by employing a transformer architecture
specifically designed to capture complex spatio-temporal inter-dependencies.
During training, it randomly masks the motion tokens of both individuals and
learns to predict them. For inference, starting from fully masked sequences, it
progressively fills in the tokens for both individuals. With its enhanced
motion representation, dedicated architecture, and effective learning strategy,
InterMask achieves state-of-the-art results, producing high-fidelity and
diverse human interactions. It outperforms previous methods, achieving an FID
of $5.154$ (vs $5.535$ of in2IN) on the InterHuman dataset and $0.399$ (vs
$5.207$ of InterGen) on the InterX dataset. Additionally, InterMask seamlessly
supports reaction generation without the need for model redesign or
fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project webpage: https://gohar-malik.github.io/intermask</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MoCoLSK: Modality Conditioned High-Resolution Downscaling for Land
  Surface Temperature 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.19835v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.19835v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qun Dai, Chunyang Yuan, Yimian Dai, Yuxuan Li, Xiang Li, Kang Ni, Jianhui Xu, Xiangbo Shu, Jian Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Land Surface Temperature (LST) is a critical parameter for environmental
studies, but directly obtaining high spatial resolution LST data remains
challenging due to the spatio-temporal trade-off in satellite remote sensing.
Guided LST downscaling has emerged as an alternative solution to overcome these
limitations, but current methods often neglect spatial non-stationarity, and
there is a lack of an open-source ecosystem for deep learning methods. In this
paper, we propose the Modality-Conditional Large Selective Kernel (MoCoLSK)
Network, a novel architecture that dynamically fuses multi-modal data through
modality-conditioned projections. MoCoLSK achieves a confluence of dynamic
receptive field adjustment and multi-modal feature fusion, leading to enhanced
LST prediction accuracy. Furthermore, we establish the GrokLST project, a
comprehensive open-source ecosystem featuring the GrokLST dataset, a
high-resolution benchmark, and the GrokLST toolkit, an open-source
PyTorch-based toolkit encapsulating MoCoLSK alongside 40+ state-of-the-art
approaches. Extensive experimental results validate MoCoLSK's effectiveness in
capturing complex dependencies and subtle variations within multispectral data,
outperforming existing methods in LST downscaling. Our code, dataset, and
toolkit are available at https://github.com/GrokCV/GrokLST.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE TGRS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TEASER: Token Enhanced Spatial Modeling for Expressions Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10982v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10982v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunfei Liu, Lei Zhu, Lijian Lin, Ye Zhu, Ailing Zhang, Yu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D facial reconstruction from a single in-the-wild image is a crucial task in
human-centered computer vision tasks. While existing methods can recover
accurate facial shapes, there remains significant space for improvement in
fine-grained expression capture. Current approaches struggle with irregular
mouth shapes, exaggerated expressions, and asymmetrical facial movements. We
present TEASER (Token EnhAnced Spatial modeling for Expressions
Reconstruction), which addresses these challenges and enhances 3D facial
geometry performance. TEASER tackles two main limitations of existing methods:
insufficient photometric loss for self-reconstruction and inaccurate
localization of subtle expressions. We introduce a multi-scale tokenizer to
extract facial appearance information. Combined with a neural renderer, these
tokens provide precise geometric guidance for expression reconstruction.
Furthermore, TEASER incorporates a pose-dependent landmark loss to further
improve geometric performances. Our approach not only significantly enhances
expression reconstruction quality but also offers interpretable tokens suitable
for various downstream applications, such as photorealistic facial video
driving, expression transfer, and identity swapping. Quantitative and
qualitative experimental results across multiple datasets demonstrate that
TEASER achieves state-of-the-art performance in precise expression
reconstruction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICLR 2025, code and demos are available at
  https://tinyurl.com/TEASER-project</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving vision-language alignment with graph spiking hybrid Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19069v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19069v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyu Zhang, Wenzhe Liu, Yeming Chen, Yiming Wu, Heming Zheng, Cheng Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To bridge the semantic gap between vision and language (VL), it is necessary
to develop a good alignment strategy, which includes handling semantic
diversity, abstract representation of visual information, and generalization
ability of models. Recent works use detector-based bounding boxes or patches
with regular partitions to represent visual semantics. While current paradigms
have made strides, they are still insufficient for fully capturing the nuanced
contextual relations among various objects. This paper proposes a comprehensive
visual semantic representation module, necessitating the utilization of
panoptic segmentation to generate coherent fine-grained semantic features.
Furthermore, we propose a novel Graph Spiking Hybrid Network (GSHN) that
integrates the complementary advantages of Spiking Neural Networks (SNNs) and
Graph Attention Networks (GATs) to encode visual semantic information.
Intriguingly, the model not only encodes the discrete and continuous latent
variables of instances but also adeptly captures both local and global
contextual features, thereby significantly enhancing the richness and diversity
of semantic representations. Leveraging the spatiotemporal properties inherent
in SNNs, we employ contrastive learning (CL) to enhance the similarity-based
representation of embeddings. This strategy alleviates the computational
overhead of the model and enriches meaningful visual representations by
constructing positive and negative sample pairs. We design an innovative
pre-training method, Spiked Text Learning (STL), which uses text features to
improve the encoding ability of discrete semantics. Experiments show that the
proposed GSHN exhibits promising results on multiple VL downstream tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Long-Text Alignment for Text-to-Image Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11817v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11817v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luping Liu, Chao Du, Tianyu Pang, Zehan Wang, Chongxuan Li, Dong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of text-to-image (T2I) diffusion models has enabled
them to generate unprecedented results from given texts. However, as text
inputs become longer, existing encoding methods like CLIP face limitations, and
aligning the generated images with long texts becomes challenging. To tackle
these issues, we propose LongAlign, which includes a segment-level encoding
method for processing long texts and a decomposed preference optimization
method for effective alignment training. For segment-level encoding, long texts
are divided into multiple segments and processed separately. This method
overcomes the maximum input length limits of pretrained encoding models. For
preference optimization, we provide decomposed CLIP-based preference models to
fine-tune diffusion models. Specifically, to utilize CLIP-based preference
models for T2I alignment, we delve into their scoring mechanisms and find that
the preference scores can be decomposed into two components: a text-relevant
part that measures T2I alignment and a text-irrelevant part that assesses other
visual aspects of human preference. Additionally, we find that the
text-irrelevant part contributes to a common overfitting problem during
fine-tuning. To address this, we propose a reweighting strategy that assigns
different weights to these two components, thereby reducing overfitting and
enhancing alignment. After fine-tuning $512 \times 512$ Stable Diffusion (SD)
v1.5 for about 20 hours using our method, the fine-tuned SD outperforms
stronger foundation models in T2I alignment, such as PixArt-$\alpha$ and
Kandinsky v2.2. The code is available at
https://github.com/luping-liu/LongAlign.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SCC-YOLO: An Improved Object Detector for Assisting in Brain Tumor
  Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.03836v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.03836v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runci Bai, Guibao Xu, Yanze Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Brain tumors can lead to neurological dysfunction, cognitive and
psychological changes, increased intracranial pressure, and seizures, posing
significant risks to health. The You Only Look Once (YOLO) series has shown
superior accuracy in medical imaging object detection. This paper presents a
novel SCC-YOLO architecture that integrates the SCConv module into YOLOv9. The
SCConv module optimizes convolutional efficiency by reducing spatial and
channel redundancy, enhancing image feature learning. We examine the effects of
different attention mechanisms with YOLOv9 for brain tumor detection using the
Br35H dataset and our custom dataset (Brain_Tumor_Dataset). Results indicate
that SCC-YOLO improved mAP50 by 0.3% on the Br35H dataset and by 0.5% on our
custom dataset compared to YOLOv9. SCC-YOLO achieves state-of-the-art
performance in brain tumor detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ IRisPath: Enhancing Costmap for Off-Road Navigation with Robust IR-RGB
  Fusion for Improved Day and Night Traversability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.03173v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.03173v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saksham Sharma, Akshit Raizada, Suresh Sundaram
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous off-road navigation is required for applications in agriculture,
construction, search and rescue and defence. Traditional on-road autonomous
methods struggle with dynamic terrains, leading to poor vehicle control in
off-road conditions. Recent deep-learning models have used perception sensors
along with kinesthetic feedback for navigation on such terrains. However, this
approach has out-of-domain uncertainty. Factors like change in time of day and
weather impacts the performance of the model. We propose a multi modal fusion
network "IRisPath" capable of using Thermal and RGB images to provide
robustness against dynamic weather and light conditions. To aid further works
in this domain, we also open-source a day-night dataset with Thermal and RGB
images along with pseudo-labels for traversability. In order to co-register for
fusion model we also develop a novel method for targetless extrinsic
calibration of Thermal, LiDAR and RGB cameras with translation accuracy of
+/-1.7cm and rotation accuracy of +/-0.827degrees.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ End-to-End Augmentation Hyperparameter Tuning for Self-Supervised
  Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.12033v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.12033v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaemin Yoo, Lingxiao Zhao, Leman Akoglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) has emerged as a promising paradigm that
presents supervisory signals to real-world problems, bypassing the extensive
cost of manual labeling. Consequently, self-supervised anomaly detection (SSAD)
has seen a recent surge of interest, since SSL is especially attractive for
unsupervised tasks. However, recent works have reported that the choice of a
data augmentation function has significant impact on the accuracy of SSAD,
posing augmentation search as an essential but nontrivial problem with the lack
of labeled validation data. In this paper, we introduce ST-SSAD, the first
systematic approach for rigorous augmentation tuning on SSAD. To this end, our
work presents two key contributions. The first is a new unsupervised validation
loss that quantifies the alignment between augmented training data and
unlabeled validation data. The second is new differentiable augmentation
functions, allowing data augmentation hyperparameter(s) to be tuned in an
end-to-end manner. Experiments on two testbeds with semantic class anomalies
and subtle industrial defects show that ST-SSAD gives significant performance
gains over existing works.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AuroraCap: Efficient, Performant Video Detailed Captioning and a New
  Benchmark 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03051v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03051v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Chai, Enxin Song, Yilun Du, Chenlin Meng, Vashisht Madhavan, Omer Bar-Tal, Jenq-Neng Hwang, Saining Xie, Christopher D. Manning
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Video detailed captioning is a key task which aims to generate comprehensive
and coherent textual descriptions of video content, benefiting both video
understanding and generation. In this paper, we propose AuroraCap, a video
captioner based on a large multimodal model. We follow the simplest
architecture design without additional parameters for temporal modeling. To
address the overhead caused by lengthy video sequences, we implement the token
merging strategy, reducing the number of input visual tokens. Surprisingly, we
found that this strategy results in little performance loss. AuroraCap shows
superior performance on various video and image captioning benchmarks, for
example, obtaining a CIDEr of 88.9 on Flickr30k, beating GPT-4V (55.3) and
Gemini-1.5 Pro (82.2). However, existing video caption benchmarks only include
simple descriptions, consisting of a few dozen words, which limits research in
this field. Therefore, we develop VDC, a video detailed captioning benchmark
with over one thousand carefully annotated structured captions. In addition, we
propose a new LLM-assisted metric VDCscore for bettering evaluation, which
adopts a divide-and-conquer strategy to transform long caption evaluation into
multiple short question-answer pairs. With the help of human Elo ranking, our
experiments show that this benchmark better correlates with human judgments of
video detailed captioning quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025. Code, docs, weight, benchmark and training
  data are all avaliable at https://rese1f.github.io/aurora-web/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EMT: A Visual <span class="highlight-title">Multi</span>-Task Benchmark <span class="highlight-title">Dataset</span> for Autonomous Driving in the
  Arab Gulf Region 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19260v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19260v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nadya Abdel Madjid, Murad Mebrahtu, Abdelmoamen Nasser, Bilal Hassan, Naoufel Werghi, Jorge Dias, Majid Khonji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the Emirates Multi-Task (EMT) dataset - the first
publicly available dataset for autonomous driving collected in the Arab Gulf
region. The EMT dataset captures the unique road topology, high traffic
congestion, and distinctive characteristics of the Gulf region, including
variations in pedestrian clothing and weather conditions. It contains over
30,000 frames from a dash-camera perspective, along with 570,000 annotated
bounding boxes, covering approximately 150 kilometers of driving routes. The
EMT dataset supports three primary tasks: tracking, trajectory forecasting and
intention prediction. Each benchmark dataset is complemented with corresponding
evaluations: (1) multi-agent tracking experiments, focusing on multi-class
scenarios and occlusion handling; (2) trajectory forecasting evaluation using
deep sequential and interaction-aware models; and (3) intention benchmark
experiments conducted for predicting agents intentions from observed
trajectories. The dataset is publicly available at avlab.io/emt-dataset, and
pre-processing scripts along with evaluation models can be accessed at
github.com/AV-Lab/emt-dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Measuring Anxiety Levels with Head Motion Patterns in Severe Depression
  Population 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.08813v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.08813v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fouad Boutaleb, Emery Pierson, Nicolas Doudeau, Clémence Nineuil, Ali Amad, Mohamed Daoudi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depression and anxiety are prevalent mental health disorders that frequently
cooccur, with anxiety significantly influencing both the manifestation and
treatment of depression. An accurate assessment of anxiety levels in
individuals with depression is crucial to develop effective and personalized
treatment plans. This study proposes a new noninvasive method for quantifying
anxiety severity by analyzing head movements -- specifically speed,
acceleration, and angular displacement -- during video-recorded interviews with
patients suffering from severe depression. Using data from a new CALYPSO
Depression Dataset, we extracted head motion characteristics and applied
regression analysis to predict clinically evaluated anxiety levels. Our results
demonstrate a high level of precision, achieving a mean absolute error (MAE) of
0.35 in predicting the severity of psychological anxiety based on head movement
patterns. This indicates that our approach can enhance the understanding of
anxiety's role in depression and assist psychiatrists in refining treatment
strategies for individuals.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19th IEEE International Conference on Automatic Face and Gesture
  Recognition (FG), 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Few-Class Arena: A Benchmark for Efficient Selection of Vision Models
  and <span class="highlight-title">Dataset</span> Difficulty Measurement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.01099v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.01099v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bryan Bo Cao, Lawrence O'Gorman, Michael Coss, Shubham Jain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose Few-Class Arena (FCA), as a unified benchmark with focus on
testing efficient image classification models for few classes. A wide variety
of benchmark datasets with many classes (80-1000) have been created to assist
Computer Vision architectural evolution. An increasing number of vision models
are evaluated with these many-class datasets. However, real-world applications
often involve substantially fewer classes of interest (2-10). This gap between
many and few classes makes it difficult to predict performance of the few-class
applications using models trained on the available many-class datasets. To
date, little has been offered to evaluate models in this Few-Class Regime. We
conduct a systematic evaluation of the ResNet family trained on ImageNet
subsets from 2 to 1000 classes, and test a wide spectrum of Convolutional
Neural Networks and Transformer architectures over ten datasets by using our
newly proposed FCA tool. Furthermore, to aid an up-front assessment of dataset
difficulty and a more efficient selection of models, we incorporate a
difficulty measure as a function of class similarity. FCA offers a new tool for
efficient machine learning in the Few-Class Regime, with goals ranging from a
new efficient class similarity proposal, to lightweight model architecture
design, to a new scaling law. FCA is user-friendly and can be easily extended
to new models and datasets, facilitating future research work. Our benchmark is
available at https://github.com/bryanbocao/fca.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 32 pages including References and Appendix, 19 figures, 8
  tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modeling and Analysis of Spatial and Temporal Land Clutter Statistics in
  SAR Imaging Based on MSTAR Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03816v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03816v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahrokh Hamidi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The statistical analysis of land clutter for Synthetic Aperture Radar (SAR)
imaging has become an increasingly important subject for research and
investigation. It is also absolutely necessary for designing robust algorithms
capable of performing the task of target detection in the background clutter.
Any attempt to extract the energy of the desired targets from the land clutter
requires complete knowledge of the statistical properties of the background
clutter. In this paper, the spatial as well as the temporal characteristics of
the land clutter are studied. Since the data for each image has been collected
based on a different aspect angle; therefore, the temporal analysis contains
variation in the aspect angle. Consequently, the temporal analysis includes the
characteristics of the radar cross section with respect to the aspect angle
based on which the data has been collected. In order to perform the statistical
analysis, several well-known and relevant distributions, namely, Weibull,
Log-normal, Gamma, and Rayleigh are considered as prime candidates to model the
land clutter. The goodness-of-fit test is based on the Kullback-Leibler (KL)
Divergence metric. The detailed analysis presented in this paper demonstrates
that the Weibull distribution is a more accurate fit for the
temporal-aspect-angle statistical analysis while the Rayleigh distribution
models the spatial characteristics of the background clutter with higher
accuracy. Finally, based on the aforementioned statistical analyses and by
utilizing the Constant False Alarm Rate (CFAR) algorithm, we perform target
detection in land clutter. The overall verification of the analysis is
performed by exploiting the Moving and Stationary Target Acquisition and
Recognition (MSTAR) data-set, which has been collected in spotlight mode at
X-band, and the results are presented.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2409.02155</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Snuffy: Efficient Whole Slide Image Classifier 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08258v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08258v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hossein Jafarinia, Alireza Alipanah, Danial Hamdi, Saeed Razavi, Nahal Mirzaie, Mohammad Hossein Rohban
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whole Slide Image (WSI) classification with multiple instance learning (MIL)
in digital pathology faces significant computational challenges. Current
methods mostly rely on extensive self-supervised learning (SSL) for
satisfactory performance, requiring long training periods and considerable
computational resources. At the same time, no pre-training affects performance
due to domain shifts from natural images to WSIs. We introduce Snuffy
architecture, a novel MIL-pooling method based on sparse transformers that
mitigates performance loss with limited pre-training and enables continual
few-shot pre-training as a competitive option. Our sparsity pattern is tailored
for pathology and is theoretically proven to be a universal approximator with
the tightest probabilistic sharp bound on the number of layers for sparse
transformers, to date. We demonstrate Snuffy's effectiveness on CAMELYON16 and
TCGA Lung cancer datasets, achieving superior WSI and patch-level accuracies.
The code is available on https://github.com/jafarinia/snuffy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for ECCV 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DynRefer: Delving into Region-level <span class="highlight-title">Multi</span>modal Tasks via Dynamic
  Resolution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.16071v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.16071v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuzhong Zhao, Feng Liu, Yue Liu, Mingxiang Liao, Chen Gong, Qixiang Ye, Fang Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One fundamental task of multimodal models is to translate referred image
regions to human preferred language descriptions. Existing methods, however,
ignore the resolution adaptability needs of different tasks, which hinders them
to find out precise language descriptions. In this study, we propose a DynRefer
approach, to pursue high-accuracy region-level referring through mimicking the
resolution adaptability of human visual cognition. During training, DynRefer
stochastically aligns language descriptions of multimodal tasks with images of
multiple resolutions, which are constructed by nesting a set of random views
around the referred region. During inference, DynRefer performs selectively
multimodal referring by sampling proper region representations for tasks from
the nested views based on image and task priors. This allows the visual
information for referring to better match human preferences, thereby improving
the representational adaptability of region-level multimodal models.
Experiments show that DynRefer brings mutual improvement upon broad tasks
including region-level captioning, open-vocabulary region recognition and
attribute detection. Furthermore, DynRefer achieves state-of-the-art results on
multiple region-level multimodal tasks using a single model. Code is available
at https://github.com/callsys/DynRefer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in CVPR 2025. Code is available at
  https://github.com/callsys/DynRefer</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Finite-State Machines for Surgical Phase Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18018v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18018v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Ding, Zhongpai Gao, Benjamin Planche, Tianyu Luan, Abhishek Sharma, Meng Zheng, Ange Lou, Terrence Chen, Mathias Unberath, Ziyan Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgical phase recognition (SPR) is crucial for applications in workflow
optimization, performance evaluation, and real-time intervention guidance.
However, current deep learning models often struggle with fragmented
predictions, failing to capture the sequential nature of surgical workflows. We
propose the Neural Finite-State Machine (NFSM), a novel approach that enforces
temporal coherence by integrating classical state-transition priors with modern
neural networks. NFSM leverages learnable global state embeddings as unique
phase identifiers and dynamic transition tables to model phase-to-phase
progressions. Additionally, a future phase forecasting mechanism employs
repeated frame padding to anticipate upcoming transitions. Implemented as a
plug-and-play module, NFSM can be integrated into existing SPR pipelines
without changing their core architectures. We demonstrate state-of-the-art
performance across multiple benchmarks, including a significant improvement on
the BernBypass70 dataset - raising video-level accuracy by 0.9 points and
phase-level precision, recall, F1-score, and mAP by 3.8, 3.1, 3.3, and 4.1,
respectively. Ablation studies confirm each component's effectiveness and the
module's adaptability to various architectures. By unifying finite-state
principles with deep learning, NFSM offers a robust path toward consistent,
long-term surgical video analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Refinement Module based on Parse Graph of Feature Map for Human Pose
  Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.11069v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.11069v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shibang Liu, Xuemei Xie, Guangming Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parse graphs of the human body can be obtained in the human brain to help
humans complete the human Pose Estimation better (HPE). It contains a
hierarchical structure, like a tree structure, and context relations among
nodes. To equip models with such capabilities, many researchers predefine the
parse graph of body structure to design HPE frameworks. However, these
frameworks struggle to adapt to instances that deviate from the predefined
parse graph and are often parameter-heavy. Unlike them, we view the feature map
holistically, much like the human body. It can be optimized using parse graphs,
where each node's feature is an implicit expression rather than a fixed one.
This allows it to adapt to more instances, unconstrained by rigid structural
features. In this paper, we design the Refinement Module based on the Parse
Graph of feature map (RMPG), which includes two stages: top-down decomposition
and bottom-up combination. In the first stage, the feature map is decomposed
into multiple sub-feature maps along the channel. In the second stage, the
context relations of sub-feature maps are calculated to obtain their respective
context information and the sub-feature maps with context information are
concatenated along channels to obtain the refined feature map. Additionally, we
design a hierarchical network with fewer parameters using multiple RMPG modules
to model the context relations and hierarchies in the parse graph of body
structure for HPE, some of which are supervised to obtain context relations
among body parts. Our network achieves excellent results on multiple mainstream
human pose datasets. More importantly, the effectiveness of RMPG is proven on
different methods. The code of RMPG will be open.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Robust Algorithms for Surgical Phase Recognition via Digital
  Twin Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20026v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20026v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Ding, Yuqian Zhang, Wenzheng Cheng, Xinyu Wang, Xu Lian, Chenhao Yu, Hongchao Shu, Ji Woong Kim, Axel Krieger, Mathias Unberath
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Surgical phase recognition (SPR) is an integral component of surgical data
science, enabling high-level surgical analysis. End-to-end trained neural
networks that predict surgical phase directly from videos have shown excellent
performance on benchmarks. However, these models struggle with robustness due
to non-causal associations in the training set. Our goal is to improve model
robustness to variations in the surgical videos by leveraging the digital twin
(DT) paradigm -- an intermediary layer to separate high-level analysis (SPR)
from low-level processing. As a proof of concept, we present a DT
representation-based framework for SPR from videos. The framework employs
vision foundation models with reliable low-level scene understanding to craft
DT representation. We embed the DT representation in place of raw video inputs
in the state-of-the-art SPR model. The framework is trained on the Cholec80
dataset and evaluated on out-of-distribution (OOD) and corrupted test samples.
Contrary to the vulnerability of the baseline model, our framework demonstrates
strong robustness on both OOD and corrupted samples, with a video-level
accuracy of 80.3 on a highly corrupted Cholec80 test set, 67.9 on the
challenging CRCD dataset, and 99.8 on an internal robotic surgery dataset,
outperforming the baseline by 3.9, 16.8, and 90.9 respectively. We also find
that using DT representation as an augmentation to the raw input can
significantly improve model robustness. Our findings lend support to the thesis
that DT representations are effective in enhancing model robustness. Future
work will seek to improve the feature informativeness and incorporate
interpretability for a more comprehensive framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SoK: Systematization and Benchmarking of Deepfake Detectors in a Unified
  Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.04364v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.04364v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Binh M. Le, Jiwon Kim, Simon S. Woo, Kristen Moore, Alsharif Abuadbba, Shahroz Tariq
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deepfakes have rapidly emerged as a serious threat to society due to their
ease of creation and dissemination, triggering the accelerated development of
detection technologies. However, many existing detectors rely on labgenerated
datasets for validation, which may not prepare them for novel, real-world
deepfakes. This paper extensively reviews and analyzes state-of-the-art
deepfake detectors, evaluating them against several critical criteria. These
criteria categorize detectors into 4 high-level groups and 13 finegrained
sub-groups, aligned with a unified conceptual framework we propose. This
classification offers practical insights into the factors affecting detector
efficacy. We evaluate the generalizability of 16 leading detectors across
comprehensive attack scenarios, including black-box, white-box, and graybox
settings. Our systematized analysis and experiments provide a deeper
understanding of deepfake detectors and their generalizability, paving the way
for future research and the development of more proactive defenses against
deepfakes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 6 figures, 7 table, Accepted at IEEE European Symposium on
  security and privacy 2025 (EuroS&P '25)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Image Watermarks are Removable Using Controllable Regeneration from
  Clean Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.05470v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.05470v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yepeng Liu, Yiren Song, Hai Ci, Yu Zhang, Haofan Wang, Mike Zheng Shou, Yuheng Bu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image watermark techniques provide an effective way to assert ownership,
deter misuse, and trace content sources, which has become increasingly
essential in the era of large generative models. A critical attribute of
watermark techniques is their robustness against various manipulations. In this
paper, we introduce a watermark removal approach capable of effectively
nullifying state-of-the-art watermarking techniques. Our primary insight
involves regenerating the watermarked image starting from a clean Gaussian
noise via a controllable diffusion model, utilizing the extracted semantic and
spatial features from the watermarked image. The semantic control adapter and
the spatial control network are specifically trained to control the denoising
process towards ensuring image quality and enhancing consistency between the
cleaned image and the original watermarked image. To achieve a smooth trade-off
between watermark removal performance and image consistency, we further propose
an adjustable and controllable regeneration scheme. This scheme adds varying
numbers of noise steps to the latent representation of the watermarked image,
followed by a controlled denoising process starting from this noisy latent
representation. As the number of noise steps increases, the latent
representation progressively approaches clean Gaussian noise, facilitating the
desired trade-off. We apply our watermark removal methods across various
watermarking techniques, and the results demonstrate that our methods offer
superior visual consistency/quality and enhanced watermark removal performance
compared to existing regeneration approaches. Our code is available at
https://github.com/yepengliu/CtrlRegen.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Neural Networks for Intelligent Data-Driven Development 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10603v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10603v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youssef Shoeb, Azarm Nowzad, Hanno Gottschalk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in machine learning methods for computer vision tasks have led to
their consideration for safety-critical applications like autonomous driving.
However, effectively integrating these methods into the automotive development
lifecycle remains challenging. Since the performance of machine learning
algorithms relies heavily on the training data provided, the data and model
development lifecycle play a key role in successfully integrating these
components into the product development lifecycle. Existing models frequently
encounter difficulties recognizing or adapting to novel instances not present
in the original training dataset. This poses a significant risk for reliable
deployment in dynamic environments. To address this challenge, we propose an
adaptive neural network architecture and an iterative development framework
that enables users to efficiently incorporate previously unknown objects into
the current perception system. Our approach builds on continuous learning,
emphasizing the necessity of dynamic updates to reflect real-world deployment
conditions. Specifically, we introduce a pipeline with three key components:
(1) a scalable network extension strategy to integrate new classes while
preserving existing performance, (2) a dynamic OoD detection component that
requires no additional retraining for newly added classes, and (3) a
retrieval-based data augmentation process tailored for safety-critical
deployments. The integration of these components establishes a pragmatic and
adaptive pipeline for the continuous evolution of perception systems in the
context of autonomous driving.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 3 figures, and 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Segment-Level Road Obstacle Detection Using Visual Foundation Model
  Priors and Likelihood Ratios 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.05707v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.05707v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youssef Shoeb, Nazir Nayal, Azarm Nowzad, Fatma Güney, Hanno Gottschalk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting road obstacles is essential for autonomous vehicles to navigate
dynamic and complex traffic environments safely. Current road obstacle
detection methods typically assign a score to each pixel and apply a threshold
to generate final predictions. However, selecting an appropriate threshold is
challenging, and the per-pixel classification approach often leads to
fragmented predictions with numerous false positives. In this work, we propose
a novel method that leverages segment-level features from visual foundation
models and likelihood ratios to predict road obstacles directly. By focusing on
segments rather than individual pixels, our approach enhances detection
accuracy, reduces false positives, and offers increased robustness to scene
variability. We benchmark our approach against existing methods on the
RoadObstacle and LostAndFound datasets, achieving state-of-the-art performance
without needing a predefined threshold.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 4 figures, and 1 table, to be published in VISAPP 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VisRAG: Vision-based Retrieval-augmented Generation on <span class="highlight-title">Multi</span>-modality
  Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10594v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10594v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) is an effective technique that enables
large language models (LLMs) to utilize external knowledge sources for
generation. However, current RAG systems are solely based on text, rendering it
impossible to utilize vision information like layout and images that play
crucial roles in real-world multi-modality documents. In this paper, we
introduce VisRAG, which tackles this issue by establishing a vision-language
model (VLM)-based RAG pipeline. In this pipeline, instead of first parsing the
document to obtain text, the document is directly embedded using a VLM as an
image and then retrieved to enhance the generation of a VLM. Compared to
traditional text-based RAG, VisRAG maximizes the retention and utilization of
the data information in the original documents, eliminating the information
loss introduced during the parsing process. We collect both open-source and
synthetic data to train the retriever in VisRAG and explore a variety of
generation methods. Experiments demonstrate that VisRAG outperforms traditional
RAG in both the retrieval and generation stages, achieving a 20--40% end-to-end
performance gain over traditional text-based RAG pipeline. Further analysis
reveals that VisRAG is efficient in utilizing training data and demonstrates
strong generalization capability, positioning it as a promising solution for
RAG on multi-modality documents. Our code and data are available at
https://github.com/openbmb/visrag.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Score Forgetting Distillation: A Swift, Data-Free Method for Machine
  Unlearning in Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.11219v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.11219v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianqi Chen, Shujian Zhang, Mingyuan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The machine learning community is increasingly recognizing the importance of
fostering trust and safety in modern generative AI (GenAI) models. We posit
machine unlearning (MU) as a crucial foundation for developing safe, secure,
and trustworthy GenAI models. Traditional MU methods often rely on stringent
assumptions and require access to real data. This paper introduces Score
Forgetting Distillation (SFD), an innovative MU approach that promotes the
forgetting of undesirable information in diffusion models by aligning the
conditional scores of "unsafe" classes or concepts with those of "safe" ones.
To eliminate the need for real data, our SFD framework incorporates a
score-based MU loss into the score distillation objective of a pretrained
diffusion model. This serves as a regularization term that preserves desired
generation capabilities while enabling the production of synthetic data through
a one-step generator. Our experiments on pretrained label-conditional and
text-to-image diffusion models demonstrate that our method effectively
accelerates the forgetting of target classes or concepts during generation,
while preserving the quality of other classes or concepts. This unlearned and
distilled diffusion not only pioneers a novel concept in MU but also
accelerates the generation speed of diffusion models. Our experiments and
studies on a range of diffusion models and datasets confirm that our approach
is generalizable, effective, and advantageous for MU in diffusion models. Code
is available at https://github.com/tqch/score-forgetting-distillation.
($\textbf{Warning:}$ This paper contains sexually explicit imagery, discussions
of pornography, racially-charged terminology, and other content that some
readers may find disturbing, distressing, and/or offensive.)
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GP-GS: Gaussian Processes for Enhanced Gaussian Splatting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.02283v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.02283v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihao Guo, Jingxuan Su, Shenglin Wang, Jinlong Fan, Jing Zhang, Liangxiu Han, Peng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  3D Gaussian Splatting has emerged as an efficient photorealistic novel view
synthesis method. However, its reliance on sparse Structure-from-Motion (SfM)
point clouds consistently compromises the scene reconstruction quality. To
address these limitations, this paper proposes a novel 3D reconstruction
framework Gaussian Processes Gaussian Splatting (GP-GS), where a multi-output
Gaussian Process model is developed to achieve adaptive and uncertainty-guided
densification of sparse SfM point clouds. Specifically, we propose a dynamic
sampling and filtering pipeline that adaptively expands the SfM point clouds by
leveraging GP-based predictions to infer new candidate points from the input 2D
pixels and depth maps. The pipeline utilizes uncertainty estimates to guide the
pruning of high-variance predictions, ensuring geometric consistency and
enabling the generation of dense point clouds. The densified point clouds
provide high-quality initial 3D Gaussians to enhance reconstruction
performance. Extensive experiments conducted on synthetic and real-world
datasets across various scales validate the effectiveness and practicality of
the proposed framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages,11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ All Seeds Are Not Equal: Enhancing Compositional Text-to-Image
  Generation with Reliable Random Seeds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.18810v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.18810v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuangqi Li, Hieu Le, Jingyi Xu, Mathieu Salzmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image diffusion models have demonstrated remarkable capability in
generating realistic images from arbitrary text prompts. However, they often
produce inconsistent results for compositional prompts such as "two dogs" or "a
penguin on the right of a bowl". Understanding these inconsistencies is crucial
for reliable image generation. In this paper, we highlight the significant role
of initial noise in these inconsistencies, where certain noise patterns are
more reliable for compositional prompts than others. Our analyses reveal that
different initial random seeds tend to guide the model to place objects in
distinct image areas, potentially adhering to specific patterns of camera
angles and image composition associated with the seed. To improve the model's
compositional ability, we propose a method for mining these reliable cases,
resulting in a curated training set of generated images without requiring any
manual annotation. By fine-tuning text-to-image models on these generated
images, we significantly enhance their compositional capabilities. For
numerical composition, we observe relative increases of 29.3% and 19.5% for
Stable Diffusion and PixArt-{\alpha}, respectively. Spatial composition sees
even larger gains, with 60.7% for Stable Diffusion and 21.1% for
PixArt-{\alpha}.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">1</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VisRAG: Vision-based Retrieval-augmented Generation on <span class="highlight-title">Multi</span>-modality
  Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10594v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10594v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shi Yu, Chaoyue Tang, Bokai Xu, Junbo Cui, Junhao Ran, Yukun Yan, Zhenghao Liu, Shuo Wang, Xu Han, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) is an effective technique that enables
large language models (LLMs) to utilize external knowledge sources for
generation. However, current RAG systems are solely based on text, rendering it
impossible to utilize vision information like layout and images that play
crucial roles in real-world multi-modality documents. In this paper, we
introduce VisRAG, which tackles this issue by establishing a vision-language
model (VLM)-based RAG pipeline. In this pipeline, instead of first parsing the
document to obtain text, the document is directly embedded using a VLM as an
image and then retrieved to enhance the generation of a VLM. Compared to
traditional text-based RAG, VisRAG maximizes the retention and utilization of
the data information in the original documents, eliminating the information
loss introduced during the parsing process. We collect both open-source and
synthetic data to train the retriever in VisRAG and explore a variety of
generation methods. Experiments demonstrate that VisRAG outperforms traditional
RAG in both the retrieval and generation stages, achieving a 20--40% end-to-end
performance gain over traditional text-based RAG pipeline. Further analysis
reveals that VisRAG is efficient in utilizing training data and demonstrates
strong generalization capability, positioning it as a promising solution for
RAG on multi-modality documents. Our code and data are available at
https://github.com/openbmb/visrag.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">45</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Eagle: Exploring The Design Space for <span class="highlight-title">Multi</span>modal <span class="highlight-title">LLM</span>s with Mixture of
  Encoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15998v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15998v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, Yilin Zhao, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, Bryan Catanzaro, Andrew Tao, Jan Kautz, Zhiding Yu, Guilin Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to accurately interpret complex visual information is a crucial
topic of multimodal large language models (MLLMs). Recent work indicates that
enhanced visual perception significantly reduces hallucinations and improves
performance on resolution-sensitive tasks, such as optical character
recognition and document analysis. A number of recent MLLMs achieve this goal
using a mixture of vision encoders. Despite their success, there is a lack of
systematic comparisons and detailed ablation studies addressing critical
aspects, such as expert selection and the integration of multiple vision
experts. This study provides an extensive exploration of the design space for
MLLMs using a mixture of vision encoders and resolutions. Our findings reveal
several underlying principles common to various existing strategies, leading to
a streamlined yet effective design approach. We discover that simply
concatenating visual tokens from a set of complementary vision encoders is as
effective as more complex mixing architectures or strategies. We additionally
introduce Pre-Alignment to bridge the gap between vision-focused encoders and
language tokens, enhancing model coherence. The resulting family of MLLMs,
Eagle, surpasses other leading open-source models on major MLLM benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Github: https://github.com/NVlabs/Eagle, HuggingFace:
  https://huggingface.co/NVEagle</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SymbolFit: Automatic Parametric Modeling with Symbolic Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.09851v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.09851v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ho Fung Tsoi, Dylan Rankin, Cecile Caillol, Miles Cranmer, Sridhara Dasu, Javier Duarte, Philip Harris, Elliot Lipeles, Vladimir Loncar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce SymbolFit, a framework that automates parametric modeling by
using symbolic regression to perform a machine-search for functions that fit
the data while simultaneously providing uncertainty estimates in a single run.
Traditionally, constructing a parametric model to accurately describe binned
data has been a manual and iterative process, requiring an adequate functional
form to be determined before the fit can be performed. The main challenge
arises when the appropriate functional forms cannot be derived from first
principles, especially when there is no underlying true closed-form function
for the distribution. In this work, we develop a framework that automates and
streamlines the process by utilizing symbolic regression, a machine learning
technique that explores a vast space of candidate functions without requiring a
predefined functional form because the functional form itself is treated as a
trainable parameter, making the process far more efficient and effortless than
traditional regression methods. We demonstrate the framework in high-energy
physics experiments at the CERN Large Hadron Collider (LHC) using five real
proton-proton collision datasets from new physics searches, including
background modeling in resonance searches for high-mass dijet, trijet,
paired-dijet, diphoton, and dimuon events. We show that our framework can
flexibly and efficiently generate a wide range of candidate functions that fit
a nontrivial distribution well using a simple fit configuration that varies
only by random seed, and that the same fit configuration, which defines a vast
function space, can also be applied to distributions of different shapes,
whereas achieving a comparable result with traditional methods would have
required extensive manual effort.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>50 pages, 35 figures. Under review. The API can be used
  out-of-the-box and is available at https://github.com/hftsoi/symbolfit</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantum time dynamics mediated by the Yang-Baxter equation and
  artificial neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.17116v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.17116v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahil Gulania, Yuri Alexeev, Stephen K. Gray, Bo Peng, Niranjan Govind
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum computing shows great potential, but errors pose a significant
challenge. This study explores new strategies for mitigating quantum errors
using artificial neural networks (ANN) and the Yang-Baxter equation (YBE).
Unlike traditional error mitigation methods, which are computationally
intensive, we investigate artificial error mitigation. We developed a novel
method that combines ANN for noise mitigation combined with the YBE to generate
noisy data. This approach effectively reduces noise in quantum simulations,
enhancing the accuracy of the results. The YBE rigorously preserves quantum
correlations and symmetries in spin chain simulations in certain classes of
integrable lattice models, enabling effective compression of quantum circuits
while retaining linear scalability with the number of qubits. This compression
facilitates both full and partial implementations, allowing the generation of
noisy quantum data on hardware alongside noiseless simulations using classical
platforms. By introducing controlled noise through the YBE, we enhance the
dataset for error mitigation. We train an ANN model on partial data from
quantum simulations, demonstrating its effectiveness in mitigating errors in
time-evolving quantum states, providing a scalable framework to enhance quantum
computation fidelity, particularly in noisy intermediate-scale quantum (NISQ)
systems. We demonstrate the efficacy of this approach by performing quantum
time dynamics simulations using the Heisenberg XY Hamiltonian on real quantum
devices.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Heterogeneous Graph Neural Network on Semantic Tree <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.13496v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.13496v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingyu Guan, Jack W. Stokes, Qinlong Luo, Fuchen Liu, Purvanshi Mehta, Elnaz Nouri, Taesoo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recent past has seen an increasing interest in Heterogeneous Graph Neural
Networks (HGNNs), since many real-world graphs are heterogeneous in nature,
from citation graphs to email graphs. However, existing methods ignore a tree
hierarchy among metapaths, naturally constituted by different node types and
relation types. In this paper, we present HetTree, a novel HGNN that models
both the graph structure and heterogeneous aspects in a scalable and effective
manner. Specifically, HetTree builds a semantic tree data structure to capture
the hierarchy among metapaths. To effectively encode the semantic tree, HetTree
uses a novel subtree attention mechanism to emphasize metapaths that are more
helpful in encoding parent-child relationships. Moreover, HetTree proposes
carefully matching pre-computed features and labels correspondingly,
constituting a complete metapath representation. Our evaluation of HetTree on a
variety of real-world datasets demonstrates that it outperforms all existing
baselines on open benchmarks and efficiently scales to large real-world graphs
with millions of nodes and edges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Disentangling Representations through <span class="highlight-title">Multi</span>-task Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11249v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11249v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pantelis Vafidis, Aman Bhargava, Antonio Rangel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intelligent perception and interaction with the world hinges on internal
representations that capture its underlying structure (''disentangled'' or
''abstract'' representations). Disentangled representations serve as world
models, isolating latent factors of variation in the world along approximately
orthogonal directions, thus facilitating feature-based generalization. We
provide experimental and theoretical results guaranteeing the emergence of
disentangled representations in agents that optimally solve multi-task evidence
accumulation classification tasks, canonical in the neuroscience literature.
The key conceptual finding is that, by producing accurate multi-task
classification estimates, a system implicitly represents a set of coordinates
specifying a disentangled representation of the underlying latent state of the
data it receives. The theory provides conditions for the emergence of these
representations in terms of noise, number of tasks, and evidence accumulation
time. We experimentally validate these predictions in RNNs trained to
multi-task, which learn disentangled representations in the form of continuous
attractors, leading to zero-shot out-of-distribution (OOD) generalization in
predicting latent factors. We demonstrate the robustness of our framework
across autoregressive architectures, decision boundary geometries and in tasks
requiring classification confidence estimation. We find that transformers are
particularly suited for disentangling representations, which might explain
their unique world understanding abilities. Overall, our framework establishes
a formal link between competence at multiple tasks and the formation of
disentangled, interpretable world models in both biological and artificial
systems, and helps explain why ANNs often arrive at human-interpretable
concepts, and how they both may acquire exceptional zero-shot generalization
capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ We Have a Package for You! A Comprehensive Analysis of Package
  Hallucinations by Code Generating <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.10279v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.10279v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Spracklen, Raveen Wijewickrama, A H M Nazmus Sakib, Anindya Maiti, Bimal Viswanath, Murtuza Jadliwala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reliance of popular programming languages such as Python and JavaScript
on centralized package repositories and open-source software, combined with the
emergence of code-generating Large Language Models (LLMs), has created a new
type of threat to the software supply chain: package hallucinations. These
hallucinations, which arise from fact-conflicting errors when generating code
using LLMs, represent a novel form of package confusion attack that poses a
critical threat to the integrity of the software supply chain. This paper
conducts a rigorous and comprehensive evaluation of package hallucinations
across different programming languages, settings, and parameters, exploring how
a diverse set of models and configurations affect the likelihood of generating
erroneous package recommendations and identifying the root causes of this
phenomenon. Using 16 popular LLMs for code generation and two unique prompt
datasets, we generate 576,000 code samples in two programming languages that we
analyze for package hallucinations. Our findings reveal that that the average
percentage of hallucinated packages is at least 5.2% for commercial models and
21.7% for open-source models, including a staggering 205,474 unique examples of
hallucinated package names, further underscoring the severity and pervasiveness
of this threat. To overcome this problem, we implement several hallucination
mitigation strategies and show that they are able to significantly reduce the
number of package hallucinations while maintaining code quality. Our
experiments and findings highlight package hallucinations as a persistent and
systemic phenomenon while using state-of-the-art LLMs for code generation, and
a significant challenge which deserves the research community's urgent
attention.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the 2025 USENIX Security Symposium. 22 pages, 14
  figures, 8 tables. Edited from original version for submission to a different
  conference. No change to original results or findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Range, not Independence, Drives Modularity in Biologically Inspired
  Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06232v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06232v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Will Dorrell, Kyle Hsu, Luke Hollingsworth, Jin Hwa Lee, Jiajun Wu, Chelsea Finn, Peter E Latham, Tim EJ Behrens, James CR Whittington
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Why do biological and artificial neurons sometimes modularise, each encoding
a single meaningful variable, and sometimes entangle their representation of
many variables? In this work, we develop a theory of when biologically inspired
networks -- those that are nonnegative and energy efficient -- modularise their
representation of source variables (sources). We derive necessary and
sufficient conditions on a sample of sources that determine whether the neurons
in an optimal biologically-inspired linear autoencoder modularise. Our theory
applies to any dataset, extending far beyond the case of statistical
independence studied in previous work. Rather we show that sources modularise
if their support is ``sufficiently spread''. From this theory, we extract and
validate predictions in a variety of empirical studies on how data distribution
affects modularisation in nonlinear feedforward and recurrent neural networks
trained on supervised and unsupervised tasks. Furthermore, we apply these ideas
to neuroscience data, showing that range independence can be used to understand
the mixing or modularising of spatial and reward information in entorhinal
recordings in seemingly conflicting experiments. Further, we use these results
to suggest alternate origins of mixed-selectivity, beyond the predominant
theory of flexible nonlinear classification. In sum, our theory prescribes
precise conditions on when neural activities modularise, providing tools for
inducing and elucidating modular representations in brains and machines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>47 pages, 17 figures. WD and KH contributed equally; LH and JHL
  contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MetaGFN: Exploring Distant Modes with Adapted Metadynamics for
  Continuous GFlowNets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.15905v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.15905v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominic Phillips, Flaviu Cipcigan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative Flow Networks (GFlowNets) are a class of generative models that
sample objects in proportion to a specified reward function through a learned
policy. They can be trained either on-policy or off-policy, needing a balance
between exploration and exploitation for fast convergence to a target
distribution. While exploration strategies for discrete GFlowNets have been
studied, exploration in the continuous case remains to be investigated, despite
the potential for novel exploration algorithms due to the local connectedness
of continuous domains. Here, we introduce Adapted Metadynamics, a variant of
metadynamics that can be applied to arbitrary black-box reward functions on
continuous domains. We use Adapted Metadynamics as an exploration strategy for
continuous GFlowNets. We show several continuous domains where the resulting
algorithm, MetaGFN, accelerates convergence to the target distribution and
discovers more distant reward modes than previous off-policy exploration
strategies used for GFlowNets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linear Diffusion Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.12381v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.12381v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Fein-Ashley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion kernels capture global dependencies. We present Linear Diffusion
Networks (LDNs), a novel architecture that reinterprets sequential data
processing as a unified diffusion process. Our model integrates adaptive
diffusion modules with localized nonlinear updates and a diffusion-inspired
attention mechanism. This design enables efficient global information
propagation while preserving fine-grained temporal details. LDN overcomes the
limitations of conventional recurrent and transformer models by allowing full
parallelization across time steps and supporting robust multi-scale temporal
representations. Experiments on benchmark sequence modeling tasks demonstrate
that LDN delivers competitive performance across ImageNet and GLUE tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ $μ$nit Scaling: Simple and Scalable FP8 <span class="highlight-title">LLM</span> Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.05967v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.05967v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saaketh Narayan, Abhay Gupta, Mansheej Paul, Davis Blalock
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Model training with 8-bit floating point (FP8) formats
promises significant efficiency improvements, but reduced numerical precision
makes training challenging. It is currently possible to train in FP8 only if
one is willing to tune various hyperparameters, reduce model scale, or accept
the overhead of computing dynamic scale factors. We demonstrate simple,
scalable FP8 training that requires no dynamic scaling factors or special
hyperparameters, even at large model sizes. Our method, $\mu$nit Scaling
($\mu$S), also enables simple hyperparameter transfer across model widths,
matched numerics across training and inference, and other desirable properties.
$\mu$nit Scaling is straightforward to implement, consisting of a set of
minimal interventions based on a first-principles analysis of common
transformer operations. We validate our method by training models from 1B to
13B parameters, performing all hidden linear layer computations in FP8. We
achieve quality equal to higher precision baselines while also training up to
33% faster.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lean Copilot: Large Language Models as Copilots for Theorem Proving in
  Lean 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.12534v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.12534v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peiyang Song, Kaiyu Yang, Anima Anandkumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural theorem proving combines large language models (LLMs) with proof
assistants such as Lean, where the correctness of formal proofs can be
rigorously verified, leaving no room for hallucination. With existing neural
theorem provers pretrained on a fixed collection of data and offering valuable
suggestions at times, it is challenging for them to continually prove novel
theorems in a fully autonomous mode, where human insights may be critical. In
this paper, we explore LLMs as copilots that assist humans in proving theorems.
We introduce Lean Copilot, an general framework for running LLM inference
natively in Lean. It enables programmers to build various LLM-based proof
automation tools that integrate seamlessly into the workflow of Lean users.
Lean users can use our pretrained models or bring their own ones that run
either locally (with or without GPUs) or on the cloud. Using Lean Copilot, we
build LLM-based tools that suggest proof steps, complete proof goals, and
select relevant premises. Experimental results on the Mathematics in Lean
textbook demonstrate the effectiveness of our method compared to existing
rule-based proof automation in Lean (aesop). When assisting humans, Lean
Copilot requires only 2.08 manually-entered proof steps on average (3.86
required by aesop); when automating the theorem proving process, Lean Copilot
automates 74.2% proof steps on average, 85% better than aesop (40.1%). We open
source all code and artifacts under a permissive MIT license to facilitate
further research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>All code and artifacts open-sourced at
  https://github.com/lean-dojo/LeanCopilot</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Two-Time-Scale Stochastic Gradient Method with Applications in
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.09660v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.09660v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sihan Zeng, Thinh T. Doan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two-time-scale optimization is a framework introduced in Zeng et al. (2024)
that abstracts a range of policy evaluation and policy optimization problems in
reinforcement learning (RL). Akin to bi-level optimization under a particular
type of stochastic oracle, the two-time-scale optimization framework has an
upper level objective whose gradient evaluation depends on the solution of a
lower level problem, which is to find the root of a strongly monotone operator.
In this work, we propose a new method for solving two-time-scale optimization
that achieves significantly faster convergence than the prior arts. The key
idea of our approach is to leverage an averaging step to improve the estimates
of the operators in both lower and upper levels before using them to update the
decision variables. These additional averaging steps eliminate the direct
coupling between the main variables, enabling the accelerated performance of
our algorithm. We characterize the finite-time convergence rates of the
proposed algorithm under various conditions of the underlying objective
function, including strong convexity, Polyak-Lojasiewicz condition, and general
non-convexity. These rates significantly improve over the best-known complexity
of the standard two-time-scale stochastic approximation algorithm. When applied
to RL, we show how the proposed algorithm specializes to novel online
sample-based methods that surpass or match the performance of the existing
state of the art. Finally, we support our theoretical results with numerical
simulations in RL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LDAdam: Adaptive Optimization from Low-Dimensional Gradient Statistics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16103v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16103v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Robert, Mher Safaryan, Ionut-Vlad Modoranu, Dan Alistarh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce LDAdam, a memory-efficient optimizer for training large models,
that performs adaptive optimization steps within lower dimensional subspaces,
while consistently exploring the full parameter space during training. This
strategy keeps the optimizer's memory footprint to a fraction of the model
size. LDAdam relies on a new projection-aware update rule for the optimizer
states that allows for transitioning between subspaces, i.e., estimation of the
statistics of the projected gradients. To mitigate the errors due to low-rank
projection, LDAdam integrates a new generalized error feedback mechanism, which
explicitly accounts for both gradient and optimizer state compression. We prove
the convergence of LDAdam under standard assumptions, and show that LDAdam
allows for accurate and efficient fine-tuning and pre-training of language
models. Code is available at https://github.com/IST-DASLab/LDAdam
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fréchet Wavelet Distance: A Domain-Agnostic Metric for Image
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.15289v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.15289v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lokesh Veeramacheneni, Moritz Wolter, Hildegard Kuehne, Juergen Gall
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern metrics for generative learning like Fr\'echet Inception Distance
(FID) and DINOv2-Fr\'echet Distance (FD-DINOv2) demonstrate impressive
performance. However, they suffer from various shortcomings, like a bias
towards specific generators and datasets. To address this problem, we propose
the Fr\'echet Wavelet Distance (FWD) as a domain-agnostic metric based on the
Wavelet Packet Transform ($W_p$). FWD provides a sight across a broad spectrum
of frequencies in images with a high resolution, preserving both spatial and
textural aspects. Specifically, we use $W_p$ to project generated and real
images to the packet coefficient space. We then compute the Fr\'echet distance
with the resultant coefficients to evaluate the quality of a generator. This
metric is general-purpose and dataset-domain agnostic, as it does not rely on
any pre-trained network, while being more interpretable due to its ability to
compute Fr\'echet distance per packet, enhancing transparency. We conclude with
an extensive evaluation of a wide variety of generators across various datasets
that the proposed FWD can generalize and improve robustness to domain shifts
and various corruptions compared to other metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kolmogorov-Arnold PointNet: Deep learning for prediction of fluid fields
  on irregular geometries 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.02950v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.02950v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Kashefi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Kolmogorov-Arnold Networks (KANs) have emerged as a promising alternative to
traditional Multilayer Perceptrons (MLPs) in deep learning. KANs have already
been integrated into various architectures, such as convolutional neural
networks, graph neural networks, and transformers, and their potential has been
assessed for predicting physical quantities. However, the combination of KANs
with point-cloud-based neural networks (e.g., PointNet) for computational
physics has not yet been explored. To address this, we present
Kolmogorov-Arnold PointNet (KA-PointNet) as a novel supervised deep learning
framework for the prediction of incompressible steady-state fluid flow fields
in irregular domains, where the predicted fields are a function of the geometry
of the domains. In KA-PointNet, we implement shared KANs in the segmentation
branch of the PointNet architecture. We utilize Jacobi polynomials to construct
shared KANs. As a benchmark test case, we consider incompressible laminar
steady-state flow over a cylinder, where the geometry of its cross-section
varies over the data set. We investigate the performance of Jacobi polynomials
with different degrees as well as special cases of Jacobi polynomials such as
Legendre polynomials, Chebyshev polynomials of the first and second kinds, and
Gegenbauer polynomials, in terms of the computational cost of training and
accuracy of prediction of the test set. Additionally, we compare the
performance of PointNet with shared KANs (i.e., KA-PointNet) and PointNet with
shared MLPs. It is observed that when the number of trainable parameters is
approximately equal, PointNet with shared KANs (i.e., KA-PointNet) outperforms
PointNet with shared MLPs. Moreover, KA-PointNet predicts the pressure and
velocity distributions along the surface of cylinders more accurately,
resulting in more precise computations of lift and drag.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Distributed</span> Speculative Inference (DSI): Speculation Parallelism for
  Provably Faster Lossless Language Model Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.14105v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.14105v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nadav Timor, Jonathan Mamou, Daniel Korat, Moshe Berchansky, Oren Pereg, Moshe Wasserblat, Tomer Galanti, Michal Gordon, David Harel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces distributed speculative inference (DSI), a novel
inference algorithm that is provably faster than speculative inference (SI)
[leviathan2023, chen2023, miao2024, sun2025, timor2025] and standard
autoregressive inference (non-SI). Like other SI algorithms, DSI operates on
frozen language models (LMs), requiring no training or architectural
modifications, and it preserves the target distribution. Prior studies on SI
have demonstrated empirical speedups over non-SI--but rely on sufficiently fast
and accurate drafters, which are often unavailable in practice. We identify a
gap where SI can be slower than non-SI if drafters are too slow or inaccurate.
We close this gap by proving that DSI is faster than both SI and non-SI--given
any drafters. DSI is therefore not only faster than SI, but also unlocks the
acceleration of LMs for which SI fails. DSI leverages speculation parallelism
(SP), a novel type of task parallelism, to orchestrate target and drafter
instances that overlap in time, establishing a new foundational tradeoff
between computational resources and latency. Our simulations show that DSI is
1.29-1.92x faster than SI in single-node setups for various off-the-shelf LMs
and tasks. We open-source all our code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2025. (Link:
  https://openreview.net/forum?id=cJd1BgZ9CS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TESGNN: Temporal Equivariant Scene Graph Neural Networks for Efficient
  and Robust <span class="highlight-title">Multi</span>-View 3D Scene Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.10509v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.10509v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quang P. M. Pham, Khoi T. N. Nguyen, Lan C. Ngo, Truong Do, Dezhen Song, Truong-Son Hy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scene graphs have proven to be highly effective for various scene
understanding tasks due to their compact and explicit representation of
relational information. However, current methods often overlook the critical
importance of preserving symmetry when generating scene graphs from 3D point
clouds, which can lead to reduced accuracy and robustness, particularly when
dealing with noisy, multi-view data. Furthermore, a major limitation of prior
approaches is the lack of temporal modeling to capture time-dependent
relationships among dynamically evolving entities in a scene. To address these
challenges, we propose Temporal Equivariant Scene Graph Neural Network
(TESGNN), consisting of two key components: (1) an Equivariant Scene Graph
Neural Network (ESGNN), which extracts information from 3D point clouds to
generate scene graph while preserving crucial symmetry properties, and (2) a
Temporal Graph Matching Network, which fuses scene graphs generated by ESGNN
across multiple time sequences into a unified global representation using an
approximate graph-matching algorithm. Our combined architecture TESGNN
outperforms current state-of-the-art methods in scene graph generation,
achieving higher accuracy and faster training convergence. Moreover, we show
that leveraging the symmetry-preserving property produces a more stable and
accurate global scene representation compared to existing approaches. Last but
not least, it is computationally efficient and easily implementable using
existing frameworks, making it well-suited for real-time applications in
robotics and computer vision. This approach paves the way for more robust and
scalable solutions to complex multi-view scene understanding challenges. Our
source code is publicly available at: https://github.com/HySonLab/TESGraph
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2407.00609</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unmasking Social Bots: How Confident Are We? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13929v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13929v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        James Giroux, Ariyarathne Gangani, Alexander C. Nwala, Cristiano Fanelli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social bots remain a major vector for spreading disinformation on social
media and a menace to the public. Despite the progress made in developing
multiple sophisticated social bot detection algorithms and tools, bot detection
remains a challenging, unsolved problem that is fraught with uncertainty due to
the heterogeneity of bot behaviors, training data, and detection algorithms.
Detection models often disagree on whether to label the same account as bot or
human-controlled. However, they do not provide any measure of uncertainty to
indicate how much we should trust their results. We propose to address both bot
detection and the quantification of uncertainty at the account level - a novel
feature of this research. This dual focus is crucial as it allows us to
leverage additional information related to the quantified uncertainty of each
prediction, thereby enhancing decision-making and improving the reliability of
bot classifications. Specifically, our approach facilitates targeted
interventions for bots when predictions are made with high confidence and
suggests caution (e.g., gathering more data) when predictions are uncertain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 6 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Optimization Landscape of SGD Across the Feature Learning Strength 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04642v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04642v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Atanasov, Alexandru Meterez, James B. Simon, Cengiz Pehlevan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider neural networks (NNs) where the final layer is down-scaled by a
fixed hyperparameter $\gamma$. Recent work has identified $\gamma$ as
controlling the strength of feature learning. As $\gamma$ increases, network
evolution changes from "lazy" kernel dynamics to "rich" feature-learning
dynamics, with a host of associated benefits including improved performance on
common tasks. In this work, we conduct a thorough empirical investigation of
the effect of scaling $\gamma$ across a variety of models and datasets in the
online training setting. We first examine the interaction of $\gamma$ with the
learning rate $\eta$, identifying several scaling regimes in the
$\gamma$-$\eta$ plane which we explain theoretically using a simple model. We
find that the optimal learning rate $\eta^*$ scales non-trivially with
$\gamma$. In particular, $\eta^* \propto \gamma^2$ when $\gamma \ll 1$ and
$\eta^* \propto \gamma^{2/L}$ when $\gamma \gg 1$ for a feed-forward network of
depth $L$. Using this optimal learning rate scaling, we proceed with an
empirical study of the under-explored "ultra-rich" $\gamma \gg 1$ regime. We
find that networks in this regime display characteristic loss curves, starting
with a long plateau followed by a drop-off, sometimes followed by one or more
additional staircase steps. We find networks of different large $\gamma$ values
optimize along similar trajectories up to a reparameterization of time. We
further find that optimal online performance is often found at large $\gamma$
and could be missed if this hyperparameter is not tuned. Our findings indicate
that analytical study of the large-$\gamma$ limit may yield useful insights
into the dynamics of representation learning in performant models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 Final Copy, 40 Pages, 45 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting Unsuccessful Students in Cybersecurity Exercises in Two
  Different Learning Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.08531v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.08531v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valdemar Švábenský, Kristián Tkáčik, Aubrey Birdwell, Richard Weiss, Ryan S. Baker, Pavel Čeleda, Jan Vykopal, Jens Mache, Ankur Chattopadhyay
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This full paper in the research track evaluates the usage of data logged from
cybersecurity exercises in order to predict students who are potentially at
risk of performing poorly. Hands-on exercises are essential for learning since
they enable students to practice their skills. In cybersecurity, hands-on
exercises are often complex and require knowledge of many topics. Therefore,
students may miss solutions due to gaps in their knowledge and become
frustrated, which impedes their learning. Targeted aid by the instructor helps,
but since the instructor's time is limited, efficient ways to detect struggling
students are needed. This paper develops automated tools to predict when a
student is having difficulty. We formed a dataset with the actions of 313
students from two countries and two learning environments: KYPO CRP and
EDURange. These data are used in machine learning algorithms to predict the
success of students in exercises deployed in these environments. After
extracting features from the data, we trained and cross-validated eight
classifiers for predicting the exercise outcome and evaluated their predictive
power. The contribution of this paper is comparing two approaches to feature
engineering, modeling, and classification performance on data from two learning
environments. Using the features from either learning environment, we were able
to detect and distinguish between successful and struggling students. A
decision tree classifier achieved the highest balanced accuracy and sensitivity
with data from both learning environments. The results show that activity data
from cybersecurity exercises are suitable for predicting student success. In a
potential application, such models can aid instructors in detecting struggling
students and providing targeted help. We publish data and code for building
these models so that others can adopt or adapt them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in the FIE 2024 conference proceedings, see
  https://doi.org/10.1109/FIE61694.2024.10893135</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentiable Weightless Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11112v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11112v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alan T. L. Bacellar, Zachary Susskind, Mauricio Breternitz Jr., Eugene John, Lizy K. John, Priscila M. V. Lima, Felipe M. G. França
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the Differentiable Weightless Neural Network (DWN), a model
based on interconnected lookup tables. Training of DWNs is enabled by a novel
Extended Finite Difference technique for approximate differentiation of binary
values. We propose Learnable Mapping, Learnable Reduction, and Spectral
Regularization to further improve the accuracy and efficiency of these models.
We evaluate DWNs in three edge computing contexts: (1) an FPGA-based hardware
accelerator, where they demonstrate superior latency, throughput, energy
efficiency, and model area compared to state-of-the-art solutions, (2) a
low-power microcontroller, where they achieve preferable accuracy to XGBoost
while subject to stringent memory constraints, and (3) ultra-low-cost chips,
where they consistently outperform small models in both accuracy and projected
hardware area. DWNs also compare favorably against leading approaches for
tabular datasets, with higher average rank. Overall, our work positions DWNs as
a pioneering solution for edge-compatible high-throughput neural networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning General-Purpose Biomedical Volume Representations using
  Randomized Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.02372v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.02372v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neel Dey, Benjamin Billot, Hallee E. Wong, Clinton J. Wang, Mengwei Ren, P. Ellen Grant, Adrian V. Dalca, Polina Golland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current volumetric biomedical foundation models struggle to generalize as
public 3D datasets are small and do not cover the broad diversity of medical
procedures, conditions, anatomical regions, and imaging protocols. We address
this by creating a representation learning method that instead anticipates
strong domain shifts at training time itself. We first propose a data engine
that synthesizes highly variable training samples that would enable
generalization to new biomedical contexts. To then train a single 3D network
for any voxel-level task, we develop a contrastive learning method that
pretrains the network to be stable against nuisance imaging variation simulated
by the data engine, a key inductive bias for generalization. This network's
features can be used as robust representations of input images for downstream
tasks and its weights provide a strong, dataset-agnostic initialization for
finetuning on new datasets. As a result, we set new standards across both
multimodality registration and few-shot segmentation, a first for any 3D
biomedical vision model, all without (pre-)training on any existing dataset of
real images.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025: International Conference on Learning Representations. Code
  and model weights available at https://github.com/neel-dey/anatomix.
  Keywords: synthetic data, representation learning, medical image analysis,
  image registration, image segmentation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prompting Fairness: Integrating Causality to Debias Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08743v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08743v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingling Li, Zeyu Tang, Xiaoyu Liu, Peter Spirtes, Kun Zhang, Liu Leqi, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs), despite their remarkable capabilities, are
susceptible to generating biased and discriminatory responses. As LLMs
increasingly influence high-stakes decision-making (e.g., hiring and
healthcare), mitigating these biases becomes critical. In this work, we propose
a causality-guided debiasing framework to tackle social biases, aiming to
reduce the objectionable dependence between LLMs' decisions and the social
information in the input. Our framework introduces a novel perspective to
identify how social information can affect an LLM's decision through different
causal pathways. Leveraging these causal insights, we outline principled
prompting strategies that regulate these pathways through selection mechanisms.
This framework not only unifies existing prompting-based debiasing techniques,
but also opens up new directions for reducing bias by encouraging the model to
prioritize fact-based reasoning over reliance on biased social cues. We
validate our framework through extensive experiments on real-world datasets
across multiple domains, demonstrating its effectiveness in debiasing LLM
decisions, even with only black-box access to the model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explain Yourself, Briefly! Self-Explaining Neural Networks with Concise
  Sufficient Reasons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.03391v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.03391v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahaf Bassan, Ron Eliav, Shlomit Gur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  *Minimal sufficient reasons* represent a prevalent form of explanation - the
smallest subset of input features which, when held constant at their
corresponding values, ensure that the prediction remains unchanged. Previous
*post-hoc* methods attempt to obtain such explanations but face two main
limitations: (1) Obtaining these subsets poses a computational challenge,
leading most scalable methods to converge towards suboptimal, less meaningful
subsets; (2) These methods heavily rely on sampling out-of-distribution input
assignments, potentially resulting in counterintuitive behaviors. To tackle
these limitations, we propose in this work a self-supervised training approach,
which we term *sufficient subset training* (SST). Using SST, we train models to
generate concise sufficient reasons for their predictions as an integral part
of their output. Our results indicate that our framework produces succinct and
faithful subsets substantially more efficiently than competing post-hoc
methods, while maintaining comparable predictive performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SymDiff: Equivariant Diffusion via Stochastic Symmetrisation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leo Zhang, Kianoosh Ashouritaklimi, Yee Whye Teh, Rob Cornish
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose SymDiff, a method for constructing equivariant diffusion models
using the framework of stochastic symmetrisation. SymDiff resembles a learned
data augmentation that is deployed at sampling time, and is lightweight,
computationally efficient, and easy to implement on top of arbitrary
off-the-shelf models. In contrast to previous work, SymDiff typically does not
require any neural network components that are intrinsically equivariant,
avoiding the need for complex parameterisations or the use of higher-order
geometric features. Instead, our method can leverage highly scalable modern
architectures as drop-in replacements for these more constrained alternatives.
We show that this additional flexibility yields significant empirical benefit
for $\mathrm{E}(3)$-equivariant molecular generation. To the best of our
knowledge, this is the first application of symmetrisation to generative
modelling, suggesting its potential in this domain more generally.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera-ready version for ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedBiP: Heterogeneous One-Shot Federated Learning with Personalized
  Latent Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04810v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04810v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haokun Chen, Hang Li, Yao Zhang, Jinhe Bi, Gengyuan Zhang, Yueqi Zhang, Philip Torr, Jindong Gu, Denis Krompass, Volker Tresp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One-Shot Federated Learning (OSFL), a special decentralized machine learning
paradigm, has recently gained significant attention. OSFL requires only a
single round of client data or model upload, which reduces communication costs
and mitigates privacy threats compared to traditional FL. Despite these
promising prospects, existing methods face challenges due to client data
heterogeneity and limited data quantity when applied to real-world OSFL
systems. Recently, Latent Diffusion Models (LDM) have shown remarkable
advancements in synthesizing high-quality images through pretraining on
large-scale datasets, thereby presenting a potential solution to overcome these
issues. However, directly applying pretrained LDM to heterogeneous OSFL results
in significant distribution shifts in synthetic data, leading to performance
degradation in classification models trained on such data. This issue is
particularly pronounced in rare domains, such as medical imaging, which are
underrepresented in LDM's pretraining data. To address this challenge, we
propose Federated Bi-Level Personalization (FedBiP), which personalizes the
pretrained LDM at both instance-level and concept-level. Hereby, FedBiP
synthesizes images following the client's local data distribution without
compromising the privacy regulations. FedBiP is also the first approach to
simultaneously address feature space heterogeneity and client data scarcity in
OSFL. Our method is validated through extensive experiments on three OSFL
benchmarks with feature space heterogeneity, as well as on challenging medical
and satellite image datasets with label heterogeneity. The results demonstrate
the effectiveness of FedBiP, which substantially outperforms other OSFL
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Understanding the Universality of Transformers for Next-Token
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.03011v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.03011v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael E. Sander, Gabriel Peyré
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal Transformers are trained to predict the next token for a given
context. While it is widely accepted that self-attention is crucial for
encoding the causal structure of sequences, the precise underlying mechanism
behind this in-context autoregressive learning ability remains unclear. In this
paper, we take a step towards understanding this phenomenon by studying the
approximation ability of Transformers for next-token prediction. Specifically,
we explore the capacity of causal Transformers to predict the next token
$x_{t+1}$ given an autoregressive sequence $(x_1, \dots, x_t)$ as a prompt,
where $ x_{t+1} = f(x_t) $, and $ f $ is a context-dependent function that
varies with each sequence. On the theoretical side, we focus on specific
instances, namely when $ f $ is linear or when $ (x_t)_{t \geq 1} $ is
periodic. We explicitly construct a Transformer (with linear, exponential, or
softmax attention) that learns the mapping $f$ in-context through a causal
kernel descent method. The causal kernel descent method we propose provably
estimates $x_{t+1} $ based solely on past and current observations $ (x_1,
\dots, x_t) $, with connections to the Kaczmarz algorithm in Hilbert spaces. We
present experimental results that validate our theoretical findings and suggest
their applicability to more general mappings $f$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025, 20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Dual Process Theory in Language Agent Framework for Real-time
  Simultaneous Human-AI Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.11882v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.11882v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shao Zhang, Xihuai Wang, Wenhao Zhang, Chaoran Li, Junru Song, Tingyu Li, Lin Qiu, Xuezhi Cao, Xunliang Cai, Wen Yao, Weinan Zhang, Xinbing Wang, Ying Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agents built on large language models (LLMs) have excelled in turn-by-turn
human-AI collaboration but struggle with simultaneous tasks requiring real-time
interaction. Latency issues and the challenge of inferring variable human
strategies hinder their ability to make autonomous decisions without explicit
instructions. Through experiments with current independent System 1 and System
2 methods, we validate the necessity of using Dual Process Theory (DPT) in
real-time tasks. We propose DPT-Agent, a novel language agent framework that
integrates System 1 and System 2 for efficient real-time simultaneous human-AI
collaboration. DPT-Agent's System 1 uses a Finite-state Machine (FSM) and
code-as-policy for fast, intuitive, and controllable decision-making.
DPT-Agent's System 2 integrates Theory of Mind (ToM) and asynchronous
reflection to infer human intentions and perform reasoning-based autonomous
decisions. We demonstrate the effectiveness of DPT-Agent through further
experiments with rule-based agents and human collaborators, showing significant
improvements over mainstream LLM-based frameworks. DPT-Agent can effectively
help LLMs convert correct slow thinking and reasoning into executable actions,
thereby improving performance. To the best of our knowledge, DPT-Agent is the
first language agent framework that achieves successful real-time simultaneous
human-AI collaboration autonomously. Code of DPT-Agent can be found in
https://github.com/sjtu-marl/DPT-Agent.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint under review. Update the experimental results of the
  DeepSeek-R1 series models, o3-mini-high and o3-mini-medium</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bidirectional Consistency Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.18035v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.18035v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liangchen Li, Jiajun He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models (DMs) are capable of generating remarkably high-quality
samples by iteratively denoising a random vector, a process that corresponds to
moving along the probability flow ordinary differential equation (PF ODE).
Interestingly, DMs can also invert an input image to noise by moving backward
along the PF ODE, a key operation for downstream tasks such as interpolation
and image editing. However, the iterative nature of this process restricts its
speed, hindering its broader application. Recently, Consistency Models (CMs)
have emerged to address this challenge by approximating the integral of the PF
ODE, largely reducing the number of iterations. Yet, the absence of an explicit
ODE solver complicates the inversion process. To resolve this, we introduce
Bidirectional Consistency Model (BCM), which learns a single neural network
that enables both forward and backward traversal along the PF ODE, efficiently
unifying generation and inversion tasks within one framework. We can train BCM
from scratch or tune it using a pretrained consistency model, which reduces the
training cost and increases scalability. We demonstrate that BCM enables
one-step generation and inversion while also allowing the use of additional
steps to enhance generation quality or reduce reconstruction error. We further
showcase BCM's capability in downstream tasks, such as interpolation and
inpainting. Our code and weights are available at
https://github.com/Mosasaur5526/BCM-iCT-torch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages, 27 figures; a shorter version of this paper was acceppted
  at the ICML 2024 Workshop on Structured Probabilistic Inference & Generative
  Modeling</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MoE-CAP: Benchmarking Cost, Accuracy and Performance of Sparse
  Mixture-of-Experts Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.07067v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.07067v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Fu, Yinsicheng Jiang, Yeqi Huang, Ping Nie, Zhan Lu, Leyang Xue, Congjie He, Man-Kit Sit, Jilong Xue, Li Dong, Ziming Miao, Kai Zou, Edoardo Ponti, Luo Mai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Mixture-of-Experts (MoE) architecture is increasingly favored for scaling
Large Language Models (LLMs). Its key feature, sparse activation, selectively
activates only a subset of parameters (experts) per token, reducing memory
bandwidth and compute FLOPs compared to dense models. To capitalize on this,
MoE designers leverage heterogeneous compute and memory hardware to lower
system costs. However, the interaction between model sparsity and hardware
heterogeneity introduces trade-offs in Cost, Accuracy, and Performance (CAP).
To address this, we introduce MoE-CAP, a benchmarking method for evaluating
sparse MoE systems across these three dimensions. Its key innovation is a
sparsity-aware CAP analysis model, the first to integrate cost, performance,
and accuracy metrics into a single diagram while estimating the impact of
sparsity on system performance. MoE-CAP helps practitioners optimize hardware
provisioning for an MoE model-or vice versa. MoE-CAP supports various MoE
models and provides more accurate metrics than existing methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Foundational Policy Acquisition via <span class="highlight-title">Multi</span>task Learning for Motor Skill
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2308.16471v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2308.16471v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Satoshi Yamamori, Jun Morimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we propose a multitask reinforcement learning algorithm for
foundational policy acquisition to generate novel motor skills.
\textcolor{\hcolor}{Learning the rich representation of the multitask policy is
a challenge in dynamic movement generation tasks because the policy needs to
cope with changes in goals or environments with different reward functions or
physical parameters. Inspired by human sensorimotor adaptation mechanisms, we
developed the learning pipeline to construct the encoder-decoder networks and
network selection to facilitate foundational policy acquisition under multiple
situations. First, we compared the proposed method with previous multitask
reinforcement learning methods in the standard multi-locomotion tasks. The
results showed that the proposed approach outperformed the baseline methods.
Then, we applied the proposed method to the ball heading task using a monopod
robot model to evaluate skill generation performance. The results showed that
the proposed method was able to adapt to novel target positions or
inexperienced ball restitution coefficients but to acquire a foundational
policy network, originally learned for heading motion, which can generate an
entirely new overhead kicking skill.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Phase-Amplitude Reduction-Based Imitation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.03735v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.03735v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Satoshi Yamamori, Jun Morimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we propose the use of the phase-amplitude reduction method to
construct an imitation learning framework. Imitating human movement
trajectories is recognized as a promising strategy for generating a range of
human-like robot movements. Unlike previous dynamical system-based imitation
learning approaches, our proposed method allows the robot not only to imitate a
limit cycle trajectory but also to replicate the transient movement from the
initial or disturbed state to the limit cycle. Consequently, our method offers
a safer imitation learning approach that avoids generating unpredictable
motions immediately after disturbances or from a specified initial state. We
first validated our proposed method by reconstructing a simple limit-cycle
attractor. We then compared the proposed approach with a conventional method on
a lemniscate trajectory tracking task with a simulated robot arm. Our findings
confirm that our proposed method can more accurately generate transient
movements to converge on a target periodic attractor compared to the previous
standard approach. Subsequently, we applied our method to a real robot arm to
imitate periodic human movements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Nonparametric Heterogeneous Long-term Causal Effect Estimation via Data
  Combination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.18960v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.18960v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weilin Chen, Ruichu Cai, Junjie Wan, Zeqin Yang, José Miguel Hernández-Lobato
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Long-term causal inference has drawn increasing attention in many scientific
domains. Existing methods mainly focus on estimating average long-term causal
effects by combining long-term observational data and short-term experimental
data. However, it is still understudied how to robustly and effectively
estimate heterogeneous long-term causal effects, significantly limiting
practical applications. In this paper, we propose several two-stage style
nonparametric estimators for heterogeneous long-term causal effect estimation,
including propensity-based, regression-based, and multiple robust estimators.
We conduct a comprehensive theoretical analysis of their asymptotic properties
under mild assumptions, with the ultimate goal of building a better
understanding of the conditions under which some estimators can be expected to
perform better. Extensive experiments across several semi-synthetic and
real-world datasets validate the theoretical results and demonstrate the
effectiveness of the proposed estimators.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TradingAgents: <span class="highlight-title">Multi</span>-Agents <span class="highlight-title">LLM</span> Financial Trading Framework <span class="chip">AAAI 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.20138v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.20138v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijia Xiao, Edward Sun, Di Luo, Wei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Significant progress has been made in automated problem-solving using
societies of agents powered by large language models (LLMs). In finance,
efforts have largely focused on single-agent systems handling specific tasks or
multi-agent frameworks independently gathering data. However, multi-agent
systems' potential to replicate real-world trading firms' collaborative
dynamics remains underexplored. TradingAgents proposes a novel stock trading
framework inspired by trading firms, featuring LLM-powered agents in
specialized roles such as fundamental analysts, sentiment analysts, technical
analysts, and traders with varied risk profiles. The framework includes Bull
and Bear researcher agents assessing market conditions, a risk management team
monitoring exposure, and traders synthesizing insights from debates and
historical data to make informed decisions. By simulating a dynamic,
collaborative trading environment, this framework aims to improve trading
performance. Detailed architecture and extensive experiments reveal its
superiority over baseline models, with notable improvements in cumulative
returns, Sharpe ratio, and maximum drawdown, highlighting the potential of
multi-agent LLM frameworks in financial trading. TradingAgents is available at
https://github.com/PioneerFintech.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Multi-Agent AI in the Real World @ AAAI 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video-Foley: Two-Stage Video-To-Sound Generation via Temporal Event
  Condition For Foley Sound 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11915v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11915v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junwon Lee, Jaekwon Im, Dabin Kim, Juhan Nam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foley sound synthesis is crucial for multimedia production, enhancing user
experience by synchronizing audio and video both temporally and semantically.
Recent studies on automating this labor-intensive process through
video-to-sound generation face significant challenges. Systems lacking explicit
temporal features suffer from poor alignment and controllability, while
timestamp-based models require costly and subjective human annotation. We
propose Video-Foley, a video-to-sound system using Root Mean Square (RMS) as an
intuitive condition with semantic timbre prompts (audio or text). RMS, a
frame-level intensity envelope closely related to audio semantics, acts as a
temporal event feature to guide audio generation from video. The
annotation-free self-supervised learning framework consists of two stages,
Video2RMS and RMS2Sound, incorporating novel ideas including RMS discretization
and RMS-ControlNet with a pretrained text-to-audio model. Our extensive
evaluation shows that Video-Foley achieves state-of-the-art performance in
audio-visual alignment and controllability for sound timing, intensity, timbre,
and nuance. Source code, model weights and demos are available on our companion
website. (https://jnwnlee.github.io/video-foley-demo)
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Audio-Visual Instance Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18709v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18709v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruohao Guo, Xianghua Ying, Yaru Chen, Dantong Niu, Guangyao Li, Liao Qu, Yanyu Qi, Jinxing Zhou, Bowei Xing, Wenzhen Yue, Ji Shi, Qixun Wang, Peiliang Zhang, Buwen Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a new multi-modal task, termed audio-visual
instance segmentation (AVIS), which aims to simultaneously identify, segment
and track individual sounding object instances in audible videos. To facilitate
this research, we introduce a high-quality benchmark named AVISeg, containing
over 90K instance masks from 26 semantic categories in 926 long videos.
Additionally, we propose a strong baseline model for this task. Our model first
localizes sound source within each frame, and condenses object-specific
contexts into concise tokens. Then it builds long-range audio-visual
dependencies between these tokens using window-based attention, and tracks
sounding objects among the entire video sequences. Extensive experiments reveal
that our method performs best on AVISeg, surpassing the existing methods from
related tasks. We further conduct the evaluation on several multi-modal large
models. Unfortunately, they exhibits subpar performance on instance-level sound
source localization and temporal perception. We expect that AVIS will inspire
the community towards a more comprehensive multi-modal understanding. Dataset
and code is available at https://github.com/ruohaoguo/avis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GAMED: Knowledge Adaptive <span class="highlight-title">Multi</span>-Experts Decoupling for <span class="highlight-title">Multi</span>modal Fake
  News Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2412.12164v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2412.12164v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingzhi Shen, Yunfei Long, Xiaohao Cai, Imran Razzak, Guanming Chen, Kang Liu, Shoaib Jameel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal fake news detection often involves modelling heterogeneous data
sources, such as vision and language. Existing detection methods typically rely
on fusion effectiveness and cross-modal consistency to model the content,
complicating understanding how each modality affects prediction accuracy.
Additionally, these methods are primarily based on static feature modelling,
making it difficult to adapt to the dynamic changes and relationships between
different data modalities. This paper develops a significantly novel approach,
GAMED, for multimodal modelling, which focuses on generating distinctive and
discriminative features through modal decoupling to enhance cross-modal
synergies, thereby optimizing overall performance in the detection process.
GAMED leverages multiple parallel expert networks to refine features and
pre-embed semantic knowledge to improve the experts' ability in information
selection and viewpoint sharing. Subsequently, the feature distribution of each
modality is adaptively adjusted based on the respective experts' opinions.
GAMED also introduces a novel classification technique to dynamically manage
contributions from different modalities, while improving the explainability of
decisions. Experimental results on the Fakeddit and Yang datasets demonstrate
that GAMED performs better than recently developed state-of-the-art models. The
source code can be accessed at https://github.com/slz0925/GAMED.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving LSH via Tensorized Random Projection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.07189v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.07189v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhisham Dev Verma, Rameshwar Pratap
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Locality sensitive hashing (LSH) is a fundamental algorithmic toolkit used by
data scientists for approximate nearest neighbour search problems that have
been used extensively in many large scale data processing applications such as
near duplicate detection, nearest neighbour search, clustering, etc. In this
work, we aim to propose faster and space efficient locality sensitive hash
functions for Euclidean distance and cosine similarity for tensor data.
Typically, the naive approach for obtaining LSH for tensor data involves first
reshaping the tensor into vectors, followed by applying existing LSH methods
for vector data $E2LSH$ and $SRP$. However, this approach becomes impractical
for higher order tensors because the size of the reshaped vector becomes
exponential in the order of the tensor. Consequently, the size of LSH
parameters increases exponentially. To address this problem, we suggest two
methods for LSH for Euclidean distance and cosine similarity, namely
$CP-E2LSH$, $TT-E2LSH$, and $CP-SRP$, $TT-SRP$, respectively, building on $CP$
and tensor train $(TT)$ decompositions techniques. Our approaches are space
efficient and can be efficiently applied to low rank $CP$ or $TT$ tensors. We
provide a rigorous theoretical analysis of our proposal on their correctness
and efficacy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Where is the Testbed for my Federated Learning Research? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.14154v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.14154v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Janez Božič, Amândio R. Faustino, Boris Radovič, Marco Canini, Veljko Pejović
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Progressing beyond centralized AI is of paramount importance, yet,
distributed AI solutions, in particular various federated learning (FL)
algorithms, are often not comprehensively assessed, which prevents the research
community from identifying the most promising approaches and practitioners from
being convinced that a certain solution is deployment-ready. The largest hurdle
towards FL algorithm evaluation is the difficulty of conducting real-world
experiments over a variety of FL client devices and different platforms, with
different datasets and data distribution, all while assessing various
dimensions of algorithm performance, such as inference accuracy, energy
consumption, and time to convergence, to name a few. In this paper, we present
CoLExT, a real-world testbed for FL research. CoLExT is designed to streamline
experimentation with custom FL algorithms in a rich testbed configuration
space, with a large number of heterogeneous edge devices, ranging from
single-board computers to smartphones, and provides real-time collection and
visualization of a variety of metrics through automatic instrumentation.
According to our evaluation, porting FL algorithms to CoLExT requires minimal
involvement from the developer, and the instrumentation introduces minimal
resource usage overhead. Furthermore, through an initial investigation
involving popular FL algorithms running on CoLExT, we reveal previously unknown
trade-offs, inefficiencies, and programming bugs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SEC 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When Attention Sink Emerges in Language Models: An Empirical View 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.10781v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.10781v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangming Gu, Tianyu Pang, Chao Du, Qian Liu, Fengzhuo Zhang, Cunxiao Du, Ye Wang, Min Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Models (LMs) assign significant attention to the first token, even
if it is not semantically important, which is known as attention sink. This
phenomenon has been widely adopted in applications such as streaming/long
context generation, KV cache optimization, inference acceleration, model
quantization, and others. Despite its widespread use, a deep understanding of
attention sink in LMs is still lacking. In this work, we first demonstrate that
attention sinks exist universally in LMs with various inputs, even in small
models. Furthermore, attention sink is observed to emerge during the LM
pre-training, motivating us to investigate how optimization, data distribution,
loss function, and model architecture in LM pre-training influence its
emergence. We highlight that attention sink emerges after effective
optimization on sufficient training data. The sink position is highly
correlated with the loss function and data distribution. Most importantly, we
find that attention sink acts more like key biases, storing extra attention
scores, which could be non-informative and not contribute to the value
computation. We also observe that this phenomenon (at least partially) stems
from tokens' inner dependence on attention scores as a result of softmax
normalization. After relaxing such dependence by replacing softmax attention
with other attention operations, such as sigmoid attention without
normalization, attention sinks do not emerge in LMs up to 1B parameters. The
code is available at https://github.com/sail-sg/Attention-Sink.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cheating Automatic <span class="highlight-title">LLM</span> Benchmarks: Null Models Achieve High Win Rates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.07137v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.07137v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Jing Jiang, Min Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic LLM benchmarks, such as AlpacaEval 2.0, Arena-Hard-Auto, and
MT-Bench, have become popular for evaluating language models due to their
cost-effectiveness and scalability compared to human evaluation. Achieving high
win rates on these benchmarks can significantly boost the promotional impact of
newly released language models. This promotional benefit may motivate tricks,
such as manipulating model output length or style to game win rates, even
though several mechanisms have been developed to control length and disentangle
style to reduce gameability. Nonetheless, we show that even a "null model" that
always outputs a constant response (irrelevant to input instructions) can cheat
automatic benchmarks and achieve top-ranked win rates: an 86.5% LC win rate on
AlpacaEval 2.0; an 83.0 score on Arena-Hard-Auto; and a 9.55 score on MT-Bench.
Moreover, the crafted cheating outputs are transferable because we assume that
the instructions of these benchmarks (e.g., 805 samples of AlpacaEval 2.0) are
private and cannot be accessed. While our experiments are primarily
proof-of-concept, an adversary could use LLMs to generate more imperceptible
cheating responses, unethically benefiting from high win rates and promotional
impact. Our findings call for the development of anti-cheating mechanisms for
reliable automatic benchmarks. The code is available at
https://github.com/sail-sg/Cheating-LLM-Benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025 (Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Transformers Dream of Electric Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.16699v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.16699v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Cheng, Lawrence Carin, Suvrit Sra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show theoretically and empirically that the linear Transformer, when
applied to graph data, can implement algorithms that solve canonical problems
such as electric flow and eigenvector decomposition. The Transformer has access
to information on the input graph only via the graph's incidence matrix. We
present explicit weight configurations for implementing each algorithm, and we
bound the constructed Transformers' errors by the errors of the underlying
algorithms. Our theoretical findings are corroborated by experiments on
synthetic data. Additionally, on a real-world molecular regression task, we
observe that the linear Transformer is capable of learning a more effective
positional encoding than the default one based on Laplacian eigenvectors. Our
work is an initial step towards elucidating the inner-workings of the
Transformer for graph data. Code is available at
https://github.com/chengxiang/LinearGraphTransformer
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerating 3D Molecule Generation via Jointly Geometric Optimal
  Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.15252v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.15252v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haokai Hong, Wanyu Lin, Kay Chen Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a new 3D molecule generation framework, called GOAT, for
fast and effective 3D molecule generation based on the flow-matching optimal
transport objective. Specifically, we formulate a geometric transport formula
for measuring the cost of mapping multi-modal features (e.g., continuous atom
coordinates and categorical atom types) between a base distribution and a
target data distribution. Our formula is solved within a joint, equivariant,
and smooth representation space. This is achieved by transforming the
multi-modal features into a continuous latent space with equivariant networks.
In addition, we find that identifying optimal distributional coupling is
necessary for fast and effective transport between any two distributions. We
further propose a mechanism for estimating and purifying optimal coupling to
train the flow model with optimal transport. By doing so, GOAT can turn
arbitrary distribution couplings into new deterministic couplings, leading to
an estimated optimal transport plan for fast 3D molecule generation. The
purification filters out the subpar molecules to ensure the ultimate generation
quality. We theoretically and empirically prove that the proposed optimal
coupling estimation and purification yield transport plan with non-increasing
cost. Finally, extensive experiments show that GOAT enjoys the efficiency of
solving geometric optimal transport, leading to a double speedup compared to
the sub-optimal method while achieving the best generation quality regarding
validity, uniqueness, and novelty. The code is available at
https://github.com/WanyuGroup/ICLR2025-GOAT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Knowledge Gradient for <span class="highlight-title">Multi</span>-Objective Bayesian Optimization with
  Decoupled Evaluations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01310v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01310v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jack M. Buckingham, Sebastian Rojas Gonzalez, Juergen Branke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-objective Bayesian optimization aims to find the Pareto front of
trade-offs between a set of expensive objectives while collecting as few
samples as possible. In some cases, it is possible to evaluate the objectives
separately, and a different latency or evaluation cost can be associated with
each objective. This decoupling of the objectives presents an opportunity to
learn the Pareto front faster by avoiding unnecessary, expensive evaluations.
We propose a scalarization based knowledge gradient acquisition function which
accounts for the different evaluation costs of the objectives. We prove
asymptotic consistency of the estimator of the optimum for an arbitrary,
D-dimensional, real compact search space and show empirically that the
algorithm performs comparably with the state of the art and significantly
outperforms versions which always evaluate both objectives.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>36 pages. This preprint has not undergone peer review (when
  applicable) or any post-submission improvements or corrections. The Version
  of Record of this contribution is published in 'Evolutionary Multi-Criterion
  Optimization', LNCS 15513, and is available online at
  https://doi.org/10.1007/978-981-96-3538-2_9</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UMGAD: Unsupervised <span class="highlight-title">Multi</span>plex Graph Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12556v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12556v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Li, Jianpeng Qi, Zhongying Zhao, Guanjie Zheng, Lei Cao, Junyu Dong, Yanwei Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph anomaly detection (GAD) is a critical task in graph machine learning,
with the primary objective of identifying anomalous nodes that deviate
significantly from the majority. This task is widely applied in various
real-world scenarios, including fraud detection and social network analysis.
However, existing GAD methods still face two major challenges: (1) They are
often limited to detecting anomalies in single-type interaction graphs and
struggle with multiple interaction types in multiplex heterogeneous graphs. (2)
In unsupervised scenarios, selecting appropriate anomaly score thresholds
remains a significant challenge for accurate anomaly detection. To address the
above challenges, we propose a novel Unsupervised Multiplex Graph Anomaly
Detection method, named UMGAD. We first learn multi-relational correlations
among nodes in multiplex heterogeneous graphs and capture anomaly information
during node attribute and structure reconstruction through graph-masked
autoencoder (GMAE). Then, to further extract abnormal information, we generate
attribute-level and subgraph-level augmented-view graphs respectively, and
perform attribute and structure reconstruction through GMAE. Finally, we learn
to optimize node attributes and structural features through contrastive
learning between original-view and augmented-view graphs to improve the model's
ability to capture anomalies. Meanwhile, we also propose a new anomaly score
threshold selection strategy, which allows the model to be independent of
ground truth information in real unsupervised scenarios. Extensive experiments
on four datasets show that our UMGAD significantly outperforms state-of-the-art
methods, achieving average improvements of 13.48% in AUC and 11.68% in Macro-F1
across all datasets.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FedBiP: Heterogeneous One-Shot Federated Learning with Personalized
  Latent Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.04810v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.04810v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haokun Chen, Hang Li, Yao Zhang, Jinhe Bi, Gengyuan Zhang, Yueqi Zhang, Philip Torr, Jindong Gu, Denis Krompass, Volker Tresp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One-Shot Federated Learning (OSFL), a special decentralized machine learning
paradigm, has recently gained significant attention. OSFL requires only a
single round of client data or model upload, which reduces communication costs
and mitigates privacy threats compared to traditional FL. Despite these
promising prospects, existing methods face challenges due to client data
heterogeneity and limited data quantity when applied to real-world OSFL
systems. Recently, Latent Diffusion Models (LDM) have shown remarkable
advancements in synthesizing high-quality images through pretraining on
large-scale datasets, thereby presenting a potential solution to overcome these
issues. However, directly applying pretrained LDM to heterogeneous OSFL results
in significant distribution shifts in synthetic data, leading to performance
degradation in classification models trained on such data. This issue is
particularly pronounced in rare domains, such as medical imaging, which are
underrepresented in LDM's pretraining data. To address this challenge, we
propose Federated Bi-Level Personalization (FedBiP), which personalizes the
pretrained LDM at both instance-level and concept-level. Hereby, FedBiP
synthesizes images following the client's local data distribution without
compromising the privacy regulations. FedBiP is also the first approach to
simultaneously address feature space heterogeneity and client data scarcity in
OSFL. Our method is validated through extensive experiments on three OSFL
benchmarks with feature space heterogeneity, as well as on challenging medical
and satellite image datasets with label heterogeneity. The results demonstrate
the effectiveness of FedBiP, which substantially outperforms other OSFL
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video-Foley: Two-Stage Video-To-Sound Generation via Temporal Event
  Condition For Foley Sound 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.11915v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.11915v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junwon Lee, Jaekwon Im, Dabin Kim, Juhan Nam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foley sound synthesis is crucial for multimedia production, enhancing user
experience by synchronizing audio and video both temporally and semantically.
Recent studies on automating this labor-intensive process through
video-to-sound generation face significant challenges. Systems lacking explicit
temporal features suffer from poor alignment and controllability, while
timestamp-based models require costly and subjective human annotation. We
propose Video-Foley, a video-to-sound system using Root Mean Square (RMS) as an
intuitive condition with semantic timbre prompts (audio or text). RMS, a
frame-level intensity envelope closely related to audio semantics, acts as a
temporal event feature to guide audio generation from video. The
annotation-free self-supervised learning framework consists of two stages,
Video2RMS and RMS2Sound, incorporating novel ideas including RMS discretization
and RMS-ControlNet with a pretrained text-to-audio model. Our extensive
evaluation shows that Video-Foley achieves state-of-the-art performance in
audio-visual alignment and controllability for sound timing, intensity, timbre,
and nuance. Source code, model weights and demos are available on our companion
website. (https://jnwnlee.github.io/video-foley-demo)
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Audio-Visual Instance Segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.18709v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.18709v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruohao Guo, Xianghua Ying, Yaru Chen, Dantong Niu, Guangyao Li, Liao Qu, Yanyu Qi, Jinxing Zhou, Bowei Xing, Wenzhen Yue, Ji Shi, Qixun Wang, Peiliang Zhang, Buwen Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a new multi-modal task, termed audio-visual
instance segmentation (AVIS), which aims to simultaneously identify, segment
and track individual sounding object instances in audible videos. To facilitate
this research, we introduce a high-quality benchmark named AVISeg, containing
over 90K instance masks from 26 semantic categories in 926 long videos.
Additionally, we propose a strong baseline model for this task. Our model first
localizes sound source within each frame, and condenses object-specific
contexts into concise tokens. Then it builds long-range audio-visual
dependencies between these tokens using window-based attention, and tracks
sounding objects among the entire video sequences. Extensive experiments reveal
that our method performs best on AVISeg, surpassing the existing methods from
related tasks. We further conduct the evaluation on several multi-modal large
models. Unfortunately, they exhibits subpar performance on instance-level sound
source localization and temporal perception. We expect that AVIS will inspire
the community towards a more comprehensive multi-modal understanding. Dataset
and code is available at https://github.com/ruohaoguo/avis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Long-Text Alignment for Text-to-Image Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11817v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11817v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luping Liu, Chao Du, Tianyu Pang, Zehan Wang, Chongxuan Li, Dong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of text-to-image (T2I) diffusion models has enabled
them to generate unprecedented results from given texts. However, as text
inputs become longer, existing encoding methods like CLIP face limitations, and
aligning the generated images with long texts becomes challenging. To tackle
these issues, we propose LongAlign, which includes a segment-level encoding
method for processing long texts and a decomposed preference optimization
method for effective alignment training. For segment-level encoding, long texts
are divided into multiple segments and processed separately. This method
overcomes the maximum input length limits of pretrained encoding models. For
preference optimization, we provide decomposed CLIP-based preference models to
fine-tune diffusion models. Specifically, to utilize CLIP-based preference
models for T2I alignment, we delve into their scoring mechanisms and find that
the preference scores can be decomposed into two components: a text-relevant
part that measures T2I alignment and a text-irrelevant part that assesses other
visual aspects of human preference. Additionally, we find that the
text-irrelevant part contributes to a common overfitting problem during
fine-tuning. To address this, we propose a reweighting strategy that assigns
different weights to these two components, thereby reducing overfitting and
enhancing alignment. After fine-tuning $512 \times 512$ Stable Diffusion (SD)
v1.5 for about 20 hours using our method, the fine-tuned SD outperforms
stronger foundation models in T2I alignment, such as PixArt-$\alpha$ and
Kandinsky v2.2. The code is available at
https://github.com/luping-liu/LongAlign.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-03-01T00:00:00Z">2025-03-01</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">39</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Masked Mixers for Language Generation and Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.01482v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.01482v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin L. Badger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attention mechanisms that confer selective focus on a strict subset of input
elements are nearly ubiquitous in language models today. We posit there to be
downside to the use of attention: most input information is lost. In support of
this idea we observe poor input representation accuracy in transformers and
more accurate representation in what we term masked mixers, which replace
self-attention with masked convolutions. The masked mixer learns causal
language modeling more efficiently than early transformer implementations and
even outperforms optimized, current transformers when training on small (<512)
but not larger context windows. Evidence is presented for the hypothesis that
differences in transformer and masked mixer training efficiencies for various
tasks are best predicted by input representation accuracy, or equivalently
global invertibility. We hypothesize that the information loss exhibited by
transformers would be more detrimental to retrieval than generation, as the
former is more closely approximated by a bijective and thus invertible
function. We find that masked mixers are more effective retrieval models both
when the pretrained embedding model is unchanged as well as when the embedding
model is modified via cosine similarity-based InfoNCE loss minimization. A
small masked mixer is shown to outperform a large and near state-of-the-art
transformer-based retrieval model, despite the latter being trained with many
orders of magnitude more data and compute.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 8 figures, 3 tables, 9 supplementary figures, 13
  supplementary tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DFlow: Diverse Dialogue Flow Simulation with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.14853v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.14853v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanyu Du, Song Feng, James Gung, Lijia Sun, Yi Zhang, Saab Mansour, Yanjun Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Developing language model-based dialogue agents requires effective data to
train models that can follow specific task logic. However, most existing data
simulation methods focus on increasing diversity in language, topics, or
dialogue acts at the utterance level, largely neglecting a critical aspect of
task logic diversity at the dialogue level. This paper proposes a novel data
simulation method designed to enhance the diversity of synthetic dialogues by
focusing on task execution logic. Our method uses LLMs to generate decision
tree-structured task plans, which enables the derivation of diverse dialogue
trajectories for a given task. Each trajectory, referred to as a "dialog flow",
guides the generation of a multi-turn dialogue that follows a unique
trajectory. We apply this method to generate a task-oriented dialogue dataset
comprising 3,886 dialogue flows across 15 different domains. We validate the
effectiveness of this dataset using the next action prediction task, where
models fine-tuned on our dataset outperform strong baselines, including GPT-4.
Upon acceptance of this paper, we plan to release the code and data publicly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MoDeGPT: Modular Decomposition for Large Language Model Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.09632v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.09632v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chi-Heng Lin, Shangqian Gao, James Seale Smith, Abhishek Patel, Shikhar Tuli, Yilin Shen, Hongxia Jin, Yen-Chang Hsu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have reshaped the landscape of artificial
intelligence by demonstrating exceptional performance across various tasks.
However, substantial computational requirements make their deployment
challenging on devices with limited resources. Recently, compression methods
using low-rank matrix techniques have shown promise, yet these often lead to
degraded accuracy or introduce significant overhead in parameters and inference
latency. This paper introduces \textbf{Mo}dular \textbf{De}composition
(MoDeGPT), a novel structured compression framework that does not need recovery
fine-tuning while resolving the above drawbacks. MoDeGPT partitions the
Transformer block into modules comprised of matrix pairs and reduces the hidden
dimensions via reconstructing the module-level outputs. MoDeGPT is developed
based on a theoretical framework that utilizes three well-established matrix
decomposition algorithms -- Nystr\"om approximation, CR decomposition, and SVD
-- and applies them to our redefined transformer modules. Our comprehensive
experiments show MoDeGPT, without backward propagation, matches or surpasses
previous structured compression methods that rely on gradient information, and
saves 98% of compute costs on compressing a 13B model. On \textsc{Llama}-2/3
and OPT models, MoDeGPT maintains 90-95% zero-shot performance with 25-30%
compression rates. Moreover, the compression can be done on a single GPU within
a few hours and increases the inference throughput by up to 46%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CREAM: Consistency Regularized Self-Rewarding Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.12735v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.12735v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyang Wang, Weilei He, Zhiyuan Liang, Xuchao Zhang, Chetan Bansal, Ying Wei, Weitong Zhang, Huaxiu Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent self-rewarding large language models (LLM) have successfully applied
LLM-as-a-Judge to iteratively improve the alignment performance without the
need of human annotations for preference data. These methods commonly utilize
the same LLM to act as both the policy model (which generates responses) and
the reward model (which scores and ranks those responses). The ranked responses
are then used as preference pairs to train the LLM via direct alignment
technologies (e.g. DPO). However, it is noteworthy that throughout this
process, there is no guarantee of accuracy in the rewarding and ranking, which
is critical for ensuring accurate rewards and high-quality preference data.
Empirical results from relatively small LLMs (e.g., 7B parameters) also
indicate that improvements from self-rewarding may diminish after several
iterations in certain situations, which we hypothesize is due to accumulated
bias in the reward system. This bias can lead to unreliable preference data for
training the LLM. To address this issue, we first formulate and analyze the
generalized iterative preference fine-tuning framework for self-rewarding
language model. We then introduce the regularization to this generalized
framework to mitigate the overconfident preference labeling in the
self-rewarding process. Based on this theoretical insight, we propose a
Consistency Regularized sElf-rewarding lAnguage Model (CREAM) that leverages
the consistency of rewards across different iterations to regularize the
self-rewarding training, helping the model to learn from more reliable
preference data. With this explicit regularization, our empirical results
demonstrate the superiority of CREAM in improving both reward consistency and
alignment performance. The code is publicly available at
https://github.com/Raibows/CREAM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Task Decomposition to Assist Humans in Competitive Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.04604v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.04604v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxin Wen, Ruiqi Zhong, Pei Ke, Zhihong Shao, Hongning Wang, Minlie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When using language models (LMs) to solve complex problems, humans might
struggle to understand the LM-generated solutions and repair the flawed ones.
To assist humans in repairing them, we propose to automatically decompose
complex solutions into multiple simpler pieces that correspond to specific
subtasks. We introduce a novel objective for learning task decomposition,
termed assistive value (AssistV), which measures the feasibility and speed for
humans to repair the decomposed solution. We collect a dataset of human repair
experiences on different decomposed solutions. Utilizing the collected data as
in-context examples, we then learn to critique, refine, and rank decomposed
solutions to improve AssistV. We validate our method under competitive
programming problems: under 177 hours of human study, our method enables
non-experts to solve 33.3\% more problems, speeds them up by 3.3x, and empowers
them to match unassisted experts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2024 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieval-Augmented Generation for Natural Language Processing: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.13193v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.13193v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangyu Wu, Ying Xiong, Yufei Cui, Haolun Wu, Can Chen, Ye Yuan, Lianming Huang, Xue Liu, Tei-Wei Kuo, Nan Guan, Chun Jason Xue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated great success in various
fields, benefiting from their huge amount of parameters that store knowledge.
However, LLMs still suffer from several key issues, such as hallucination
problems, knowledge update issues, and lacking domain-specific expertise. The
appearance of retrieval-augmented generation (RAG), which leverages an external
knowledge database to augment LLMs, makes up those drawbacks of LLMs. This
paper reviews all significant techniques of RAG, especially in the retriever
and the retrieval fusions. Besides, tutorial codes are provided for
implementing the representative techniques in RAG. This paper further discusses
the RAG update, including RAG with/without knowledge update. Then, we introduce
RAG evaluation and benchmarking, as well as the application of RAG in
representative NLP tasks and industrial scenarios. Finally, this paper
discusses RAG's future directions and challenges for promoting this field's
development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Word Embeddings in the <span class="highlight-title">LLM</span> Era 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19607v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19607v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yash Mahajan, Matthew Freestone, Sathyanarayanan Aakur, Santu Karmaker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have recently shown remarkable advancement in
various NLP tasks. As such, a popular trend has emerged lately where NLP
researchers extract word/sentence/document embeddings from these large
decoder-only models and use them for various inference tasks with promising
results. However, it is still unclear whether the performance improvement of
LLM-induced embeddings is merely because of scale or whether underlying
embeddings they produce significantly differ from classical encoding models
like Word2Vec, GloVe, Sentence-BERT (SBERT) or Universal Sentence Encoder
(USE). This is the central question we investigate in the paper by
systematically comparing classical decontextualized and contextualized word
embeddings with the same for LLM-induced embeddings. Our results show that LLMs
cluster semantically related words more tightly and perform better on analogy
tasks in decontextualized settings. However, in contextualized settings,
classical models like SimCSE often outperform LLMs in sentence-level similarity
assessment tasks, highlighting their continued relevance for fine-grained
semantics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work was intended as a replacement of the older version,
  arXiv:2402.11094, and any subsequent updates will appear there</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Word Embeddings in the <span class="highlight-title">LLM</span> Era 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.11094v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.11094v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yash Mahajan, Matthew Freestone, Naman Bansal, Sathyanarayanan Aakur, Shubhra Kanti Karmaker Santu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have recently shown remarkable advancement in
various NLP tasks. As such, a popular trend has emerged lately where NLP
researchers extract word/sentence/document embeddings from these large
decoder-only models and use them for various inference tasks with promising
results. However, it is still unclear whether the performance improvement of
LLM-induced embeddings is merely because of scale or whether underlying
embeddings they produce significantly differ from classical encoding models
like Word2Vec, GloVe, Sentence-BERT (SBERT) or Universal Sentence Encoder
(USE). This is the central question we investigate in the paper by
systematically comparing classical decontextualized and contextualized word
embeddings with the same for LLM-induced embeddings. Our results show that LLMs
cluster semantically related words more tightly and perform better on analogy
tasks in decontextualized settings. However, in contextualized settings,
classical models like SimCSE often outperform LLMs in sentence-level similarity
assessment tasks, highlighting their continued relevance for fine-grained
semantics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is an updated version of the older version: 2402.11094. We
  accidentally submitted this article as a new submission (2502.19607), which
  we have requested to withdraw. This version has 30 pages and 22 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TeaMs-<span class="highlight-title">RL</span>: Teaching <span class="highlight-title">LLM</span>s to Generate Better Instruction <span class="highlight-title">Dataset</span>s via
  Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.08694v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.08694v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shangding Gu, Alois Knoll, Ming Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of Large Language Models (LLMs) often confronts challenges
stemming from the heavy reliance on human annotators in the reinforcement
learning with human feedback (RLHF) framework, or the frequent and costly
external queries tied to the self-instruct paradigm. In this work, we pivot to
Reinforcement Learning (RL) -- but with a twist. Diverging from the typical
RLHF, which refines LLMs following instruction data training, we use RL to
directly generate the foundational instruction dataset that alone suffices for
fine-tuning. Our method, TeaMs-RL, uses a suite of textual operations and
rules, prioritizing the diversification of training datasets. It facilitates
the generation of high-quality data without excessive reliance on external
advanced models, paving the way for a single fine-tuning step and negating the
need for subsequent RLHF stages. Our findings highlight key advantages of our
approach: reduced need for human involvement and fewer model queries (only
5.73% of the strong baseline's total), along with enhanced capabilities of LLMs
in crafting and comprehending complex instructions compared to strong
baselines, and substantially improved model privacy protection. Code is
available at the link: https://github.com/SafeRL-Lab/TeaMs-RL
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Instructional Segment Embedding: Improving <span class="highlight-title">LLM</span> Safety with Instruction
  Hierarchy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.09102v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.09102v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tong Wu, Shujian Zhang, Kaiqiang Song, Silei Xu, Sanqiang Zhao, Ravi Agrawal, Sathish Reddy Indurthi, Chong Xiang, Prateek Mittal, Wenxuan Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are susceptible to security and safety threats,
such as prompt injection, prompt extraction, and harmful requests. One major
cause of these vulnerabilities is the lack of an instruction hierarchy. Modern
LLM architectures treat all inputs equally, failing to distinguish between and
prioritize various types of instructions, such as system messages, user
prompts, and data. As a result, lower-priority user prompts may override more
critical system instructions, including safety protocols. Existing approaches
to achieving instruction hierarchy, such as delimiters and instruction-based
training, do not address this issue at the architectural level. We introduce
the Instructional Segment Embedding (ISE) technique, inspired by BERT, to
modern large language models, which embeds instruction priority information
directly into the model. This approach enables models to explicitly
differentiate and prioritize various instruction types, significantly improving
safety against malicious prompts that attempt to override priority rules. Our
experiments on the Structured Query and Instruction Hierarchy benchmarks
demonstrate an average robust accuracy increase of up to 15.75% and 18.68%,
respectively. Furthermore, we observe an improvement in instruction-following
capability of up to 4.1% evaluated on AlpacaEval. Overall, our approach offers
a promising direction for enhancing the safety and effectiveness of LLM
architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-correction is Not An Innate Capability in Large Language Models: A
  Case Study of Moral Self-correction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.20513v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.20513v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zimo Qi, Guangliang Liu, Kristen Marie Johnson, Lu Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Though there has been intensive attention to the self-correction capability
of Large Language Models (LLMs), conclusions regarding its effectiveness remain
varied. In this paper, we investigate a fundamental question: is moral
self-correction an innate capability in LLMs? To explore this, we conduct (1) a
mechanistic analysis of how key components of self-correction, such as
Chain-of-Thought (CoT) reasoning and external feedback, interact to enable
moral self-correction; and (2) a behavioral analysis of LLMs' ability to
distinguish between desired and undesired outputs, introducing a
self-distinguish framework. Our mechanistic analysis reveals that LLMs struggle
to effectively leverage helpful feedback, and conflicts can arise between
feedback and CoT reasoning. These limitations suggest that LLMs fail to
identify useful contextual information, instead prioritizing their own internal
knowledge. Additionally, our behavioral analysis indicates that LLMs struggle
to differentiate among their own outputs. Based on these empirical findings
across two analytical dimensions, mechanism and behavior, we argue that moral
self-correction is not an innate capability of LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dual Process Learning: Controlling Use of In-Context vs. In-Weights
  Strategies with Weight Forgetting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00053v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00053v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suraj Anand, Michael A. Lepori, Jack Merullo, Ellie Pavlick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language models have the ability to perform in-context learning (ICL),
allowing them to flexibly adapt their behavior based on context. This contrasts
with in-weights learning (IWL), where memorized information is encoded in model
parameters after iterated observations of data. An ideal model should be able
to flexibly deploy both of these abilities. Despite their apparent ability to
learn in-context, language models are known to struggle when faced with unseen
or rarely seen tokens (Land & Bartolo, 2024). Hence, we study
$\textbf{structural in-context learning}$, which we define as the ability of a
model to execute in-context learning on arbitrary novel tokens -- so called
because the model must generalize on the basis of e.g. sentence structure or
task structure, rather than content encoded in token embeddings. We study
structural in-context algorithms on both synthetic and naturalistic tasks using
toy models, masked language models, and autoregressive language models. We find
that structural ICL appears before quickly disappearing early in LM
pretraining. While it has been shown that ICL can diminish during training
(Singh et al., 2023), we find that prior work does not account for structural
ICL. Building on Chen et al. (2024) 's active forgetting method, we introduce
pretraining and finetuning methods that can modulate the preference for
structural ICL and IWL. Importantly, this allows us to induce a $\textit{dual
process strategy}$ where in-context and in-weights solutions coexist within a
single model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Complex Reasoning over Knowledge Graph with Logic-Aware
  Curriculum Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.01649v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.01649v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianle Xia, Liang Ding, Guojia Wan, Yibing Zhan, Bo Du, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Answering complex queries over incomplete knowledge graphs (KGs) is a
challenging job. Most previous works have focused on learning entity/relation
embeddings and simulating first-order logic operators with various neural
networks. However, they are bottlenecked by the inability to share world
knowledge to improve logical reasoning, thus resulting in suboptimal
performance. In this paper, we propose a complex reasoning schema over KG upon
large language models (LLMs), containing a curriculum-based logical-aware
instruction tuning framework, named LACT. Specifically, we augment the
arbitrary first-order logical queries via binary tree decomposition, to
stimulate the reasoning capability of LLMs. To address the difficulty gap among
different types of complex queries, we design a simple and flexible logic-aware
curriculum learning framework. Experiments across widely used datasets
demonstrate that LACT has substantial improvements~(brings an average +5.5% MRR
score) over advanced methods, achieving the new state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cheems: A Practical Guidance for Building and Evaluating Chinese Reward
  Models from Scratch 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17173v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17173v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xueru Wen, Jie Lou, Zichao Li, Yaojie Lu, Xing Yu, Yuqiu Ji, Guohai Xu, Hongyu Lin, Ben He, Xianpei Han, Le Sun, Debing Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reward models (RMs) are crucial for aligning large language models (LLMs)
with human preferences. However, most RM research is centered on English and
relies heavily on synthetic resources, which leads to limited and less reliable
datasets and benchmarks for Chinese. To address this gap, we introduce
CheemsBench, a fully human-annotated RM evaluation benchmark within Chinese
contexts, and CheemsPreference, a large-scale and diverse preference dataset
annotated through human-machine collaboration to support Chinese RM training.
We systematically evaluate open-source discriminative and generative RMs on
CheemsBench and observe significant limitations in their ability to capture
human preferences in Chinese scenarios. Additionally, based on
CheemsPreference, we construct an RM that achieves state-of-the-art performance
on CheemsBench, demonstrating the necessity of human supervision in RM
training. Our findings reveal that scaled AI-generated data struggles to fully
capture human preferences, emphasizing the importance of high-quality human
supervision in RM development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ JudgeLM: Fine-tuned Large Language Models are Scalable Judges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2310.17631v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2310.17631v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lianghui Zhu, Xinggang Wang, Xinlong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating Large Language Models (LLMs) in open-ended scenarios is
challenging because existing benchmarks and metrics can not measure them
comprehensively. To address this problem, we propose to fine-tune LLMs as
scalable judges (JudgeLM) to evaluate LLMs efficiently and effectively in
open-ended benchmarks. We first propose a comprehensive, large-scale,
high-quality dataset containing task seeds, LLMs-generated answers, and
GPT-4-generated judgments for fine-tuning high-performance judges, as well as a
new benchmark for evaluating the judges. We train JudgeLM at different scales
from 7B, 13B, to 33B parameters, and conduct a systematic analysis of its
capabilities and behaviors. We then analyze the key biases in fine-tuning LLM
as a judge and consider them as position bias, knowledge bias, and format bias.
To address these issues, JudgeLM introduces a bag of techniques including swap
augmentation, reference support, and reference drop, which clearly enhance the
judge's performance. JudgeLM obtains the state-of-the-art judge performance on
both the existing PandaLM benchmark and our proposed new benchmark. Our JudgeLM
is efficient and the JudgeLM-7B only needs 3 minutes to judge 5K samples with 8
A100 GPUs. JudgeLM obtains high agreement with the teacher judge, achieving an
agreement exceeding 90% that even surpasses human-to-human agreement. JudgeLM
also demonstrates extended capabilities in being judges of the single answer,
multimodal models, multiple answers, multi-turn chat, etc. Code is available at
https://github.com/baaivision/JudgeLM.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>JudgeLM is accepted by ICLR2025. Code is available at
  https://github.com/baaivision/JudgeLM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Limitations of <span class="highlight-title">LLM</span> as Annotator for Low Resource Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.17637v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.17637v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Suramya Jadhav, Abhay Shanbhag, Amogh Thakurdesai, Ridhima Sinare, Raviraj Joshi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-resource languages face significant challenges due to the lack of
sufficient linguistic data, resources, and tools for tasks such as supervised
learning, annotation, and classification. This shortage hinders the development
of accurate models and datasets, making it difficult to perform critical NLP
tasks like sentiment analysis or hate speech detection. To bridge this gap,
Large Language Models (LLMs) present an opportunity for potential annotators,
capable of generating datasets and resources for these underrepresented
languages. In this paper, we focus on Marathi, a low-resource language, and
evaluate the performance of both closed-source and open-source LLMs as
annotators, while also comparing these results with fine-tuned BERT models. We
assess models such as GPT-4o and Gemini 1.0 Pro, Gemma 2 (2B and 9B), and Llama
3.1 (8B and 405B) on classification tasks including sentiment analysis, news
classification, and hate speech detection. Our findings reveal that while LLMs
excel in annotation tasks for high-resource languages like English, they still
fall short when applied to Marathi. Even advanced models like GPT-4o and Llama
3.1 405B underperform compared to fine-tuned BERT-based baselines, with GPT-4o
and Llama 3.1 405B trailing fine-tuned BERT by accuracy margins of 10.2% and
14.1%, respectively. This highlights the limitations of LLMs as annotators for
low-resource languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoT2Align: Cross-Chain of Thought Distillation via Optimal Transport
  Alignment for Language Models with Different Tokenizers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16806v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16806v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anh Duc Le, Tu Vu, Nam Le Hai, Nguyen Thi Ngoc Diep, Linh Ngo Van, Trung Le, Thien Huu Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) achieve state-of-the-art performance across
various NLP tasks but face deployment challenges due to high computational
costs and memory constraints. Knowledge distillation (KD) is a promising
solution, transferring knowledge from large teacher models to smaller student
models. However, existing KD methods often assume shared vocabularies and
tokenizers, limiting their flexibility. While approaches like Universal Logit
Distillation (ULD) and Dual-Space Knowledge Distillation (DSKD) address
vocabulary mismatches, they overlook the critical \textbf{reasoning-aware
distillation} aspect. To bridge this gap, we propose CoT2Align a universal KD
framework that integrates Chain-of-Thought (CoT) augmentation and introduces
Cross-CoT Alignment to enhance reasoning transfer. Additionally, we extend
Optimal Transport beyond token-wise alignment to a sequence-level and
layer-wise alignment approach that adapts to varying sequence lengths while
preserving contextual integrity. Comprehensive experiments demonstrate that
CoT2Align outperforms existing KD methods across different vocabulary settings,
improving reasoning capabilities and robustness in domain-specific tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QAEA-DR: A Unified Text Augmentation Framework for Dense Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.20207v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.20207v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongming Tan, Shaoxiong Zhan, Hai Lin, Hai-Tao Zheng, Wai Kin Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In dense retrieval, embedding long texts into dense vectors can result in
information loss, leading to inaccurate query-text matching. Additionally,
low-quality texts with excessive noise or sparse key information are unlikely
to align well with relevant queries. Recent studies mainly focus on improving
the sentence embedding model or retrieval process. In this work, we introduce a
novel text augmentation framework for dense retrieval. This framework
transforms raw documents into information-dense text formats, which supplement
the original texts to effectively address the aforementioned issues without
modifying embedding or retrieval methodologies. Two text representations are
generated via large language models (LLMs) zero-shot prompting: question-answer
pairs and element-driven events. We term this approach QAEA-DR: unifying
question-answer generation and event extraction in a text augmentation
framework for dense retrieval. To further enhance the quality of generated
texts, a scoring-based evaluation and regeneration mechanism is introduced in
LLM prompting. Our QAEA-DR model has a positive impact on dense retrieval,
supported by both theoretical analysis and empirical experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From RAGs to riches: Utilizing large language models to write documents
  for clinical trials 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.16406v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.16406v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nigel Markey, Ilyass El-Mansouri, Gaetan Rensonnet, Casper van Langen, Christoph Meier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This manuscript has now been published: - Link to article on journal website:
https://journals.sagepub.com/doi/10.1177/17407745251320806 - Pubmed link:
https://pubmed.ncbi.nlm.nih.gov/40013826/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LLaMA-Omni: Seamless Speech Interaction with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2409.06666v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2409.06666v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingkai Fang, Shoutao Guo, Yan Zhou, Zhengrui Ma, Shaolei Zhang, Yang Feng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models like GPT-4o enable real-time interaction with large language models
(LLMs) through speech, significantly enhancing user experience compared to
traditional text-based interaction. However, there is still a lack of
exploration on how to build speech interaction models based on open-source
LLMs. To address this, we propose LLaMA-Omni, a novel model architecture
designed for low-latency and high-quality speech interaction with LLMs.
LLaMA-Omni integrates a pretrained speech encoder, a speech adaptor, an LLM,
and a streaming speech decoder. It eliminates the need for speech
transcription, and can simultaneously generate text and speech responses
directly from speech instructions with extremely low latency. We build our
model based on the latest Llama-3.1-8B-Instruct model. To align the model with
speech interaction scenarios, we construct a dataset named InstructS2S-200K,
which includes 200K speech instructions and corresponding speech responses.
Experimental results show that compared to previous speech-language models,
LLaMA-Omni provides better responses in both content and style, with a response
latency as low as 226ms. Additionally, training LLaMA-Omni takes less than 3
days on just 4 GPUs, paving the way for the efficient development of
speech-language models in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FCoReBench: Can Large Language Models Solve Challenging First-Order
  Combinatorial Reasoning Problems? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2402.02611v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2402.02611v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chinmay Mittal, Krishna Kartik,  Mausam, Parag Singla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Can the large language models (LLMs) solve challenging first-order
combinatorial reasoning problems such as graph coloring, knapsack, and
cryptarithmetic? By first-order, we mean these problems can be instantiated
with potentially an infinite number of problem instances of varying sizes. They
are also challenging being NP-hard and requiring several reasoning steps to
reach a solution. While existing work has focused on coming up with datasets
with hard benchmarks, there is limited work which exploits the first-order
nature of the problem structure. To address this challenge, we present
FCoReBench, a dataset of 40 such challenging problems, along with scripts to
generate problem instances of varying sizes and automatically verify and
generate their solutions. We first observe that LLMs, even when aided by
symbolic solvers, perform rather poorly on our dataset, being unable to
leverage the underlying structure of these problems. We specifically observe a
drop in performance with increasing problem size. In response, we propose a new
approach, SymPro-LM, which combines LLMs with both symbolic solvers and program
interpreters, along with feedback from a few solved examples, to achieve huge
performance gains. Our proposed approach is robust to changes in the problem
size, and has the unique characteristic of not requiring any LLM call during
inference time, unlike earlier approaches. As an additional experiment, we also
demonstrate SymPro-LM's effectiveness on other logical reasoning benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Survey</span>ing the Landscape of Image Captioning Evaluation: A Comprehensive
  Taxonomy, Trends and Metrics Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04909v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04909v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Uri Berger, Gabriel Stanovsky, Omri Abend, Lea Frermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of image captioning has recently been gaining popularity, and with
it the complex task of evaluating the quality of image captioning models. In
this work, we present the first survey and taxonomy of over 70 different image
captioning metrics and their usage in hundreds of papers, specifically designed
to help users select the most suitable metric for their needs. We find that
despite the diversity of proposed metrics, the vast majority of studies rely on
only five popular metrics, which we show to be weakly correlated with human
ratings. We hypothesize that combining a diverse set of metrics can enhance
correlation with human ratings. As an initial step, we demonstrate that a
linear regression-based ensemble method, which we call EnsembEval, trained on
one human ratings dataset, achieves improved correlation across five additional
datasets, showing there is a lot of room for improvement by leveraging a
diverse set of metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Who Wrote This? The Key to Zero-Shot <span class="highlight-title">LLM</span>-Generated Text Detection Is
  GECScore 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.04286v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.04286v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junchao Wu, Runzhe Zhan, Derek F. Wong, Shu Yang, Xuebo Liu, Lidia S. Chao, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The efficacy of detectors for texts generated by large language models (LLMs)
substantially depends on the availability of large-scale training data.
However, white-box zero-shot detectors, which require no such data, are limited
by the accessibility of the source model of the LLM-generated text. In this
paper, we propose a simple yet effective black-box zero-shot detection approach
based on the observation that, from the perspective of LLMs, human-written
texts typically contain more grammatical errors than LLM-generated texts. This
approach involves calculating the Grammar Error Correction Score (GECScore) for
the given text to differentiate between human-written and LLM-generated text.
Experimental results show that our method outperforms current state-of-the-art
(SOTA) zero-shot and supervised methods, achieving an average AUROC of 98.62%
across XSum and Writing Prompts dataset. Additionally, our approach
demonstrates strong reliability in the wild, exhibiting robust generalization
and resistance to paraphrasing attacks. Data and code are available at:
https://github.com/NLP2CT/GECScore.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLING 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ URO-Bench: A Comprehensive Benchmark for End-to-End Spoken Dialogue
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17810v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17810v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiqi Yan, Xiquan Li, Wenxi Chen, Zhikang Niu, Chen Yang, Ziyang Ma, Kai Yu, Xie Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, with advances in large language models (LLMs), end-to-end
spoken dialogue models (SDMs) have made significant strides. Compared to
text-based LLMs, the evaluation of SDMs needs to take speech-related aspects
into account, such as paralinguistic information and speech quality. However,
there is still a lack of comprehensive evaluations for SDMs in speech-to-speech
(S2S) scenarios. To address this gap, we propose URO-Bench, an extensive
benchmark for SDMs. Notably, URO-Bench is the first S2S benchmark that covers
evaluations about multilingualism, multi-round dialogues, and paralinguistics.
Our benchmark is divided into two difficulty levels: basic track and pro track,
consisting of 16 and 20 datasets respectively, evaluating the model's abilities
in Understanding, Reasoning, and Oral conversation. Evaluations on our proposed
benchmark reveal that current open-source SDMs perform rather well in daily QA
tasks, but lag behind their backbone LLMs in terms of instruction-following
ability and also suffer from catastrophic forgetting. Their performance in
advanced evaluations of paralinguistic information and audio understanding
remains subpar, highlighting the need for further research in this direction.
We hope that URO-Bench can effectively facilitate the development of spoken
dialogue models by providing a multifaceted evaluation of existing models and
helping to track progress in this area.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Multi</span>-objective Representation for Numbers in Clinical Narratives: A
  CamemBERT-Bio-Based Alternative to Large-Scale <span class="highlight-title">LLM</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.18448v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.18448v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boammani Aser Lompo, Thanh-Dung Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The processing of numerical values is a rapidly developing area in the field
of Language Models (LLMs). Despite numerous advancements achieved by previous
research, significant challenges persist, particularly within the healthcare
domain. This paper investigates the limitations of Transformer models in
understanding numerical values. \textit{Objective:} this research aims to
categorize numerical values extracted from medical documents into eight
specific physiological categories using CamemBERT-bio. \textit{Methods:} In a
context where scalable methods and Large Language Models (LLMs) are emphasized,
we explore lifting the limitations of transformer-based models. We examine two
strategies: fine-tuning CamemBERT-bio on a small medical dataset, integrating
Label Embedding for Self-Attention (LESA), and combining LESA with additional
enhancement techniques such as Xval. Given that CamemBERT-bio is already
pre-trained on a large medical dataset, the first approach aims to update its
encoder with the newly added label embeddings technique. In contrast, the
second approach seeks to develop multiple representations of numbers
(contextual and magnitude-based) to achieve more robust number embeddings.
\textit{Results:} As anticipated, fine-tuning the standard CamemBERT-bio on our
small medical dataset did not improve F1 scores. However, significant
improvements were observed with CamemBERT-bio + LESA, resulting in an over 13\%
increase. Similar enhancements were noted when combining LESA with Xval,
outperforming conventional methods and giving comparable results to GPT-4
\textit{Conclusions and Novelty:} This study introduces two innovative
techniques for handling numerical data, which are also applicable to other
modalities. We illustrate how these techniques can improve the performance of
Transformer-based models, achieving more reliable classification results even
with small datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under the revision. arXiv admin note: substantial text overlap with
  arXiv:2404.10171</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QA-Calibration of Language Model Confidence Scores 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.06615v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.06615v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Putra Manggala, Atalanti Mastakouri, Elke Kirschbaum, Shiva Prasad Kasiviswanathan, Aaditya Ramdas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To use generative question-and-answering (QA) systems for decision-making and
in any critical application, these systems need to provide well-calibrated
confidence scores that reflect the correctness of their answers. Existing
calibration methods aim to ensure that the confidence score is, *on average*,
indicative of the likelihood that the answer is correct. We argue, however,
that this standard (average-case) notion of calibration is difficult to
interpret for decision-making in generative QA. To address this, we generalize
the standard notion of average calibration and introduce QA-calibration, which
ensures calibration holds across different question-and-answer groups. We then
propose discretized posthoc calibration schemes for achieving QA-calibration.
We establish distribution-free guarantees on the performance of this method and
validate our method on confidence scores returned by elicitation prompts across
multiple QA benchmarks and large language models (LLMs).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MMKE-Bench: A <span class="highlight-title">Multi</span>modal Editing Benchmark for Diverse Visual Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.19870v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.19870v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuntao Du, Kailin Jiang, Zhi Gao, Chenrui Shi, Zilong Zheng, Siyuan Qi, Qing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge editing techniques have emerged as essential tools for updating the
factual knowledge of large language models (LLMs) and multimodal models (LMMs),
allowing them to correct outdated or inaccurate information without retraining
from scratch. However, existing benchmarks for multimodal knowledge editing
primarily focus on entity-level knowledge represented as simple triplets, which
fail to capture the complexity of real-world multimodal information. To address
this issue, we introduce MMKE-Bench, a comprehensive MultiModal Knowledge
Editing Benchmark, designed to evaluate the ability of LMMs to edit diverse
visual knowledge in real-world scenarios. MMKE-Bench addresses these
limitations by incorporating three types of editing tasks: visual entity
editing, visual semantic editing, and user-specific editing. Besides,
MMKE-Bench uses free-form natural language to represent and edit knowledge,
offering a more flexible and effective format. The benchmark consists of 2,940
pieces of knowledge and 8,363 images across 33 broad categories, with
evaluation questions automatically generated and human-verified. We assess five
state-of-the-art knowledge editing methods on three prominent LMMs, revealing
that no method excels across all criteria, and that visual and user-specific
edits are particularly challenging. MMKE-Bench sets a new standard for
evaluating the robustness of multimodal knowledge editing techniques, driving
progress in this rapidly evolving field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accept to ICLR2025. Project Page: https://mmke-bench-iclr.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LongPO: Long Context Self-Evolution of Large Language Models through
  Short-to-Long Preference Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.13922v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.13922v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guanzheng Chen, Xin Li, Michael Qizhe Shieh, Lidong Bing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated remarkable capabilities
through pretraining and alignment. However, superior short-context LLMs may
underperform in long-context scenarios due to insufficient long-context
alignment. This alignment process remains challenging due to the impracticality
of human annotation for extended contexts and the difficulty in balancing
short- and long-context performance. To address these challenges, we introduce
LongPO, that enables short-context LLMs to self-evolve to excel on long-context
tasks by internally transferring short-context capabilities. LongPO harnesses
LLMs to learn from self-generated short-to-long preference data, comprising
paired responses generated for identical instructions with long-context inputs
and their compressed short-context counterparts, respectively. This preference
reveals capabilities and potentials of LLMs cultivated during short-context
alignment that may be diminished in under-aligned long-context scenarios.
Additionally, LongPO incorporates a short-to-long KL constraint to mitigate
short-context performance decline during long-context alignment. When applied
to Mistral-7B-Instruct-v0.2 from 128K to 512K context lengths, LongPO fully
retains short-context performance and largely outperforms naive SFT and DPO in
both long- and short-context tasks. Specifically, LongPO-trained models can
achieve results on long-context benchmarks comparable to, or even surpassing,
those of superior LLMs (e.g., GPT-4-128K) that involve extensive long-context
annotation and larger parameter scales. Our code is available at
https://github.com/DAMO-NLP-SG/LongPO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Post-training an <span class="highlight-title">LLM</span> for RAG? Train on Self-Generated Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.10596v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.10596v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Finlayson, Ilia Kulikov, Daniel M. Bikel, Barlas Oguz, Xilun Chen, Aasish Pappu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) often struggle with knowledge intensive NLP
tasks, such as answering "Who won the latest World Cup?" because the knowledge
they learn during training may be insufficient or outdated. Conditioning
generation on retrieved documents -- a technique known as retrieval augmented
generation (RAG) -- mitigates these shortcomings by allowing the model to
leverage in-context information. Practitioners can improve LLM RAG performance
by fine-tuning on retrieval-augmented instructions, but must beware that this
can cause undesirable model behaviors like hallucinations. We attribute this
degradation to the fact that the training data is likely to be
out-of-distribution for the model and may suffer from quality issues, such as
misalignment between retrievals and target responses (since retrievals are
frequently added post-hoc). We propose a recipe for training RAG-enabled LLMs
using self-generated demonstrations, thereby avoiding training on
out-of-distribution text and integrating retrievals into the LLM responses. We
evaluate our method on knowledge intensive question answering (QA) tasks and
show that our method teaches LLMs to properly handle in-context retrievals and
abstain from questions it will likely get wrong. Compared to conventional RA-IT
methods, our method prevents model degradation in non-RAG settings while
exhibiting superior QA performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prompting Medical Large Vision-Language Models to Diagnose Pathologies
  by Visual Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.21368v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.21368v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danfeng Guo, Demetri Terzopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Vision-Language Models (LVLMs) have achieved significant success in
recent years, and they have been extended to the medical domain. Although
demonstrating satisfactory performance on medical Visual Question Answering
(VQA) tasks, Medical LVLMs (MLVLMs) suffer from the hallucination problem,
which makes them fail to diagnose complex pathologies. Moreover, they readily
fail to learn minority pathologies due to imbalanced training data. We propose
two prompting strategies for MLVLMs that reduce hallucination and improve VQA
performance. In the first strategy, we provide a detailed explanation of the
queried pathology. In the second strategy, we fine-tune a cheap, weak learner
to achieve high performance on a specific metric, and textually provide its
judgment to the MLVLM. Tested on the MIMIC-CXR-JPG and Chexpert datasets, our
methods significantly improve the diagnostic F1 score, with the highest
increase being 0.27. We also demonstrate that our prompting strategies can be
extended to general LVLM domains. Based on POPE metrics, it effectively
suppresses the false negative predictions of existing LVLMs and improves Recall
by approximately 0.07.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CORAL: Learning Consistent Representations across <span class="highlight-title">Multi</span>-step Training
  with Lighter Speculative Drafter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.16880v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.16880v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yepeng Weng, Dianwen Mei, Huishi Qiu, Xujie Chen, Li Liu, Jiang Tian, Zhongchao Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speculative decoding is a powerful technique that accelerates Large Language
Model (LLM) inference by leveraging a lightweight speculative draft model.
However, existing designs suffers in performance due to misalignment between
training and inference. Recent methods have tried to solve this issue by
adopting a multi-step training strategy, but the complex inputs of different
training steps make it harder for the draft model to converge. To address this,
we propose CORAL, a novel framework that improves both accuracy and efficiency
in speculative drafting. CORAL introduces Cross-Step Representation Alignment,
a method that enhances consistency across multiple training steps,
significantly improving speculative drafting performance. Additionally, we
identify the LM head as a major bottleneck in the inference speed of the draft
model. We introduce a weight-grouping mechanism that selectively activates a
subset of LM head parameters during inference, substantially reducing the
latency of the draft model. We evaluate CORAL on three LLM families and three
benchmark datasets, achieving speedup ratios of 2.50x-4.07x, outperforming
state-of-the-art methods such as EAGLE-2 and HASS. Our results demonstrate that
CORAL effectively mitigates training-inference misalignment and delivers
significant speedup for modern LLMs with large vocabularies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ s1: Simple test-time scaling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.19393v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.19393v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, Tatsunori Hashimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-time scaling is a promising new approach to language modeling that uses
extra test-time compute to improve performance. Recently, OpenAI's o1 model
showed this capability but did not publicly share its methodology, leading to
many replication efforts. We seek the simplest approach to achieve test-time
scaling and strong reasoning performance. First, we curate a small dataset s1K
of 1,000 questions paired with reasoning traces relying on three criteria we
validate through ablations: difficulty, diversity, and quality. Second, we
develop budget forcing to control test-time compute by forcefully terminating
the model's thinking process or lengthening it by appending "Wait" multiple
times to the model's generation when it tries to end. This can lead the model
to double-check its answer, often fixing incorrect reasoning steps. After
supervised finetuning the Qwen2.5-32B-Instruct language model on s1K and
equipping it with budget forcing, our model s1-32B exceeds o1-preview on
competition math questions by up to 27% (MATH and AIME24). Further, scaling
s1-32B with budget forcing allows extrapolating beyond its performance without
test-time intervention: from 50% to 57% on AIME24. Our model, data, and code
are open-source at https://github.com/simplescaling/s1
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>46 pages (9 main), 10 figures, 15 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LABOR-<span class="highlight-title">LLM</span>: Language-Based Occupational Representations with Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.17972v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.17972v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Susan Athey, Herman Brunborg, Tianyu Du, Ayush Kanodia, Keyon Vafa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vafa et al. (2024) introduced a transformer-based econometric model, CAREER,
that predicts a worker's next job as a function of career history (an
"occupation model"). CAREER was initially estimated ("pre-trained") using a
large, unrepresentative resume dataset, which served as a "foundation model,"
and parameter estimation was continued ("fine-tuned") using data from a
representative survey. CAREER had better predictive performance than
benchmarks. This paper considers an alternative where the resume-based
foundation model is replaced by a large language model (LLM). We convert
tabular data from the survey into text files that resemble resumes and
fine-tune the LLMs using these text files with the objective to predict the
next token (word). The resulting fine-tuned LLM is used as an input to an
occupation model. Its predictive performance surpasses all prior models. We
demonstrate the value of fine-tuning and further show that by adding more
career data from a different population, fine-tuning smaller LLMs surpasses the
performance of fine-tuning larger models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Guided Skill Discovery 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.06615v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.06615v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seungeun Rho, Laura Smith, Tianyu Li, Sergey Levine, Xue Bin Peng, Sehoon Ha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skill discovery methods enable agents to learn diverse emergent behaviors
without explicit rewards. To make learned skills useful for unknown downstream
tasks, obtaining a semantically diverse repertoire of skills is essential.
While some approaches introduce a discriminator to distinguish skills and
others aim to increase state coverage, no existing work directly addresses the
"semantic diversity" of skills. We hypothesize that leveraging the semantic
knowledge of large language models (LLMs) can lead us to improve semantic
diversity of resulting behaviors. In this sense, we introduce Language Guided
Skill Discovery (LGSD), a skill discovery framework that aims to directly
maximize the semantic diversity between skills. LGSD takes user prompts as
input and outputs a set of semantically distinctive skills. The prompts serve
as a means to constrain the search space into a semantically desired subspace,
and the generated LLM outputs guide the agent to visit semantically diverse
states within the subspace. We demonstrate that LGSD enables legged robots to
visit different user-intended areas on a plane by simply changing the prompt.
Furthermore, we show that language guidance aids in discovering more diverse
skills compared to five existing skill discovery methods in robot-arm
manipulation environments. Lastly, LGSD provides a simple way of utilizing
learned skills via natural language.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semantic Operators: A Declarative Model for Rich, AI-based Data
  Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.11418v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.11418v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liana Patel, Siddharth Jha, Melissa Pan, Harshit Gupta, Parth Asawa, Carlos Guestrin, Matei Zaharia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The semantic capabilities of large language models (LLMs) have the potential
to enable rich analytics and reasoning over vast knowledge corpora.
Unfortunately, existing systems either empirically optimize expensive
LLM-powered operations with no performance guarantees, or serve a limited set
of row-wise LLM operations, providing limited robustness, expressiveness and
usability. We introduce semantic operators, the first formalism for declarative
and general-purpose AI-based transformations based on natural language
specifications (e.g., filtering, sorting, joining or aggregating records using
natural language criteria). Each operator opens a rich space for execution
plans, similar to relational operators. Our model specifies the expected
behavior of each operator with a high-quality gold algorithm, and we develop an
optimization framework that reduces cost, while providing accuracy guarantees
with respect to a gold algorithm. Using this approach, we propose several novel
optimizations to accelerate semantic filtering, joining, group-by and top-k
operations by up to $1,000\times$. We implement semantic operators in the LOTUS
system and demonstrate LOTUS' effectiveness on real, bulk-semantic processing
applications, including fact-checking, biomedical multi-label classification,
search, and topic analysis. We show that the semantic operator model is
expressive, capturing state-of-the-art AI pipelines in a few operator calls,
and making it easy to express new pipelines that match or exceed quality of
recent LLM-based analytic systems by up to $170\%$, while offering accuracy
guarantees. Overall, LOTUS programs match or exceed the accuracy of
state-of-the-art AI pipelines for each task while running up to $3.6\times$
faster than the highest-quality baselines. LOTUS is publicly available at
https://github.com/lotus-data/lotus.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Useful is Continued Pre-Training for Generative Unsupervised Domain
  Adaptation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.17514v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.17514v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rheeya Uppaal, Yixuan Li, Junjie Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent breakthroughs in scale have enabled the emergence of powerful
generative language models, and the ability to fine-tune these models on
various tasks by casting them into prompts or instructions. In this landscape,
the problem of Unsupervised Domain Adaptation (UDA), or the problem of
leveraging knowledge from a labeled source domain to an unlabeled target
domain, has been left behind, with recent UDA methods still addressing
discriminative classification. In particular, two popular UDA approaches,
involving Continued Pre-Training (CPT) and learning domain invariant
representations, have been under-explored in the generative setting, signaling
a gap. In this work, we evaluate the utility of CPT for generative UDA. We
first perform an empirical evaluation to measure the trade-offs between CPT and
strong methods promoting domain invariance. We further evaluate how well the
benefits of CPT extend to different architectures, tuning methods and data
regimes. We then motivate the use of CPT by studying to what degree it benefits
classification performance on the target domain. Finally, we attempt to
understand the mechanism behind which CPT improves classification performance
on the unlabeled target domain. Our findings suggest that a implicitly learns
the downstream task while predicting masked words informative to that task. Our
work connects the body of UDA research with that of instruction tuning,
enabling an initial step towards a wider applicability of modern language
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to RepL4NLP at ACL 2024</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Model Editing as a Robust and Denoised variant of DPO: A Case Study on
  Toxicity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2405.13967v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2405.13967v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rheeya Uppaal, Apratim Dey, Yiting He, Yiqiao Zhong, Junjie Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent alignment algorithms such as direct preference optimization (DPO) have
been developed to improve the safety of large language models (LLMs) by
training these models to match human behaviors exemplified by preference data.
However, these methods are both computationally intensive and lacking in
controllability and transparency, inhibiting their widespread use. Furthermore,
these tuning-based methods require large-scale preference data for training and
are susceptible to noisy preference data. In this paper, we introduce a
tuning-free alignment alternative, ProFS (Projection Filter for Subspaces), and
demonstrate its effectiveness under the use case of toxicity reduction.
Grounded on theory from factor analysis, ProFS is a sample-efficient model
editing approach that identifies a toxic subspace in the model parameter space
and reduces model toxicity by projecting away the detected subspace. The toxic
subspace is identified by extracting preference data embeddings from the
language model, and removing non-toxic information from these embeddings. We
show that ProFS is more sample-efficient than DPO, further showcasing greater
robustness to noisy data. Finally, we attempt to connect tuning based alignment
with editing, by establishing both theoretical and empirical connections
between ProFS and DPO, showing that ProFS can be interpreted as a denoised
version of a single DPO step.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Building Trust in Mental Health Chatbots: Safety Metrics and <span class="highlight-title">LLM</span>-Based
  Evaluation Tools 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2408.04650v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2408.04650v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jung In Park, Mahyar Abbasian, Iman Azimi, Dawn T. Bounds, Angela Jun, Jaesu Han, Robert M. McCarron, Jessica Borelli, Parmida Safavi, Sanaz Mirbaha, Jia Li, Mona Mahmoudi, Carmen Wiedenhoeft, Amir M. Rahmani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Objective: This study aims to develop and validate an evaluation framework to
ensure the safety and reliability of mental health chatbots, which are
increasingly popular due to their accessibility, human-like interactions, and
context-aware support. Materials and Methods: We created an evaluation
framework with 100 benchmark questions and ideal responses, and five guideline
questions for chatbot responses. This framework, validated by mental health
experts, was tested on a GPT-3.5-turbo-based chatbot. Automated evaluation
methods explored included large language model (LLM)-based scoring, an agentic
approach using real-time data, and embedding models to compare chatbot
responses against ground truth standards. Results: The results highlight the
importance of guidelines and ground truth for improving LLM evaluation
accuracy. The agentic method, dynamically accessing reliable information,
demonstrated the best alignment with human assessments. Adherence to a
standardized, expert-validated framework significantly enhanced chatbot
response safety and reliability. Discussion: Our findings emphasize the need
for comprehensive, expert-tailored safety evaluation metrics for mental health
chatbots. While LLMs have significant potential, careful implementation is
necessary to mitigate risks. The superior performance of the agentic approach
underscores the importance of real-time data access in enhancing chatbot
reliability. Conclusion: The study validated an evaluation framework for mental
health chatbots, proving its effectiveness in improving safety and reliability.
Future work should extend evaluations to accuracy, bias, empathy, and privacy
to ensure holistic assessment and responsible integration into healthcare.
Standardized evaluations will build trust among users and professionals,
facilitating broader adoption and improved mental health support through
technology.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating Paraphrase Attacks on Machine-Text Detectors via Paraphrase
  Inversion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.21637v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.21637v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafael Rivera Soto, Barry Chen, Nicholas Andrews
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality paraphrases are easy to produce using instruction-tuned language
models or specialized paraphrasing models. Although this capability has a
variety of benign applications, paraphrasing
attacks$\unicode{x2013}$paraphrases applied to machine-generated
texts$\unicode{x2013}$are known to significantly degrade the performance of
machine-text detectors. This motivates us to consider the novel problem of
paraphrase inversion, where, given paraphrased text, the objective is to
recover an approximation of the original text. The closer the approximation is
to the original text, the better machine-text detectors will perform. We
propose an approach which frames the problem as translation from paraphrased
text back to the original text, which requires examples of texts and
corresponding paraphrases to train the inversion model. Fortunately, such
training data can easily be generated, given a corpus of original texts and one
or more paraphrasing models. We find that language models such as GPT-4 and
Llama-3 exhibit biases when paraphrasing which an inversion model can learn
with a modest amount of data. Perhaps surprisingly, we also find that such
models generalize well, including to paraphrase models unseen at training time.
Finally, we show that when combined with a paraphrased-text detector, our
inversion models provide an effective defense against paraphrasing attacks, and
overall our approach yields an average improvement of +22% AUROC across seven
machine-text detectors and three different domains.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Computer Vision and Pattern Recognition <span class="chip" style="font-size: 60%">10</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Joint Person Identity, Gender and Age Estimation from Hand Images using
  Deep <span class="highlight-title">Multi</span>-Task Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.15263v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.15263v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nathanael L. Baisa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a multi-task representation learning framework to
jointly estimate the identity, gender and age of individuals from their hand
images for the purpose of criminal investigations since the hand images are
often the only available information in cases of serious crime such as sexual
abuse. We investigate different up-to-date deep learning architectures and
compare their performance for joint estimation of identity, gender and age from
hand images of perpetrators of serious crime. To simplify the age prediction,
we create age groups for the age estimation. We make extensive evaluations and
comparisons of both convolution-based and transformer-based deep learning
architectures on a publicly available 11k hands dataset. Our experimental
analysis shows that it is possible to efficiently estimate not only identity
but also other attributes such as gender and age of suspects jointly from hand
images for criminal investigations, which is crucial in assisting international
police forces in the court to identify and convict abusers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2209.04821</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ViViDex: Learning Vision-based Dexterous Manipulation from Human Videos <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.15709v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.15709v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zerui Chen, Shizhe Chen, Etienne Arlaud, Ivan Laptev, Cordelia Schmid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we aim to learn a unified vision-based policy for
multi-fingered robot hands to manipulate a variety of objects in diverse poses.
Though prior work has shown benefits of using human videos for policy learning,
performance gains have been limited by the noise in estimated trajectories.
Moreover, reliance on privileged object information such as ground-truth object
states further limits the applicability in realistic scenarios. To address
these limitations, we propose a new framework ViViDex to improve vision-based
policy learning from human videos. It first uses reinforcement learning with
trajectory guided rewards to train state-based policies for each video,
obtaining both visually natural and physically plausible trajectories from the
video. We then rollout successful episodes from state-based policies and train
a unified visual policy without using any privileged information. We propose
coordinate transformation to further enhance the visual point cloud
representation, and compare behavior cloning and diffusion policy for the
visual policy training. Experiments both in simulation and on the real robot
demonstrate that ViViDex outperforms state-of-the-art approaches on three
dexterous manipulation tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICRA 2025. Project Page:
  https://zerchen.github.io/projects/vividex.html</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ VDT-Auto: End-to-end Autonomous Driving with VLM-Guided Diffusion
  Transformers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20108v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20108v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziang Guo, Konstantin Gubernatorov, Selamawit Asfaw, Zakhar Yagudin, Dzmitry Tsetserukou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In autonomous driving, dynamic environment and corner cases pose significant
challenges to the robustness of ego vehicle's decision-making. To address these
challenges, commencing with the representation of state-action mapping in the
end-to-end autonomous driving paradigm, we introduce a novel pipeline,
VDT-Auto. Leveraging the advancement of the state understanding of Visual
Language Model (VLM), incorporating with diffusion Transformer-based action
generation, our VDT-Auto parses the environment geometrically and contextually
for the conditioning of the diffusion process. Geometrically, we use a
bird's-eye view (BEV) encoder to extract feature grids from the surrounding
images. Contextually, the structured output of our fine-tuned VLM is processed
into textual embeddings and noisy paths. During our diffusion process, the
added noise for the forward process is sampled from the noisy path output of
the fine-tuned VLM, while the extracted BEV feature grids and embedded texts
condition the reverse process of our diffusion Transformers. Our VDT-Auto
achieved 0.52m on average L2 errors and 21% on average collision rate in the
nuScenes open-loop planning evaluation. Moreover, the real-world demonstration
exhibited prominent generalizability of our VDT-Auto. The code and dataset will
be released after acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HDKD: Hybrid Data-Efficient Knowledge Distillation Network for Medical
  Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.07516v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.07516v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar S. EL-Assiouti, Ghada Hamed, Dina Khattab, Hala M. Ebied
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision Transformers (ViTs) have achieved significant advancement in computer
vision tasks due to their powerful modeling capacity. However, their
performance notably degrades when trained with insufficient data due to lack of
inherent inductive biases. Distilling knowledge and inductive biases from a
Convolutional Neural Network (CNN) teacher has emerged as an effective strategy
for enhancing the generalization of ViTs on limited datasets. Previous
approaches to Knowledge Distillation (KD) have pursued two primary paths: some
focused solely on distilling the logit distribution from CNN teacher to ViT
student, neglecting the rich semantic information present in intermediate
features due to the structural differences between them. Others integrated
feature distillation along with logit distillation, yet this introduced
alignment operations that limits the amount of knowledge transferred due to
mismatched architectures and increased the computational overhead. To this end,
this paper presents Hybrid Data-efficient Knowledge Distillation (HDKD)
paradigm which employs a CNN teacher and a hybrid student. The choice of hybrid
student serves two main aspects. First, it leverages the strengths of both
convolutions and transformers while sharing the convolutional structure with
the teacher model. Second, this shared structure enables the direct application
of feature distillation without any information loss or additional
computational overhead. Additionally, we propose an efficient light-weight
convolutional block named Mobile Channel-Spatial Attention (MBCSA), which
serves as the primary convolutional block in both teacher and student models.
Extensive experiments on two medical public datasets showcase the superiority
of HDKD over other state-of-the-art models and its computational efficiency.
Source code at: https://github.com/omarsherif200/HDKD
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Text-to-Image Evaluation with Gecko: On Metrics, Prompts, and
  Human Ratings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2404.16820v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2404.16820v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olivia Wiles, Chuhan Zhang, Isabela Albuquerque, Ivana Kajić, Su Wang, Emanuele Bugliarello, Yasumasa Onoe, Chris Knutsen, Cyrus Rashtchian, Jordi Pont-Tuset, Aida Nematzadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While text-to-image (T2I) generative models have become ubiquitous, they do
not necessarily generate images that align with a given prompt. While previous
work has evaluated T2I alignment by proposing metrics, benchmarks, and
templates for collecting human judgements, the quality of these components is
not systematically measured. Human-rated prompt sets are generally small and
the reliability of the ratings -- and thereby the prompt set used to compare
models -- is not evaluated. We address this gap by performing an extensive
study evaluating auto-eval metrics and human templates. We provide three main
contributions: (1) We introduce a comprehensive skills-based benchmark that can
discriminate models across different human templates. This skills-based
benchmark categorises prompts into sub-skills, allowing a practitioner to
pinpoint not only which skills are challenging, but at what level of complexity
a skill becomes challenging. (2) We gather human ratings across four templates
and four T2I models for a total of >100K annotations. This allows us to
understand where differences arise due to inherent ambiguity in the prompt and
where they arise due to differences in metric and model quality. (3) Finally,
we introduce a new QA-based auto-eval metric that is better correlated with
human ratings than existing metrics for our new dataset, across different human
templates, and on TIFA160.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2025 (Spotlight)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FitDiff: Robust monocular 3D facial shape and reflectance estimation
  using Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2312.04465v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2312.04465v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stathis Galanakis, Alexandros Lattas, Stylianos Moschoglou, Stefanos Zafeiriou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable progress in 3D face reconstruction has resulted in high-detail
and photorealistic facial representations. Recently, Diffusion Models have
revolutionized the capabilities of generative methods by surpassing the
performance of GANs. In this work, we present FitDiff, a diffusion-based 3D
facial avatar generative model. Leveraging diffusion principles, our model
accurately generates relightable facial avatars, utilizing an identity
embedding extracted from an "in-the-wild" 2D facial image. The introduced
multi-modal diffusion model is the first to concurrently output facial
reflectance maps (diffuse and specular albedo and normals) and shapes,
showcasing great generalization capabilities. It is solely trained on an
annotated subset of a public facial dataset, paired with 3D reconstructions. We
revisit the typical 3D facial fitting approach by guiding a reverse diffusion
process using perceptual and face recognition losses. Being the first 3D LDM
conditioned on face recognition embeddings, FitDiff reconstructs relightable
human avatars, that can be used as-is in common rendering engines, starting
only from an unconstrained facial image, and achieving state-of-the-art
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Generalizable Vision-Language Robotic Manipulation: A Benchmark
  and <span class="highlight-title">LLM</span>-guided 3D Policy <span class="chip">ICRA 2025</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.01345v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.01345v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ricardo Garcia, Shizhe Chen, Cordelia Schmid
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalizing language-conditioned robotic policies to new tasks remains a
significant challenge, hampered by the lack of suitable simulation benchmarks.
In this paper, we address this gap by introducing GemBench, a novel benchmark
to assess generalization capabilities of vision-language robotic manipulation
policies. GemBench incorporates seven general action primitives and four levels
of generalization, spanning novel placements, rigid and articulated objects,
and complex long-horizon tasks. We evaluate state-of-the-art approaches on
GemBench and also introduce a new method. Our approach 3D-LOTUS leverages rich
3D information for action prediction conditioned on language. While 3D-LOTUS
excels in both efficiency and performance on seen tasks, it struggles with
novel tasks. To address this, we present 3D-LOTUS++, a framework that
integrates 3D-LOTUS's motion planning capabilities with the task planning
capabilities of LLMs and the object grounding accuracy of VLMs. 3D-LOTUS++
achieves state-of-the-art performance on novel tasks of GemBench, setting a new
standard for generalization in robotic manipulation. The benchmark, codes and
trained models are available at
https://www.di.ens.fr/willow/research/gembench/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Vision Language Models for Specialized Agricultural Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.19617v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.19617v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Arbab Arshad, Talukder Zaki Jubery, Tirtho Roy, Rim Nassiri, Asheesh K. Singh, Arti Singh, Chinmay Hegde, Baskar Ganapathysubramanian, Aditya Balu, Adarsh Krishnamurthy, Soumik Sarkar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Vision Language Models (VLMs) become increasingly accessible to farmers
and agricultural experts, there is a growing need to evaluate their potential
in specialized tasks. We present AgEval, a comprehensive benchmark for
assessing VLMs' capabilities in plant stress phenotyping, offering a solution
to the challenge of limited annotated data in agriculture. Our study explores
how general-purpose VLMs can be leveraged for domain-specific tasks with only a
few annotated examples, providing insights into their behavior and
adaptability. AgEval encompasses 12 diverse plant stress phenotyping tasks,
evaluating zero-shot and few-shot in-context learning performance of
state-of-the-art models including Claude, GPT, Gemini, and LLaVA. Our results
demonstrate VLMs' rapid adaptability to specialized tasks, with the
best-performing model showing an increase in F1 scores from 46.24% to 73.37% in
8-shot identification. To quantify performance disparities across classes, we
introduce metrics such as the coefficient of variation (CV), revealing that
VLMs' training impacts classes differently, with CV ranging from 26.02% to
58.03%. We also find that strategic example selection enhances model
reliability, with exact category examples improving F1 scores by 15.38% on
average. AgEval establishes a framework for assessing VLMs in agricultural
applications, offering valuable benchmarks for future evaluations. Our findings
suggest that VLMs, with minimal few-shot examples, show promise as a viable
alternative to traditional specialized models in plant stress phenotyping,
while also highlighting areas for further refinement. Results and benchmark
details are available at: https://github.com/arbab-ml/AgEval
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at WACV 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ET-Former: Efficient Triplane Deformable Attention for 3D Semantic Scene
  Completion From Monocular Camera 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.11019v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.11019v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Liang, He Yin, Xuewei Qi, Jong Jin Park, Min Sun, Rajasimman Madhivanan, Dinesh Manocha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce ET-Former, a novel end-to-end algorithm for semantic scene
completion using a single monocular camera. Our approach generates a semantic
occupancy map from single RGB observation while simultaneously providing
uncertainty estimates for semantic predictions. By designing a triplane-based
deformable attention mechanism, our approach improves geometric understanding
of the scene than other SOTA approaches and reduces noise in semantic
predictions. Additionally, through the use of a Conditional Variational
AutoEncoder (CVAE), we estimate the uncertainties of these predictions. The
generated semantic and uncertainty maps will help formulate navigation
strategies that facilitate safe and permissible decision making in the future.
Evaluated on the Semantic-KITTI dataset, ET-Former achieves the highest
Intersection over Union (IoU) and mean IoU (mIoU) scores while maintaining the
lowest GPU memory usage, surpassing state-of-the-art (SOTA) methods. It
improves the SOTA scores of IoU from 44.71 to 51.49 and mIoU from 15.04 to
16.30 on SeamnticKITTI test, with a notably low training memory consumption of
10.9 GB. Project page: https://github.com/jingGM/ET-Former.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards Hierarchical Rectified Flow 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.17436v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.17436v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yichi Zhang, Yici Yan, Alex Schwing, Zhizhen Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We formulate a hierarchical rectified flow to model data distributions. It
hierarchically couples multiple ordinary differential equations (ODEs) and
defines a time-differentiable stochastic process that generates a data
distribution from a known source distribution. Each ODE resembles the ODE that
is solved in a classic rectified flow, but differs in its domain, i.e.,
location, velocity, acceleration, etc. Unlike the classic rectified flow
formulation, which formulates a single ODE in the location domain and only
captures the expected velocity field (sufficient to capture a multi-modal data
distribution), the hierarchical rectified flow formulation models the
multi-modal random velocity field, acceleration field, etc., in their entirety.
This more faithful modeling of the random velocity field enables integration
paths to intersect when the underlying ODE is solved during data generation.
Intersecting paths in turn lead to integration trajectories that are more
straight than those obtained in the classic rectified flow formulation, where
integration paths cannot intersect. This leads to modeling of data
distributions with fewer neural function evaluations. We empirically verify
this on synthetic 1D and 2D data as well as MNIST, CIFAR-10, and ImageNet-32
data. Our code is available at: https://riccizz.github.io/HRF/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025; Project Page: https://riccizz.github.io/HRF/</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">4</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixture of Structural-and-Textual Retrieval over Text-rich Graph
  Knowledge Bases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20317v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20317v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongjia Lei, Haoyu Han, Ryan A. Rossi, Franck Dernoncourt, Nedim Lipka, Mahantesh M Halappanavar, Jiliang Tang, Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-rich Graph Knowledge Bases (TG-KBs) have become increasingly crucial for
answering queries by providing textual and structural knowledge. However,
current retrieval methods often retrieve these two types of knowledge in
isolation without considering their mutual reinforcement and some hybrid
methods even bypass structural retrieval entirely after neighboring
aggregation. To fill in this gap, we propose a Mixture of
Structural-and-Textual Retrieval (MoR) to retrieve these two types of knowledge
via a Planning-Reasoning-Organizing framework. In the Planning stage, MoR
generates textual planning graphs delineating the logic for answering queries.
Following planning graphs, in the Reasoning stage, MoR interweaves structural
traversal and textual matching to obtain candidates from TG-KBs. In the
Organizing stage, MoR further reranks fetched candidates based on their
structural trajectory. Extensive experiments demonstrate the superiority of MoR
in harmonizing structural and textual retrieval with insights, including uneven
retrieving performance across different query logics and the benefits of
integrating structural trajectories for candidate reranking. Our code is
available at https://github.com/Yoega/MoR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ICPE: An Item Cluster-Wise Pareto-Efficient Framework for Recommendation
  Debiasing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.12887v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.12887v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yule Wang, Xin Xin, Yue Ding, Yunzhe Li, Dong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recommender system based on historical user-item interactions is of vital
importance for web-based services. However, the observed data used to train the
recommender model suffers from severe bias issues. Practically, the item
frequency distribution of the dataset is a highly skewed power-law
distribution. Interactions of a small fraction of head items account for almost
the whole training data. The normal training paradigm from such biased data
tends to repetitively generate recommendations from the head items, which
further exacerbates the biases and affects the exploration of potentially
interesting items from the niche set. In this work, we innovatively explore the
central theme of recommendation debiasing from an item cluster-wise
multi-objective optimization perspective. Aiming to balance the learning on
various item clusters that differ in popularity during the training process, we
propose a model-agnostic framework namely Item Cluster-Wise Pareto-Efficient
Recommendation (ICPE). In detail, we define our item cluster-wise optimization
target as the recommender model should balance all item clusters that differ in
popularity, thus we set the model learning on each item cluster as a unique
optimization objective. To achieve this goal, we first explore items'
popularity levels from a novel causal reasoning perspective. Then, we devise
popularity discrepancy-based bisecting clustering to separate the item
clusters. Next, we adaptively find the overall harmonious gradient direction
for cluster-wise optimization objectives from a Pareto-efficient solver.
Finally, in the prediction stage, we perform counterfactual inference to
further eliminate the impact of global propensity. Extensive experimental
results verify the superiorities of ICPE on overall recommendation performance
and biases elimination.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ QAEA-DR: A Unified Text Augmentation Framework for Dense Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.20207v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.20207v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongming Tan, Shaoxiong Zhan, Hai Lin, Hai-Tao Zheng, Wai Kin Chan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In dense retrieval, embedding long texts into dense vectors can result in
information loss, leading to inaccurate query-text matching. Additionally,
low-quality texts with excessive noise or sparse key information are unlikely
to align well with relevant queries. Recent studies mainly focus on improving
the sentence embedding model or retrieval process. In this work, we introduce a
novel text augmentation framework for dense retrieval. This framework
transforms raw documents into information-dense text formats, which supplement
the original texts to effectively address the aforementioned issues without
modifying embedding or retrieval methodologies. Two text representations are
generated via large language models (LLMs) zero-shot prompting: question-answer
pairs and element-driven events. We term this approach QAEA-DR: unifying
question-answer generation and event extraction in a text augmentation
framework for dense retrieval. To further enhance the quality of generated
texts, a scoring-based evaluation and regeneration mechanism is introduced in
LLM prompting. Our QAEA-DR model has a positive impact on dense retrieval,
supported by both theoretical analysis and empirical experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Search and Society: Reimagining Information Access for Radical Futures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2403.17901v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2403.17901v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bhaskar Mitra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information retrieval (IR) research must understand and contend with the
social implications of the technology it produces. Instead of adopting a
reactionary strategy of trying to mitigate potential social harms from emerging
technologies, the community should aim to proactively set the research agenda
for the kinds of systems we should build inspired by diverse explicitly stated
sociotechnical imaginaries. The sociotechnical imaginaries that underpin the
design and development of information access technologies needs to be
explicitly articulated, and we need to develop theories of change in context of
these diverse perspectives. Our guiding future imaginaries must be informed by
other academic fields, such as human-computer interaction, information
sciences, media studies, design, science and technology studies, social
sciences, humanities, democratic theory, and critical theory, as well as legal
and policy experts, civil rights and social justice activists, and artists,
among others. In this perspective paper, we motivate why the community must
consider this radical shift in how we do research and what we work on, and
sketch a path forward towards this transformation.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2025-02-28T00:00:00Z">2025-02-28</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Information Retrieval <span class="chip" style="font-size: 60%">17</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Modeling in Recommendations: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21195v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21195v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangyu Zhao, Yichao Wang, Bo Chen, Jingtong Gao, Yuhao Wang, Xiaopeng Li, Pengyue Jia, Qidong Liu, Huifeng Guo, Ruiming Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In today's digital landscape, Deep Recommender Systems (DRS) play a crucial
role in navigating and customizing online content for individual preferences.
However, conventional methods, which mainly depend on single recommendation
task, scenario, data modality and user behavior, are increasingly seen as
insufficient due to their inability to accurately reflect users' complex and
changing preferences. This gap underscores the need for joint modeling
approaches, which are central to overcoming these limitations by integrating
diverse tasks, scenarios, modalities, and behaviors in the recommendation
process, thus promising significant enhancements in recommendation precision,
efficiency, and customization. In this paper, we comprehensively survey the
joint modeling methods in recommendations. We begin by defining the scope of
joint modeling through four distinct dimensions: multi-task, multi-scenario,
multi-modal, and multi-behavior modeling. Subsequently, we examine these
methods in depth, identifying and summarizing their underlying paradigms based
on the latest advancements and potential research trajectories. Ultimately, we
highlight several promising avenues for future exploration in joint modeling
for recommendations and provide a concise conclusion to our findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2302.03525</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimizing Large Language Models for ESG Activity Detection in Financial
  Texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21112v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21112v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mattia Birti, Francesco Osborne, Andrea Maurino
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of Environmental, Social, and Governance (ESG) factors into
corporate decision-making is a fundamental aspect of sustainable finance.
However, ensuring that business practices align with evolving regulatory
frameworks remains a persistent challenge. AI-driven solutions for
automatically assessing the alignment of sustainability reports and
non-financial disclosures with specific ESG activities could greatly support
this process. Yet, this task remains complex due to the limitations of
general-purpose Large Language Models (LLMs) in domain-specific contexts and
the scarcity of structured, high-quality datasets. In this paper, we
investigate the ability of current-generation LLMs to identify text related to
environmental activities. Furthermore, we demonstrate that their performance
can be significantly enhanced through fine-tuning on a combination of original
and synthetically generated data. To this end, we introduce ESG-Activities, a
benchmark dataset containing 1,325 labelled text segments classified according
to the EU ESG taxonomy. Our experimental results show that fine-tuning on
ESG-Activities significantly enhances classification accuracy, with open models
such as Llama 7B and Gemma 7B outperforming large proprietary solutions in
specific configurations. These findings have important implications for
financial analysts, policymakers, and AI researchers seeking to enhance ESG
transparency and compliance through advanced natural language processing
techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast 3D point clouds retrieval for Large-scale 3D Place Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chahine-Nicolas Zede, Laurent Carrafa, Valérie Gouet-Brunet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval in 3D point clouds is a challenging task that consists in
retrieving the most similar point clouds to a given query within a reference of
3D points. Current methods focus on comparing descriptors of point clouds in
order to identify similar ones. Due to the complexity of this latter step, here
we focus on the acceleration of the retrieval by adapting the Differentiable
Search Index (DSI), a transformer-based approach initially designed for text
information retrieval, for 3D point clouds retrieval. Our approach generates 1D
identifiers based on the point descriptors, enabling direct retrieval in
constant time. To adapt DSI to 3D data, we integrate Vision Transformers to map
descriptors to these identifiers while incorporating positional and semantic
encoding. The approach is evaluated for place recognition on a public benchmark
comparing its retrieval capabilities against state-of-the-art methods, in terms
of quality and speed of returned point clouds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 1 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extending Dense Passage Retrieval with Temporal Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.21024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.21024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abdelrahman Abdallah, Bhawna Piryani, Jonas Wallat, Avishek Anand, Adam Jatowt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal awareness is crucial in many information retrieval tasks,
particularly in scenarios where the relevance of documents depends on their
alignment with the query's temporal context. Traditional retrieval methods such
as BM25 and Dense Passage Retrieval (DPR) excel at capturing lexical and
semantic relevance but fall short in addressing time-sensitive queries. To
bridge this gap, we introduce the temporal retrieval model that integrates
explicit temporal signals by incorporating query timestamps and document dates
into the representation space. Our approach ensures that retrieved passages are
not only topically relevant but also temporally aligned with user intent. We
evaluate our approach on two large-scale benchmark datasets, ArchivalQA and
ChroniclingAmericaQA, achieving substantial performance gains over standard
retrieval baselines. In particular, our model improves Top-1 retrieval accuracy
by 6.63% and NDCG@10 by 3.79% on ArchivalQA, while yielding a 9.56% boost in
Top-1 retrieval accuracy and 4.68% in NDCG@10 on ChroniclingAmericaQA.
Additionally, we introduce a time-sensitive negative sampling strategy, which
refines the model's ability to distinguish between temporally relevant and
irrelevant documents during training. Our findings highlight the importance of
explicitly modeling time in retrieval systems and set a new standard for
handling temporally grounded queries.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The RAG Paradox: A Black-Box Attack Exploiting Unintentional
  Vulnerabilities in Retrieval-Augmented Generation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20995v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20995v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chanwoo Choi, Jinsoo Kim, Sukmin Cho, Soyeong Jeong, Buru Chang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the growing adoption of retrieval-augmented generation (RAG) systems,
recent studies have introduced attack methods aimed at degrading their
performance. However, these methods rely on unrealistic white-box assumptions,
such as attackers having access to RAG systems' internal processes. To address
this issue, we introduce a realistic black-box attack scenario based on the RAG
paradox, where RAG systems inadvertently expose vulnerabilities while
attempting to enhance trustworthiness. Because RAG systems reference external
documents during response generation, our attack targets these sources without
requiring internal access. Our approach first identifies the external sources
disclosed by RAG systems and then automatically generates poisoned documents
with misinformation designed to match these sources. Finally, these poisoned
documents are newly published on the disclosed sources, disrupting the RAG
system's response generation process. Both offline and online experiments
confirm that this attack significantly reduces RAG performance without
requiring internal access. Furthermore, from an insider perspective within the
RAG system, we propose a re-ranking method that acts as a fundamental
safeguard, offering minimal protection against unforeseen attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Variations in Relevance Judgments and the Shelf Life of Test Collections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Parry, Maik Fröbe, Harrisen Scells, Ferdinand Schlatt, Guglielmo Faggioli, Saber Zerhoudi, Sean MacAvaney, Eugene Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The fundamental property of Cranfield-style evaluations, that system rankings
are stable even when assessors disagree on individual relevance decisions, was
validated on traditional test collections. However, the paradigm shift towards
neural retrieval models affected the characteristics of modern test
collections, e.g., documents are short, judged with four grades of relevance,
and information needs have no descriptions or narratives. Under these changes,
it is unclear whether assessor disagreement remains negligible for system
comparisons. We investigate this aspect under the additional condition that the
few modern test collections are heavily re-used. Given more possible query
interpretations due to less formalized information needs, an ''expiration
date'' for test collections might be needed if top-effectiveness requires
overfitting to a single interpretation of relevance. We run a reproducibility
study and re-annotate the relevance judgments of the 2019 TREC Deep Learning
track. We can reproduce prior work in the neural retrieval setting, showing
that assessor disagreement does not affect system rankings. However, we observe
that some models substantially degrade with our new relevance judgments, and
some have already reached the effectiveness of humans as rankers, providing
evidence that test collections can expire.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 tables, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WebFAQ: A <span class="highlight-title">Multi</span>lingual Collection of Natural Q&A <span class="highlight-title">Dataset</span>s for Dense
  Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20936v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20936v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Dinzinger, Laura Caspari, Kanishka Ghosh Dastidar, Jelena Mitrović, Michael Granitzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present WebFAQ, a large-scale collection of open-domain question answering
datasets derived from FAQ-style schema.org annotations. In total, the data
collection consists of 96 million natural question-answer (QA) pairs across 75
languages, including 47 million (49%) non-English samples. WebFAQ further
serves as the foundation for 20 monolingual retrieval benchmarks with a total
size of 11.2 million QA pairs (5.9 million non-English). These datasets are
carefully curated through refined filtering and near-duplicate detection,
yielding high-quality resources for training and evaluating multilingual dense
retrieval models. To empirically confirm WebFAQ's efficacy, we use the
collected QAs to fine-tune an in-domain pretrained XLM-RoBERTa model. Through
this process of dataset-specific fine-tuning, the model achieves significant
retrieval performance gains, which generalize - beyond WebFAQ - to other
multilingual retrieval benchmarks evaluated in zero-shot setting. Last but not
least, we utilize WebFAQ to construct a set of QA-aligned bilingual corpora
spanning over 1000 language pairs using state-of-the-art bitext mining and
automated LLM-assessed translation evaluation. Due to our advanced, automated
method of bitext dataset generation, the resulting bilingual corpora
demonstrate higher translation quality compared to similar datasets. WebFAQ and
all associated resources are publicly available on GitHub and HuggingFace.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 3 figures, 7 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoTMR: Chain-of-Thought <span class="highlight-title">Multi</span>-Scale Reasoning for Training-Free
  Zero-Shot Composed Image Retrieval 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20826v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20826v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zelong Sun, Dong Jing, Zhiwu Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-Shot Composed Image Retrieval (ZS-CIR) aims to retrieve target images by
integrating information from a composed query (reference image and modification
text) without training samples. Existing methods primarily combine caption
models and large language models (LLMs) to generate target captions based on
composed queries but face various issues such as incompatibility, visual
information loss, and insufficient reasoning. In this work, we propose CoTMR, a
training-free framework crafted for ZS-CIR with novel Chain-of-thought (CoT)
and Multi-scale Reasoning. Instead of relying on caption models for modality
transformation, CoTMR employs the Large Vision-Language Model (LVLM) to achieve
unified understanding and reasoning for composed queries. To enhance the
reasoning reliability, we devise CIRCoT, which guides the LVLM through a
step-by-step inference process using predefined subtasks. Considering that
existing approaches focus solely on global-level reasoning, our CoTMR
incorporates multi-scale reasoning to achieve more comprehensive inference via
fine-grained predictions about the presence or absence of key elements at the
object scale. Further, we design a Multi-Grained Scoring (MGS) mechanism, which
integrates CLIP similarity scores of the above reasoning outputs with candidate
images to realize precise retrieval. Extensive experiments demonstrate that our
CoTMR not only drastically outperforms previous methods across four prominent
benchmarks but also offers appealing interpretability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Ove<span class="highlight-title">rl</span>oad-Aware Graph-Based Index Construction for
  10-Billion-Scale Vector Similarity Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Shi, Yiping Sun, Jiaolong Du, Xiaocheng Zhong, Zhiyong Wang, Yao Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Approximate Nearest Neighbor Search (ANNS) is essential for modern
data-driven applications that require efficient retrieval of top-k results from
massive vector databases. Although existing graph-based ANNS algorithms achieve
a high recall rate on billion-scale datasets, their slow construction speed and
limited scalability hinder their applicability to large-scale industrial
scenarios. In this paper, we introduce SOGAIC, the first Scalable
Overload-Aware Graph-Based ANNS Index Construction system tailored for
ultra-large-scale vector databases: 1) We propose a dynamic data partitioning
algorithm with overload constraints that adaptively introduces overlaps among
subsets; 2) To enable efficient distributed subgraph construction, we employ a
load-balancing task scheduling framework combined with an agglomerative merging
strategy; 3) Extensive experiments on various datasets demonstrate a reduction
of 47.3% in average construction time compared to existing methods. The
proposed method has also been successfully deployed in a real-world industrial
search engine, managing over 10 billion daily updated vectors and serving
hundreds of millions of users.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by WWW'25</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unleashing the Potential of Two-Tower Models: Diffusion-Based
  Cross-Interaction for Large-Scale Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20687v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20687v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihan Wang, Fei Xiong, Zhexin Han, Qi Song, Kaiqiao Zhan, Ben Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Two-tower models are widely adopted in the industrial-scale matching stage
across a broad range of application domains, such as content recommendations,
advertisement systems, and search engines. This model efficiently handles
large-scale candidate item screening by separating user and item
representations. However, the decoupling network also leads to a neglect of
potential information interaction between the user and item representations.
Current state-of-the-art (SOTA) approaches include adding a shallow fully
connected layer(i.e., COLD), which is limited by performance and can only be
used in the ranking stage. For performance considerations, another approach
attempts to capture historical positive interaction information from the other
tower by regarding them as the input features(i.e., DAT). Later research showed
that the gains achieved by this method are still limited because of lacking the
guidance on the next user intent. To address the aforementioned challenges, we
propose a "cross-interaction decoupling architecture" within our matching
paradigm. This user-tower architecture leverages a diffusion module to
reconstruct the next positive intention representation and employs a
mixed-attention module to facilitate comprehensive cross-interaction. During
the next positive intention generation, we further enhance the accuracy of its
reconstruction by explicitly extracting the temporal drift within user behavior
sequences. Experiments on two real-world datasets and one industrial dataset
demonstrate that our method outperforms the SOTA two-tower models
significantly, and our diffusion approach outperforms other generative models
in reconstructing item representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LexRAG: Benchmarking Retrieval-Augmented Generation in <span class="highlight-title">Multi</span>-Turn Legal
  Consultation Conversation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haitao Li, Yifan Chen, Yiran Hu, Qingyao Ai, Junjie Chen, Xiaoyu Yang, Jianhui Yang, Yueyue Wu, Zeyang Liu, Yiqun Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) has proven highly effective in improving
large language models (LLMs) across various domains. However, there is no
benchmark specifically designed to assess the effectiveness of RAG in the legal
domain, which restricts progress in this area. To fill this gap, we propose
LexRAG, the first benchmark to evaluate RAG systems for multi-turn legal
consultations. LexRAG consists of 1,013 multi-turn dialogue samples and 17,228
candidate legal articles. Each sample is annotated by legal experts and
consists of five rounds of progressive questioning. LexRAG includes two key
tasks: (1) Conversational knowledge retrieval, requiring accurate retrieval of
relevant legal articles based on multi-turn context. (2) Response generation,
focusing on producing legally sound answers. To ensure reliable
reproducibility, we develop LexiT, a legal RAG toolkit that provides a
comprehensive implementation of RAG system components tailored for the legal
domain. Additionally, we introduce an LLM-as-a-judge evaluation pipeline to
enable detailed and effective assessment. Through experimental analysis of
various LLMs and retrieval methods, we reveal the key limitations of existing
RAG systems in handling legal consultation conversations. LexRAG establishes a
new benchmark for the practical application of RAG systems in the legal domain,
with its code and data available at https://github.com/CSHaitao/LexRAG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2410.02642v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2410.02642v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijie Chen, Bernal Jiménez Gutiérrez, Yu Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information retrieval (IR) systems have played a vital role in modern digital
life and have cemented their continued usefulness in this new era of generative
AI via retrieval-augmented generation. With strong language processing
capabilities and remarkable versatility, large language models (LLMs) have
become popular choices for zero-shot re-ranking in IR systems. So far,
LLM-based re-ranking methods rely on strong generative capabilities, which
restricts their use to either specialized or powerful proprietary models. Given
these restrictions, we ask: is autoregressive generation necessary and optimal
for LLMs to perform re-ranking? We hypothesize that there are abundant signals
relevant to re-ranking within LLMs that might not be used to their full
potential via generation. To more directly leverage such signals, we propose
in-context re-ranking (ICR), a novel method that leverages the change in
attention pattern caused by the search query for accurate and efficient
re-ranking. To mitigate the intrinsic biases in LLMs, we propose a calibration
method using a content-free query. Due to the absence of generation, ICR only
requires two ($O(1)$) forward passes to re-rank $N$ documents, making it
substantially more efficient than generative re-ranking methods that require at
least $O(N)$ forward passes. Our novel design also enables ICR to be applied to
any LLM without specialized training while guaranteeing a well-formed ranking.
Extensive experiments with two popular open-weight LLMs on standard single-hop
and multi-hop information retrieval benchmarks show that ICR outperforms
RankGPT while cutting the latency by more than 60% in practice. Through
detailed analyses, we show that ICR's performance is specially strong on tasks
that require more complex re-ranking signals. Our findings call for further
exploration on novel ways of utilizing open-weight LLMs beyond text generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TSPRank: Bridging Pairwise and Listwise Methods with a Bilinear
  Travelling Salesman Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2411.12064v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2411.12064v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weixian Waylon Li, Yftah Ziser, Yifei Xie, Shay B. Cohen, Tiejun Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional Learning-To-Rank (LETOR) approaches, including pairwise methods
like RankNet and LambdaMART, often fall short by solely focusing on pairwise
comparisons, leading to sub-optimal global rankings. Conversely, deep learning
based listwise methods, while aiming to optimise entire lists, require complex
tuning and yield only marginal improvements over robust pairwise models. To
overcome these limitations, we introduce Travelling Salesman Problem Rank
(TSPRank), a hybrid pairwise-listwise ranking method. TSPRank reframes the
ranking problem as a Travelling Salesman Problem (TSP), a well-known
combinatorial optimisation challenge that has been extensively studied for its
numerous solution algorithms and applications. This approach enables the
modelling of pairwise relationships and leverages combinatorial optimisation to
determine the listwise ranking. This approach can be directly integrated as an
additional component into embeddings generated by existing backbone models to
enhance ranking performance. Our extensive experiments across three backbone
models on diverse tasks, including stock ranking, information retrieval, and
historical events ordering, demonstrate that TSPRank significantly outperforms
both pure pairwise and listwise methods. Our qualitative analysis reveals that
TSPRank's main advantage over existing methods is its ability to harness global
information better while ranking. TSPRank's robustness and superior performance
across different domains highlight its potential as a versatile and effective
LETOR solution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM SIGKDD 2025 Research Track. The code and preprocessed
  data are available at https://github.com/waylonli/TSPRank-KDD2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ LoRec: Large Language Model for Robust Sequential Recommendation against
  Poisoning Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2401.17723v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2401.17723v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaike Zhang, Qi Cao, Yunfan Wu, Fei Sun, Huawei Shen, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential recommender systems stand out for their ability to capture users'
dynamic interests and the patterns of item-to-item transitions. However, the
inherent openness of sequential recommender systems renders them vulnerable to
poisoning attacks, where fraudulent users are injected into the training data
to manipulate learned patterns. Traditional defense strategies predominantly
depend on predefined assumptions or rules extracted from specific known
attacks, limiting their generalizability to unknown attack types. To solve the
above problems, considering the rich open-world knowledge encapsulated in Large
Language Models (LLMs), our research initially focuses on the capabilities of
LLMs in the detection of unknown fraudulent activities within recommender
systems, a strategy we denote as LLM4Dec. Empirical evaluations demonstrate the
substantial capability of LLMs in identifying unknown fraudsters, leveraging
their expansive, open-world knowledge.
  Building upon this, we propose the integration of LLMs into defense
strategies to extend their effectiveness beyond the confines of known attacks.
We propose LoRec, an advanced framework that employs LLM-Enhanced Calibration
to strengthen the robustness of sequential recommender systems against
poisoning attacks. LoRec integrates an LLM-enhanced CalibraTor (LCT) that
refines the training process of sequential recommender systems with knowledge
derived from LLMs, applying a user-wise reweighting to diminish the impact of
fraudsters injected by attacks. By incorporating LLMs' open-world knowledge,
the LCT effectively converts the limited, specific priors or rules into a more
general pattern of fraudsters, offering improved defenses against poisoning
attacks. Our comprehensive experiments validate that LoRec, as a general
framework, significantly strengthens the robustness of sequential recommender
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Eliciting In-context Retrieval and Reasoning for Long-context Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2501.08248v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2501.08248v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifu Qiu, Varun Embar, Yizhe Zhang, Navdeep Jaitly, Shay B. Cohen, Benjamin Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in long-context language models (LCLMs) promise to
transform Retrieval-Augmented Generation (RAG) by simplifying pipelines. With
their expanded context windows, LCLMs can process entire knowledge bases and
perform retrieval and reasoning directly -- a capability we define as
In-Context Retrieval and Reasoning (ICR^2). However, existing benchmarks like
LOFT often overestimate LCLM performance by providing overly simplified
contexts. To address this, we introduce ICR^2, a benchmark that evaluates LCLMs
in more realistic scenarios by including confounding passages retrieved with
strong retrievers. We then propose three methods to enhance LCLM performance:
(1) retrieve-then-generate fine-tuning, (2) retrieval-attention-probing, which
uses attention heads to filter and de-noise long contexts during decoding, and
(3) joint retrieval head training alongside the generation head. Our evaluation
of five well-known LCLMs on LOFT and ICR^2 demonstrates significant gains with
our best approach applied to Mistral-7B: +17 and +15 points by Exact Match on
LOFT, and +13 and +2 points on ICR^2, compared to vanilla RAG and supervised
fine-tuning, respectively. It even outperforms GPT-4-Turbo on most tasks
despite being a much smaller model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ColPali: Efficient Document Retrieval with Vision Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2407.01449v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2407.01449v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, Céline Hudelot, Pierre Colombo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Documents are visually rich structures that convey information through text,
but also figures, page layouts, tables, or even fonts. Since modern retrieval
systems mainly rely on the textual information they extract from document pages
to index documents -often through lengthy and brittle processes-, they struggle
to exploit key visual cues efficiently. This limits their capabilities in many
practical document retrieval applications such as Retrieval Augmented
Generation (RAG). To benchmark current systems on visually rich document
retrieval, we introduce the Visual Document Retrieval Benchmark ViDoRe,
composed of various page-level retrieval tasks spanning multiple domains,
languages, and practical settings. The inherent complexity and performance
shortcomings of modern systems motivate a new concept; doing document retrieval
by directly embedding the images of the document pages. We release ColPali, a
Vision Language Model trained to produce high-quality multi-vector embeddings
from images of document pages. Combined with a late interaction matching
mechanism, ColPali largely outperforms modern document retrieval pipelines
while being drastically simpler, faster and end-to-end trainable. We release
models, data, code and benchmarks under open licenses at https://hf.co/vidore.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a conference paper at ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Theory for Token-Level Harmonization in Retrieval-Augmented Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2406.00944v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2406.00944v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shicheng Xu, Liang Pang, Huawei Shen, Xueqi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Retrieval-augmented generation (RAG) utilizes retrieved texts to enhance
large language models (LLMs). Studies show that while RAG provides valuable
external information (benefit), it may also mislead LLMs (detriment) with noisy
or incorrect retrieved texts. Although many existing methods attempt to
preserve benefit and avoid detriment, they lack a theoretical explanation for
RAG. The benefit and detriment in the next token prediction of RAG remain a
black box that cannot be quantified or compared in an explainable manner, so
existing methods are data-driven, need additional utility evaluators or
post-hoc. This paper takes the first step towards providing a theory to explain
and trade off the benefit and detriment in RAG. First, we model RAG as the
fusion between distribution of LLMs knowledge and distribution of retrieved
texts. Then, we formalize the trade-off between the value of external knowledge
(benefit) and its potential risk of misleading LLMs (detriment) in next token
prediction of RAG by distribution difference in this fusion. Finally, we prove
that the actual effect of RAG on the token, which is the comparison between
benefit and detriment, can be predicted without any training or accessing the
utility of retrieval. Based on our theory, we propose a practical novel method,
Tok-RAG, which achieves collaborative generation between the pure LLM and RAG
at token level to preserve benefit and avoid detriment. Experiments in
real-world tasks using LLMs such as OPT, LLaMA-2, and Mistral show the
effectiveness of our method and support our theoretical findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2025</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Multimedia <span class="chip" style="font-size: 60%">3</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DiffBrush:Just Painting the Art by Your Hands 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20904v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20904v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaming Chu, Lei Jin, Tao Wang, Junliang Xing, Jian Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid development of image generation and editing algorithms in recent
years has enabled ordinary user to produce realistic images. However, the
current AI painting ecosystem predominantly relies on text-driven diffusion
models (T2I), which pose challenges in accurately capturing user requirements.
Furthermore, achieving compatibility with other modalities incurs substantial
training costs. To this end, we introduce DiffBrush, which is compatible with
T2I models and allows users to draw and edit images. By manipulating and
adapting the internal representation of the diffusion model, DiffBrush guides
the model-generated images to converge towards the user's hand-drawn sketches
for user's specific needs without additional training. DiffBrush achieves
control over the color, semantic, and instance of objects in images by
continuously guiding the latent and instance-level attention map during the
denoising process of the diffusion model. Besides, we propose a latent
regeneration, which refines the randomly sampled noise in the diffusion model,
obtaining a better image generation layout. Finally, users only need to roughly
draw the mask of the instance (acceptable colors) on the canvas, DiffBrush can
naturally generate the corresponding instance at the corresponding location.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EyEar: Learning Audio Synchronized Human Gaze Trajectory Based on
  Physics-Informed Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaochuan Liu, Xin Cheng, Yuchong Sun, Xiaoxue Wu, Ruihua Song, Hao Sun, Denghao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Imitating how humans move their gaze in a visual scene is a vital research
problem for both visual understanding and psychology, kindling crucial
applications such as building alive virtual characters. Previous studies aim to
predict gaze trajectories when humans are free-viewing an image, searching for
required targets, or looking for clues to answer questions in an image. While
these tasks focus on visual-centric scenarios, humans move their gaze also
along with audio signal inputs in more common scenarios. To fill this gap, we
introduce a new task that predicts human gaze trajectories in a visual scene
with synchronized audio inputs and provide a new dataset containing 20k gaze
points from 8 subjects. To effectively integrate audio information and simulate
the dynamic process of human gaze motion, we propose a novel learning framework
called EyEar (Eye moving while Ear listening) based on physics-informed
dynamics, which considers three key factors to predict gazes: eye inherent
motion tendency, vision salient attraction, and audio semantic attraction. We
also propose a probability density score to overcome the high individual
variability of gaze trajectories, thereby improving the stabilization of
optimization and the reliability of the evaluation. Experimental results show
that EyEar outperforms all the baselines in the context of all evaluation
metrics, thanks to the proposed components in the learning model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HAIC: Improving Human Action Understanding and Generation with Better
  Captions for <span class="highlight-title">Multi</span>-modal Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2502.20811v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2502.20811v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Wang, Jingyun Hua, Weihong Lin, Yuanxing Zhang, Fuzheng Zhang, Jianlong Wu, Di Zhang, Liqiang Nie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent Multi-modal Large Language Models (MLLMs) have made great progress in
video understanding. However, their performance on videos involving human
actions is still limited by the lack of high-quality data. To address this, we
introduce a two-stage data annotation pipeline. First, we design strategies to
accumulate videos featuring clear human actions from the Internet. Second,
videos are annotated in a standardized caption format that uses human
attributes to distinguish individuals and chronologically details their actions
and interactions. Through this pipeline, we curate two datasets, namely
HAICTrain and HAICBench. \textbf{HAICTrain} comprises 126K video-caption pairs
generated by Gemini-Pro and verified for training purposes. Meanwhile,
\textbf{HAICBench} includes 500 manually annotated video-caption pairs and
1,400 QA pairs, for a comprehensive evaluation of human action understanding.
Experimental results demonstrate that training with HAICTrain not only
significantly enhances human understanding abilities across 4 benchmarks, but
can also improve text-to-video generation results. Both the HAICTrain and
HAICBench are released at https://huggingface.co/datasets/KuaishouHAIC/HAIC.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2025-03-08T05:24:16.777477792Z">
            2025-03-08 05:24:16 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
